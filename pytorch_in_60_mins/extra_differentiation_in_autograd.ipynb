{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/7201krap/Introduction_to_Pytorch/blob/main/differentiation_in_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QdbunkVcKgUM"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AcFNOq4JKpMB"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at how autograd collects gradients. \n",
    "# We create two tensors a and b with requires_grad = True. \n",
    "# This signals to autograd that every operation on them should be tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "du3uSvmALdJc"
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n2n_tvduL5i5"
   },
   "outputs": [],
   "source": [
    "# We create another tensor Q from a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ezxSbMCUL6uY"
   },
   "outputs": [],
   "source": [
    "Q = (3 * a ** 3) - (b ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aXB0uKCrMbDP"
   },
   "outputs": [],
   "source": [
    "# Let's assume a and b to be parameters of an NN, and Q to be the error.\n",
    "# In NN training, we want gradients of the error w.r.t. parameters, \n",
    "\n",
    "# delta_Q / delta_a = 9*a**2\n",
    "# delta_Q / delta_b = -2*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OdsNJuCuM0ka"
   },
   "outputs": [],
   "source": [
    "# When we call .backward() on Q, autograd calculates these gradients and stores them \n",
    "# in the respective tensor's .grad attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rF3uW6p-NBJL"
   },
   "outputs": [],
   "source": [
    "# 이 부분은 잘 이해 안간다. \n",
    "# We need to explicitly pass a gradent argument in Q.backward because it is a vector.\n",
    "# gradient is a tensor of the same shape as Q, and it represents the gradient of Q w.r.t.\n",
    "# itself. delta_Q / delta_Q = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "imTy4SSuQQkZ"
   },
   "outputs": [],
   "source": [
    "# 이 방법으로 이해 하면 된다. \n",
    "# Equivalently, we can also aggregate Q into a scalar and call backward implicitly like\n",
    "# Q.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "P-adAo2GQiAG"
   },
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TtiPkqV1QvTS"
   },
   "outputs": [],
   "source": [
    "# Gradient are now deposited in a.grad and b.grad\n",
    "# check if collected gradients are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t4aOJl68Q4mZ",
    "outputId": "74d60e03-4be7-4286-dd17-086ac900aedd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "print(9*a**2 == a.grad)\n",
    "print(-2*b   == b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFjJpffOSCi-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNI6CRQzbfMJrgrtJP+QuFE",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "differentiation_in_autograd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
