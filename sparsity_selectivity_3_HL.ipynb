{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparsity_selectivity_3_HL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/sparsity_selectivity_3_HL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7STrWa0P3z_",
        "outputId": "ca96d023-794c-4149-faee-0fbddc406667"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfTylLHSgiBJ",
        "outputId": "936d8cc0-f94d-4342-e0e6-164e72fd8912"
      },
      "source": [
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "root_dir = './'\n",
        "torchvision.datasets.MNIST(root=root_dir,download=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-16 00:53:31--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
            "--2021-03-16 00:53:32--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘MNIST.tar.gz’\n",
            "\n",
            "MNIST.tar.gz            [        <=>         ]  33.20M  6.10MB/s    in 16s     \n",
            "\n",
            "2021-03-16 00:53:48 (2.09 MB/s) - ‘MNIST.tar.gz’ saved [34813078]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte\n",
            "MNIST/raw/train-images-idx3-ubyte.gz\n",
            "MNIST/processed/\n",
            "MNIST/processed/training.pt\n",
            "MNIST/processed/test.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./\n",
              "    Split: Train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4j9WoP-UnAm"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApOU7hvb95W4"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTW5TOUnP5XY"
      },
      "source": [
        "mnist_trainset = torchvision.datasets.MNIST(root=root_dir, train=True, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "mnist_testset  = torchvision.datasets.MNIST(root=root_dir, \n",
        "                                train=False, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(mnist_trainset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=True)\n",
        "\n",
        "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXTkEUJ5P6kU"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# Define the model \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # modify this section for later use \n",
        "        self.linear_1 = torch.nn.Linear(784, 256)\n",
        "        self.linear_2 = torch.nn.Linear(256, 256)\n",
        "        self.linear_3 = torch.nn.Linear(256, 256)\n",
        "        self.linear_4 = torch.nn.Linear(256, 10)\n",
        "        self.sigmoid12  = torch.nn.Sigmoid()\n",
        "        self.sigmoid23  = torch.nn.Sigmoid()\n",
        "        self.sigmoid34  = torch.nn.Sigmoid()\n",
        "\n",
        "        self.layer_activations = dict()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # modify this section for later use \n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.sigmoid12(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.sigmoid23(x)\n",
        "        x = self.linear_3(x)\n",
        "        x = self.sigmoid34(x)\n",
        "        pred = self.linear_4(x)\n",
        "        return pred\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfgvKH6eP9Ou"
      },
      "source": [
        "def get_activation(model, layer_name):    \n",
        "    def hook(module, input, output):\n",
        "        model.layer_activations[layer_name] = output\n",
        "    return hook"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILRIzQaSgn5G"
      },
      "source": [
        "def sparsity_calculator(final_spareness):\n",
        "    sparseness_list = list()\n",
        "    for single_epoch_spareness in final_spareness:\n",
        "\n",
        "        hidden_layer_activation_list = single_epoch_spareness\n",
        "        hidden_layer_activation_list = torch.stack(hidden_layer_activation_list)\n",
        "        layer_activations_list = torch.reshape(hidden_layer_activation_list, (10000, 256))\n",
        "\n",
        "        layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
        "        num_neurons = layer_activations_list.shape[1]\n",
        "        population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
        "        mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
        "\n",
        "        sparseness_list.append(mean_sparseness_per_epoch)\n",
        "\n",
        "    return sparseness_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvHGO5RSvi6I"
      },
      "source": [
        "def selectivity(hidden_layer_each_neuron):\n",
        "    __selectivity__ = list()\n",
        "    # I will now try to find the average of each class for each neuron.\n",
        "    # check out the next cell \n",
        "    avg_activations = [dict() for x in range(256)]\n",
        "    for i, neuron in enumerate(hidden_layer_each_neuron):\n",
        "        for k, v in neuron.items():\n",
        "            # v is the list of activations for hidden layer's neuron k \n",
        "            avg_activations[i][k] = sum(v) / float(len(v))\n",
        "\n",
        "    # generate 256 lists to get only values in avg_activations\n",
        "    only_activation_vals = [list() for x in range(256)]\n",
        "\n",
        "    # get only values from avg_activations\n",
        "    for i, avg_activation in enumerate(avg_activations):\n",
        "        for value in avg_activation.values():\n",
        "            only_activation_vals[i].append(value)\n",
        "\n",
        "\n",
        "    for activation_val in only_activation_vals:\n",
        "        # find u_max \n",
        "        u_max = np.max(activation_val)\n",
        "\n",
        "        # find u_minus_max \n",
        "        u_minus_max = (np.sum(activation_val) - u_max) / 9\n",
        "\n",
        "        # find selectivity \n",
        "        selectivity = (u_max - u_minus_max) / (u_max + u_minus_max)\n",
        "\n",
        "        # append selectivity value to selectivity\n",
        "        __selectivity__.append(selectivity)\n",
        "\n",
        "    avg_selectivity = np.average(__selectivity__)\n",
        "    std_selectivity = np.std(__selectivity__)\n",
        "                                 \n",
        "    return avg_selectivity, std_selectivity"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGjUpHBIgp1u"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# add a parameter to the function and calculate avg and std. Do not forget to change division by 2, 3, 4, or 5 \n",
        "def avg_std_calculator(_hidden_layer_each_neuron_12, _hidden_layer_each_neuron_23, _hidden_layer_each_neuron_34):\n",
        "\n",
        "    avg_selectivity12, std_selectivity12 = selectivity(_hidden_layer_each_neuron_12)\n",
        "    avg_selectivity23, std_selectivity23 = selectivity(_hidden_layer_each_neuron_23)\n",
        "    avg_selectivity34, std_selectivity34 = selectivity(_hidden_layer_each_neuron_34)\n",
        "\n",
        "    final_selectivity_avg = (avg_selectivity12 + avg_selectivity23 + avg_selectivity34) / 3 \n",
        "    final_selecvitity_std = (std_selectivity12 + std_selectivity23 + std_selectivity34) / 3 \n",
        "\n",
        "    return final_selectivity_avg, final_selecvitity_std\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5PUiBNqUImf"
      },
      "source": [
        "def model_factory(optimizer_name):\n",
        "    '''\n",
        "    optimizer_name : choose one of Adagrad, Adadelta, SGD, and Adam \n",
        "\n",
        "    '''\n",
        "    my_model = Model()\n",
        "    print(\"my_model:\", my_model)\n",
        "    my_model.to(device)\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    # chagen sigmoid34 an 's34'\n",
        "    my_model.sigmoid12.register_forward_hook(get_activation(my_model, 's12'))\n",
        "    my_model.sigmoid23.register_forward_hook(get_activation(my_model, 's23'))\n",
        "    my_model.sigmoid34.register_forward_hook(get_activation(my_model, 's34'))\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        my_optimizer = torch.optim.Adadelta(my_model.parameters(), lr=1.0)\n",
        "\n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        my_optimizer = torch.optim.Adagrad(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        my_optimizer = torch.optim.SGD(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        my_optimizer = torch.optim.Adam(my_model.parameters(), lr=0.001)\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")\n",
        "    \n",
        "    print(\"my_optimizer:\", my_optimizer)\n",
        "    test_acc, sparsity_avg, selectivity_list_avg, selectivity_list_std = selectivity_trainer(optimizer=my_optimizer, model=my_model)\n",
        "    # ************* modify this section for later use *************\n",
        "    # change name of the file \n",
        "    file_saver = open(f\"3HL_selectivity_sparsity_{optimizer_name}.txt\", \"w\")\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver.write(str(test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(selectivity_list_avg)+'\\n'+str(selectivity_list_std)+'\\n\\n')\n",
        "    file_saver.close()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        !cp 3HL_selectivity_sparsity_Adadelta.txt /content/drive/MyDrive\n",
        "    \n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        !cp 3HL_selectivity_sparsity_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        !cp 3HL_selectivity_sparsity_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        !cp 3HL_selectivity_sparsity_Adam.txt /content/drive/MyDrive\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXOpwTXEQFKY"
      },
      "source": [
        "no_epochs = 50\n",
        "def selectivity_trainer(optimizer, model):\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    train_loss = list()\n",
        "    test_loss  = list()\n",
        "    test_acc   = list()\n",
        "\n",
        "    best_test_loss = 1\n",
        "\n",
        "    selectivity_avg_list = list()\n",
        "    selectivity_std_list = list()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    final_spareness_12 = list()\n",
        "    final_spareness_23 = list()\n",
        "    final_spareness_34 = list()\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    for epoch in range(no_epochs):\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_each_neuron_12 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_12 = np.array(hidden_layer_each_neuron_12)\n",
        "\n",
        "        hidden_layer_each_neuron_23 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_23 = np.array(hidden_layer_each_neuron_23)\n",
        "\n",
        "        hidden_layer_each_neuron_34 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_34 = np.array(hidden_layer_each_neuron_34)\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_activation_list_12 = list()\n",
        "        hidden_layer_activation_list_23 = list()\n",
        "        hidden_layer_activation_list_34 = list()\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "\n",
        "        total_train_loss = 0\n",
        "        total_test_loss = 0\n",
        "\n",
        "        # training\n",
        "        # set up training mode \n",
        "        model.train()\n",
        "\n",
        "        for itr, (images, labels) in enumerate(train_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_train_loss = total_train_loss / (itr + 1)\n",
        "        train_loss.append(total_train_loss)\n",
        "\n",
        "        # testing \n",
        "        # change to evaluation mode \n",
        "        model.eval()\n",
        "        total = 0\n",
        "        for itr, (images, labels) in enumerate(test_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # we now need softmax because we are testing.\n",
        "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "            for i, p in enumerate(pred):\n",
        "                if labels[i] == torch.max(p.data, 0)[1]:\n",
        "                    total = total + 1\n",
        "\n",
        "            # ***************** sparsity calculation ***************** #\n",
        "            hidden_layer_activation_list_12.append(model.layer_activations['s12'])\n",
        "            hidden_layer_activation_list_23.append(model.layer_activations['s23'])\n",
        "            hidden_layer_activation_list_34.append(model.layer_activations['s34'])\n",
        "\n",
        "            # ************* modify this section for later use *************\n",
        "            # Do not forget to change hidden_layer_each_neuron_12 name \n",
        "            for activation, label in zip(model.layer_activations['s12'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_12[i][label].append(activation[i])\n",
        "\n",
        "            for activation, label in zip(model.layer_activations['s23'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_23[i][label].append(activation[i])\n",
        "            \n",
        "            for activation, label in zip(model.layer_activations['s34'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_34[i][label].append(activation[i])\n",
        "\n",
        "        # Add one more parameter \n",
        "        selectivity_avg, selecvitity_std = avg_std_calculator(hidden_layer_each_neuron_12, hidden_layer_each_neuron_23, hidden_layer_each_neuron_34)\n",
        "        # ************* modify this section for later use *************\n",
        "        \n",
        "        selectivity_avg_list.append(selectivity_avg)\n",
        "        selectivity_std_list.append(selecvitity_std)\n",
        "\n",
        "        # this conains activations for all epochs \n",
        "        final_spareness_12.append(hidden_layer_activation_list_12)\n",
        "        final_spareness_23.append(hidden_layer_activation_list_23)\n",
        "        final_spareness_34.append(hidden_layer_activation_list_34)\n",
        "        # ***************** sparsity calculation ***************** #\n",
        "\n",
        "        # caculate accuracy \n",
        "        accuracy = total / len(mnist_testset)\n",
        "\n",
        "        # append accuracy here\n",
        "        test_acc.append(accuracy)\n",
        "\n",
        "        # append test loss here \n",
        "        total_test_loss = total_test_loss / (itr + 1)\n",
        "        test_loss.append(total_test_loss)\n",
        "\n",
        "        print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Test Loss: {:.8f}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, total_train_loss, total_test_loss, accuracy))\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "    sparsity_list12 = sparsity_calculator(final_spareness_12)\n",
        "    sparsity_list23 = sparsity_calculator(final_spareness_23)\n",
        "    sparsity_list34 = sparsity_calculator(final_spareness_34)\n",
        "\n",
        "    print(sparsity_list12)\n",
        "    print(sparsity_list23)\n",
        "    print(sparsity_list34)\n",
        "\n",
        "    average_sparsity = list()\n",
        "    for i in range(no_epochs):\n",
        "        average_sparsity.append( (sparsity_list12[i].item() + sparsity_list23[i].item() + sparsity_list34[i].item()) / 3 )\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "\n",
        "    print(\"average_sparsity:\", average_sparsity)\n",
        "\n",
        "    return test_acc, average_sparsity, selectivity_avg_list, selectivity_std_list"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILIJTJb2UdfI"
      },
      "source": [
        "# Adadelta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UH0qDnFUfaD",
        "outputId": "5b2e8f0f-1e53-490b-c6f1-ee4b6ad8f768"
      },
      "source": [
        "model_factory('Adadelta')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-06\n",
            "    lr: 1.0\n",
            "    rho: 0.9\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.00404090, Test Loss: 0.74558227, Test Accuracy: 0.75990000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 0.49531609, Test Loss: 0.32430531, Test Accuracy: 0.90510000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 0.25243455, Test Loss: 0.18649505, Test Accuracy: 0.94610000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 0.16455400, Test Loss: 0.14236793, Test Accuracy: 0.95980000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.12499757, Test Loss: 0.13524991, Test Accuracy: 0.95950000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.10178312, Test Loss: 0.10451699, Test Accuracy: 0.96890000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.08460891, Test Loss: 0.10305050, Test Accuracy: 0.96980000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.07082212, Test Loss: 0.09327359, Test Accuracy: 0.97390000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.06076949, Test Loss: 0.11816178, Test Accuracy: 0.96630000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.05300455, Test Loss: 0.08111801, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.04599879, Test Loss: 0.08253325, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.03949730, Test Loss: 0.08995864, Test Accuracy: 0.97550000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.03378946, Test Loss: 0.08632489, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.02918231, Test Loss: 0.08396863, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.02498637, Test Loss: 0.10888980, Test Accuracy: 0.97170000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.02179227, Test Loss: 0.09581160, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.01880488, Test Loss: 0.10313362, Test Accuracy: 0.97620000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.01606354, Test Loss: 0.09538353, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.01227083, Test Loss: 0.09657165, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.01026177, Test Loss: 0.10043788, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.00876970, Test Loss: 0.10082645, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.00797083, Test Loss: 0.10513566, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.00664106, Test Loss: 0.11610138, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.00477135, Test Loss: 0.13657618, Test Accuracy: 0.97390000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.00379971, Test Loss: 0.10919007, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.00344470, Test Loss: 0.11753429, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.00276358, Test Loss: 0.11387338, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.00173252, Test Loss: 0.11814134, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00122349, Test Loss: 0.11374982, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00111464, Test Loss: 0.11828282, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00077661, Test Loss: 0.11785374, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00058708, Test Loss: 0.11621848, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00057788, Test Loss: 0.11976590, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00040221, Test Loss: 0.12110651, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00036758, Test Loss: 0.12245296, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00027149, Test Loss: 0.12417324, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00022266, Test Loss: 0.12227909, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00019528, Test Loss: 0.12517220, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00016439, Test Loss: 0.12363472, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00015661, Test Loss: 0.12516996, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00014246, Test Loss: 0.12495537, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00013277, Test Loss: 0.12555036, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00012579, Test Loss: 0.12642874, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00011862, Test Loss: 0.12713138, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00011308, Test Loss: 0.12746127, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00010799, Test Loss: 0.12789654, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00010312, Test Loss: 0.12849783, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00009851, Test Loss: 0.12850587, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00009486, Test Loss: 0.12918988, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00009086, Test Loss: 0.12961810, Test Accuracy: 0.97940000\n",
            "[tensor(0.1429, grad_fn=<MeanBackward0>), tensor(0.3164, grad_fn=<MeanBackward0>), tensor(0.3794, grad_fn=<MeanBackward0>), tensor(0.4041, grad_fn=<MeanBackward0>), tensor(0.4294, grad_fn=<MeanBackward0>), tensor(0.4389, grad_fn=<MeanBackward0>), tensor(0.4496, grad_fn=<MeanBackward0>), tensor(0.4543, grad_fn=<MeanBackward0>), tensor(0.4573, grad_fn=<MeanBackward0>), tensor(0.4643, grad_fn=<MeanBackward0>), tensor(0.4679, grad_fn=<MeanBackward0>), tensor(0.4723, grad_fn=<MeanBackward0>), tensor(0.4704, grad_fn=<MeanBackward0>), tensor(0.4767, grad_fn=<MeanBackward0>), tensor(0.4758, grad_fn=<MeanBackward0>), tensor(0.4826, grad_fn=<MeanBackward0>), tensor(0.4818, grad_fn=<MeanBackward0>), tensor(0.4825, grad_fn=<MeanBackward0>), tensor(0.4834, grad_fn=<MeanBackward0>), tensor(0.4830, grad_fn=<MeanBackward0>), tensor(0.4821, grad_fn=<MeanBackward0>), tensor(0.4810, grad_fn=<MeanBackward0>), tensor(0.4858, grad_fn=<MeanBackward0>), tensor(0.4845, grad_fn=<MeanBackward0>), tensor(0.4841, grad_fn=<MeanBackward0>), tensor(0.4829, grad_fn=<MeanBackward0>), tensor(0.4820, grad_fn=<MeanBackward0>), tensor(0.4810, grad_fn=<MeanBackward0>), tensor(0.4811, grad_fn=<MeanBackward0>), tensor(0.4807, grad_fn=<MeanBackward0>), tensor(0.4797, grad_fn=<MeanBackward0>), tensor(0.4792, grad_fn=<MeanBackward0>), tensor(0.4789, grad_fn=<MeanBackward0>), tensor(0.4786, grad_fn=<MeanBackward0>), tensor(0.4784, grad_fn=<MeanBackward0>), tensor(0.4786, grad_fn=<MeanBackward0>), tensor(0.4779, grad_fn=<MeanBackward0>), tensor(0.4774, grad_fn=<MeanBackward0>), tensor(0.4773, grad_fn=<MeanBackward0>), tensor(0.4770, grad_fn=<MeanBackward0>), tensor(0.4768, grad_fn=<MeanBackward0>), tensor(0.4766, grad_fn=<MeanBackward0>), tensor(0.4764, grad_fn=<MeanBackward0>), tensor(0.4763, grad_fn=<MeanBackward0>), tensor(0.4761, grad_fn=<MeanBackward0>), tensor(0.4760, grad_fn=<MeanBackward0>), tensor(0.4759, grad_fn=<MeanBackward0>), tensor(0.4756, grad_fn=<MeanBackward0>), tensor(0.4755, grad_fn=<MeanBackward0>), tensor(0.4754, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.1202, grad_fn=<MeanBackward0>), tensor(0.1617, grad_fn=<MeanBackward0>), tensor(0.1550, grad_fn=<MeanBackward0>), tensor(0.1579, grad_fn=<MeanBackward0>), tensor(0.1539, grad_fn=<MeanBackward0>), tensor(0.1659, grad_fn=<MeanBackward0>), tensor(0.1645, grad_fn=<MeanBackward0>), tensor(0.1733, grad_fn=<MeanBackward0>), tensor(0.1781, grad_fn=<MeanBackward0>), tensor(0.1857, grad_fn=<MeanBackward0>), tensor(0.1871, grad_fn=<MeanBackward0>), tensor(0.1865, grad_fn=<MeanBackward0>), tensor(0.1931, grad_fn=<MeanBackward0>), tensor(0.1985, grad_fn=<MeanBackward0>), tensor(0.2079, grad_fn=<MeanBackward0>), tensor(0.2009, grad_fn=<MeanBackward0>), tensor(0.2077, grad_fn=<MeanBackward0>), tensor(0.2059, grad_fn=<MeanBackward0>), tensor(0.2053, grad_fn=<MeanBackward0>), tensor(0.2058, grad_fn=<MeanBackward0>), tensor(0.2099, grad_fn=<MeanBackward0>), tensor(0.2141, grad_fn=<MeanBackward0>), tensor(0.2143, grad_fn=<MeanBackward0>), tensor(0.2163, grad_fn=<MeanBackward0>), tensor(0.2102, grad_fn=<MeanBackward0>), tensor(0.2118, grad_fn=<MeanBackward0>), tensor(0.2111, grad_fn=<MeanBackward0>), tensor(0.2115, grad_fn=<MeanBackward0>), tensor(0.2116, grad_fn=<MeanBackward0>), tensor(0.2117, grad_fn=<MeanBackward0>), tensor(0.2106, grad_fn=<MeanBackward0>), tensor(0.2113, grad_fn=<MeanBackward0>), tensor(0.2088, grad_fn=<MeanBackward0>), tensor(0.2084, grad_fn=<MeanBackward0>), tensor(0.2087, grad_fn=<MeanBackward0>), tensor(0.2070, grad_fn=<MeanBackward0>), tensor(0.2079, grad_fn=<MeanBackward0>), tensor(0.2072, grad_fn=<MeanBackward0>), tensor(0.2073, grad_fn=<MeanBackward0>), tensor(0.2072, grad_fn=<MeanBackward0>), tensor(0.2070, grad_fn=<MeanBackward0>), tensor(0.2065, grad_fn=<MeanBackward0>), tensor(0.2063, grad_fn=<MeanBackward0>), tensor(0.2071, grad_fn=<MeanBackward0>), tensor(0.2064, grad_fn=<MeanBackward0>), tensor(0.2058, grad_fn=<MeanBackward0>), tensor(0.2057, grad_fn=<MeanBackward0>), tensor(0.2056, grad_fn=<MeanBackward0>), tensor(0.2056, grad_fn=<MeanBackward0>), tensor(0.2053, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2654, grad_fn=<MeanBackward0>), tensor(0.2848, grad_fn=<MeanBackward0>), tensor(0.3003, grad_fn=<MeanBackward0>), tensor(0.3180, grad_fn=<MeanBackward0>), tensor(0.3424, grad_fn=<MeanBackward0>), tensor(0.3297, grad_fn=<MeanBackward0>), tensor(0.3513, grad_fn=<MeanBackward0>), tensor(0.3360, grad_fn=<MeanBackward0>), tensor(0.3464, grad_fn=<MeanBackward0>), tensor(0.3330, grad_fn=<MeanBackward0>), tensor(0.3340, grad_fn=<MeanBackward0>), tensor(0.3476, grad_fn=<MeanBackward0>), tensor(0.3314, grad_fn=<MeanBackward0>), tensor(0.3319, grad_fn=<MeanBackward0>), tensor(0.3129, grad_fn=<MeanBackward0>), tensor(0.3272, grad_fn=<MeanBackward0>), tensor(0.3227, grad_fn=<MeanBackward0>), tensor(0.3244, grad_fn=<MeanBackward0>), tensor(0.3275, grad_fn=<MeanBackward0>), tensor(0.3220, grad_fn=<MeanBackward0>), tensor(0.3206, grad_fn=<MeanBackward0>), tensor(0.3088, grad_fn=<MeanBackward0>), tensor(0.3105, grad_fn=<MeanBackward0>), tensor(0.3051, grad_fn=<MeanBackward0>), tensor(0.3121, grad_fn=<MeanBackward0>), tensor(0.3119, grad_fn=<MeanBackward0>), tensor(0.3073, grad_fn=<MeanBackward0>), tensor(0.3093, grad_fn=<MeanBackward0>), tensor(0.3047, grad_fn=<MeanBackward0>), tensor(0.3025, grad_fn=<MeanBackward0>), tensor(0.3054, grad_fn=<MeanBackward0>), tensor(0.3024, grad_fn=<MeanBackward0>), tensor(0.3023, grad_fn=<MeanBackward0>), tensor(0.3021, grad_fn=<MeanBackward0>), tensor(0.3019, grad_fn=<MeanBackward0>), tensor(0.3050, grad_fn=<MeanBackward0>), tensor(0.3020, grad_fn=<MeanBackward0>), tensor(0.3018, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3024, grad_fn=<MeanBackward0>), tensor(0.3020, grad_fn=<MeanBackward0>), tensor(0.3028, grad_fn=<MeanBackward0>), tensor(0.3030, grad_fn=<MeanBackward0>), tensor(0.3013, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3029, grad_fn=<MeanBackward0>), tensor(0.3029, grad_fn=<MeanBackward0>), tensor(0.3032, grad_fn=<MeanBackward0>), tensor(0.3029, grad_fn=<MeanBackward0>), tensor(0.3033, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.17615080376466116, 0.2542933175961177, 0.27825139462947845, 0.2932901531457901, 0.3085531045993169, 0.3114845355351766, 0.32178034881750744, 0.32122020920117694, 0.3272866755723953, 0.32766737043857574, 0.3296687553326289, 0.33545590937137604, 0.33166930576165515, 0.3356894602378209, 0.33215657373269397, 0.3368883778651555, 0.33739426732063293, 0.33758897085984546, 0.3386994848648707, 0.33691903452078503, 0.3375251690546672, 0.33463555574417114, 0.33687544365723926, 0.3352927813927333, 0.3354453643163045, 0.33555396894613904, 0.3334560791651408, 0.33393975098927814, 0.3325079878171285, 0.33165930211544037, 0.3318818112214406, 0.330964316924413, 0.3300062467654546, 0.3297375440597534, 0.32970641553401947, 0.33019407093524933, 0.329274242122968, 0.3288218180338542, 0.3290872126817703, 0.32885489364465076, 0.3286224603652954, 0.3286282569169998, 0.32858701050281525, 0.3282288610935211, 0.3283767153819402, 0.32823632657527924, 0.32814020415147144, 0.3281175543864568, 0.3280070622762044, 0.32799873252709705]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hXfQe4vMDKB"
      },
      "source": [
        "# AdaGrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb-4TPM5MGuE",
        "outputId": "a06c2887-108d-444e-a31a-f3aab512cd27"
      },
      "source": [
        "model_factory('Adagrad')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adagrad (\n",
            "Parameter Group 0\n",
            "    eps: 1e-10\n",
            "    initial_accumulator_value: 0\n",
            "    lr: 0.1\n",
            "    lr_decay: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.43297447, Test Loss: 2.33329262, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.33133696, Test Loss: 2.31113662, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.32438050, Test Loss: 2.33262855, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.32198116, Test Loss: 2.31648885, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 1.57443711, Test Loss: 0.38308425, Test Accuracy: 0.91690000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.20637552, Test Loss: 0.14843431, Test Accuracy: 0.95780000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.11044675, Test Loss: 0.11095761, Test Accuracy: 0.96810000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.07525161, Test Loss: 0.10652698, Test Accuracy: 0.96930000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.05501122, Test Loss: 0.09353709, Test Accuracy: 0.97300000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.04096059, Test Loss: 0.09066880, Test Accuracy: 0.97330000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.03073827, Test Loss: 0.08869772, Test Accuracy: 0.97560000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.02324941, Test Loss: 0.09035329, Test Accuracy: 0.97530000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.01752179, Test Loss: 0.09233054, Test Accuracy: 0.97530000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.01323451, Test Loss: 0.09858931, Test Accuracy: 0.97480000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.01054830, Test Loss: 0.09254653, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.00840160, Test Loss: 0.09544620, Test Accuracy: 0.97630000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.00683248, Test Loss: 0.09771999, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.00552852, Test Loss: 0.09854212, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.00433376, Test Loss: 0.10307136, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.00344527, Test Loss: 0.10353016, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.00247749, Test Loss: 0.10640430, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.00203980, Test Loss: 0.10771432, Test Accuracy: 0.97680000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.00167924, Test Loss: 0.11045176, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.00137265, Test Loss: 0.11128655, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.00117342, Test Loss: 0.11134518, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.00098612, Test Loss: 0.11458879, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.00085778, Test Loss: 0.11664393, Test Accuracy: 0.97680000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.00073961, Test Loss: 0.11703827, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00062171, Test Loss: 0.11889147, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00052643, Test Loss: 0.12024523, Test Accuracy: 0.97680000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00046437, Test Loss: 0.12035824, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00037837, Test Loss: 0.12142344, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00031379, Test Loss: 0.12277332, Test Accuracy: 0.97680000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00027592, Test Loss: 0.12404068, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00024791, Test Loss: 0.12474389, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00021019, Test Loss: 0.12561699, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00018247, Test Loss: 0.12712666, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00016336, Test Loss: 0.12750083, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00014831, Test Loss: 0.12876000, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00013654, Test Loss: 0.12985752, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00012471, Test Loss: 0.13020935, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00011520, Test Loss: 0.13082197, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00010716, Test Loss: 0.13143594, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00009942, Test Loss: 0.13190644, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00009287, Test Loss: 0.13314592, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00008686, Test Loss: 0.13311792, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00008149, Test Loss: 0.13370741, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00007705, Test Loss: 0.13464122, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00007261, Test Loss: 0.13533696, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00006896, Test Loss: 0.13578124, Test Accuracy: 0.97840000\n",
            "[tensor(0.2636, grad_fn=<MeanBackward0>), tensor(0.2585, grad_fn=<MeanBackward0>), tensor(0.2636, grad_fn=<MeanBackward0>), tensor(0.3055, grad_fn=<MeanBackward0>), tensor(0.4171, grad_fn=<MeanBackward0>), tensor(0.4333, grad_fn=<MeanBackward0>), tensor(0.4337, grad_fn=<MeanBackward0>), tensor(0.4392, grad_fn=<MeanBackward0>), tensor(0.4377, grad_fn=<MeanBackward0>), tensor(0.4395, grad_fn=<MeanBackward0>), tensor(0.4375, grad_fn=<MeanBackward0>), tensor(0.4406, grad_fn=<MeanBackward0>), tensor(0.4398, grad_fn=<MeanBackward0>), tensor(0.4385, grad_fn=<MeanBackward0>), tensor(0.4387, grad_fn=<MeanBackward0>), tensor(0.4391, grad_fn=<MeanBackward0>), tensor(0.4380, grad_fn=<MeanBackward0>), tensor(0.4384, grad_fn=<MeanBackward0>), tensor(0.4385, grad_fn=<MeanBackward0>), tensor(0.4384, grad_fn=<MeanBackward0>), tensor(0.4384, grad_fn=<MeanBackward0>), tensor(0.4380, grad_fn=<MeanBackward0>), tensor(0.4377, grad_fn=<MeanBackward0>), tensor(0.4382, grad_fn=<MeanBackward0>), tensor(0.4384, grad_fn=<MeanBackward0>), tensor(0.4383, grad_fn=<MeanBackward0>), tensor(0.4383, grad_fn=<MeanBackward0>), tensor(0.4381, grad_fn=<MeanBackward0>), tensor(0.4381, grad_fn=<MeanBackward0>), tensor(0.4381, grad_fn=<MeanBackward0>), tensor(0.4379, grad_fn=<MeanBackward0>), tensor(0.4379, grad_fn=<MeanBackward0>), tensor(0.4379, grad_fn=<MeanBackward0>), tensor(0.4378, grad_fn=<MeanBackward0>), tensor(0.4378, grad_fn=<MeanBackward0>), tensor(0.4378, grad_fn=<MeanBackward0>), tensor(0.4378, grad_fn=<MeanBackward0>), tensor(0.4378, grad_fn=<MeanBackward0>), tensor(0.4378, grad_fn=<MeanBackward0>), tensor(0.4377, grad_fn=<MeanBackward0>), tensor(0.4377, grad_fn=<MeanBackward0>), tensor(0.4377, grad_fn=<MeanBackward0>), tensor(0.4377, grad_fn=<MeanBackward0>), tensor(0.4377, grad_fn=<MeanBackward0>), tensor(0.4377, grad_fn=<MeanBackward0>), tensor(0.4377, grad_fn=<MeanBackward0>), tensor(0.4377, grad_fn=<MeanBackward0>), tensor(0.4377, grad_fn=<MeanBackward0>), tensor(0.4377, grad_fn=<MeanBackward0>), tensor(0.4377, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3015, grad_fn=<MeanBackward0>), tensor(0.3009, grad_fn=<MeanBackward0>), tensor(0.3015, grad_fn=<MeanBackward0>), tensor(0.3049, grad_fn=<MeanBackward0>), tensor(0.3629, grad_fn=<MeanBackward0>), tensor(0.4705, grad_fn=<MeanBackward0>), tensor(0.4963, grad_fn=<MeanBackward0>), tensor(0.5094, grad_fn=<MeanBackward0>), tensor(0.5084, grad_fn=<MeanBackward0>), tensor(0.5061, grad_fn=<MeanBackward0>), tensor(0.5126, grad_fn=<MeanBackward0>), tensor(0.5148, grad_fn=<MeanBackward0>), tensor(0.5133, grad_fn=<MeanBackward0>), tensor(0.5164, grad_fn=<MeanBackward0>), tensor(0.5103, grad_fn=<MeanBackward0>), tensor(0.5110, grad_fn=<MeanBackward0>), tensor(0.5132, grad_fn=<MeanBackward0>), tensor(0.5131, grad_fn=<MeanBackward0>), tensor(0.5123, grad_fn=<MeanBackward0>), tensor(0.5124, grad_fn=<MeanBackward0>), tensor(0.5104, grad_fn=<MeanBackward0>), tensor(0.5075, grad_fn=<MeanBackward0>), tensor(0.5100, grad_fn=<MeanBackward0>), tensor(0.5043, grad_fn=<MeanBackward0>), tensor(0.5066, grad_fn=<MeanBackward0>), tensor(0.5063, grad_fn=<MeanBackward0>), tensor(0.5041, grad_fn=<MeanBackward0>), tensor(0.5031, grad_fn=<MeanBackward0>), tensor(0.5015, grad_fn=<MeanBackward0>), tensor(0.5008, grad_fn=<MeanBackward0>), tensor(0.4999, grad_fn=<MeanBackward0>), tensor(0.4990, grad_fn=<MeanBackward0>), tensor(0.4942, grad_fn=<MeanBackward0>), tensor(0.4975, grad_fn=<MeanBackward0>), tensor(0.4967, grad_fn=<MeanBackward0>), tensor(0.4958, grad_fn=<MeanBackward0>), tensor(0.4948, grad_fn=<MeanBackward0>), tensor(0.4935, grad_fn=<MeanBackward0>), tensor(0.4937, grad_fn=<MeanBackward0>), tensor(0.4938, grad_fn=<MeanBackward0>), tensor(0.4926, grad_fn=<MeanBackward0>), tensor(0.4930, grad_fn=<MeanBackward0>), tensor(0.4927, grad_fn=<MeanBackward0>), tensor(0.4923, grad_fn=<MeanBackward0>), tensor(0.4911, grad_fn=<MeanBackward0>), tensor(0.4904, grad_fn=<MeanBackward0>), tensor(0.4894, grad_fn=<MeanBackward0>), tensor(0.4896, grad_fn=<MeanBackward0>), tensor(0.4893, grad_fn=<MeanBackward0>), tensor(0.4892, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3333, grad_fn=<MeanBackward0>), tensor(0.3333, grad_fn=<MeanBackward0>), tensor(0.3394, grad_fn=<MeanBackward0>), tensor(0.3394, grad_fn=<MeanBackward0>), tensor(0.3236, grad_fn=<MeanBackward0>), tensor(0.3220, grad_fn=<MeanBackward0>), tensor(0.3251, grad_fn=<MeanBackward0>), tensor(0.3312, grad_fn=<MeanBackward0>), tensor(0.3330, grad_fn=<MeanBackward0>), tensor(0.3290, grad_fn=<MeanBackward0>), tensor(0.3400, grad_fn=<MeanBackward0>), tensor(0.3361, grad_fn=<MeanBackward0>), tensor(0.3383, grad_fn=<MeanBackward0>), tensor(0.3343, grad_fn=<MeanBackward0>), tensor(0.3326, grad_fn=<MeanBackward0>), tensor(0.3311, grad_fn=<MeanBackward0>), tensor(0.3251, grad_fn=<MeanBackward0>), tensor(0.3300, grad_fn=<MeanBackward0>), tensor(0.3270, grad_fn=<MeanBackward0>), tensor(0.3223, grad_fn=<MeanBackward0>), tensor(0.3262, grad_fn=<MeanBackward0>), tensor(0.3248, grad_fn=<MeanBackward0>), tensor(0.3254, grad_fn=<MeanBackward0>), tensor(0.3223, grad_fn=<MeanBackward0>), tensor(0.3246, grad_fn=<MeanBackward0>), tensor(0.3237, grad_fn=<MeanBackward0>), tensor(0.3222, grad_fn=<MeanBackward0>), tensor(0.3228, grad_fn=<MeanBackward0>), tensor(0.3220, grad_fn=<MeanBackward0>), tensor(0.3205, grad_fn=<MeanBackward0>), tensor(0.3236, grad_fn=<MeanBackward0>), tensor(0.3222, grad_fn=<MeanBackward0>), tensor(0.3239, grad_fn=<MeanBackward0>), tensor(0.3225, grad_fn=<MeanBackward0>), tensor(0.3232, grad_fn=<MeanBackward0>), tensor(0.3226, grad_fn=<MeanBackward0>), tensor(0.3228, grad_fn=<MeanBackward0>), tensor(0.3232, grad_fn=<MeanBackward0>), tensor(0.3232, grad_fn=<MeanBackward0>), tensor(0.3238, grad_fn=<MeanBackward0>), tensor(0.3239, grad_fn=<MeanBackward0>), tensor(0.3245, grad_fn=<MeanBackward0>), tensor(0.3247, grad_fn=<MeanBackward0>), tensor(0.3251, grad_fn=<MeanBackward0>), tensor(0.3252, grad_fn=<MeanBackward0>), tensor(0.3256, grad_fn=<MeanBackward0>), tensor(0.3259, grad_fn=<MeanBackward0>), tensor(0.3260, grad_fn=<MeanBackward0>), tensor(0.3264, grad_fn=<MeanBackward0>), tensor(0.3266, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.29946330189704895, 0.2975699206193288, 0.30151234070460003, 0.31660065054893494, 0.3678521712621053, 0.4086116949717204, 0.4183807671070099, 0.4265923301378886, 0.4263956844806671, 0.4248661696910858, 0.43001942833264667, 0.4305083354314168, 0.4304845631122589, 0.4297158221403758, 0.4271954794724782, 0.42705586552619934, 0.4254184166590373, 0.42714551091194153, 0.4259164035320282, 0.424375722805659, 0.42500696579615277, 0.4234263002872467, 0.4243938426176707, 0.42161176602045697, 0.42320098479588825, 0.42274468143781024, 0.421514630317688, 0.42134889960289, 0.4205235441525777, 0.4197731912136078, 0.42045759161313373, 0.4196840624014537, 0.4186605711778005, 0.41925476988156635, 0.41921426852544147, 0.4187325636545817, 0.41845764716466266, 0.41816437244415283, 0.41820146640141803, 0.41845618685086566, 0.41807706157366437, 0.4183886746565501, 0.41836122671763104, 0.41837310791015625, 0.4179986317952474, 0.4178746740023295, 0.4176291525363922, 0.4177386860052745, 0.4177890717983246, 0.41782883803049725]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmLJ4Zr2MnoS"
      },
      "source": [
        "# SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ObsEJHuMoPy",
        "outputId": "1639c737-5770-4ec9-b671-eae4122182cb"
      },
      "source": [
        "model_factory('SGD')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            ")\n",
            "my_optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.32532845, Test Loss: 2.31208871, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30529008, Test Loss: 2.29400768, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.09060714, Test Loss: 1.51504961, Test Accuracy: 0.43530000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 1.15780424, Test Loss: 0.92878846, Test Accuracy: 0.69310000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.73544378, Test Loss: 0.61645775, Test Accuracy: 0.81670000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.56971211, Test Loss: 0.50624602, Test Accuracy: 0.85860000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.48048691, Test Loss: 0.43317789, Test Accuracy: 0.88170000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.41316074, Test Loss: 0.38933342, Test Accuracy: 0.89460000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.36106235, Test Loss: 0.32562608, Test Accuracy: 0.91190000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.31813434, Test Loss: 0.29498430, Test Accuracy: 0.91750000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.28134766, Test Loss: 0.28739367, Test Accuracy: 0.92050000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.24858592, Test Loss: 0.26864402, Test Accuracy: 0.92110000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.21990306, Test Loss: 0.21969077, Test Accuracy: 0.93980000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.19893319, Test Loss: 0.19158922, Test Accuracy: 0.94450000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.17917664, Test Loss: 0.17869401, Test Accuracy: 0.95000000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.16519953, Test Loss: 0.16411226, Test Accuracy: 0.95270000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.15278488, Test Loss: 0.16242116, Test Accuracy: 0.95200000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.14202907, Test Loss: 0.16607916, Test Accuracy: 0.94980000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.13320350, Test Loss: 0.13930785, Test Accuracy: 0.95950000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.12522590, Test Loss: 0.13881749, Test Accuracy: 0.95800000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.11762799, Test Loss: 0.13809037, Test Accuracy: 0.95890000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.11100591, Test Loss: 0.14160439, Test Accuracy: 0.95810000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.10406828, Test Loss: 0.12224484, Test Accuracy: 0.96400000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.09816111, Test Loss: 0.12370116, Test Accuracy: 0.96480000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.09334046, Test Loss: 0.11687362, Test Accuracy: 0.96540000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.08873709, Test Loss: 0.13687615, Test Accuracy: 0.96070000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.08343246, Test Loss: 0.11195592, Test Accuracy: 0.96740000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.07959808, Test Loss: 0.11098211, Test Accuracy: 0.96710000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.07490557, Test Loss: 0.10301682, Test Accuracy: 0.96980000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.07064912, Test Loss: 0.11123683, Test Accuracy: 0.96760000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.06670394, Test Loss: 0.10556572, Test Accuracy: 0.96870000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.06362269, Test Loss: 0.10338117, Test Accuracy: 0.96980000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.06039618, Test Loss: 0.10444765, Test Accuracy: 0.96920000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.05755416, Test Loss: 0.11162742, Test Accuracy: 0.96770000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.05481909, Test Loss: 0.09923855, Test Accuracy: 0.97160000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.05109029, Test Loss: 0.10074976, Test Accuracy: 0.97160000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.04947395, Test Loss: 0.12790379, Test Accuracy: 0.96320000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.04544718, Test Loss: 0.09784573, Test Accuracy: 0.97220000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.04388826, Test Loss: 0.09627195, Test Accuracy: 0.97200000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.04134829, Test Loss: 0.09861339, Test Accuracy: 0.97140000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.03897853, Test Loss: 0.09710683, Test Accuracy: 0.97120000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.03686709, Test Loss: 0.09839267, Test Accuracy: 0.97270000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.03512137, Test Loss: 0.10930836, Test Accuracy: 0.96930000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.03295690, Test Loss: 0.09996101, Test Accuracy: 0.97150000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.03136570, Test Loss: 0.09862370, Test Accuracy: 0.97260000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.02954736, Test Loss: 0.10324888, Test Accuracy: 0.97170000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.02767032, Test Loss: 0.09431137, Test Accuracy: 0.97440000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.02622695, Test Loss: 0.09770340, Test Accuracy: 0.97350000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.02496538, Test Loss: 0.09780007, Test Accuracy: 0.97400000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.02310412, Test Loss: 0.10582321, Test Accuracy: 0.97210000\n",
            "[tensor(0.0056, grad_fn=<MeanBackward0>), tensor(0.0086, grad_fn=<MeanBackward0>), tensor(0.0374, grad_fn=<MeanBackward0>), tensor(0.0659, grad_fn=<MeanBackward0>), tensor(0.1019, grad_fn=<MeanBackward0>), tensor(0.1243, grad_fn=<MeanBackward0>), tensor(0.1341, grad_fn=<MeanBackward0>), tensor(0.1394, grad_fn=<MeanBackward0>), tensor(0.1465, grad_fn=<MeanBackward0>), tensor(0.1560, grad_fn=<MeanBackward0>), tensor(0.1661, grad_fn=<MeanBackward0>), tensor(0.1763, grad_fn=<MeanBackward0>), tensor(0.1843, grad_fn=<MeanBackward0>), tensor(0.1919, grad_fn=<MeanBackward0>), tensor(0.1977, grad_fn=<MeanBackward0>), tensor(0.2024, grad_fn=<MeanBackward0>), tensor(0.2064, grad_fn=<MeanBackward0>), tensor(0.2098, grad_fn=<MeanBackward0>), tensor(0.2143, grad_fn=<MeanBackward0>), tensor(0.2187, grad_fn=<MeanBackward0>), tensor(0.2220, grad_fn=<MeanBackward0>), tensor(0.2246, grad_fn=<MeanBackward0>), tensor(0.2282, grad_fn=<MeanBackward0>), tensor(0.2308, grad_fn=<MeanBackward0>), tensor(0.2335, grad_fn=<MeanBackward0>), tensor(0.2365, grad_fn=<MeanBackward0>), tensor(0.2384, grad_fn=<MeanBackward0>), tensor(0.2412, grad_fn=<MeanBackward0>), tensor(0.2433, grad_fn=<MeanBackward0>), tensor(0.2448, grad_fn=<MeanBackward0>), tensor(0.2470, grad_fn=<MeanBackward0>), tensor(0.2487, grad_fn=<MeanBackward0>), tensor(0.2504, grad_fn=<MeanBackward0>), tensor(0.2525, grad_fn=<MeanBackward0>), tensor(0.2535, grad_fn=<MeanBackward0>), tensor(0.2549, grad_fn=<MeanBackward0>), tensor(0.2564, grad_fn=<MeanBackward0>), tensor(0.2584, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2610, grad_fn=<MeanBackward0>), tensor(0.2616, grad_fn=<MeanBackward0>), tensor(0.2629, grad_fn=<MeanBackward0>), tensor(0.2635, grad_fn=<MeanBackward0>), tensor(0.2647, grad_fn=<MeanBackward0>), tensor(0.2650, grad_fn=<MeanBackward0>), tensor(0.2659, grad_fn=<MeanBackward0>), tensor(0.2669, grad_fn=<MeanBackward0>), tensor(0.2676, grad_fn=<MeanBackward0>), tensor(0.2681, grad_fn=<MeanBackward0>), tensor(0.2682, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0122, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0250, grad_fn=<MeanBackward0>), tensor(0.0435, grad_fn=<MeanBackward0>), tensor(0.0562, grad_fn=<MeanBackward0>), tensor(0.0604, grad_fn=<MeanBackward0>), tensor(0.0599, grad_fn=<MeanBackward0>), tensor(0.0575, grad_fn=<MeanBackward0>), tensor(0.0563, grad_fn=<MeanBackward0>), tensor(0.0555, grad_fn=<MeanBackward0>), tensor(0.0519, grad_fn=<MeanBackward0>), tensor(0.0518, grad_fn=<MeanBackward0>), tensor(0.0505, grad_fn=<MeanBackward0>), tensor(0.0505, grad_fn=<MeanBackward0>), tensor(0.0501, grad_fn=<MeanBackward0>), tensor(0.0498, grad_fn=<MeanBackward0>), tensor(0.0495, grad_fn=<MeanBackward0>), tensor(0.0495, grad_fn=<MeanBackward0>), tensor(0.0481, grad_fn=<MeanBackward0>), tensor(0.0489, grad_fn=<MeanBackward0>), tensor(0.0479, grad_fn=<MeanBackward0>), tensor(0.0493, grad_fn=<MeanBackward0>), tensor(0.0480, grad_fn=<MeanBackward0>), tensor(0.0474, grad_fn=<MeanBackward0>), tensor(0.0469, grad_fn=<MeanBackward0>), tensor(0.0468, grad_fn=<MeanBackward0>), tensor(0.0470, grad_fn=<MeanBackward0>), tensor(0.0475, grad_fn=<MeanBackward0>), tensor(0.0462, grad_fn=<MeanBackward0>), tensor(0.0456, grad_fn=<MeanBackward0>), tensor(0.0462, grad_fn=<MeanBackward0>), tensor(0.0451, grad_fn=<MeanBackward0>), tensor(0.0455, grad_fn=<MeanBackward0>), tensor(0.0441, grad_fn=<MeanBackward0>), tensor(0.0447, grad_fn=<MeanBackward0>), tensor(0.0443, grad_fn=<MeanBackward0>), tensor(0.0442, grad_fn=<MeanBackward0>), tensor(0.0449, grad_fn=<MeanBackward0>), tensor(0.0448, grad_fn=<MeanBackward0>), tensor(0.0447, grad_fn=<MeanBackward0>), tensor(0.0446, grad_fn=<MeanBackward0>), tensor(0.0441, grad_fn=<MeanBackward0>), tensor(0.0441, grad_fn=<MeanBackward0>), tensor(0.0440, grad_fn=<MeanBackward0>), tensor(0.0437, grad_fn=<MeanBackward0>), tensor(0.0431, grad_fn=<MeanBackward0>), tensor(0.0436, grad_fn=<MeanBackward0>), tensor(0.0432, grad_fn=<MeanBackward0>), tensor(0.0425, grad_fn=<MeanBackward0>), tensor(0.0426, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0066, grad_fn=<MeanBackward0>), tensor(0.0045, grad_fn=<MeanBackward0>), tensor(0.0798, grad_fn=<MeanBackward0>), tensor(0.1015, grad_fn=<MeanBackward0>), tensor(0.1034, grad_fn=<MeanBackward0>), tensor(0.1108, grad_fn=<MeanBackward0>), tensor(0.1151, grad_fn=<MeanBackward0>), tensor(0.1161, grad_fn=<MeanBackward0>), tensor(0.1165, grad_fn=<MeanBackward0>), tensor(0.1175, grad_fn=<MeanBackward0>), tensor(0.1138, grad_fn=<MeanBackward0>), tensor(0.1118, grad_fn=<MeanBackward0>), tensor(0.1113, grad_fn=<MeanBackward0>), tensor(0.1107, grad_fn=<MeanBackward0>), tensor(0.1107, grad_fn=<MeanBackward0>), tensor(0.1111, grad_fn=<MeanBackward0>), tensor(0.1127, grad_fn=<MeanBackward0>), tensor(0.1121, grad_fn=<MeanBackward0>), tensor(0.1118, grad_fn=<MeanBackward0>), tensor(0.1133, grad_fn=<MeanBackward0>), tensor(0.1115, grad_fn=<MeanBackward0>), tensor(0.1136, grad_fn=<MeanBackward0>), tensor(0.1139, grad_fn=<MeanBackward0>), tensor(0.1122, grad_fn=<MeanBackward0>), tensor(0.1130, grad_fn=<MeanBackward0>), tensor(0.1133, grad_fn=<MeanBackward0>), tensor(0.1124, grad_fn=<MeanBackward0>), tensor(0.1130, grad_fn=<MeanBackward0>), tensor(0.1121, grad_fn=<MeanBackward0>), tensor(0.1111, grad_fn=<MeanBackward0>), tensor(0.1108, grad_fn=<MeanBackward0>), tensor(0.1110, grad_fn=<MeanBackward0>), tensor(0.1118, grad_fn=<MeanBackward0>), tensor(0.1121, grad_fn=<MeanBackward0>), tensor(0.1119, grad_fn=<MeanBackward0>), tensor(0.1127, grad_fn=<MeanBackward0>), tensor(0.1134, grad_fn=<MeanBackward0>), tensor(0.1119, grad_fn=<MeanBackward0>), tensor(0.1132, grad_fn=<MeanBackward0>), tensor(0.1128, grad_fn=<MeanBackward0>), tensor(0.1142, grad_fn=<MeanBackward0>), tensor(0.1132, grad_fn=<MeanBackward0>), tensor(0.1129, grad_fn=<MeanBackward0>), tensor(0.1144, grad_fn=<MeanBackward0>), tensor(0.1136, grad_fn=<MeanBackward0>), tensor(0.1138, grad_fn=<MeanBackward0>), tensor(0.1135, grad_fn=<MeanBackward0>), tensor(0.1145, grad_fn=<MeanBackward0>), tensor(0.1135, grad_fn=<MeanBackward0>), tensor(0.1149, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.008115171144406, 0.008248130790889263, 0.0474033293624719, 0.07029881204168002, 0.08717720583081245, 0.09851342191298802, 0.10304588948686917, 0.10434707129995029, 0.10643112162748973, 0.10966777304808299, 0.1105986771484216, 0.11328446740905444, 0.11536489675442378, 0.11773036544521649, 0.11951422070463498, 0.12110075478752454, 0.12288488944371541, 0.12381494541962941, 0.12472747638821602, 0.12694906070828438, 0.1271382992466291, 0.12915366142988205, 0.1300439921518167, 0.13015053421258926, 0.13112017636497816, 0.1321705679098765, 0.1325846587618192, 0.13386932015419006, 0.13387522225578627, 0.13382425904273987, 0.13467319558064142, 0.1349253828326861, 0.13591916486620903, 0.13623670736948648, 0.1366950236260891, 0.13730356593926749, 0.13799000531435013, 0.13840408995747566, 0.13923619811733565, 0.1395187166829904, 0.14014748111367226, 0.14007887740929922, 0.14015852535764375, 0.14100074023008347, 0.14077546944220862, 0.14093995342652002, 0.14133269463976225, 0.14175866295893988, 0.14139373352130255, 0.14190143470962843]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvQxaN_fRXLq"
      },
      "source": [
        "# Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkqfFoVkRXxP",
        "outputId": "53d8ee2f-5674-4803-82f1-1b7e7112583b"
      },
      "source": [
        "model_factory('Adam')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 0.53387665, Test Loss: 0.22672456, Test Accuracy: 0.93340000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 0.17671410, Test Loss: 0.14660383, Test Accuracy: 0.95600000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 0.12163184, Test Loss: 0.11540960, Test Accuracy: 0.96490000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 0.09115731, Test Loss: 0.10252395, Test Accuracy: 0.96910000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.07290089, Test Loss: 0.09184266, Test Accuracy: 0.97110000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.05646879, Test Loss: 0.08709985, Test Accuracy: 0.97310000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.04764986, Test Loss: 0.08260359, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.03745656, Test Loss: 0.09358410, Test Accuracy: 0.97340000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.03126319, Test Loss: 0.10327442, Test Accuracy: 0.97260000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.02690324, Test Loss: 0.09627856, Test Accuracy: 0.97350000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.02183411, Test Loss: 0.08748319, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.01857902, Test Loss: 0.08327075, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.01575469, Test Loss: 0.08859389, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.01513554, Test Loss: 0.09890987, Test Accuracy: 0.97660000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.01354384, Test Loss: 0.10785309, Test Accuracy: 0.97560000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.01157518, Test Loss: 0.10576002, Test Accuracy: 0.97630000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.01003486, Test Loss: 0.10296812, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.01080955, Test Loss: 0.09670066, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.00790836, Test Loss: 0.11101465, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.00981442, Test Loss: 0.10907616, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.00509494, Test Loss: 0.14253406, Test Accuracy: 0.97260000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.00817656, Test Loss: 0.10571221, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.00645968, Test Loss: 0.10320981, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.00587529, Test Loss: 0.12202683, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.00726575, Test Loss: 0.12060207, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.00643080, Test Loss: 0.09857835, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.00508339, Test Loss: 0.13689073, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.00670790, Test Loss: 0.10914707, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00449612, Test Loss: 0.11520653, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00619041, Test Loss: 0.11430205, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00591654, Test Loss: 0.13649324, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00419765, Test Loss: 0.12382365, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00413260, Test Loss: 0.13795083, Test Accuracy: 0.97670000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00382971, Test Loss: 0.12198096, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00588127, Test Loss: 0.13184561, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00447558, Test Loss: 0.12852260, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00359541, Test Loss: 0.12137408, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00467749, Test Loss: 0.15551787, Test Accuracy: 0.97540000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00268383, Test Loss: 0.11644943, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00505820, Test Loss: 0.12215037, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00425593, Test Loss: 0.12617655, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00276675, Test Loss: 0.12070085, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00241917, Test Loss: 0.12895896, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00431883, Test Loss: 0.13566129, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00282843, Test Loss: 0.12569955, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00417160, Test Loss: 0.16915664, Test Accuracy: 0.97530000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00286742, Test Loss: 0.12933992, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00352090, Test Loss: 0.14295196, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00347901, Test Loss: 0.15795133, Test Accuracy: 0.97580000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00320643, Test Loss: 0.13278869, Test Accuracy: 0.98060000\n",
            "[tensor(0.2724, grad_fn=<MeanBackward0>), tensor(0.2974, grad_fn=<MeanBackward0>), tensor(0.3150, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3339, grad_fn=<MeanBackward0>), tensor(0.3332, grad_fn=<MeanBackward0>), tensor(0.3429, grad_fn=<MeanBackward0>), tensor(0.3459, grad_fn=<MeanBackward0>), tensor(0.3527, grad_fn=<MeanBackward0>), tensor(0.3517, grad_fn=<MeanBackward0>), tensor(0.3551, grad_fn=<MeanBackward0>), tensor(0.3554, grad_fn=<MeanBackward0>), tensor(0.3575, grad_fn=<MeanBackward0>), tensor(0.3588, grad_fn=<MeanBackward0>), tensor(0.3624, grad_fn=<MeanBackward0>), tensor(0.3657, grad_fn=<MeanBackward0>), tensor(0.3657, grad_fn=<MeanBackward0>), tensor(0.3635, grad_fn=<MeanBackward0>), tensor(0.3686, grad_fn=<MeanBackward0>), tensor(0.3756, grad_fn=<MeanBackward0>), tensor(0.3721, grad_fn=<MeanBackward0>), tensor(0.3736, grad_fn=<MeanBackward0>), tensor(0.3720, grad_fn=<MeanBackward0>), tensor(0.3731, grad_fn=<MeanBackward0>), tensor(0.3829, grad_fn=<MeanBackward0>), tensor(0.3765, grad_fn=<MeanBackward0>), tensor(0.3793, grad_fn=<MeanBackward0>), tensor(0.3778, grad_fn=<MeanBackward0>), tensor(0.3770, grad_fn=<MeanBackward0>), tensor(0.3818, grad_fn=<MeanBackward0>), tensor(0.3768, grad_fn=<MeanBackward0>), tensor(0.3784, grad_fn=<MeanBackward0>), tensor(0.3743, grad_fn=<MeanBackward0>), tensor(0.3847, grad_fn=<MeanBackward0>), tensor(0.3839, grad_fn=<MeanBackward0>), tensor(0.3805, grad_fn=<MeanBackward0>), tensor(0.3846, grad_fn=<MeanBackward0>), tensor(0.3864, grad_fn=<MeanBackward0>), tensor(0.3895, grad_fn=<MeanBackward0>), tensor(0.3835, grad_fn=<MeanBackward0>), tensor(0.3845, grad_fn=<MeanBackward0>), tensor(0.3842, grad_fn=<MeanBackward0>), tensor(0.3878, grad_fn=<MeanBackward0>), tensor(0.3867, grad_fn=<MeanBackward0>), tensor(0.3912, grad_fn=<MeanBackward0>), tensor(0.3873, grad_fn=<MeanBackward0>), tensor(0.3854, grad_fn=<MeanBackward0>), tensor(0.3909, grad_fn=<MeanBackward0>), tensor(0.3907, grad_fn=<MeanBackward0>), tensor(0.3900, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2097, grad_fn=<MeanBackward0>), tensor(0.2195, grad_fn=<MeanBackward0>), tensor(0.2280, grad_fn=<MeanBackward0>), tensor(0.2315, grad_fn=<MeanBackward0>), tensor(0.2420, grad_fn=<MeanBackward0>), tensor(0.2445, grad_fn=<MeanBackward0>), tensor(0.2580, grad_fn=<MeanBackward0>), tensor(0.2542, grad_fn=<MeanBackward0>), tensor(0.2659, grad_fn=<MeanBackward0>), tensor(0.2723, grad_fn=<MeanBackward0>), tensor(0.2723, grad_fn=<MeanBackward0>), tensor(0.2833, grad_fn=<MeanBackward0>), tensor(0.2900, grad_fn=<MeanBackward0>), tensor(0.2948, grad_fn=<MeanBackward0>), tensor(0.2999, grad_fn=<MeanBackward0>), tensor(0.3048, grad_fn=<MeanBackward0>), tensor(0.3087, grad_fn=<MeanBackward0>), tensor(0.3172, grad_fn=<MeanBackward0>), tensor(0.3191, grad_fn=<MeanBackward0>), tensor(0.3270, grad_fn=<MeanBackward0>), tensor(0.3272, grad_fn=<MeanBackward0>), tensor(0.3400, grad_fn=<MeanBackward0>), tensor(0.3386, grad_fn=<MeanBackward0>), tensor(0.3456, grad_fn=<MeanBackward0>), tensor(0.3502, grad_fn=<MeanBackward0>), tensor(0.3580, grad_fn=<MeanBackward0>), tensor(0.3634, grad_fn=<MeanBackward0>), tensor(0.3640, grad_fn=<MeanBackward0>), tensor(0.3654, grad_fn=<MeanBackward0>), tensor(0.3672, grad_fn=<MeanBackward0>), tensor(0.3806, grad_fn=<MeanBackward0>), tensor(0.3804, grad_fn=<MeanBackward0>), tensor(0.3856, grad_fn=<MeanBackward0>), tensor(0.3813, grad_fn=<MeanBackward0>), tensor(0.3830, grad_fn=<MeanBackward0>), tensor(0.3876, grad_fn=<MeanBackward0>), tensor(0.3837, grad_fn=<MeanBackward0>), tensor(0.3841, grad_fn=<MeanBackward0>), tensor(0.3858, grad_fn=<MeanBackward0>), tensor(0.3889, grad_fn=<MeanBackward0>), tensor(0.3901, grad_fn=<MeanBackward0>), tensor(0.3901, grad_fn=<MeanBackward0>), tensor(0.3951, grad_fn=<MeanBackward0>), tensor(0.4015, grad_fn=<MeanBackward0>), tensor(0.4015, grad_fn=<MeanBackward0>), tensor(0.4008, grad_fn=<MeanBackward0>), tensor(0.4068, grad_fn=<MeanBackward0>), tensor(0.4060, grad_fn=<MeanBackward0>), tensor(0.4112, grad_fn=<MeanBackward0>), tensor(0.4033, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2250, grad_fn=<MeanBackward0>), tensor(0.2391, grad_fn=<MeanBackward0>), tensor(0.2541, grad_fn=<MeanBackward0>), tensor(0.2588, grad_fn=<MeanBackward0>), tensor(0.2796, grad_fn=<MeanBackward0>), tensor(0.2779, grad_fn=<MeanBackward0>), tensor(0.2909, grad_fn=<MeanBackward0>), tensor(0.2904, grad_fn=<MeanBackward0>), tensor(0.2968, grad_fn=<MeanBackward0>), tensor(0.2980, grad_fn=<MeanBackward0>), tensor(0.3034, grad_fn=<MeanBackward0>), tensor(0.3053, grad_fn=<MeanBackward0>), tensor(0.3062, grad_fn=<MeanBackward0>), tensor(0.3147, grad_fn=<MeanBackward0>), tensor(0.3189, grad_fn=<MeanBackward0>), tensor(0.3145, grad_fn=<MeanBackward0>), tensor(0.3162, grad_fn=<MeanBackward0>), tensor(0.3222, grad_fn=<MeanBackward0>), tensor(0.3182, grad_fn=<MeanBackward0>), tensor(0.3244, grad_fn=<MeanBackward0>), tensor(0.3244, grad_fn=<MeanBackward0>), tensor(0.3265, grad_fn=<MeanBackward0>), tensor(0.3315, grad_fn=<MeanBackward0>), tensor(0.3284, grad_fn=<MeanBackward0>), tensor(0.3334, grad_fn=<MeanBackward0>), tensor(0.3355, grad_fn=<MeanBackward0>), tensor(0.3367, grad_fn=<MeanBackward0>), tensor(0.3382, grad_fn=<MeanBackward0>), tensor(0.3349, grad_fn=<MeanBackward0>), tensor(0.3395, grad_fn=<MeanBackward0>), tensor(0.3424, grad_fn=<MeanBackward0>), tensor(0.3420, grad_fn=<MeanBackward0>), tensor(0.3434, grad_fn=<MeanBackward0>), tensor(0.3392, grad_fn=<MeanBackward0>), tensor(0.3449, grad_fn=<MeanBackward0>), tensor(0.3464, grad_fn=<MeanBackward0>), tensor(0.3503, grad_fn=<MeanBackward0>), tensor(0.3526, grad_fn=<MeanBackward0>), tensor(0.3506, grad_fn=<MeanBackward0>), tensor(0.3561, grad_fn=<MeanBackward0>), tensor(0.3538, grad_fn=<MeanBackward0>), tensor(0.3569, grad_fn=<MeanBackward0>), tensor(0.3517, grad_fn=<MeanBackward0>), tensor(0.3541, grad_fn=<MeanBackward0>), tensor(0.3529, grad_fn=<MeanBackward0>), tensor(0.3544, grad_fn=<MeanBackward0>), tensor(0.3531, grad_fn=<MeanBackward0>), tensor(0.3569, grad_fn=<MeanBackward0>), tensor(0.3566, grad_fn=<MeanBackward0>), tensor(0.3581, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.23572911322116852, 0.25198499858379364, 0.2656945337851842, 0.27053878208001453, 0.28516269226868945, 0.28519579271475476, 0.29726694027582806, 0.2968510488669078, 0.3051580786705017, 0.30734776457150775, 0.3102847735087077, 0.3146771291891734, 0.31789036591847736, 0.32276450594266254, 0.3270687758922577, 0.32833073536554974, 0.3302231927712758, 0.33429011702537537, 0.3353017369906108, 0.3423231740792592, 0.34123434623082477, 0.34672897060712177, 0.34736956159273785, 0.34903745849927265, 0.35551194349924725, 0.35668429732322693, 0.35980814695358276, 0.3600078721841176, 0.3590935667355855, 0.3628036677837372, 0.3666069606939952, 0.3669352134068807, 0.3677522341410319, 0.368401159842809, 0.3705910344918569, 0.3715032438437144, 0.3728732466697693, 0.374368021885554, 0.37533626953760785, 0.37619871894518536, 0.3761005798975627, 0.37704436977704364, 0.37821892897288006, 0.38073508938153583, 0.38190142313639325, 0.3808538317680359, 0.38179826736450195, 0.384581983089447, 0.38617925842603046, 0.3837879200776418]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}