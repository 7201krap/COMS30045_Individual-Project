{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_1_gradient_descent_with_autograd_and_backprop.ipynb.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbr4pvAzgRdz/Jpy2CZmOL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_introduction_to_pytorch/blob/main/5_1_gradient_descent_with_autograd_and_backprop_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVyrlR2GbOra"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhlLy3jJbhf-",
        "outputId": "21398193-a234-4dee-854d-365f3411091d"
      },
      "source": [
        "# linear regression\n",
        "# f = 2 * x\n",
        "\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "y = 2 * X\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "    return w * x \n",
        "\n",
        "# loss\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted - y)**2).mean()\n",
        "\n",
        "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
        "\n",
        "# training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # prediction = forward pass \n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss \n",
        "    l = loss(y, y_pred)\n",
        "\n",
        "    # gradients = backward pass\n",
        "    l.backward()    # dloss/dw\n",
        "\n",
        "    # update weights\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad \n",
        "\n",
        "    # zero gradients \n",
        "    w.grad.zero_()\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        print(f'epoch: {epoch+1}: w: {w:.3f}, loss: {l:.8f}') \n",
        "\n",
        "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch: 1: w: 0.300, loss: 30.00000000\n",
            "epoch: 2: w: 0.555, loss: 21.67499924\n",
            "epoch: 3: w: 0.772, loss: 15.66018772\n",
            "epoch: 4: w: 0.956, loss: 11.31448650\n",
            "epoch: 5: w: 1.113, loss: 8.17471695\n",
            "epoch: 6: w: 1.246, loss: 5.90623236\n",
            "epoch: 7: w: 1.359, loss: 4.26725292\n",
            "epoch: 8: w: 1.455, loss: 3.08308983\n",
            "epoch: 9: w: 1.537, loss: 2.22753215\n",
            "epoch: 10: w: 1.606, loss: 1.60939169\n",
            "epoch: 11: w: 1.665, loss: 1.16278565\n",
            "epoch: 12: w: 1.716, loss: 0.84011245\n",
            "epoch: 13: w: 1.758, loss: 0.60698116\n",
            "epoch: 14: w: 1.794, loss: 0.43854395\n",
            "epoch: 15: w: 1.825, loss: 0.31684780\n",
            "epoch: 16: w: 1.851, loss: 0.22892261\n",
            "epoch: 17: w: 1.874, loss: 0.16539653\n",
            "epoch: 18: w: 1.893, loss: 0.11949898\n",
            "epoch: 19: w: 1.909, loss: 0.08633806\n",
            "epoch: 20: w: 1.922, loss: 0.06237914\n",
            "epoch: 21: w: 1.934, loss: 0.04506890\n",
            "epoch: 22: w: 1.944, loss: 0.03256231\n",
            "epoch: 23: w: 1.952, loss: 0.02352631\n",
            "epoch: 24: w: 1.960, loss: 0.01699772\n",
            "epoch: 25: w: 1.966, loss: 0.01228084\n",
            "epoch: 26: w: 1.971, loss: 0.00887291\n",
            "epoch: 27: w: 1.975, loss: 0.00641066\n",
            "epoch: 28: w: 1.979, loss: 0.00463169\n",
            "epoch: 29: w: 1.982, loss: 0.00334642\n",
            "epoch: 30: w: 1.985, loss: 0.00241778\n",
            "epoch: 31: w: 1.987, loss: 0.00174685\n",
            "epoch: 32: w: 1.989, loss: 0.00126211\n",
            "epoch: 33: w: 1.991, loss: 0.00091188\n",
            "epoch: 34: w: 1.992, loss: 0.00065882\n",
            "epoch: 35: w: 1.993, loss: 0.00047601\n",
            "epoch: 36: w: 1.994, loss: 0.00034392\n",
            "epoch: 37: w: 1.995, loss: 0.00024848\n",
            "epoch: 38: w: 1.996, loss: 0.00017952\n",
            "epoch: 39: w: 1.996, loss: 0.00012971\n",
            "epoch: 40: w: 1.997, loss: 0.00009371\n",
            "epoch: 41: w: 1.997, loss: 0.00006770\n",
            "epoch: 42: w: 1.998, loss: 0.00004891\n",
            "epoch: 43: w: 1.998, loss: 0.00003534\n",
            "epoch: 44: w: 1.998, loss: 0.00002553\n",
            "epoch: 45: w: 1.999, loss: 0.00001845\n",
            "epoch: 46: w: 1.999, loss: 0.00001333\n",
            "epoch: 47: w: 1.999, loss: 0.00000963\n",
            "epoch: 48: w: 1.999, loss: 0.00000696\n",
            "epoch: 49: w: 1.999, loss: 0.00000503\n",
            "epoch: 50: w: 1.999, loss: 0.00000363\n",
            "epoch: 51: w: 1.999, loss: 0.00000262\n",
            "epoch: 52: w: 2.000, loss: 0.00000190\n",
            "epoch: 53: w: 2.000, loss: 0.00000137\n",
            "epoch: 54: w: 2.000, loss: 0.00000099\n",
            "epoch: 55: w: 2.000, loss: 0.00000071\n",
            "epoch: 56: w: 2.000, loss: 0.00000052\n",
            "epoch: 57: w: 2.000, loss: 0.00000037\n",
            "epoch: 58: w: 2.000, loss: 0.00000027\n",
            "epoch: 59: w: 2.000, loss: 0.00000019\n",
            "epoch: 60: w: 2.000, loss: 0.00000014\n",
            "epoch: 61: w: 2.000, loss: 0.00000010\n",
            "epoch: 62: w: 2.000, loss: 0.00000007\n",
            "epoch: 63: w: 2.000, loss: 0.00000005\n",
            "epoch: 64: w: 2.000, loss: 0.00000004\n",
            "epoch: 65: w: 2.000, loss: 0.00000003\n",
            "epoch: 66: w: 2.000, loss: 0.00000002\n",
            "epoch: 67: w: 2.000, loss: 0.00000001\n",
            "epoch: 68: w: 2.000, loss: 0.00000001\n",
            "epoch: 69: w: 2.000, loss: 0.00000001\n",
            "epoch: 70: w: 2.000, loss: 0.00000001\n",
            "epoch: 71: w: 2.000, loss: 0.00000000\n",
            "epoch: 72: w: 2.000, loss: 0.00000000\n",
            "epoch: 73: w: 2.000, loss: 0.00000000\n",
            "epoch: 74: w: 2.000, loss: 0.00000000\n",
            "epoch: 75: w: 2.000, loss: 0.00000000\n",
            "epoch: 76: w: 2.000, loss: 0.00000000\n",
            "epoch: 77: w: 2.000, loss: 0.00000000\n",
            "epoch: 78: w: 2.000, loss: 0.00000000\n",
            "epoch: 79: w: 2.000, loss: 0.00000000\n",
            "epoch: 80: w: 2.000, loss: 0.00000000\n",
            "epoch: 81: w: 2.000, loss: 0.00000000\n",
            "epoch: 82: w: 2.000, loss: 0.00000000\n",
            "epoch: 83: w: 2.000, loss: 0.00000000\n",
            "epoch: 84: w: 2.000, loss: 0.00000000\n",
            "epoch: 85: w: 2.000, loss: 0.00000000\n",
            "epoch: 86: w: 2.000, loss: 0.00000000\n",
            "epoch: 87: w: 2.000, loss: 0.00000000\n",
            "epoch: 88: w: 2.000, loss: 0.00000000\n",
            "epoch: 89: w: 2.000, loss: 0.00000000\n",
            "epoch: 90: w: 2.000, loss: 0.00000000\n",
            "epoch: 91: w: 2.000, loss: 0.00000000\n",
            "epoch: 92: w: 2.000, loss: 0.00000000\n",
            "epoch: 93: w: 2.000, loss: 0.00000000\n",
            "epoch: 94: w: 2.000, loss: 0.00000000\n",
            "epoch: 95: w: 2.000, loss: 0.00000000\n",
            "epoch: 96: w: 2.000, loss: 0.00000000\n",
            "epoch: 97: w: 2.000, loss: 0.00000000\n",
            "epoch: 98: w: 2.000, loss: 0.00000000\n",
            "epoch: 99: w: 2.000, loss: 0.00000000\n",
            "epoch: 100: w: 2.000, loss: 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5ECplhDgKGj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}