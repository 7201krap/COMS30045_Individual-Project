{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_sort.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/mnist_sort.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7STrWa0P3z_"
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHdKug7ua8fs",
        "outputId": "1aab507c-08de-4412-a40d-9d1147a71fdc"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTW5TOUnP5XY"
      },
      "source": [
        "mnist_trainset = datasets.MNIST(root='./data', train=True, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "mnist_testset  = datasets.MNIST(root='./data', \n",
        "                                train=False, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr7XOzii6d40"
      },
      "source": [
        "new_mnist_trainset =  [ [[],[]] for i in range(10)]\n",
        "new_mnist_testset  =  [ [[],[]] for i in range(10)]\n",
        "\n",
        "for i in range(60000):\n",
        "    for j in range(10):\n",
        "        if mnist_trainset[i][1] == j:\n",
        "            # image \n",
        "            new_mnist_trainset[j][0].append(mnist_trainset[i][0])  \n",
        "            # label\n",
        "            new_mnist_trainset[j][1].append(mnist_trainset[i][1])\n",
        "\n",
        "for i in range(10000):\n",
        "    for j in range(10):\n",
        "        if mnist_testset[i][1] == j:\n",
        "            # image \n",
        "            new_mnist_testset[j][0].append(mnist_testset[i][0])  \n",
        "            # label\n",
        "            new_mnist_testset[j][1].append(mnist_testset[i][1])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wbVvijODEac"
      },
      "source": [
        "image_trainset = list()\n",
        "label_trainset = list()\n",
        "\n",
        "image_testset = list()\n",
        "label_testset = list()\n",
        "\n",
        "for i in range(10):\n",
        "    image_trainset.append(new_mnist_trainset[i][0])\n",
        "    label_trainset.append(new_mnist_trainset[i][1])\n",
        "\n",
        "for i in range(10):\n",
        "    image_testset.append(new_mnist_testset[i][0])\n",
        "    label_testset.append(new_mnist_testset[i][1])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9OvyQlGGTJN"
      },
      "source": [
        "flattened_image_train = list()\n",
        "flattened_label_train = list()\n",
        "\n",
        "flattened_image_test = list()\n",
        "flattened_label_test = list()\n",
        "\n",
        "# flattening image \n",
        "for sublist in image_trainset:\n",
        "    for val in sublist:\n",
        "        flattened_image_train.append(val)\n",
        "\n",
        "# flattening label\n",
        "for sublist in label_trainset:\n",
        "    for val in sublist:\n",
        "        flattened_label_train.append(val)\n",
        "\n",
        "# flattening image \n",
        "for sublist in image_testset:\n",
        "    for val in sublist:\n",
        "        flattened_image_test.append(val)\n",
        "\n",
        "# flattening label\n",
        "for sublist in label_testset:\n",
        "    for val in sublist:\n",
        "        flattened_label_test.append(val)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "893t0fq-Mx_D"
      },
      "source": [
        "flattened_image_train = torch.stack(flattened_image_train)\n",
        "flattened_label_train = torch.Tensor(flattened_label_train)\n",
        "flattened_label_train = flattened_label_train.type(torch.LongTensor)\n",
        "\n",
        "flattened_image_test = torch.stack(flattened_image_test)\n",
        "flattened_label_test = torch.Tensor(flattened_label_test)\n",
        "flattened_label_test = flattened_label_test.type(torch.LongTensor)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q0RanWqOUYS"
      },
      "source": [
        "train_dataset = TensorDataset(flattened_image_train, flattened_label_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=50)\n",
        "\n",
        "test_dataset = TensorDataset(flattened_image_test, flattened_label_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=50)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXTkEUJ5P6kU"
      },
      "source": [
        "# Define the model \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.linear_1 = torch.nn.Linear(784, 256)\n",
        "        self.linear_2 = torch.nn.Linear(256, 10)\n",
        "        self.sigmoid  = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        pred = self.linear_2(x)\n",
        "\n",
        "        return pred\n",
        "\n",
        "model = Model()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfgvKH6eP9Ou",
        "outputId": "b2faa36c-85da-410f-c78f-24e0d0fbb42d"
      },
      "source": [
        "def get_activation():    \n",
        "    def hook(module, input, output):\n",
        "        model.layer_activations = output\n",
        "    return hook\n",
        "    \n",
        "model.sigmoid.register_forward_hook(get_activation())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.hooks.RemovableHandle at 0x7fccb95172e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "savMT9-gP_3m"
      },
      "source": [
        "# adagrad \n",
        "optimizer  = torch.optim.Adagrad(model.parameters(), lr=0.1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXOpwTXEQFKY",
        "outputId": "2f4148b9-59f6-4a25-ac17-941375d01c93"
      },
      "source": [
        "f = open(\"sorted_mnist_adagrad.txt\", \"w\")\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "no_epochs = 200\n",
        "train_loss = list()\n",
        "test_loss  = list()\n",
        "test_acc   = list()\n",
        "final_spareness = list()\n",
        "\n",
        "# define activation list \n",
        "\n",
        "best_test_loss = 1\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    total_train_loss = 0\n",
        "    total_test_loss = 0\n",
        "\n",
        "    hidden_layer_activation_list = list()\n",
        "\n",
        "    # training\n",
        "    # set up training mode \n",
        "    model.train()\n",
        "\n",
        "    for itr, (image, label) in enumerate(train_dataloader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(image)\n",
        "\n",
        "        loss = criterion(pred, label)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print/Append activation of the hidden layer \n",
        "        # print(model.layer_activations.shape)\n",
        "        hidden_layer_activation_list.append(model.layer_activations)\n",
        "    \n",
        "    # this conains activations for all epochs \n",
        "    final_spareness.append(hidden_layer_activation_list)\n",
        "\n",
        "    total_train_loss = total_train_loss / (itr + 1)\n",
        "    train_loss.append(total_train_loss)\n",
        "\n",
        "    # testing \n",
        "    # change to evaluation mode \n",
        "    model.eval()\n",
        "    total = 0\n",
        "    for itr, (image, label) in enumerate(test_dataloader):\n",
        "\n",
        "        pred = model(image)\n",
        "\n",
        "        loss = criterion(pred, label)\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "        # we now need softmax because we are testing.\n",
        "        pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "        for i, p in enumerate(pred):\n",
        "            if label[i] == torch.max(p.data, 0)[1]:\n",
        "                total = total + 1\n",
        "\n",
        "    # caculate accuracy \n",
        "    accuracy = total / len(mnist_testset)\n",
        "\n",
        "    # append accuracy here\n",
        "    test_acc.append(accuracy)\n",
        "\n",
        "    # append test loss here \n",
        "    total_test_loss = total_test_loss / (itr + 1)\n",
        "    test_loss.append(total_test_loss)\n",
        "\n",
        "    print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Test Loss: {:.8f}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, total_train_loss, total_test_loss, accuracy))\n",
        "\n",
        "    if total_test_loss < best_test_loss:\n",
        "        best_test_loss = total_test_loss\n",
        "        print(\"Saving the model state dictionary for Epoch: {} with Test loss: {:.8f}\".format(epoch + 1, total_test_loss))\n",
        "        torch.save(model.state_dict(), \"model.dth\")\n",
        "\n",
        "for single_epoch_spareness in final_spareness:\n",
        "\n",
        "    hidden_layer_activation_list = single_epoch_spareness\n",
        "    hidden_layer_activation_list = torch.stack(hidden_layer_activation_list)\n",
        "    layer_activations_list = torch.reshape(hidden_layer_activation_list, (60000, 256))\n",
        "\n",
        "    layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
        "    num_neurons = layer_activations_list.shape[1]\n",
        "    population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
        "    mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
        "\n",
        "    f = open(\"sorted_mnist_adagrad.txt\", \"a\")\n",
        "    f.write(str(mean_sparseness_per_epoch)+'\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1/200, Train Loss: 0.12857285, Test Loss: 7.54030974, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 2/200, Train Loss: 0.09614688, Test Loss: 5.83546543, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 3/200, Train Loss: 0.05961758, Test Loss: 4.87126793, Test Accuracy: 0.10680000\n",
            "\n",
            "Epoch: 4/200, Train Loss: 0.04469364, Test Loss: 4.00259415, Test Accuracy: 0.17380000\n",
            "\n",
            "Epoch: 5/200, Train Loss: 0.03706994, Test Loss: 3.68464195, Test Accuracy: 0.23810000\n",
            "\n",
            "Epoch: 6/200, Train Loss: 0.03419592, Test Loss: 3.44937062, Test Accuracy: 0.27560000\n",
            "\n",
            "Epoch: 7/200, Train Loss: 0.03199480, Test Loss: 3.33122518, Test Accuracy: 0.31660000\n",
            "\n",
            "Epoch: 8/200, Train Loss: 0.03029283, Test Loss: 3.24316522, Test Accuracy: 0.33450000\n",
            "\n",
            "Epoch: 9/200, Train Loss: 0.03035927, Test Loss: 3.11401232, Test Accuracy: 0.35900000\n",
            "\n",
            "Epoch: 10/200, Train Loss: 0.02979577, Test Loss: 3.02490805, Test Accuracy: 0.36240000\n",
            "\n",
            "Epoch: 11/200, Train Loss: 0.02894702, Test Loss: 2.87128641, Test Accuracy: 0.37450000\n",
            "\n",
            "Epoch: 12/200, Train Loss: 0.02840261, Test Loss: 2.77943250, Test Accuracy: 0.38750000\n",
            "\n",
            "Epoch: 13/200, Train Loss: 0.02800738, Test Loss: 2.82036618, Test Accuracy: 0.39110000\n",
            "\n",
            "Epoch: 14/200, Train Loss: 0.02741626, Test Loss: 2.71905478, Test Accuracy: 0.39880000\n",
            "\n",
            "Epoch: 15/200, Train Loss: 0.02630488, Test Loss: 2.71480153, Test Accuracy: 0.41010000\n",
            "\n",
            "Epoch: 16/200, Train Loss: 0.02481582, Test Loss: 2.66726733, Test Accuracy: 0.41840000\n",
            "\n",
            "Epoch: 17/200, Train Loss: 0.02376651, Test Loss: 2.46959515, Test Accuracy: 0.44540000\n",
            "\n",
            "Epoch: 18/200, Train Loss: 0.02342469, Test Loss: 2.53808600, Test Accuracy: 0.43580000\n",
            "\n",
            "Epoch: 19/200, Train Loss: 0.02204429, Test Loss: 2.44302529, Test Accuracy: 0.44700000\n",
            "\n",
            "Epoch: 20/200, Train Loss: 0.02092179, Test Loss: 2.47149318, Test Accuracy: 0.43770000\n",
            "\n",
            "Epoch: 21/200, Train Loss: 0.02073655, Test Loss: 2.46389750, Test Accuracy: 0.46110000\n",
            "\n",
            "Epoch: 22/200, Train Loss: 0.01965076, Test Loss: 2.35670147, Test Accuracy: 0.47160000\n",
            "\n",
            "Epoch: 23/200, Train Loss: 0.01870370, Test Loss: 2.35512685, Test Accuracy: 0.48070000\n",
            "\n",
            "Epoch: 24/200, Train Loss: 0.01829916, Test Loss: 2.25232214, Test Accuracy: 0.48590000\n",
            "\n",
            "Epoch: 25/200, Train Loss: 0.01735408, Test Loss: 2.15184073, Test Accuracy: 0.48930000\n",
            "\n",
            "Epoch: 26/200, Train Loss: 0.01628401, Test Loss: 2.28917267, Test Accuracy: 0.48280000\n",
            "\n",
            "Epoch: 27/200, Train Loss: 0.01630422, Test Loss: 2.08499637, Test Accuracy: 0.51150000\n",
            "\n",
            "Epoch: 28/200, Train Loss: 0.01567850, Test Loss: 2.01604602, Test Accuracy: 0.51430000\n",
            "\n",
            "Epoch: 29/200, Train Loss: 0.01548605, Test Loss: 1.85121363, Test Accuracy: 0.54710000\n",
            "\n",
            "Epoch: 30/200, Train Loss: 0.01464745, Test Loss: 2.10446147, Test Accuracy: 0.51140000\n",
            "\n",
            "Epoch: 31/200, Train Loss: 0.01435935, Test Loss: 1.85793997, Test Accuracy: 0.53410000\n",
            "\n",
            "Epoch: 32/200, Train Loss: 0.01377520, Test Loss: 1.98443613, Test Accuracy: 0.52760000\n",
            "\n",
            "Epoch: 33/200, Train Loss: 0.01346432, Test Loss: 1.90052832, Test Accuracy: 0.54210000\n",
            "\n",
            "Epoch: 34/200, Train Loss: 0.01305478, Test Loss: 1.90535243, Test Accuracy: 0.53040000\n",
            "\n",
            "Epoch: 35/200, Train Loss: 0.01305696, Test Loss: 1.76596102, Test Accuracy: 0.55710000\n",
            "\n",
            "Epoch: 36/200, Train Loss: 0.01269360, Test Loss: 1.87416470, Test Accuracy: 0.53130000\n",
            "\n",
            "Epoch: 37/200, Train Loss: 0.01216319, Test Loss: 1.76717501, Test Accuracy: 0.55860000\n",
            "\n",
            "Epoch: 38/200, Train Loss: 0.01181884, Test Loss: 1.70074378, Test Accuracy: 0.57170000\n",
            "\n",
            "Epoch: 39/200, Train Loss: 0.01139271, Test Loss: 1.69405534, Test Accuracy: 0.57300000\n",
            "\n",
            "Epoch: 40/200, Train Loss: 0.01151972, Test Loss: 1.62011423, Test Accuracy: 0.58780000\n",
            "\n",
            "Epoch: 41/200, Train Loss: 0.01128211, Test Loss: 1.59120140, Test Accuracy: 0.59180000\n",
            "\n",
            "Epoch: 42/200, Train Loss: 0.01120109, Test Loss: 1.51629633, Test Accuracy: 0.59960000\n",
            "\n",
            "Epoch: 43/200, Train Loss: 0.01082011, Test Loss: 1.52237128, Test Accuracy: 0.60130000\n",
            "\n",
            "Epoch: 44/200, Train Loss: 0.01060213, Test Loss: 1.46634856, Test Accuracy: 0.61210000\n",
            "\n",
            "Epoch: 45/200, Train Loss: 0.01083180, Test Loss: 1.41774444, Test Accuracy: 0.61930000\n",
            "\n",
            "Epoch: 46/200, Train Loss: 0.01022302, Test Loss: 1.41452523, Test Accuracy: 0.62050000\n",
            "\n",
            "Epoch: 47/200, Train Loss: 0.01030944, Test Loss: 1.36592796, Test Accuracy: 0.62950000\n",
            "\n",
            "Epoch: 48/200, Train Loss: 0.01027058, Test Loss: 1.31232498, Test Accuracy: 0.64150000\n",
            "\n",
            "Epoch: 49/200, Train Loss: 0.01024295, Test Loss: 1.27476829, Test Accuracy: 0.64980000\n",
            "\n",
            "Epoch: 50/200, Train Loss: 0.00980825, Test Loss: 1.32234648, Test Accuracy: 0.63880000\n",
            "\n",
            "Epoch: 51/200, Train Loss: 0.00927295, Test Loss: 1.23579679, Test Accuracy: 0.65790000\n",
            "\n",
            "Epoch: 52/200, Train Loss: 0.00871693, Test Loss: 1.29094337, Test Accuracy: 0.64830000\n",
            "\n",
            "Epoch: 53/200, Train Loss: 0.00830244, Test Loss: 1.25634158, Test Accuracy: 0.65690000\n",
            "\n",
            "Epoch: 54/200, Train Loss: 0.00828347, Test Loss: 1.17688698, Test Accuracy: 0.67280000\n",
            "\n",
            "Epoch: 55/200, Train Loss: 0.00852409, Test Loss: 1.13791621, Test Accuracy: 0.68400000\n",
            "\n",
            "Epoch: 56/200, Train Loss: 0.00826888, Test Loss: 1.10782254, Test Accuracy: 0.69200000\n",
            "\n",
            "Epoch: 57/200, Train Loss: 0.00793845, Test Loss: 1.12653099, Test Accuracy: 0.68730000\n",
            "\n",
            "Epoch: 58/200, Train Loss: 0.00772433, Test Loss: 1.12090634, Test Accuracy: 0.69080000\n",
            "\n",
            "Epoch: 59/200, Train Loss: 0.00746550, Test Loss: 1.08022428, Test Accuracy: 0.70080000\n",
            "\n",
            "Epoch: 60/200, Train Loss: 0.00757360, Test Loss: 1.06431135, Test Accuracy: 0.70580000\n",
            "\n",
            "Epoch: 61/200, Train Loss: 0.00759696, Test Loss: 1.02593807, Test Accuracy: 0.71550000\n",
            "\n",
            "Epoch: 62/200, Train Loss: 0.00735668, Test Loss: 1.09664896, Test Accuracy: 0.70050000\n",
            "\n",
            "Epoch: 63/200, Train Loss: 0.00702114, Test Loss: 1.05318815, Test Accuracy: 0.70960000\n",
            "\n",
            "Epoch: 64/200, Train Loss: 0.00732332, Test Loss: 1.00589510, Test Accuracy: 0.72400000\n",
            "\n",
            "Epoch: 65/200, Train Loss: 0.00751732, Test Loss: 0.93809644, Test Accuracy: 0.73630000\n",
            "Saving the model state dictionary for Epoch: 65 with Test loss: 0.93809644\n",
            "\n",
            "Epoch: 66/200, Train Loss: 0.00748830, Test Loss: 0.93235323, Test Accuracy: 0.73740000\n",
            "Saving the model state dictionary for Epoch: 66 with Test loss: 0.93235323\n",
            "\n",
            "Epoch: 67/200, Train Loss: 0.00717335, Test Loss: 0.92555758, Test Accuracy: 0.74190000\n",
            "Saving the model state dictionary for Epoch: 67 with Test loss: 0.92555758\n",
            "\n",
            "Epoch: 68/200, Train Loss: 0.00701365, Test Loss: 0.91279627, Test Accuracy: 0.74600000\n",
            "Saving the model state dictionary for Epoch: 68 with Test loss: 0.91279627\n",
            "\n",
            "Epoch: 69/200, Train Loss: 0.00725321, Test Loss: 0.89238344, Test Accuracy: 0.75080000\n",
            "Saving the model state dictionary for Epoch: 69 with Test loss: 0.89238344\n",
            "\n",
            "Epoch: 70/200, Train Loss: 0.00716204, Test Loss: 0.88098504, Test Accuracy: 0.75270000\n",
            "Saving the model state dictionary for Epoch: 70 with Test loss: 0.88098504\n",
            "\n",
            "Epoch: 71/200, Train Loss: 0.00682800, Test Loss: 0.84677397, Test Accuracy: 0.76270000\n",
            "Saving the model state dictionary for Epoch: 71 with Test loss: 0.84677397\n",
            "\n",
            "Epoch: 72/200, Train Loss: 0.00670151, Test Loss: 0.86969438, Test Accuracy: 0.75760000\n",
            "\n",
            "Epoch: 73/200, Train Loss: 0.00650272, Test Loss: 0.88935822, Test Accuracy: 0.75410000\n",
            "\n",
            "Epoch: 74/200, Train Loss: 0.00645608, Test Loss: 0.82697428, Test Accuracy: 0.76920000\n",
            "Saving the model state dictionary for Epoch: 74 with Test loss: 0.82697428\n",
            "\n",
            "Epoch: 75/200, Train Loss: 0.00618008, Test Loss: 0.79739052, Test Accuracy: 0.77710000\n",
            "Saving the model state dictionary for Epoch: 75 with Test loss: 0.79739052\n",
            "\n",
            "Epoch: 76/200, Train Loss: 0.00642032, Test Loss: 0.77996548, Test Accuracy: 0.78340000\n",
            "Saving the model state dictionary for Epoch: 76 with Test loss: 0.77996548\n",
            "\n",
            "Epoch: 77/200, Train Loss: 0.00640241, Test Loss: 0.79502314, Test Accuracy: 0.78020000\n",
            "\n",
            "Epoch: 78/200, Train Loss: 0.00639097, Test Loss: 0.79609294, Test Accuracy: 0.77950000\n",
            "\n",
            "Epoch: 79/200, Train Loss: 0.00616044, Test Loss: 0.75568250, Test Accuracy: 0.79190000\n",
            "Saving the model state dictionary for Epoch: 79 with Test loss: 0.75568250\n",
            "\n",
            "Epoch: 80/200, Train Loss: 0.00616803, Test Loss: 0.71749689, Test Accuracy: 0.80130000\n",
            "Saving the model state dictionary for Epoch: 80 with Test loss: 0.71749689\n",
            "\n",
            "Epoch: 81/200, Train Loss: 0.00610511, Test Loss: 0.69605030, Test Accuracy: 0.80700000\n",
            "Saving the model state dictionary for Epoch: 81 with Test loss: 0.69605030\n",
            "\n",
            "Epoch: 82/200, Train Loss: 0.00608613, Test Loss: 0.68081819, Test Accuracy: 0.81070000\n",
            "Saving the model state dictionary for Epoch: 82 with Test loss: 0.68081819\n",
            "\n",
            "Epoch: 83/200, Train Loss: 0.00591853, Test Loss: 0.68579712, Test Accuracy: 0.81110000\n",
            "\n",
            "Epoch: 84/200, Train Loss: 0.00558272, Test Loss: 0.67498200, Test Accuracy: 0.81120000\n",
            "Saving the model state dictionary for Epoch: 84 with Test loss: 0.67498200\n",
            "\n",
            "Epoch: 85/200, Train Loss: 0.00540064, Test Loss: 0.64169687, Test Accuracy: 0.82250000\n",
            "Saving the model state dictionary for Epoch: 85 with Test loss: 0.64169687\n",
            "\n",
            "Epoch: 86/200, Train Loss: 0.00563442, Test Loss: 0.63832401, Test Accuracy: 0.82130000\n",
            "Saving the model state dictionary for Epoch: 86 with Test loss: 0.63832401\n",
            "\n",
            "Epoch: 87/200, Train Loss: 0.00543005, Test Loss: 0.64064521, Test Accuracy: 0.82160000\n",
            "\n",
            "Epoch: 88/200, Train Loss: 0.00545956, Test Loss: 0.61607143, Test Accuracy: 0.82720000\n",
            "Saving the model state dictionary for Epoch: 88 with Test loss: 0.61607143\n",
            "\n",
            "Epoch: 89/200, Train Loss: 0.00551426, Test Loss: 0.62416877, Test Accuracy: 0.82490000\n",
            "\n",
            "Epoch: 90/200, Train Loss: 0.00547508, Test Loss: 0.59751811, Test Accuracy: 0.83220000\n",
            "Saving the model state dictionary for Epoch: 90 with Test loss: 0.59751811\n",
            "\n",
            "Epoch: 91/200, Train Loss: 0.00519057, Test Loss: 0.59880938, Test Accuracy: 0.83180000\n",
            "\n",
            "Epoch: 92/200, Train Loss: 0.00521999, Test Loss: 0.59577354, Test Accuracy: 0.83310000\n",
            "Saving the model state dictionary for Epoch: 92 with Test loss: 0.59577354\n",
            "\n",
            "Epoch: 93/200, Train Loss: 0.00488131, Test Loss: 0.61889106, Test Accuracy: 0.82770000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUgkscNqb0Z3"
      },
      "source": [
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygc9ge4OcVwH"
      },
      "source": [
        "!cp sorted_mnist_adagrad.txt /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7vqWo82Jm9c"
      },
      "source": [
        "fig=plt.figure(figsize=(20, 10))\n",
        "plt.plot(np.arange(1, no_epochs+1), sparseness_list, label=\"Sparseness\", color='g')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Sparsity')\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Sparsity Plot\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACiOqR2mMbUX"
      },
      "source": [
        "fig=plt.figure(figsize=(20, 10))\n",
        "plt.plot(np.arange(1, no_epochs+1), test_acc, label=\"Adagrad Test Accuracy\", color='g')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Accuracy Plot\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYXSOIXIVu2F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}