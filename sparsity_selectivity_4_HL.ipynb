{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparsity_selectivity_4_HL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/sparsity_selectivity_4_HL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7STrWa0P3z_",
        "outputId": "097e0a8e-9c35-4370-d8ab-90a3c787259c"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf9vduGvh3Q_",
        "outputId": "c633ac23-2863-41e4-b2ea-d0a353f7a52a"
      },
      "source": [
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "root_dir = './'\n",
        "torchvision.datasets.MNIST(root=root_dir,download=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-16 01:00:51--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
            "--2021-03-16 01:00:51--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘MNIST.tar.gz’\n",
            "\n",
            "MNIST.tar.gz            [            <=>     ]  33.20M  6.42MB/s    in 15s     \n",
            "\n",
            "2021-03-16 01:01:06 (2.28 MB/s) - ‘MNIST.tar.gz’ saved [34813078]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte\n",
            "MNIST/raw/train-images-idx3-ubyte.gz\n",
            "MNIST/processed/\n",
            "MNIST/processed/training.pt\n",
            "MNIST/processed/test.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./\n",
              "    Split: Train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4j9WoP-UnAm"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApOU7hvb95W4"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTW5TOUnP5XY"
      },
      "source": [
        "mnist_trainset = torchvision.datasets.MNIST(root=root_dir, train=True, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "mnist_testset  = torchvision.datasets.MNIST(root=root_dir, \n",
        "                                train=False, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(mnist_trainset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=True)\n",
        "\n",
        "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXTkEUJ5P6kU"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# Define the model \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # modify this section for later use \n",
        "        self.linear_1 = torch.nn.Linear(784, 256)\n",
        "        self.linear_2 = torch.nn.Linear(256, 256)\n",
        "        self.linear_3 = torch.nn.Linear(256, 256)\n",
        "        self.linear_4 = torch.nn.Linear(256, 256)\n",
        "        self.linear_5 = torch.nn.Linear(256, 10)\n",
        "        self.sigmoid12  = torch.nn.Sigmoid()\n",
        "        self.sigmoid23  = torch.nn.Sigmoid()\n",
        "        self.sigmoid34  = torch.nn.Sigmoid()\n",
        "        self.sigmoid45  = torch.nn.Sigmoid()\n",
        "\n",
        "        self.layer_activations = dict()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # modify this section for later use \n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.sigmoid12(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.sigmoid23(x)\n",
        "        x = self.linear_3(x)\n",
        "        x = self.sigmoid34(x)\n",
        "        x = self.linear_4(x)\n",
        "        x = self.sigmoid45(x)\n",
        "        pred = self.linear_5(x)\n",
        "        return pred\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfgvKH6eP9Ou"
      },
      "source": [
        "def get_activation(model, layer_name):    \n",
        "    def hook(module, input, output):\n",
        "        model.layer_activations[layer_name] = output\n",
        "    return hook"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gykz9nwHiQua"
      },
      "source": [
        "def sparsity_calculator(final_spareness):\n",
        "    sparseness_list = list()\n",
        "    for single_epoch_spareness in final_spareness:\n",
        "\n",
        "        hidden_layer_activation_list = single_epoch_spareness\n",
        "        hidden_layer_activation_list = torch.stack(hidden_layer_activation_list)\n",
        "        layer_activations_list = torch.reshape(hidden_layer_activation_list, (10000, 256))\n",
        "\n",
        "        layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
        "        num_neurons = layer_activations_list.shape[1]\n",
        "        population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
        "        mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
        "\n",
        "        sparseness_list.append(mean_sparseness_per_epoch)\n",
        "\n",
        "    return sparseness_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvHGO5RSvi6I"
      },
      "source": [
        "def selectivity(hidden_layer_each_neuron):\n",
        "    __selectivity__ = list()\n",
        "    # I will now try to find the average of each class for each neuron.\n",
        "    # check out the next cell \n",
        "    avg_activations = [dict() for x in range(256)]\n",
        "    for i, neuron in enumerate(hidden_layer_each_neuron):\n",
        "        for k, v in neuron.items():\n",
        "            # v is the list of activations for hidden layer's neuron k \n",
        "            avg_activations[i][k] = sum(v) / float(len(v))\n",
        "\n",
        "    # generate 256 lists to get only values in avg_activations\n",
        "    only_activation_vals = [list() for x in range(256)]\n",
        "\n",
        "    # get only values from avg_activations\n",
        "    for i, avg_activation in enumerate(avg_activations):\n",
        "        for value in avg_activation.values():\n",
        "            only_activation_vals[i].append(value)\n",
        "\n",
        "    for activation_val in only_activation_vals:\n",
        "        # find u_max \n",
        "        u_max = np.max(activation_val)\n",
        "\n",
        "        # find u_minus_max \n",
        "        u_minus_max = (np.sum(activation_val) - u_max) / 9\n",
        "\n",
        "        # find selectivity \n",
        "        selectivity = (u_max - u_minus_max) / (u_max + u_minus_max)\n",
        "\n",
        "        # append selectivity value to selectivity\n",
        "        __selectivity__.append(selectivity)\n",
        "\n",
        "    avg_selectivity = np.average(__selectivity__)\n",
        "    std_selectivity = np.std(__selectivity__)\n",
        "                                 \n",
        "    return avg_selectivity, std_selectivity"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4q4e0IBiUd4"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# add a parameter to the function and calculate avg and std. Do not forget to change division by 2, 3, 4, or 5 \n",
        "def avg_std_calculator(_hidden_layer_each_neuron_12, _hidden_layer_each_neuron_23, _hidden_layer_each_neuron_34, _hidden_layer_each_neuron_45):\n",
        "\n",
        "    avg_selectivity12, std_selectivity12 = selectivity(_hidden_layer_each_neuron_12)\n",
        "    avg_selectivity23, std_selectivity23 = selectivity(_hidden_layer_each_neuron_23)\n",
        "    avg_selectivity34, std_selectivity34 = selectivity(_hidden_layer_each_neuron_34)\n",
        "    avg_selectivity45, std_selectivity45 = selectivity(_hidden_layer_each_neuron_45)\n",
        "\n",
        "    final_selectivity_avg = (avg_selectivity12 + avg_selectivity23 + avg_selectivity34 + avg_selectivity45) / 4\n",
        "    final_selecvitity_std = (std_selectivity12 + std_selectivity23 + std_selectivity34 + std_selectivity45) / 4\n",
        "\n",
        "    return final_selectivity_avg, final_selecvitity_std\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5PUiBNqUImf"
      },
      "source": [
        "def model_factory(optimizer_name):\n",
        "    '''\n",
        "    optimizer_name : choose one of Adagrad, Adadelta, SGD, and Adam \n",
        "\n",
        "    '''\n",
        "    my_model = Model()\n",
        "    print(\"my_model:\", my_model)\n",
        "    my_model.to(device)\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    # chagen sigmoid34 an 's34'\n",
        "    my_model.sigmoid12.register_forward_hook(get_activation(my_model, 's12'))\n",
        "    my_model.sigmoid23.register_forward_hook(get_activation(my_model, 's23'))\n",
        "    my_model.sigmoid34.register_forward_hook(get_activation(my_model, 's34'))\n",
        "    my_model.sigmoid45.register_forward_hook(get_activation(my_model, 's45'))\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        my_optimizer = torch.optim.Adadelta(my_model.parameters(), lr=1.0)\n",
        "\n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        my_optimizer = torch.optim.Adagrad(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        my_optimizer = torch.optim.SGD(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        my_optimizer = torch.optim.Adam(my_model.parameters(), lr=0.001)\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")\n",
        "    \n",
        "    print(\"my_optimizer:\", my_optimizer)\n",
        "    test_acc, sparsity_avg, selectivity_list_avg, selectivity_list_std = selectivity_trainer(optimizer=my_optimizer, model=my_model)\n",
        "    # ************* modify this section for later use *************\n",
        "    # change name of the file \n",
        "    file_saver = open(f\"4HL_selectivity_sparsity_{optimizer_name}.txt\", \"w\")\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver.write(str(test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(selectivity_list_avg)+'\\n'+str(selectivity_list_std)+'\\n\\n')\n",
        "    file_saver.close()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        !cp 4HL_selectivity_sparsity_Adadelta.txt /content/drive/MyDrive\n",
        "    \n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        !cp 4HL_selectivity_sparsity_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        !cp 4HL_selectivity_sparsity_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        !cp 4HL_selectivity_sparsity_Adam.txt /content/drive/MyDrive\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXOpwTXEQFKY"
      },
      "source": [
        "no_epochs = 50\n",
        "def selectivity_trainer(optimizer, model):\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    train_loss = list()\n",
        "    test_loss  = list()\n",
        "    test_acc   = list()\n",
        "\n",
        "    best_test_loss = 1\n",
        "\n",
        "    selectivity_avg_list = list()\n",
        "    selectivity_std_list = list()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    final_spareness_12 = list()\n",
        "    final_spareness_23 = list()\n",
        "    final_spareness_34 = list()\n",
        "    final_spareness_45 = list()\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    for epoch in range(no_epochs):\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_activation_list_12 = list()\n",
        "        hidden_layer_activation_list_23 = list()\n",
        "        hidden_layer_activation_list_34 = list()\n",
        "        hidden_layer_activation_list_45 = list()\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_each_neuron_12 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_12 = np.array(hidden_layer_each_neuron_12)\n",
        "\n",
        "        hidden_layer_each_neuron_23 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_23 = np.array(hidden_layer_each_neuron_23)\n",
        "\n",
        "        hidden_layer_each_neuron_34 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_34 = np.array(hidden_layer_each_neuron_34)\n",
        "\n",
        "        hidden_layer_each_neuron_45 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_45 = np.array(hidden_layer_each_neuron_45)\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "\n",
        "        total_train_loss = 0\n",
        "        total_test_loss = 0\n",
        "\n",
        "        # training\n",
        "        # set up training mode \n",
        "        model.train()\n",
        "\n",
        "        for itr, (images, labels) in enumerate(train_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_train_loss = total_train_loss / (itr + 1)\n",
        "        train_loss.append(total_train_loss)\n",
        "\n",
        "        # testing \n",
        "        # change to evaluation mode \n",
        "        model.eval()\n",
        "        total = 0\n",
        "        for itr, (images, labels) in enumerate(test_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # we now need softmax because we are testing.\n",
        "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "            for i, p in enumerate(pred):\n",
        "                if labels[i] == torch.max(p.data, 0)[1]:\n",
        "                    total = total + 1\n",
        "\n",
        "            # ***************** sparsity calculation ***************** #\n",
        "            hidden_layer_activation_list_12.append(model.layer_activations['s12'])\n",
        "            hidden_layer_activation_list_23.append(model.layer_activations['s23'])\n",
        "            hidden_layer_activation_list_34.append(model.layer_activations['s34'])\n",
        "            hidden_layer_activation_list_45.append(model.layer_activations['s45'])\n",
        "\n",
        "            # ************* modify this section for later use *************\n",
        "            # Do not forget to change hidden_layer_each_neuron_12 name \n",
        "            for activation, label in zip(model.layer_activations['s12'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_12[i][label].append(activation[i])\n",
        "\n",
        "            for activation, label in zip(model.layer_activations['s23'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_23[i][label].append(activation[i])\n",
        "            \n",
        "            for activation, label in zip(model.layer_activations['s34'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_34[i][label].append(activation[i])\n",
        "\n",
        "            for activation, label in zip(model.layer_activations['s45'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_45[i][label].append(activation[i])\n",
        "\n",
        "        # Add one more parameter \n",
        "        selectivity_avg, selecvitity_std = avg_std_calculator(hidden_layer_each_neuron_12, hidden_layer_each_neuron_23, hidden_layer_each_neuron_34, hidden_layer_each_neuron_45)\n",
        "        # ************* modify this section for later use *************\n",
        "        \n",
        "        selectivity_avg_list.append(selectivity_avg)\n",
        "        selectivity_std_list.append(selecvitity_std)\n",
        "\n",
        "        final_spareness_12.append(hidden_layer_activation_list_12)\n",
        "        final_spareness_23.append(hidden_layer_activation_list_23)\n",
        "        final_spareness_34.append(hidden_layer_activation_list_34)\n",
        "        final_spareness_45.append(hidden_layer_activation_list_45)\n",
        "        # ***************** sparsity calculation ***************** #\n",
        "\n",
        "        # caculate accuracy \n",
        "        accuracy = total / len(mnist_testset)\n",
        "\n",
        "        # append accuracy here\n",
        "        test_acc.append(accuracy)\n",
        "\n",
        "        # append test loss here \n",
        "        total_test_loss = total_test_loss / (itr + 1)\n",
        "        test_loss.append(total_test_loss)\n",
        "\n",
        "        print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Test Loss: {:.8f}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, total_train_loss, total_test_loss, accuracy))\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "    sparsity_list12 = sparsity_calculator(final_spareness_12)\n",
        "    sparsity_list23 = sparsity_calculator(final_spareness_23)\n",
        "    sparsity_list34 = sparsity_calculator(final_spareness_34)\n",
        "    sparsity_list45 = sparsity_calculator(final_spareness_45)\n",
        "\n",
        "    print(sparsity_list12)\n",
        "    print(sparsity_list23)\n",
        "    print(sparsity_list34)\n",
        "    print(sparsity_list45)\n",
        "\n",
        "    average_sparsity = list()\n",
        "    for i in range(no_epochs):\n",
        "        average_sparsity.append( (sparsity_list12[i].item() + sparsity_list23[i].item() + sparsity_list34[i].item() + sparsity_list45[i].item()) / 4 )\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "\n",
        "    print(\"average_sparsity:\", average_sparsity)\n",
        "\n",
        "    return test_acc, average_sparsity, selectivity_avg_list, selectivity_std_list"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILIJTJb2UdfI"
      },
      "source": [
        "# Adadelta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UH0qDnFUfaD",
        "outputId": "31c354cf-853b-45c3-97dd-1be2eb57d433"
      },
      "source": [
        "model_factory('Adadelta')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-06\n",
            "    lr: 1.0\n",
            "    rho: 0.9\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.30746806, Test Loss: 2.30202827, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30127321, Test Loss: 2.29988618, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.18322148, Test Loss: 1.78346998, Test Accuracy: 0.35970000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 1.38807317, Test Loss: 1.05196509, Test Accuracy: 0.58940000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.73925288, Test Loss: 0.40720915, Test Accuracy: 0.89020000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.35055134, Test Loss: 0.30235354, Test Accuracy: 0.91540000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.22971163, Test Loss: 0.18080255, Test Accuracy: 0.94850000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.16830976, Test Loss: 0.16070983, Test Accuracy: 0.95200000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.13468076, Test Loss: 0.14689419, Test Accuracy: 0.95620000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.11125832, Test Loss: 0.13769227, Test Accuracy: 0.96050000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.09575471, Test Loss: 0.12158668, Test Accuracy: 0.96510000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.08345701, Test Loss: 0.10658812, Test Accuracy: 0.96770000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.07242749, Test Loss: 0.10226166, Test Accuracy: 0.97110000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.06346309, Test Loss: 0.12266047, Test Accuracy: 0.96430000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.05714226, Test Loss: 0.11184846, Test Accuracy: 0.97000000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.04942244, Test Loss: 0.09444458, Test Accuracy: 0.97360000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.04504926, Test Loss: 0.08744071, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.03980238, Test Loss: 0.09039968, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.03666457, Test Loss: 0.09473740, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.03226412, Test Loss: 0.08644524, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.02791062, Test Loss: 0.09336870, Test Accuracy: 0.97580000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.02439865, Test Loss: 0.10836605, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.02249776, Test Loss: 0.09321603, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.01903939, Test Loss: 0.09567913, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.01696302, Test Loss: 0.09537915, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.01522317, Test Loss: 0.09634081, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.01188203, Test Loss: 0.10121603, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.01161051, Test Loss: 0.09870636, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00893058, Test Loss: 0.09959495, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00833516, Test Loss: 0.12736073, Test Accuracy: 0.97410000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00774268, Test Loss: 0.11226941, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00714412, Test Loss: 0.10984368, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00553139, Test Loss: 0.11530146, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00521357, Test Loss: 0.10872274, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00435523, Test Loss: 0.11609443, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00414847, Test Loss: 0.11266749, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00290722, Test Loss: 0.11466608, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00279187, Test Loss: 0.11495049, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00241140, Test Loss: 0.11872076, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00187569, Test Loss: 0.12077873, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00167831, Test Loss: 0.12387241, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00149886, Test Loss: 0.13416952, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00119602, Test Loss: 0.12152271, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00110594, Test Loss: 0.12464444, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00093754, Test Loss: 0.12232344, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00115545, Test Loss: 0.12480746, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00093265, Test Loss: 0.13523230, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00087843, Test Loss: 0.12731616, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00091637, Test Loss: 0.12694788, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00066378, Test Loss: 0.12831847, Test Accuracy: 0.97920000\n",
            "[tensor(0.0054, grad_fn=<MeanBackward0>), tensor(0.0078, grad_fn=<MeanBackward0>), tensor(0.0597, grad_fn=<MeanBackward0>), tensor(0.2470, grad_fn=<MeanBackward0>), tensor(0.3866, grad_fn=<MeanBackward0>), tensor(0.4355, grad_fn=<MeanBackward0>), tensor(0.4512, grad_fn=<MeanBackward0>), tensor(0.4639, grad_fn=<MeanBackward0>), tensor(0.4669, grad_fn=<MeanBackward0>), tensor(0.4713, grad_fn=<MeanBackward0>), tensor(0.4783, grad_fn=<MeanBackward0>), tensor(0.4782, grad_fn=<MeanBackward0>), tensor(0.4864, grad_fn=<MeanBackward0>), tensor(0.4855, grad_fn=<MeanBackward0>), tensor(0.4901, grad_fn=<MeanBackward0>), tensor(0.4931, grad_fn=<MeanBackward0>), tensor(0.4926, grad_fn=<MeanBackward0>), tensor(0.4947, grad_fn=<MeanBackward0>), tensor(0.4964, grad_fn=<MeanBackward0>), tensor(0.4953, grad_fn=<MeanBackward0>), tensor(0.4937, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.4984, grad_fn=<MeanBackward0>), tensor(0.4980, grad_fn=<MeanBackward0>), tensor(0.4993, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.5000, grad_fn=<MeanBackward0>), tensor(0.5003, grad_fn=<MeanBackward0>), tensor(0.4990, grad_fn=<MeanBackward0>), tensor(0.5004, grad_fn=<MeanBackward0>), tensor(0.4991, grad_fn=<MeanBackward0>), tensor(0.4988, grad_fn=<MeanBackward0>), tensor(0.4986, grad_fn=<MeanBackward0>), tensor(0.5010, grad_fn=<MeanBackward0>), tensor(0.5001, grad_fn=<MeanBackward0>), tensor(0.4996, grad_fn=<MeanBackward0>), tensor(0.4997, grad_fn=<MeanBackward0>), tensor(0.5002, grad_fn=<MeanBackward0>), tensor(0.4990, grad_fn=<MeanBackward0>), tensor(0.4987, grad_fn=<MeanBackward0>), tensor(0.4990, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.4974, grad_fn=<MeanBackward0>), tensor(0.4971, grad_fn=<MeanBackward0>), tensor(0.4963, grad_fn=<MeanBackward0>), tensor(0.4968, grad_fn=<MeanBackward0>), tensor(0.4954, grad_fn=<MeanBackward0>), tensor(0.4957, grad_fn=<MeanBackward0>), tensor(0.4947, grad_fn=<MeanBackward0>), tensor(0.4959, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0556, grad_fn=<MeanBackward0>), tensor(0.1241, grad_fn=<MeanBackward0>), tensor(0.1722, grad_fn=<MeanBackward0>), tensor(0.2126, grad_fn=<MeanBackward0>), tensor(0.2321, grad_fn=<MeanBackward0>), tensor(0.2425, grad_fn=<MeanBackward0>), tensor(0.2504, grad_fn=<MeanBackward0>), tensor(0.2448, grad_fn=<MeanBackward0>), tensor(0.2561, grad_fn=<MeanBackward0>), tensor(0.2542, grad_fn=<MeanBackward0>), tensor(0.2559, grad_fn=<MeanBackward0>), tensor(0.2572, grad_fn=<MeanBackward0>), tensor(0.2665, grad_fn=<MeanBackward0>), tensor(0.2631, grad_fn=<MeanBackward0>), tensor(0.2645, grad_fn=<MeanBackward0>), tensor(0.2626, grad_fn=<MeanBackward0>), tensor(0.2641, grad_fn=<MeanBackward0>), tensor(0.2623, grad_fn=<MeanBackward0>), tensor(0.2689, grad_fn=<MeanBackward0>), tensor(0.2643, grad_fn=<MeanBackward0>), tensor(0.2626, grad_fn=<MeanBackward0>), tensor(0.2623, grad_fn=<MeanBackward0>), tensor(0.2604, grad_fn=<MeanBackward0>), tensor(0.2614, grad_fn=<MeanBackward0>), tensor(0.2594, grad_fn=<MeanBackward0>), tensor(0.2573, grad_fn=<MeanBackward0>), tensor(0.2528, grad_fn=<MeanBackward0>), tensor(0.2539, grad_fn=<MeanBackward0>), tensor(0.2568, grad_fn=<MeanBackward0>), tensor(0.2515, grad_fn=<MeanBackward0>), tensor(0.2485, grad_fn=<MeanBackward0>), tensor(0.2485, grad_fn=<MeanBackward0>), tensor(0.2487, grad_fn=<MeanBackward0>), tensor(0.2503, grad_fn=<MeanBackward0>), tensor(0.2485, grad_fn=<MeanBackward0>), tensor(0.2459, grad_fn=<MeanBackward0>), tensor(0.2466, grad_fn=<MeanBackward0>), tensor(0.2423, grad_fn=<MeanBackward0>), tensor(0.2456, grad_fn=<MeanBackward0>), tensor(0.2442, grad_fn=<MeanBackward0>), tensor(0.2417, grad_fn=<MeanBackward0>), tensor(0.2400, grad_fn=<MeanBackward0>), tensor(0.2407, grad_fn=<MeanBackward0>), tensor(0.2397, grad_fn=<MeanBackward0>), tensor(0.2380, grad_fn=<MeanBackward0>), tensor(0.2379, grad_fn=<MeanBackward0>), tensor(0.2387, grad_fn=<MeanBackward0>), tensor(0.2369, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0097, grad_fn=<MeanBackward0>), tensor(0.0070, grad_fn=<MeanBackward0>), tensor(0.0459, grad_fn=<MeanBackward0>), tensor(0.2062, grad_fn=<MeanBackward0>), tensor(0.2501, grad_fn=<MeanBackward0>), tensor(0.2244, grad_fn=<MeanBackward0>), tensor(0.2312, grad_fn=<MeanBackward0>), tensor(0.2474, grad_fn=<MeanBackward0>), tensor(0.2480, grad_fn=<MeanBackward0>), tensor(0.2218, grad_fn=<MeanBackward0>), tensor(0.2491, grad_fn=<MeanBackward0>), tensor(0.2435, grad_fn=<MeanBackward0>), tensor(0.2594, grad_fn=<MeanBackward0>), tensor(0.2383, grad_fn=<MeanBackward0>), tensor(0.2686, grad_fn=<MeanBackward0>), tensor(0.2571, grad_fn=<MeanBackward0>), tensor(0.2511, grad_fn=<MeanBackward0>), tensor(0.2618, grad_fn=<MeanBackward0>), tensor(0.2489, grad_fn=<MeanBackward0>), tensor(0.2520, grad_fn=<MeanBackward0>), tensor(0.2580, grad_fn=<MeanBackward0>), tensor(0.2727, grad_fn=<MeanBackward0>), tensor(0.2611, grad_fn=<MeanBackward0>), tensor(0.2641, grad_fn=<MeanBackward0>), tensor(0.2590, grad_fn=<MeanBackward0>), tensor(0.2691, grad_fn=<MeanBackward0>), tensor(0.2755, grad_fn=<MeanBackward0>), tensor(0.2791, grad_fn=<MeanBackward0>), tensor(0.2784, grad_fn=<MeanBackward0>), tensor(0.2960, grad_fn=<MeanBackward0>), tensor(0.2906, grad_fn=<MeanBackward0>), tensor(0.2764, grad_fn=<MeanBackward0>), tensor(0.2752, grad_fn=<MeanBackward0>), tensor(0.2889, grad_fn=<MeanBackward0>), tensor(0.2943, grad_fn=<MeanBackward0>), tensor(0.2964, grad_fn=<MeanBackward0>), tensor(0.2951, grad_fn=<MeanBackward0>), tensor(0.2958, grad_fn=<MeanBackward0>), tensor(0.2987, grad_fn=<MeanBackward0>), tensor(0.2933, grad_fn=<MeanBackward0>), tensor(0.3024, grad_fn=<MeanBackward0>), tensor(0.3060, grad_fn=<MeanBackward0>), tensor(0.2993, grad_fn=<MeanBackward0>), tensor(0.2968, grad_fn=<MeanBackward0>), tensor(0.2998, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3022, grad_fn=<MeanBackward0>), tensor(0.3024, grad_fn=<MeanBackward0>), tensor(0.3048, grad_fn=<MeanBackward0>), tensor(0.3009, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0020, grad_fn=<MeanBackward0>), tensor(0.0044, grad_fn=<MeanBackward0>), tensor(0.4402, grad_fn=<MeanBackward0>), tensor(0.5325, grad_fn=<MeanBackward0>), tensor(0.5191, grad_fn=<MeanBackward0>), tensor(0.5348, grad_fn=<MeanBackward0>), tensor(0.5508, grad_fn=<MeanBackward0>), tensor(0.5308, grad_fn=<MeanBackward0>), tensor(0.5373, grad_fn=<MeanBackward0>), tensor(0.5763, grad_fn=<MeanBackward0>), tensor(0.5497, grad_fn=<MeanBackward0>), tensor(0.5700, grad_fn=<MeanBackward0>), tensor(0.5578, grad_fn=<MeanBackward0>), tensor(0.5734, grad_fn=<MeanBackward0>), tensor(0.5504, grad_fn=<MeanBackward0>), tensor(0.5682, grad_fn=<MeanBackward0>), tensor(0.5699, grad_fn=<MeanBackward0>), tensor(0.5607, grad_fn=<MeanBackward0>), tensor(0.5811, grad_fn=<MeanBackward0>), tensor(0.5779, grad_fn=<MeanBackward0>), tensor(0.5733, grad_fn=<MeanBackward0>), tensor(0.5605, grad_fn=<MeanBackward0>), tensor(0.5779, grad_fn=<MeanBackward0>), tensor(0.5730, grad_fn=<MeanBackward0>), tensor(0.5815, grad_fn=<MeanBackward0>), tensor(0.5695, grad_fn=<MeanBackward0>), tensor(0.5699, grad_fn=<MeanBackward0>), tensor(0.5683, grad_fn=<MeanBackward0>), tensor(0.5717, grad_fn=<MeanBackward0>), tensor(0.5505, grad_fn=<MeanBackward0>), tensor(0.5592, grad_fn=<MeanBackward0>), tensor(0.5703, grad_fn=<MeanBackward0>), tensor(0.5780, grad_fn=<MeanBackward0>), tensor(0.5608, grad_fn=<MeanBackward0>), tensor(0.5567, grad_fn=<MeanBackward0>), tensor(0.5532, grad_fn=<MeanBackward0>), tensor(0.5547, grad_fn=<MeanBackward0>), tensor(0.5561, grad_fn=<MeanBackward0>), tensor(0.5495, grad_fn=<MeanBackward0>), tensor(0.5578, grad_fn=<MeanBackward0>), tensor(0.5470, grad_fn=<MeanBackward0>), tensor(0.5454, grad_fn=<MeanBackward0>), tensor(0.5532, grad_fn=<MeanBackward0>), tensor(0.5559, grad_fn=<MeanBackward0>), tensor(0.5532, grad_fn=<MeanBackward0>), tensor(0.5520, grad_fn=<MeanBackward0>), tensor(0.5530, grad_fn=<MeanBackward0>), tensor(0.5525, grad_fn=<MeanBackward0>), tensor(0.5488, grad_fn=<MeanBackward0>), tensor(0.5552, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.007432477781549096, 0.007616111310198903, 0.15035023167729378, 0.27743260003626347, 0.33196355402469635, 0.3518451191484928, 0.3663039058446884, 0.3711462840437889, 0.3756405636668205, 0.37856297194957733, 0.38328251242637634, 0.38648318499326706, 0.38989969342947006, 0.3886025659739971, 0.3938790634274483, 0.3953825682401657, 0.3945397213101387, 0.39492326974868774, 0.3976452127099037, 0.39690595865249634, 0.3985053300857544, 0.39890460669994354, 0.39997973293066025, 0.39935216307640076, 0.4000226557254791, 0.3995169624686241, 0.40120695531368256, 0.40125924348831177, 0.400505967438221, 0.40019381791353226, 0.40144745260477066, 0.39925579726696014, 0.40008214116096497, 0.39980510622262955, 0.3999709002673626, 0.39988719671964645, 0.3994910307228565, 0.3994886018335819, 0.3984619006514549, 0.39804790914058685, 0.39850206300616264, 0.39840542525053024, 0.39792224764823914, 0.3974546156823635, 0.3975033834576607, 0.39780454710125923, 0.3971572630107403, 0.39710136502981186, 0.39673660323023796, 0.3972465954720974]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hXfQe4vMDKB"
      },
      "source": [
        "# AdaGrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb-4TPM5MGuE",
        "outputId": "819475d1-8876-4878-d8b3-c5ada14620b7"
      },
      "source": [
        "model_factory('Adagrad')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adagrad (\n",
            "Parameter Group 0\n",
            "    eps: 1e-10\n",
            "    initial_accumulator_value: 0\n",
            "    lr: 0.1\n",
            "    lr_decay: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.41819022, Test Loss: 2.32539536, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.32925108, Test Loss: 2.33123923, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.32166392, Test Loss: 2.32314516, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.31882394, Test Loss: 2.32085734, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.31751979, Test Loss: 2.32146750, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.31592446, Test Loss: 2.31645982, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.31481103, Test Loss: 2.31098961, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.31389957, Test Loss: 2.31366414, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.31191634, Test Loss: 2.31590306, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.31286573, Test Loss: 2.30808157, Test Accuracy: 0.08920000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.31184440, Test Loss: 2.32162372, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.31107149, Test Loss: 2.31959062, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.31080975, Test Loss: 2.32261588, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.31069644, Test Loss: 2.31980920, Test Accuracy: 0.08920000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.30973258, Test Loss: 2.30672049, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.30907274, Test Loss: 2.30789469, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.30989215, Test Loss: 2.31089048, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.06967501, Test Loss: 1.79579359, Test Accuracy: 0.30290000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 1.36391798, Test Loss: 0.68855165, Test Accuracy: 0.76890000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.30257350, Test Loss: 0.17812443, Test Accuracy: 0.95210000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.12702743, Test Loss: 0.12733839, Test Accuracy: 0.96650000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.08497407, Test Loss: 0.12222327, Test Accuracy: 0.96680000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.06324314, Test Loss: 0.10979649, Test Accuracy: 0.97100000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.04882699, Test Loss: 0.11148373, Test Accuracy: 0.97320000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.03828752, Test Loss: 0.10659371, Test Accuracy: 0.97300000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.03004818, Test Loss: 0.10611726, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.02447630, Test Loss: 0.11308604, Test Accuracy: 0.97280000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.01982690, Test Loss: 0.11356062, Test Accuracy: 0.97470000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.01734117, Test Loss: 0.11529768, Test Accuracy: 0.97410000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.01404227, Test Loss: 0.12361706, Test Accuracy: 0.97370000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.01252323, Test Loss: 0.12372039, Test Accuracy: 0.97480000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00998399, Test Loss: 0.12418616, Test Accuracy: 0.97480000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00903993, Test Loss: 0.12977210, Test Accuracy: 0.97480000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00758339, Test Loss: 0.13564425, Test Accuracy: 0.97330000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00760029, Test Loss: 0.13117826, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00612100, Test Loss: 0.13309895, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00525761, Test Loss: 0.13545938, Test Accuracy: 0.97490000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00410157, Test Loss: 0.13759412, Test Accuracy: 0.97550000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00373752, Test Loss: 0.14452383, Test Accuracy: 0.97360000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00341219, Test Loss: 0.14853351, Test Accuracy: 0.97380000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00303024, Test Loss: 0.15659665, Test Accuracy: 0.97160000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00265175, Test Loss: 0.14970841, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00241900, Test Loss: 0.15266190, Test Accuracy: 0.97390000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00223889, Test Loss: 0.15512001, Test Accuracy: 0.97440000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00210107, Test Loss: 0.16041171, Test Accuracy: 0.97350000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00196907, Test Loss: 0.15874036, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00179892, Test Loss: 0.16095162, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00165242, Test Loss: 0.16210078, Test Accuracy: 0.97440000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00150407, Test Loss: 0.16595333, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00146326, Test Loss: 0.16547024, Test Accuracy: 0.97420000\n",
            "[tensor(0.2704, grad_fn=<MeanBackward0>), tensor(0.2705, grad_fn=<MeanBackward0>), tensor(0.2705, grad_fn=<MeanBackward0>), tensor(0.2706, grad_fn=<MeanBackward0>), tensor(0.2707, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.3013, grad_fn=<MeanBackward0>), tensor(0.4508, grad_fn=<MeanBackward0>), tensor(0.4567, grad_fn=<MeanBackward0>), tensor(0.4476, grad_fn=<MeanBackward0>), tensor(0.4509, grad_fn=<MeanBackward0>), tensor(0.4498, grad_fn=<MeanBackward0>), tensor(0.4509, grad_fn=<MeanBackward0>), tensor(0.4511, grad_fn=<MeanBackward0>), tensor(0.4531, grad_fn=<MeanBackward0>), tensor(0.4507, grad_fn=<MeanBackward0>), tensor(0.4520, grad_fn=<MeanBackward0>), tensor(0.4505, grad_fn=<MeanBackward0>), tensor(0.4514, grad_fn=<MeanBackward0>), tensor(0.4517, grad_fn=<MeanBackward0>), tensor(0.4521, grad_fn=<MeanBackward0>), tensor(0.4515, grad_fn=<MeanBackward0>), tensor(0.4523, grad_fn=<MeanBackward0>), tensor(0.4532, grad_fn=<MeanBackward0>), tensor(0.4521, grad_fn=<MeanBackward0>), tensor(0.4517, grad_fn=<MeanBackward0>), tensor(0.4520, grad_fn=<MeanBackward0>), tensor(0.4516, grad_fn=<MeanBackward0>), tensor(0.4512, grad_fn=<MeanBackward0>), tensor(0.4516, grad_fn=<MeanBackward0>), tensor(0.4514, grad_fn=<MeanBackward0>), tensor(0.4516, grad_fn=<MeanBackward0>), tensor(0.4513, grad_fn=<MeanBackward0>), tensor(0.4514, grad_fn=<MeanBackward0>), tensor(0.4512, grad_fn=<MeanBackward0>), tensor(0.4511, grad_fn=<MeanBackward0>), tensor(0.4514, grad_fn=<MeanBackward0>), tensor(0.4517, grad_fn=<MeanBackward0>), tensor(0.4516, grad_fn=<MeanBackward0>), tensor(0.4518, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3174, grad_fn=<MeanBackward0>), tensor(0.3175, grad_fn=<MeanBackward0>), tensor(0.3176, grad_fn=<MeanBackward0>), tensor(0.3178, grad_fn=<MeanBackward0>), tensor(0.3178, grad_fn=<MeanBackward0>), tensor(0.3140, grad_fn=<MeanBackward0>), tensor(0.3140, grad_fn=<MeanBackward0>), tensor(0.3141, grad_fn=<MeanBackward0>), tensor(0.3141, grad_fn=<MeanBackward0>), tensor(0.3142, grad_fn=<MeanBackward0>), tensor(0.3142, grad_fn=<MeanBackward0>), tensor(0.3143, grad_fn=<MeanBackward0>), tensor(0.3143, grad_fn=<MeanBackward0>), tensor(0.3144, grad_fn=<MeanBackward0>), tensor(0.3144, grad_fn=<MeanBackward0>), tensor(0.3145, grad_fn=<MeanBackward0>), tensor(0.3164, grad_fn=<MeanBackward0>), tensor(0.4237, grad_fn=<MeanBackward0>), tensor(0.4780, grad_fn=<MeanBackward0>), tensor(0.4874, grad_fn=<MeanBackward0>), tensor(0.4924, grad_fn=<MeanBackward0>), tensor(0.5023, grad_fn=<MeanBackward0>), tensor(0.5021, grad_fn=<MeanBackward0>), tensor(0.5004, grad_fn=<MeanBackward0>), tensor(0.4925, grad_fn=<MeanBackward0>), tensor(0.4954, grad_fn=<MeanBackward0>), tensor(0.4943, grad_fn=<MeanBackward0>), tensor(0.4949, grad_fn=<MeanBackward0>), tensor(0.4973, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.4948, grad_fn=<MeanBackward0>), tensor(0.4949, grad_fn=<MeanBackward0>), tensor(0.4959, grad_fn=<MeanBackward0>), tensor(0.4951, grad_fn=<MeanBackward0>), tensor(0.4957, grad_fn=<MeanBackward0>), tensor(0.4934, grad_fn=<MeanBackward0>), tensor(0.4941, grad_fn=<MeanBackward0>), tensor(0.4864, grad_fn=<MeanBackward0>), tensor(0.4892, grad_fn=<MeanBackward0>), tensor(0.4897, grad_fn=<MeanBackward0>), tensor(0.4905, grad_fn=<MeanBackward0>), tensor(0.4904, grad_fn=<MeanBackward0>), tensor(0.4877, grad_fn=<MeanBackward0>), tensor(0.4869, grad_fn=<MeanBackward0>), tensor(0.4889, grad_fn=<MeanBackward0>), tensor(0.4890, grad_fn=<MeanBackward0>), tensor(0.4893, grad_fn=<MeanBackward0>), tensor(0.4878, grad_fn=<MeanBackward0>), tensor(0.4848, grad_fn=<MeanBackward0>), tensor(0.4853, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3289, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3831, grad_fn=<MeanBackward0>), tensor(0.4928, grad_fn=<MeanBackward0>), tensor(0.5302, grad_fn=<MeanBackward0>), tensor(0.5366, grad_fn=<MeanBackward0>), tensor(0.5262, grad_fn=<MeanBackward0>), tensor(0.5304, grad_fn=<MeanBackward0>), tensor(0.5322, grad_fn=<MeanBackward0>), tensor(0.5325, grad_fn=<MeanBackward0>), tensor(0.5362, grad_fn=<MeanBackward0>), tensor(0.5420, grad_fn=<MeanBackward0>), tensor(0.5400, grad_fn=<MeanBackward0>), tensor(0.5378, grad_fn=<MeanBackward0>), tensor(0.5425, grad_fn=<MeanBackward0>), tensor(0.5392, grad_fn=<MeanBackward0>), tensor(0.5397, grad_fn=<MeanBackward0>), tensor(0.5306, grad_fn=<MeanBackward0>), tensor(0.5405, grad_fn=<MeanBackward0>), tensor(0.5345, grad_fn=<MeanBackward0>), tensor(0.5333, grad_fn=<MeanBackward0>), tensor(0.5354, grad_fn=<MeanBackward0>), tensor(0.5316, grad_fn=<MeanBackward0>), tensor(0.5321, grad_fn=<MeanBackward0>), tensor(0.5317, grad_fn=<MeanBackward0>), tensor(0.5273, grad_fn=<MeanBackward0>), tensor(0.5255, grad_fn=<MeanBackward0>), tensor(0.5208, grad_fn=<MeanBackward0>), tensor(0.5258, grad_fn=<MeanBackward0>), tensor(0.5242, grad_fn=<MeanBackward0>), tensor(0.5220, grad_fn=<MeanBackward0>), tensor(0.5203, grad_fn=<MeanBackward0>), tensor(0.5173, grad_fn=<MeanBackward0>), tensor(0.5183, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.4009, grad_fn=<MeanBackward0>), tensor(0.4158, grad_fn=<MeanBackward0>), tensor(0.4021, grad_fn=<MeanBackward0>), tensor(0.4036, grad_fn=<MeanBackward0>), tensor(0.4064, grad_fn=<MeanBackward0>), tensor(0.4054, grad_fn=<MeanBackward0>), tensor(0.4003, grad_fn=<MeanBackward0>), tensor(0.4077, grad_fn=<MeanBackward0>), tensor(0.4097, grad_fn=<MeanBackward0>), tensor(0.4080, grad_fn=<MeanBackward0>), tensor(0.4072, grad_fn=<MeanBackward0>), tensor(0.4086, grad_fn=<MeanBackward0>), tensor(0.4088, grad_fn=<MeanBackward0>), tensor(0.4082, grad_fn=<MeanBackward0>), tensor(0.4071, grad_fn=<MeanBackward0>), tensor(0.4039, grad_fn=<MeanBackward0>), tensor(0.4099, grad_fn=<MeanBackward0>), tensor(0.4112, grad_fn=<MeanBackward0>), tensor(0.4108, grad_fn=<MeanBackward0>), tensor(0.4103, grad_fn=<MeanBackward0>), tensor(0.4071, grad_fn=<MeanBackward0>), tensor(0.4072, grad_fn=<MeanBackward0>), tensor(0.4047, grad_fn=<MeanBackward0>), tensor(0.4059, grad_fn=<MeanBackward0>), tensor(0.4072, grad_fn=<MeanBackward0>), tensor(0.4071, grad_fn=<MeanBackward0>), tensor(0.4066, grad_fn=<MeanBackward0>), tensor(0.4062, grad_fn=<MeanBackward0>), tensor(0.4085, grad_fn=<MeanBackward0>), tensor(0.4079, grad_fn=<MeanBackward0>), tensor(0.4067, grad_fn=<MeanBackward0>), tensor(0.4075, grad_fn=<MeanBackward0>), tensor(0.4073, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.32059696316719055, 0.3206307664513588, 0.32067976146936417, 0.32073453813791275, 0.3207795023918152, 0.3249920681118965, 0.3250023126602173, 0.3250133842229843, 0.3250245228409767, 0.32503629475831985, 0.32504870742559433, 0.3250611573457718, 0.3250742256641388, 0.3250885531306267, 0.3251046761870384, 0.32512906938791275, 0.3284998759627342, 0.4014400690793991, 0.43339086323976517, 0.4574742540717125, 0.4692729264497757, 0.47377123683691025, 0.47116807103157043, 0.4705396220088005, 0.47138598561286926, 0.4720718339085579, 0.4726187661290169, 0.47365985810756683, 0.4743337258696556, 0.474080391228199, 0.4743865951895714, 0.47319700568914413, 0.47293689101934433, 0.47218628227710724, 0.47487449645996094, 0.4726182594895363, 0.47239939868450165, 0.4701022058725357, 0.46980106085538864, 0.4695197120308876, 0.4698679596185684, 0.4691237509250641, 0.4679178521037102, 0.4664338454604149, 0.46800675243139267, 0.4682137742638588, 0.46765486150979996, 0.466618612408638, 0.46530651301145554, 0.4656742289662361]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmLJ4Zr2MnoS"
      },
      "source": [
        "# SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ObsEJHuMoPy",
        "outputId": "56dfadec-6913-450b-f569-042814bba3a5"
      },
      "source": [
        "model_factory('SGD')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.32315335, Test Loss: 2.30903590, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30906891, Test Loss: 2.30911063, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.30566493, Test Loss: 2.30473551, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.30376772, Test Loss: 2.30548497, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.30315410, Test Loss: 2.30266628, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.30291336, Test Loss: 2.30233923, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.30252485, Test Loss: 2.30283901, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.30234118, Test Loss: 2.30234135, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.30218777, Test Loss: 2.30202645, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.30217589, Test Loss: 2.30245288, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.30208508, Test Loss: 2.30125003, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.30207717, Test Loss: 2.30117370, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.30191832, Test Loss: 2.30243047, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.30188111, Test Loss: 2.30183062, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.30170921, Test Loss: 2.30127634, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.30160856, Test Loss: 2.30088890, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.30137673, Test Loss: 2.30096326, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.30104898, Test Loss: 2.30090060, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 2.30040888, Test Loss: 2.29950048, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 2.29918755, Test Loss: 2.29756449, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 2.29594026, Test Loss: 2.29298709, Test Accuracy: 0.21030000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 2.27940428, Test Loss: 2.24576280, Test Accuracy: 0.14350000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 2.07198362, Test Loss: 1.89169017, Test Accuracy: 0.29000000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 1.79864557, Test Loss: 1.69034996, Test Accuracy: 0.35440000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 1.66169387, Test Loss: 1.59261252, Test Accuracy: 0.38050000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 1.57599361, Test Loss: 1.51248771, Test Accuracy: 0.39980000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 1.50176132, Test Loss: 1.42308507, Test Accuracy: 0.44480000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 1.41070338, Test Loss: 1.32790823, Test Accuracy: 0.49670000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 1.24923304, Test Loss: 0.98909124, Test Accuracy: 0.66370000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.85563599, Test Loss: 0.73560081, Test Accuracy: 0.74180000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.70487264, Test Loss: 0.61985340, Test Accuracy: 0.81770000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.60342645, Test Loss: 0.55436289, Test Accuracy: 0.83800000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.52255753, Test Loss: 0.49487989, Test Accuracy: 0.86330000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.46595178, Test Loss: 0.41601222, Test Accuracy: 0.89550000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.42113654, Test Loss: 0.40295435, Test Accuracy: 0.89890000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.38326159, Test Loss: 0.35731590, Test Accuracy: 0.90860000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.35746804, Test Loss: 0.36892897, Test Accuracy: 0.90110000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.32986220, Test Loss: 0.32568321, Test Accuracy: 0.91700000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.30912760, Test Loss: 0.29942438, Test Accuracy: 0.92330000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.29016519, Test Loss: 0.28343526, Test Accuracy: 0.92690000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.27181026, Test Loss: 0.26919584, Test Accuracy: 0.92910000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.25264537, Test Loss: 0.28705800, Test Accuracy: 0.92650000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.23388326, Test Loss: 0.24178720, Test Accuracy: 0.93540000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.21865698, Test Loss: 0.25449849, Test Accuracy: 0.93520000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.20357712, Test Loss: 0.23846809, Test Accuracy: 0.93960000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.19323417, Test Loss: 0.22680931, Test Accuracy: 0.94070000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.18324133, Test Loss: 0.20620589, Test Accuracy: 0.94470000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.17500244, Test Loss: 0.20668234, Test Accuracy: 0.94450000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.16426174, Test Loss: 0.20672508, Test Accuracy: 0.94380000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.15846913, Test Loss: 0.22579610, Test Accuracy: 0.93880000\n",
            "[tensor(0.0050, grad_fn=<MeanBackward0>), tensor(0.0050, grad_fn=<MeanBackward0>), tensor(0.0051, grad_fn=<MeanBackward0>), tensor(0.0051, grad_fn=<MeanBackward0>), tensor(0.0052, grad_fn=<MeanBackward0>), tensor(0.0052, grad_fn=<MeanBackward0>), tensor(0.0053, grad_fn=<MeanBackward0>), tensor(0.0053, grad_fn=<MeanBackward0>), tensor(0.0054, grad_fn=<MeanBackward0>), tensor(0.0055, grad_fn=<MeanBackward0>), tensor(0.0056, grad_fn=<MeanBackward0>), tensor(0.0057, grad_fn=<MeanBackward0>), tensor(0.0058, grad_fn=<MeanBackward0>), tensor(0.0060, grad_fn=<MeanBackward0>), tensor(0.0063, grad_fn=<MeanBackward0>), tensor(0.0066, grad_fn=<MeanBackward0>), tensor(0.0071, grad_fn=<MeanBackward0>), tensor(0.0077, grad_fn=<MeanBackward0>), tensor(0.0087, grad_fn=<MeanBackward0>), tensor(0.0104, grad_fn=<MeanBackward0>), tensor(0.0134, grad_fn=<MeanBackward0>), tensor(0.0218, grad_fn=<MeanBackward0>), tensor(0.0409, grad_fn=<MeanBackward0>), tensor(0.0313, grad_fn=<MeanBackward0>), tensor(0.0412, grad_fn=<MeanBackward0>), tensor(0.0518, grad_fn=<MeanBackward0>), tensor(0.0599, grad_fn=<MeanBackward0>), tensor(0.0686, grad_fn=<MeanBackward0>), tensor(0.0810, grad_fn=<MeanBackward0>), tensor(0.0908, grad_fn=<MeanBackward0>), tensor(0.1047, grad_fn=<MeanBackward0>), tensor(0.1211, grad_fn=<MeanBackward0>), tensor(0.1349, grad_fn=<MeanBackward0>), tensor(0.1472, grad_fn=<MeanBackward0>), tensor(0.1576, grad_fn=<MeanBackward0>), tensor(0.1678, grad_fn=<MeanBackward0>), tensor(0.1790, grad_fn=<MeanBackward0>), tensor(0.1871, grad_fn=<MeanBackward0>), tensor(0.1947, grad_fn=<MeanBackward0>), tensor(0.2019, grad_fn=<MeanBackward0>), tensor(0.2072, grad_fn=<MeanBackward0>), tensor(0.2111, grad_fn=<MeanBackward0>), tensor(0.2143, grad_fn=<MeanBackward0>), tensor(0.2168, grad_fn=<MeanBackward0>), tensor(0.2190, grad_fn=<MeanBackward0>), tensor(0.2222, grad_fn=<MeanBackward0>), tensor(0.2251, grad_fn=<MeanBackward0>), tensor(0.2277, grad_fn=<MeanBackward0>), tensor(0.2303, grad_fn=<MeanBackward0>), tensor(0.2334, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0108, grad_fn=<MeanBackward0>), tensor(0.0108, grad_fn=<MeanBackward0>), tensor(0.0109, grad_fn=<MeanBackward0>), tensor(0.0110, grad_fn=<MeanBackward0>), tensor(0.0110, grad_fn=<MeanBackward0>), tensor(0.0111, grad_fn=<MeanBackward0>), tensor(0.0111, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0111, grad_fn=<MeanBackward0>), tensor(0.0109, grad_fn=<MeanBackward0>), tensor(0.0104, grad_fn=<MeanBackward0>), tensor(0.0096, grad_fn=<MeanBackward0>), tensor(0.0083, grad_fn=<MeanBackward0>), tensor(0.0364, grad_fn=<MeanBackward0>), tensor(0.0584, grad_fn=<MeanBackward0>), tensor(0.0592, grad_fn=<MeanBackward0>), tensor(0.0599, grad_fn=<MeanBackward0>), tensor(0.0628, grad_fn=<MeanBackward0>), tensor(0.0646, grad_fn=<MeanBackward0>), tensor(0.0654, grad_fn=<MeanBackward0>), tensor(0.0740, grad_fn=<MeanBackward0>), tensor(0.0681, grad_fn=<MeanBackward0>), tensor(0.0617, grad_fn=<MeanBackward0>), tensor(0.0612, grad_fn=<MeanBackward0>), tensor(0.0648, grad_fn=<MeanBackward0>), tensor(0.0650, grad_fn=<MeanBackward0>), tensor(0.0669, grad_fn=<MeanBackward0>), tensor(0.0663, grad_fn=<MeanBackward0>), tensor(0.0691, grad_fn=<MeanBackward0>), tensor(0.0695, grad_fn=<MeanBackward0>), tensor(0.0720, grad_fn=<MeanBackward0>), tensor(0.0728, grad_fn=<MeanBackward0>), tensor(0.0756, grad_fn=<MeanBackward0>), tensor(0.0760, grad_fn=<MeanBackward0>), tensor(0.0800, grad_fn=<MeanBackward0>), tensor(0.0807, grad_fn=<MeanBackward0>), tensor(0.0831, grad_fn=<MeanBackward0>), tensor(0.0804, grad_fn=<MeanBackward0>), tensor(0.0827, grad_fn=<MeanBackward0>), tensor(0.0811, grad_fn=<MeanBackward0>), tensor(0.0815, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0134, grad_fn=<MeanBackward0>), tensor(0.0139, grad_fn=<MeanBackward0>), tensor(0.0139, grad_fn=<MeanBackward0>), tensor(0.0137, grad_fn=<MeanBackward0>), tensor(0.0135, grad_fn=<MeanBackward0>), tensor(0.0133, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0124, grad_fn=<MeanBackward0>), tensor(0.0122, grad_fn=<MeanBackward0>), tensor(0.0121, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0110, grad_fn=<MeanBackward0>), tensor(0.0106, grad_fn=<MeanBackward0>), tensor(0.0100, grad_fn=<MeanBackward0>), tensor(0.0092, grad_fn=<MeanBackward0>), tensor(0.0076, grad_fn=<MeanBackward0>), tensor(0.0045, grad_fn=<MeanBackward0>), tensor(0.0089, grad_fn=<MeanBackward0>), tensor(0.0271, grad_fn=<MeanBackward0>), tensor(0.0492, grad_fn=<MeanBackward0>), tensor(0.0776, grad_fn=<MeanBackward0>), tensor(0.1144, grad_fn=<MeanBackward0>), tensor(0.1507, grad_fn=<MeanBackward0>), tensor(0.1149, grad_fn=<MeanBackward0>), tensor(0.1370, grad_fn=<MeanBackward0>), tensor(0.1387, grad_fn=<MeanBackward0>), tensor(0.1326, grad_fn=<MeanBackward0>), tensor(0.1346, grad_fn=<MeanBackward0>), tensor(0.1533, grad_fn=<MeanBackward0>), tensor(0.1561, grad_fn=<MeanBackward0>), tensor(0.1558, grad_fn=<MeanBackward0>), tensor(0.1505, grad_fn=<MeanBackward0>), tensor(0.1610, grad_fn=<MeanBackward0>), tensor(0.1578, grad_fn=<MeanBackward0>), tensor(0.1601, grad_fn=<MeanBackward0>), tensor(0.1565, grad_fn=<MeanBackward0>), tensor(0.1667, grad_fn=<MeanBackward0>), tensor(0.1541, grad_fn=<MeanBackward0>), tensor(0.1667, grad_fn=<MeanBackward0>), tensor(0.1635, grad_fn=<MeanBackward0>), tensor(0.1682, grad_fn=<MeanBackward0>), tensor(0.1536, grad_fn=<MeanBackward0>), tensor(0.1598, grad_fn=<MeanBackward0>), tensor(0.1528, grad_fn=<MeanBackward0>), tensor(0.1598, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0070, grad_fn=<MeanBackward0>), tensor(0.0037, grad_fn=<MeanBackward0>), tensor(0.0022, grad_fn=<MeanBackward0>), tensor(0.0016, grad_fn=<MeanBackward0>), tensor(0.0012, grad_fn=<MeanBackward0>), tensor(0.0009, grad_fn=<MeanBackward0>), tensor(0.0009, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0006, grad_fn=<MeanBackward0>), tensor(0.0006, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0010, grad_fn=<MeanBackward0>), tensor(0.0013, grad_fn=<MeanBackward0>), tensor(0.0022, grad_fn=<MeanBackward0>), tensor(0.0040, grad_fn=<MeanBackward0>), tensor(0.0055, grad_fn=<MeanBackward0>), tensor(0.0089, grad_fn=<MeanBackward0>), tensor(0.0100, grad_fn=<MeanBackward0>), tensor(0.0434, grad_fn=<MeanBackward0>), tensor(0.1329, grad_fn=<MeanBackward0>), tensor(0.2291, grad_fn=<MeanBackward0>), tensor(0.2611, grad_fn=<MeanBackward0>), tensor(0.2884, grad_fn=<MeanBackward0>), tensor(0.2644, grad_fn=<MeanBackward0>), tensor(0.2654, grad_fn=<MeanBackward0>), tensor(0.3009, grad_fn=<MeanBackward0>), tensor(0.3415, grad_fn=<MeanBackward0>), tensor(0.3508, grad_fn=<MeanBackward0>), tensor(0.3402, grad_fn=<MeanBackward0>), tensor(0.3454, grad_fn=<MeanBackward0>), tensor(0.3526, grad_fn=<MeanBackward0>), tensor(0.3745, grad_fn=<MeanBackward0>), tensor(0.3528, grad_fn=<MeanBackward0>), tensor(0.3629, grad_fn=<MeanBackward0>), tensor(0.3652, grad_fn=<MeanBackward0>), tensor(0.3762, grad_fn=<MeanBackward0>), tensor(0.3611, grad_fn=<MeanBackward0>), tensor(0.3795, grad_fn=<MeanBackward0>), tensor(0.3584, grad_fn=<MeanBackward0>), tensor(0.3594, grad_fn=<MeanBackward0>), tensor(0.3586, grad_fn=<MeanBackward0>), tensor(0.3809, grad_fn=<MeanBackward0>), tensor(0.3740, grad_fn=<MeanBackward0>), tensor(0.3874, grad_fn=<MeanBackward0>), tensor(0.3816, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.009035350172780454, 0.008382769534364343, 0.008020294189918786, 0.007867652806453407, 0.0077304086298681796, 0.007618976800586097, 0.007577700278488919, 0.00750357007200364, 0.007474989251932129, 0.007454087128280662, 0.007402252449537627, 0.007390893952106126, 0.007367406702542212, 0.007398379180813208, 0.007412120990920812, 0.007467655843356624, 0.00756479540723376, 0.007666759367566556, 0.007968981633894145, 0.008500382769852877, 0.009004134451970458, 0.010887250420637429, 0.024038777453824878, 0.04004678688943386, 0.07060703821480274, 0.10460407938808203, 0.12456480041146278, 0.14306248724460602, 0.13143153116106987, 0.14178095385432243, 0.15309470146894455, 0.16423431877046824, 0.1703993109986186, 0.17634671926498413, 0.1810316052287817, 0.18575500696897507, 0.19259764067828655, 0.1925139669328928, 0.1962234266102314, 0.19976499304175377, 0.20316686294972897, 0.20360421389341354, 0.2059666644781828, 0.20545107685029507, 0.20565216802060604, 0.20799775049090385, 0.20996900834143162, 0.21103624068200588, 0.21292820759117603, 0.21407873183488846]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvQxaN_fRXLq"
      },
      "source": [
        "# Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkqfFoVkRXxP",
        "outputId": "58545b05-cdeb-4b69-d83d-b65fdb61625a"
      },
      "source": [
        "model_factory('Adam')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 0.63060462, Test Loss: 0.26807360, Test Accuracy: 0.92430000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 0.20602424, Test Loss: 0.17091358, Test Accuracy: 0.95030000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 0.13963423, Test Loss: 0.13862532, Test Accuracy: 0.96050000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 0.10679276, Test Loss: 0.11708062, Test Accuracy: 0.96550000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.08605078, Test Loss: 0.10161783, Test Accuracy: 0.96890000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.06997541, Test Loss: 0.10106351, Test Accuracy: 0.96990000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.05697623, Test Loss: 0.09878754, Test Accuracy: 0.97310000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.04864489, Test Loss: 0.08308130, Test Accuracy: 0.97660000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.03917246, Test Loss: 0.09710505, Test Accuracy: 0.97200000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.03353929, Test Loss: 0.08978832, Test Accuracy: 0.97520000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.02944324, Test Loss: 0.09079082, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.02727311, Test Loss: 0.08651221, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.02212837, Test Loss: 0.10875838, Test Accuracy: 0.97380000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.02065332, Test Loss: 0.11554108, Test Accuracy: 0.97220000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.01728293, Test Loss: 0.10393809, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.01743503, Test Loss: 0.09364917, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.01488689, Test Loss: 0.09518447, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.01465772, Test Loss: 0.09294552, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.01331280, Test Loss: 0.10719148, Test Accuracy: 0.97680000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.01146981, Test Loss: 0.10636781, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.01076229, Test Loss: 0.11568037, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.01094787, Test Loss: 0.10368003, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.01096126, Test Loss: 0.10372334, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.00916949, Test Loss: 0.11437551, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.00867400, Test Loss: 0.10149641, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.00768714, Test Loss: 0.10892842, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.01032192, Test Loss: 0.10701216, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.00594464, Test Loss: 0.11558755, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00722419, Test Loss: 0.11979975, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00767958, Test Loss: 0.14004651, Test Accuracy: 0.97530000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00643361, Test Loss: 0.11410860, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00658088, Test Loss: 0.11896787, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00584123, Test Loss: 0.11278648, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00427163, Test Loss: 0.11660295, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00587032, Test Loss: 0.13375498, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00775411, Test Loss: 0.11942773, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00561368, Test Loss: 0.12555562, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00341692, Test Loss: 0.11896444, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00621822, Test Loss: 0.12563199, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00496862, Test Loss: 0.12525794, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00447093, Test Loss: 0.12711130, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00512971, Test Loss: 0.11264677, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00365913, Test Loss: 0.14619701, Test Accuracy: 0.97620000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00633194, Test Loss: 0.11377818, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00433969, Test Loss: 0.12402723, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00490925, Test Loss: 0.12014301, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00344085, Test Loss: 0.12659953, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00493614, Test Loss: 0.12884805, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00439996, Test Loss: 0.13358994, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00216132, Test Loss: 0.10985982, Test Accuracy: 0.98270000\n",
            "[tensor(0.2696, grad_fn=<MeanBackward0>), tensor(0.2998, grad_fn=<MeanBackward0>), tensor(0.3202, grad_fn=<MeanBackward0>), tensor(0.3305, grad_fn=<MeanBackward0>), tensor(0.3425, grad_fn=<MeanBackward0>), tensor(0.3504, grad_fn=<MeanBackward0>), tensor(0.3562, grad_fn=<MeanBackward0>), tensor(0.3599, grad_fn=<MeanBackward0>), tensor(0.3647, grad_fn=<MeanBackward0>), tensor(0.3658, grad_fn=<MeanBackward0>), tensor(0.3748, grad_fn=<MeanBackward0>), tensor(0.3764, grad_fn=<MeanBackward0>), tensor(0.3740, grad_fn=<MeanBackward0>), tensor(0.3744, grad_fn=<MeanBackward0>), tensor(0.3807, grad_fn=<MeanBackward0>), tensor(0.3795, grad_fn=<MeanBackward0>), tensor(0.3847, grad_fn=<MeanBackward0>), tensor(0.3840, grad_fn=<MeanBackward0>), tensor(0.3830, grad_fn=<MeanBackward0>), tensor(0.3900, grad_fn=<MeanBackward0>), tensor(0.3870, grad_fn=<MeanBackward0>), tensor(0.3894, grad_fn=<MeanBackward0>), tensor(0.3911, grad_fn=<MeanBackward0>), tensor(0.3888, grad_fn=<MeanBackward0>), tensor(0.3949, grad_fn=<MeanBackward0>), tensor(0.3946, grad_fn=<MeanBackward0>), tensor(0.3957, grad_fn=<MeanBackward0>), tensor(0.3983, grad_fn=<MeanBackward0>), tensor(0.3938, grad_fn=<MeanBackward0>), tensor(0.3977, grad_fn=<MeanBackward0>), tensor(0.3988, grad_fn=<MeanBackward0>), tensor(0.4008, grad_fn=<MeanBackward0>), tensor(0.3961, grad_fn=<MeanBackward0>), tensor(0.3989, grad_fn=<MeanBackward0>), tensor(0.3989, grad_fn=<MeanBackward0>), tensor(0.4029, grad_fn=<MeanBackward0>), tensor(0.4030, grad_fn=<MeanBackward0>), tensor(0.4029, grad_fn=<MeanBackward0>), tensor(0.4038, grad_fn=<MeanBackward0>), tensor(0.4041, grad_fn=<MeanBackward0>), tensor(0.4054, grad_fn=<MeanBackward0>), tensor(0.4075, grad_fn=<MeanBackward0>), tensor(0.4096, grad_fn=<MeanBackward0>), tensor(0.4057, grad_fn=<MeanBackward0>), tensor(0.4054, grad_fn=<MeanBackward0>), tensor(0.4065, grad_fn=<MeanBackward0>), tensor(0.4103, grad_fn=<MeanBackward0>), tensor(0.4130, grad_fn=<MeanBackward0>), tensor(0.4075, grad_fn=<MeanBackward0>), tensor(0.4096, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2190, grad_fn=<MeanBackward0>), tensor(0.2337, grad_fn=<MeanBackward0>), tensor(0.2423, grad_fn=<MeanBackward0>), tensor(0.2540, grad_fn=<MeanBackward0>), tensor(0.2635, grad_fn=<MeanBackward0>), tensor(0.2729, grad_fn=<MeanBackward0>), tensor(0.2759, grad_fn=<MeanBackward0>), tensor(0.2840, grad_fn=<MeanBackward0>), tensor(0.2922, grad_fn=<MeanBackward0>), tensor(0.3040, grad_fn=<MeanBackward0>), tensor(0.3014, grad_fn=<MeanBackward0>), tensor(0.3143, grad_fn=<MeanBackward0>), tensor(0.3291, grad_fn=<MeanBackward0>), tensor(0.3350, grad_fn=<MeanBackward0>), tensor(0.3314, grad_fn=<MeanBackward0>), tensor(0.3494, grad_fn=<MeanBackward0>), tensor(0.3511, grad_fn=<MeanBackward0>), tensor(0.3548, grad_fn=<MeanBackward0>), tensor(0.3622, grad_fn=<MeanBackward0>), tensor(0.3625, grad_fn=<MeanBackward0>), tensor(0.3687, grad_fn=<MeanBackward0>), tensor(0.3743, grad_fn=<MeanBackward0>), tensor(0.3802, grad_fn=<MeanBackward0>), tensor(0.3894, grad_fn=<MeanBackward0>), tensor(0.3908, grad_fn=<MeanBackward0>), tensor(0.3923, grad_fn=<MeanBackward0>), tensor(0.4022, grad_fn=<MeanBackward0>), tensor(0.4045, grad_fn=<MeanBackward0>), tensor(0.4039, grad_fn=<MeanBackward0>), tensor(0.4086, grad_fn=<MeanBackward0>), tensor(0.4089, grad_fn=<MeanBackward0>), tensor(0.4159, grad_fn=<MeanBackward0>), tensor(0.4169, grad_fn=<MeanBackward0>), tensor(0.4194, grad_fn=<MeanBackward0>), tensor(0.4265, grad_fn=<MeanBackward0>), tensor(0.4258, grad_fn=<MeanBackward0>), tensor(0.4286, grad_fn=<MeanBackward0>), tensor(0.4295, grad_fn=<MeanBackward0>), tensor(0.4389, grad_fn=<MeanBackward0>), tensor(0.4330, grad_fn=<MeanBackward0>), tensor(0.4348, grad_fn=<MeanBackward0>), tensor(0.4382, grad_fn=<MeanBackward0>), tensor(0.4345, grad_fn=<MeanBackward0>), tensor(0.4396, grad_fn=<MeanBackward0>), tensor(0.4378, grad_fn=<MeanBackward0>), tensor(0.4327, grad_fn=<MeanBackward0>), tensor(0.4389, grad_fn=<MeanBackward0>), tensor(0.4390, grad_fn=<MeanBackward0>), tensor(0.4455, grad_fn=<MeanBackward0>), tensor(0.4409, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2400, grad_fn=<MeanBackward0>), tensor(0.2629, grad_fn=<MeanBackward0>), tensor(0.2751, grad_fn=<MeanBackward0>), tensor(0.2840, grad_fn=<MeanBackward0>), tensor(0.2988, grad_fn=<MeanBackward0>), tensor(0.3061, grad_fn=<MeanBackward0>), tensor(0.3156, grad_fn=<MeanBackward0>), tensor(0.3217, grad_fn=<MeanBackward0>), tensor(0.3310, grad_fn=<MeanBackward0>), tensor(0.3293, grad_fn=<MeanBackward0>), tensor(0.3435, grad_fn=<MeanBackward0>), tensor(0.3477, grad_fn=<MeanBackward0>), tensor(0.3476, grad_fn=<MeanBackward0>), tensor(0.3560, grad_fn=<MeanBackward0>), tensor(0.3654, grad_fn=<MeanBackward0>), tensor(0.3631, grad_fn=<MeanBackward0>), tensor(0.3696, grad_fn=<MeanBackward0>), tensor(0.3744, grad_fn=<MeanBackward0>), tensor(0.3783, grad_fn=<MeanBackward0>), tensor(0.3848, grad_fn=<MeanBackward0>), tensor(0.3819, grad_fn=<MeanBackward0>), tensor(0.3869, grad_fn=<MeanBackward0>), tensor(0.3914, grad_fn=<MeanBackward0>), tensor(0.3920, grad_fn=<MeanBackward0>), tensor(0.3937, grad_fn=<MeanBackward0>), tensor(0.3920, grad_fn=<MeanBackward0>), tensor(0.3945, grad_fn=<MeanBackward0>), tensor(0.3978, grad_fn=<MeanBackward0>), tensor(0.4039, grad_fn=<MeanBackward0>), tensor(0.4058, grad_fn=<MeanBackward0>), tensor(0.4050, grad_fn=<MeanBackward0>), tensor(0.4055, grad_fn=<MeanBackward0>), tensor(0.4059, grad_fn=<MeanBackward0>), tensor(0.4074, grad_fn=<MeanBackward0>), tensor(0.4043, grad_fn=<MeanBackward0>), tensor(0.4109, grad_fn=<MeanBackward0>), tensor(0.4097, grad_fn=<MeanBackward0>), tensor(0.4111, grad_fn=<MeanBackward0>), tensor(0.4137, grad_fn=<MeanBackward0>), tensor(0.4135, grad_fn=<MeanBackward0>), tensor(0.4128, grad_fn=<MeanBackward0>), tensor(0.4115, grad_fn=<MeanBackward0>), tensor(0.4175, grad_fn=<MeanBackward0>), tensor(0.4151, grad_fn=<MeanBackward0>), tensor(0.4173, grad_fn=<MeanBackward0>), tensor(0.4217, grad_fn=<MeanBackward0>), tensor(0.4221, grad_fn=<MeanBackward0>), tensor(0.4221, grad_fn=<MeanBackward0>), tensor(0.4165, grad_fn=<MeanBackward0>), tensor(0.4190, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2533, grad_fn=<MeanBackward0>), tensor(0.2697, grad_fn=<MeanBackward0>), tensor(0.2828, grad_fn=<MeanBackward0>), tensor(0.2953, grad_fn=<MeanBackward0>), tensor(0.3106, grad_fn=<MeanBackward0>), tensor(0.3130, grad_fn=<MeanBackward0>), tensor(0.3203, grad_fn=<MeanBackward0>), tensor(0.3307, grad_fn=<MeanBackward0>), tensor(0.3361, grad_fn=<MeanBackward0>), tensor(0.3447, grad_fn=<MeanBackward0>), tensor(0.3463, grad_fn=<MeanBackward0>), tensor(0.3526, grad_fn=<MeanBackward0>), tensor(0.3552, grad_fn=<MeanBackward0>), tensor(0.3658, grad_fn=<MeanBackward0>), tensor(0.3595, grad_fn=<MeanBackward0>), tensor(0.3645, grad_fn=<MeanBackward0>), tensor(0.3681, grad_fn=<MeanBackward0>), tensor(0.3715, grad_fn=<MeanBackward0>), tensor(0.3729, grad_fn=<MeanBackward0>), tensor(0.3738, grad_fn=<MeanBackward0>), tensor(0.3776, grad_fn=<MeanBackward0>), tensor(0.3747, grad_fn=<MeanBackward0>), tensor(0.3774, grad_fn=<MeanBackward0>), tensor(0.3830, grad_fn=<MeanBackward0>), tensor(0.3842, grad_fn=<MeanBackward0>), tensor(0.3859, grad_fn=<MeanBackward0>), tensor(0.3946, grad_fn=<MeanBackward0>), tensor(0.3910, grad_fn=<MeanBackward0>), tensor(0.3880, grad_fn=<MeanBackward0>), tensor(0.3894, grad_fn=<MeanBackward0>), tensor(0.3958, grad_fn=<MeanBackward0>), tensor(0.3983, grad_fn=<MeanBackward0>), tensor(0.4041, grad_fn=<MeanBackward0>), tensor(0.3950, grad_fn=<MeanBackward0>), tensor(0.3985, grad_fn=<MeanBackward0>), tensor(0.4056, grad_fn=<MeanBackward0>), tensor(0.4044, grad_fn=<MeanBackward0>), tensor(0.3989, grad_fn=<MeanBackward0>), tensor(0.4044, grad_fn=<MeanBackward0>), tensor(0.4084, grad_fn=<MeanBackward0>), tensor(0.4113, grad_fn=<MeanBackward0>), tensor(0.4175, grad_fn=<MeanBackward0>), tensor(0.4126, grad_fn=<MeanBackward0>), tensor(0.4206, grad_fn=<MeanBackward0>), tensor(0.4174, grad_fn=<MeanBackward0>), tensor(0.4190, grad_fn=<MeanBackward0>), tensor(0.4205, grad_fn=<MeanBackward0>), tensor(0.4306, grad_fn=<MeanBackward0>), tensor(0.4327, grad_fn=<MeanBackward0>), tensor(0.4294, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.245490662753582, 0.2665325775742531, 0.2800970524549484, 0.2909601852297783, 0.3038611635565758, 0.3106103837490082, 0.31699584424495697, 0.32405439019203186, 0.33098888397216797, 0.3359668552875519, 0.34152156859636307, 0.3477576896548271, 0.3514605611562729, 0.35779203474521637, 0.3592500165104866, 0.36414529383182526, 0.3683718219399452, 0.3711651489138603, 0.374090239405632, 0.37777721881866455, 0.3788049668073654, 0.3813071846961975, 0.38500411808490753, 0.38830310106277466, 0.3908807933330536, 0.3911946937441826, 0.3967643305659294, 0.3978904187679291, 0.3974302038550377, 0.4003729596734047, 0.4021322950720787, 0.4051128402352333, 0.40576814115047455, 0.40517374128103256, 0.4070633202791214, 0.41128092259168625, 0.4114430844783783, 0.4106030911207199, 0.415207177400589, 0.41475196182727814, 0.4160715192556381, 0.4186890199780464, 0.418548159301281, 0.42023955285549164, 0.4194611757993698, 0.4199855625629425, 0.4229374974966049, 0.4261610582470894, 0.4255417138338089, 0.42472393065690994]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}