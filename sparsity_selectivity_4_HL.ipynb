{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparsity_selectivity_4_HL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/sparsity_selectivity_4_HL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7STrWa0P3z_",
        "outputId": "ac7650a9-d676-40ab-e9c1-554fe2f7749d"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYoWaxyTpG6m",
        "outputId": "17ac81e6-a92e-456f-efe5-4a0c280e1158"
      },
      "source": [
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "root_dir = './'\n",
        "torchvision.datasets.MNIST(root=root_dir,download=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-16 20:36:48--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
            "--2021-03-16 20:36:48--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘MNIST.tar.gz.2’\n",
            "\n",
            "MNIST.tar.gz.2          [          <=>       ]  33.20M  15.6MB/s    in 2.1s    \n",
            "\n",
            "2021-03-16 20:36:51 (15.6 MB/s) - ‘MNIST.tar.gz.2’ saved [34813078]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte\n",
            "MNIST/raw/train-images-idx3-ubyte.gz\n",
            "MNIST/processed/\n",
            "MNIST/processed/training.pt\n",
            "MNIST/processed/test.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./\n",
              "    Split: Train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4j9WoP-UnAm"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTW5TOUnP5XY"
      },
      "source": [
        "mnist_trainset = torchvision.datasets.MNIST(root=root_dir, train=True, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "mnist_testset  = torchvision.datasets.MNIST(root=root_dir, \n",
        "                                train=False, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(mnist_trainset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=True)\n",
        "\n",
        "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXTkEUJ5P6kU"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# Define the model \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # modify this section for later use \n",
        "        self.linear_1 = torch.nn.Linear(784, 256)\n",
        "        self.linear_2 = torch.nn.Linear(256, 256)\n",
        "        self.linear_3 = torch.nn.Linear(256, 256)\n",
        "        self.linear_4 = torch.nn.Linear(256, 256)\n",
        "        self.linear_5 = torch.nn.Linear(256, 10)\n",
        "        self.sigmoid12  = torch.nn.Sigmoid()\n",
        "        self.sigmoid23  = torch.nn.Sigmoid()\n",
        "        self.sigmoid34  = torch.nn.Sigmoid()\n",
        "        self.sigmoid45  = torch.nn.Sigmoid()\n",
        "\n",
        "        self.layer_activations = dict()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # modify this section for later use \n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.sigmoid12(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.sigmoid23(x)\n",
        "        x = self.linear_3(x)\n",
        "        x = self.sigmoid34(x)\n",
        "        x = self.linear_4(x)\n",
        "        x = self.sigmoid45(x)\n",
        "        pred = self.linear_5(x)\n",
        "        return pred\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfgvKH6eP9Ou"
      },
      "source": [
        "def get_activation(model, layer_name):    \n",
        "    def hook(module, input, output):\n",
        "        model.layer_activations[layer_name] = output\n",
        "    return hook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOAaaXKupOys"
      },
      "source": [
        "def sparsity_calculator(final_spareness):\n",
        "    sparseness_list = list()\n",
        "    for single_epoch_spareness in final_spareness:\n",
        "\n",
        "        hidden_layer_activation_list = single_epoch_spareness\n",
        "        hidden_layer_activation_list = torch.stack(hidden_layer_activation_list)\n",
        "        layer_activations_list = torch.reshape(hidden_layer_activation_list, (10000, 256))\n",
        "\n",
        "        layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
        "        num_neurons = layer_activations_list.shape[1]\n",
        "        population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
        "        mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
        "\n",
        "        sparseness_list.append(mean_sparseness_per_epoch)\n",
        "\n",
        "    return sparseness_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvHGO5RSvi6I"
      },
      "source": [
        "def selectivity(hidden_layer_each_neuron):\n",
        "    __selectivity__ = list()\n",
        "    # I will now try to find the average of each class for each neuron.\n",
        "    # check out the next cell \n",
        "    avg_activations = [dict() for x in range(256)]\n",
        "    for i, neuron in enumerate(hidden_layer_each_neuron):\n",
        "        for k, v in neuron.items():\n",
        "            # v is the list of activations for hidden layer's neuron k \n",
        "            avg_activations[i][k] = sum(v) / float(len(v))\n",
        "\n",
        "    # generate 256 lists to get only values in avg_activations\n",
        "    only_activation_vals = [list() for x in range(256)]\n",
        "\n",
        "    # get only values from avg_activations\n",
        "    for i, avg_activation in enumerate(avg_activations):\n",
        "        for value in avg_activation.values():\n",
        "            only_activation_vals[i].append(value)\n",
        "\n",
        "    for activation_val in only_activation_vals:\n",
        "        # find u_max \n",
        "        u_max = np.max(activation_val)\n",
        "\n",
        "        # find u_minus_max \n",
        "        u_minus_max = (np.sum(activation_val) - u_max) / 9\n",
        "\n",
        "        # find selectivity \n",
        "        selectivity = (u_max - u_minus_max) / (u_max + u_minus_max)\n",
        "\n",
        "        # append selectivity value to selectivity\n",
        "        __selectivity__.append(selectivity)\n",
        "\n",
        "    avg_selectivity = np.average(__selectivity__)\n",
        "    std_selectivity = np.std(__selectivity__)\n",
        "                                 \n",
        "    return avg_selectivity, std_selectivity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcKnp4rEpQ1q"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# add a parameter to the function and calculate avg and std. Do not forget to change division by 2, 3, 4, or 5 \n",
        "def avg_std_calculator(_hidden_layer_each_neuron_12, _hidden_layer_each_neuron_23, _hidden_layer_each_neuron_34, _hidden_layer_each_neuron_45):\n",
        "\n",
        "    avg_selectivity12, std_selectivity12 = selectivity(_hidden_layer_each_neuron_12)\n",
        "    avg_selectivity23, std_selectivity23 = selectivity(_hidden_layer_each_neuron_23)\n",
        "    avg_selectivity34, std_selectivity34 = selectivity(_hidden_layer_each_neuron_34)\n",
        "    avg_selectivity45, std_selectivity45 = selectivity(_hidden_layer_each_neuron_45)\n",
        "\n",
        "    final_selectivity_avg = (avg_selectivity12 + avg_selectivity23 + avg_selectivity34 + avg_selectivity45) / 4\n",
        "    final_selecvitity_std = (std_selectivity12 + std_selectivity23 + std_selectivity34 + std_selectivity45) / 4\n",
        "\n",
        "    return final_selectivity_avg, final_selecvitity_std, avg_selectivity12, std_selectivity12, avg_selectivity23, std_selectivity23, avg_selectivity34, std_selectivity34, avg_selectivity45, std_selectivity45 \n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5PUiBNqUImf"
      },
      "source": [
        "def model_factory(optimizer_name):\n",
        "    '''\n",
        "    optimizer_name : choose one of Adagrad, Adadelta, SGD, and Adam \n",
        "\n",
        "    '''\n",
        "    my_model = Model()\n",
        "    print(\"my_model:\", my_model)\n",
        "    my_model.to(device)\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    # chagen sigmoid34 an 's34'\n",
        "    my_model.sigmoid12.register_forward_hook(get_activation(my_model, 's12'))\n",
        "    my_model.sigmoid23.register_forward_hook(get_activation(my_model, 's23'))\n",
        "    my_model.sigmoid34.register_forward_hook(get_activation(my_model, 's34'))\n",
        "    my_model.sigmoid45.register_forward_hook(get_activation(my_model, 's45'))\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        my_optimizer = torch.optim.Adadelta(my_model.parameters(), lr=1.0)\n",
        "\n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        my_optimizer = torch.optim.Adagrad(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        my_optimizer = torch.optim.SGD(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        my_optimizer = torch.optim.Adam(my_model.parameters(), lr=0.001)\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")\n",
        "    \n",
        "    print(\"my_optimizer:\", my_optimizer)\n",
        "    test_acc, sparseness_list, spar12, spar23, spar34, spar45, selectivity_list_avg, selectivity_list_std, selec_12_avg, selec_12_std, selec_23_avg, selec_23_std, selec_34_avg, selec_34_std, selec_45_avg, selec_45_std = selectivity_trainer(optimizer=my_optimizer, model=my_model)\n",
        "    # ************* modify this section for later use *************\n",
        "    # change name of the file \n",
        "    file_saver = open(f\"4HL_sparsity_selectivity_new_{optimizer_name}.txt\", \"w\")\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver.write(str(test_acc)+'\\n'+str(sparseness_list)+'\\n'+str(spar12)+'\\n'+str(spar23)+'\\n'+str(spar34)+'\\n'+str(spar45)+'\\n'+str(selectivity_list_avg)+'\\n'+str(selectivity_list_std)+'\\n'+str(selec_12_avg)+'\\n'+str(selec_12_std)+'\\n'+str(selec_23_avg)+'\\n'+str(selec_23_std)+'\\n'+str(selec_34_avg)+'\\n'+str(selec_34_std)+'\\n'+str(selec_45_avg)+'\\n'+str(selec_45_std)+'\\n\\n')\n",
        "    file_saver.close()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        !cp 4HL_sparsity_selectivity_new_Adadelta.txt /content/drive/MyDrive\n",
        "    \n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        !cp 4HL_sparsity_selectivity_new_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        !cp 4HL_sparsity_selectivity_new_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        !cp 4HL_sparsity_selectivity_new_Adam.txt /content/drive/MyDrive\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXOpwTXEQFKY"
      },
      "source": [
        "no_epochs = 50\n",
        "def selectivity_trainer(optimizer, model):\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    train_loss = list()\n",
        "    test_loss  = list()\n",
        "    test_acc   = list()\n",
        "\n",
        "    best_test_loss = 1\n",
        "\n",
        "    selectivity_avg_list = list()\n",
        "    selectivity_std_list = list()\n",
        "\n",
        "    selec_12_avg = list()\n",
        "    selec_12_std = list()\n",
        "    selec_23_avg = list()\n",
        "    selec_23_std = list()\n",
        "    selec_34_avg = list()\n",
        "    selec_34_std = list()\n",
        "    selec_45_avg = list()\n",
        "    selec_45_std = list()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    final_spareness_12 = list()\n",
        "    final_spareness_23 = list()\n",
        "    final_spareness_34 = list()\n",
        "    final_spareness_45 = list()\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    for epoch in range(no_epochs):\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_each_neuron_12 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_12 = np.array(hidden_layer_each_neuron_12)\n",
        "\n",
        "        hidden_layer_each_neuron_23 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_23 = np.array(hidden_layer_each_neuron_23)\n",
        "\n",
        "        hidden_layer_each_neuron_34 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_34 = np.array(hidden_layer_each_neuron_34)\n",
        "\n",
        "        hidden_layer_each_neuron_45 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_45 = np.array(hidden_layer_each_neuron_45)\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_activation_list_12 = list()\n",
        "        hidden_layer_activation_list_23 = list()\n",
        "        hidden_layer_activation_list_34 = list()\n",
        "        hidden_layer_activation_list_45 = list()\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        total_train_loss = 0\n",
        "        total_test_loss = 0\n",
        "\n",
        "        # training\n",
        "        # set up training mode \n",
        "        model.train()\n",
        "\n",
        "        for itr, (images, labels) in enumerate(train_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_train_loss = total_train_loss / (itr + 1)\n",
        "        train_loss.append(total_train_loss)\n",
        "\n",
        "        # testing \n",
        "        # change to evaluation mode \n",
        "        model.eval()\n",
        "        total = 0\n",
        "        for itr, (images, labels) in enumerate(test_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # we now need softmax because we are testing.\n",
        "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "            for i, p in enumerate(pred):\n",
        "                if labels[i] == torch.max(p.data, 0)[1]:\n",
        "                    total = total + 1\n",
        "\n",
        "            # ***************** sparsity calculation ***************** #\n",
        "            hidden_layer_activation_list_12.append(model.layer_activations['s12'])\n",
        "            hidden_layer_activation_list_23.append(model.layer_activations['s23'])\n",
        "            hidden_layer_activation_list_34.append(model.layer_activations['s34'])\n",
        "            hidden_layer_activation_list_45.append(model.layer_activations['s45'])\n",
        "\n",
        "            # ************* modify this section for later use *************\n",
        "            # Do not forget to change hidden_layer_each_neuron_12 name \n",
        "            for activation, label in zip(model.layer_activations['s12'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_12[i][label].append(activation[i])\n",
        "\n",
        "            for activation, label in zip(model.layer_activations['s23'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_23[i][label].append(activation[i])\n",
        "            \n",
        "            for activation, label in zip(model.layer_activations['s34'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_34[i][label].append(activation[i])\n",
        "\n",
        "            for activation, label in zip(model.layer_activations['s45'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_45[i][label].append(activation[i])\n",
        "\n",
        "        # Add one more parameter \n",
        "        selectivity_avg, selectivity_std, avg_selectivity12, std_selectivity12, avg_selectivity23, std_selectivity23, avg_selectivity34, std_selectivity34, avg_selectivity45, std_selectivity45 = avg_std_calculator(hidden_layer_each_neuron_12, hidden_layer_each_neuron_23, hidden_layer_each_neuron_34, hidden_layer_each_neuron_45)\n",
        "        # ************* modify this section for later use *************\n",
        "        \n",
        "        selec_12_avg.append(avg_selectivity12)\n",
        "        selec_12_std.append(std_selectivity12)\n",
        "        selec_23_avg.append(avg_selectivity23)\n",
        "        selec_23_std.append(std_selectivity23)\n",
        "        selec_34_avg.append(avg_selectivity34)\n",
        "        selec_34_std.append(std_selectivity34)\n",
        "        selec_45_avg.append(avg_selectivity45)\n",
        "        selec_45_std.append(std_selectivity45)\n",
        "\n",
        "        selectivity_avg_list.append(selectivity_avg)\n",
        "        selectivity_std_list.append(selectivity_std)\n",
        "\n",
        "        final_spareness_12.append(hidden_layer_activation_list_12)\n",
        "        final_spareness_23.append(hidden_layer_activation_list_23)\n",
        "        final_spareness_34.append(hidden_layer_activation_list_34)\n",
        "        final_spareness_45.append(hidden_layer_activation_list_45)\n",
        "        # ***************** sparsity calculation ***************** #\n",
        "\n",
        "        # caculate accuracy \n",
        "        accuracy = total / len(mnist_testset)\n",
        "\n",
        "        # append accuracy here\n",
        "        test_acc.append(accuracy)\n",
        "\n",
        "        # append test loss here \n",
        "        total_test_loss = total_test_loss / (itr + 1)\n",
        "        test_loss.append(total_test_loss)\n",
        "\n",
        "        print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Test Loss: {:.8f}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, total_train_loss, total_test_loss, accuracy))\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "    sparsity_list12 = sparsity_calculator(final_spareness_12)\n",
        "    sparsity_list23 = sparsity_calculator(final_spareness_23)\n",
        "    sparsity_list34 = sparsity_calculator(final_spareness_34)\n",
        "    sparsity_list45 = sparsity_calculator(final_spareness_45)\n",
        "\n",
        "    print(sparsity_list12)\n",
        "    print(sparsity_list23)\n",
        "    print(sparsity_list34)\n",
        "    print(sparsity_list45)\n",
        "\n",
        "    average_sparsity = list()\n",
        "    sparsity12 = list()\n",
        "    sparsity23 = list()\n",
        "    sparsity34 = list()\n",
        "    sparsity45 = list()\n",
        "\n",
        "    for i in range(no_epochs):\n",
        "        sparsity12.append(sparsity_list12[i].item())\n",
        "        sparsity23.append(sparsity_list23[i].item())\n",
        "        sparsity34.append(sparsity_list34[i].item())\n",
        "        sparsity45.append(sparsity_list45[i].item())\n",
        "        average_sparsity.append( (sparsity_list12[i].item() + sparsity_list23[i].item() + sparsity_list34[i].item() + sparsity_list45[i].item()) / 4 )\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "\n",
        "    print(\"average_sparsity:\", average_sparsity)\n",
        "\n",
        "    return test_acc, average_sparsity, sparsity12, sparsity23, sparsity34, sparsity45, selectivity_avg_list, selectivity_std_list, selec_12_avg, selec_12_std, selec_23_avg, selec_23_std, selec_34_avg, selec_34_std, selec_45_avg, selec_45_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmLiu0sSp-CJ",
        "outputId": "e0d09d59-8256-4a0d-80b5-4e777a3284c0"
      },
      "source": [
        "model_factory('Adadelta')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-06\n",
            "    lr: 1.0\n",
            "    rho: 0.9\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.30746806, Test Loss: 2.30202827, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30127321, Test Loss: 2.29988618, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.18322148, Test Loss: 1.78346998, Test Accuracy: 0.35970000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 1.38807317, Test Loss: 1.05196509, Test Accuracy: 0.58940000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.73925288, Test Loss: 0.40720915, Test Accuracy: 0.89020000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.35055134, Test Loss: 0.30235354, Test Accuracy: 0.91540000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.22971163, Test Loss: 0.18080255, Test Accuracy: 0.94850000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.16830976, Test Loss: 0.16070983, Test Accuracy: 0.95200000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.13468076, Test Loss: 0.14689419, Test Accuracy: 0.95620000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.11125832, Test Loss: 0.13769227, Test Accuracy: 0.96050000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.09575471, Test Loss: 0.12158668, Test Accuracy: 0.96510000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.08345701, Test Loss: 0.10658812, Test Accuracy: 0.96770000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.07242749, Test Loss: 0.10226166, Test Accuracy: 0.97110000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.06346309, Test Loss: 0.12266047, Test Accuracy: 0.96430000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.05714226, Test Loss: 0.11184846, Test Accuracy: 0.97000000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.04942244, Test Loss: 0.09444458, Test Accuracy: 0.97360000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.04504926, Test Loss: 0.08744071, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.03980238, Test Loss: 0.09039968, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.03666457, Test Loss: 0.09473740, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.03226412, Test Loss: 0.08644524, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.02791062, Test Loss: 0.09336870, Test Accuracy: 0.97580000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.02439865, Test Loss: 0.10836605, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.02249776, Test Loss: 0.09321603, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.01903939, Test Loss: 0.09567913, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.01696302, Test Loss: 0.09537915, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.01522317, Test Loss: 0.09634081, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.01188203, Test Loss: 0.10121603, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.01161051, Test Loss: 0.09870636, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00893058, Test Loss: 0.09959495, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00833516, Test Loss: 0.12736073, Test Accuracy: 0.97410000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00774268, Test Loss: 0.11226941, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00714412, Test Loss: 0.10984368, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00553139, Test Loss: 0.11530146, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00521357, Test Loss: 0.10872274, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00435523, Test Loss: 0.11609443, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00414847, Test Loss: 0.11266749, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00290722, Test Loss: 0.11466608, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00279187, Test Loss: 0.11495049, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00241140, Test Loss: 0.11872076, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00187569, Test Loss: 0.12077873, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00167831, Test Loss: 0.12387241, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00149886, Test Loss: 0.13416952, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00119602, Test Loss: 0.12152271, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00110594, Test Loss: 0.12464444, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00093754, Test Loss: 0.12232344, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00115545, Test Loss: 0.12480746, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00093265, Test Loss: 0.13523230, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00087843, Test Loss: 0.12731616, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00091637, Test Loss: 0.12694788, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00066378, Test Loss: 0.12831847, Test Accuracy: 0.97920000\n",
            "[tensor(0.0054, grad_fn=<MeanBackward0>), tensor(0.0078, grad_fn=<MeanBackward0>), tensor(0.0597, grad_fn=<MeanBackward0>), tensor(0.2470, grad_fn=<MeanBackward0>), tensor(0.3866, grad_fn=<MeanBackward0>), tensor(0.4355, grad_fn=<MeanBackward0>), tensor(0.4512, grad_fn=<MeanBackward0>), tensor(0.4639, grad_fn=<MeanBackward0>), tensor(0.4669, grad_fn=<MeanBackward0>), tensor(0.4713, grad_fn=<MeanBackward0>), tensor(0.4783, grad_fn=<MeanBackward0>), tensor(0.4782, grad_fn=<MeanBackward0>), tensor(0.4864, grad_fn=<MeanBackward0>), tensor(0.4855, grad_fn=<MeanBackward0>), tensor(0.4901, grad_fn=<MeanBackward0>), tensor(0.4931, grad_fn=<MeanBackward0>), tensor(0.4926, grad_fn=<MeanBackward0>), tensor(0.4947, grad_fn=<MeanBackward0>), tensor(0.4964, grad_fn=<MeanBackward0>), tensor(0.4953, grad_fn=<MeanBackward0>), tensor(0.4937, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.4984, grad_fn=<MeanBackward0>), tensor(0.4980, grad_fn=<MeanBackward0>), tensor(0.4993, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.5000, grad_fn=<MeanBackward0>), tensor(0.5003, grad_fn=<MeanBackward0>), tensor(0.4990, grad_fn=<MeanBackward0>), tensor(0.5004, grad_fn=<MeanBackward0>), tensor(0.4991, grad_fn=<MeanBackward0>), tensor(0.4988, grad_fn=<MeanBackward0>), tensor(0.4986, grad_fn=<MeanBackward0>), tensor(0.5010, grad_fn=<MeanBackward0>), tensor(0.5001, grad_fn=<MeanBackward0>), tensor(0.4996, grad_fn=<MeanBackward0>), tensor(0.4997, grad_fn=<MeanBackward0>), tensor(0.5002, grad_fn=<MeanBackward0>), tensor(0.4990, grad_fn=<MeanBackward0>), tensor(0.4987, grad_fn=<MeanBackward0>), tensor(0.4990, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.4974, grad_fn=<MeanBackward0>), tensor(0.4971, grad_fn=<MeanBackward0>), tensor(0.4963, grad_fn=<MeanBackward0>), tensor(0.4968, grad_fn=<MeanBackward0>), tensor(0.4954, grad_fn=<MeanBackward0>), tensor(0.4957, grad_fn=<MeanBackward0>), tensor(0.4947, grad_fn=<MeanBackward0>), tensor(0.4959, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0556, grad_fn=<MeanBackward0>), tensor(0.1241, grad_fn=<MeanBackward0>), tensor(0.1722, grad_fn=<MeanBackward0>), tensor(0.2126, grad_fn=<MeanBackward0>), tensor(0.2321, grad_fn=<MeanBackward0>), tensor(0.2425, grad_fn=<MeanBackward0>), tensor(0.2504, grad_fn=<MeanBackward0>), tensor(0.2448, grad_fn=<MeanBackward0>), tensor(0.2561, grad_fn=<MeanBackward0>), tensor(0.2542, grad_fn=<MeanBackward0>), tensor(0.2559, grad_fn=<MeanBackward0>), tensor(0.2572, grad_fn=<MeanBackward0>), tensor(0.2665, grad_fn=<MeanBackward0>), tensor(0.2631, grad_fn=<MeanBackward0>), tensor(0.2645, grad_fn=<MeanBackward0>), tensor(0.2626, grad_fn=<MeanBackward0>), tensor(0.2641, grad_fn=<MeanBackward0>), tensor(0.2623, grad_fn=<MeanBackward0>), tensor(0.2689, grad_fn=<MeanBackward0>), tensor(0.2643, grad_fn=<MeanBackward0>), tensor(0.2626, grad_fn=<MeanBackward0>), tensor(0.2623, grad_fn=<MeanBackward0>), tensor(0.2604, grad_fn=<MeanBackward0>), tensor(0.2614, grad_fn=<MeanBackward0>), tensor(0.2594, grad_fn=<MeanBackward0>), tensor(0.2573, grad_fn=<MeanBackward0>), tensor(0.2528, grad_fn=<MeanBackward0>), tensor(0.2539, grad_fn=<MeanBackward0>), tensor(0.2568, grad_fn=<MeanBackward0>), tensor(0.2515, grad_fn=<MeanBackward0>), tensor(0.2485, grad_fn=<MeanBackward0>), tensor(0.2485, grad_fn=<MeanBackward0>), tensor(0.2487, grad_fn=<MeanBackward0>), tensor(0.2503, grad_fn=<MeanBackward0>), tensor(0.2485, grad_fn=<MeanBackward0>), tensor(0.2459, grad_fn=<MeanBackward0>), tensor(0.2466, grad_fn=<MeanBackward0>), tensor(0.2423, grad_fn=<MeanBackward0>), tensor(0.2456, grad_fn=<MeanBackward0>), tensor(0.2442, grad_fn=<MeanBackward0>), tensor(0.2417, grad_fn=<MeanBackward0>), tensor(0.2400, grad_fn=<MeanBackward0>), tensor(0.2407, grad_fn=<MeanBackward0>), tensor(0.2397, grad_fn=<MeanBackward0>), tensor(0.2380, grad_fn=<MeanBackward0>), tensor(0.2379, grad_fn=<MeanBackward0>), tensor(0.2387, grad_fn=<MeanBackward0>), tensor(0.2369, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0097, grad_fn=<MeanBackward0>), tensor(0.0070, grad_fn=<MeanBackward0>), tensor(0.0459, grad_fn=<MeanBackward0>), tensor(0.2062, grad_fn=<MeanBackward0>), tensor(0.2501, grad_fn=<MeanBackward0>), tensor(0.2244, grad_fn=<MeanBackward0>), tensor(0.2312, grad_fn=<MeanBackward0>), tensor(0.2474, grad_fn=<MeanBackward0>), tensor(0.2480, grad_fn=<MeanBackward0>), tensor(0.2218, grad_fn=<MeanBackward0>), tensor(0.2491, grad_fn=<MeanBackward0>), tensor(0.2435, grad_fn=<MeanBackward0>), tensor(0.2594, grad_fn=<MeanBackward0>), tensor(0.2383, grad_fn=<MeanBackward0>), tensor(0.2686, grad_fn=<MeanBackward0>), tensor(0.2571, grad_fn=<MeanBackward0>), tensor(0.2511, grad_fn=<MeanBackward0>), tensor(0.2618, grad_fn=<MeanBackward0>), tensor(0.2489, grad_fn=<MeanBackward0>), tensor(0.2520, grad_fn=<MeanBackward0>), tensor(0.2580, grad_fn=<MeanBackward0>), tensor(0.2727, grad_fn=<MeanBackward0>), tensor(0.2611, grad_fn=<MeanBackward0>), tensor(0.2641, grad_fn=<MeanBackward0>), tensor(0.2590, grad_fn=<MeanBackward0>), tensor(0.2691, grad_fn=<MeanBackward0>), tensor(0.2755, grad_fn=<MeanBackward0>), tensor(0.2791, grad_fn=<MeanBackward0>), tensor(0.2784, grad_fn=<MeanBackward0>), tensor(0.2960, grad_fn=<MeanBackward0>), tensor(0.2906, grad_fn=<MeanBackward0>), tensor(0.2764, grad_fn=<MeanBackward0>), tensor(0.2752, grad_fn=<MeanBackward0>), tensor(0.2889, grad_fn=<MeanBackward0>), tensor(0.2943, grad_fn=<MeanBackward0>), tensor(0.2964, grad_fn=<MeanBackward0>), tensor(0.2951, grad_fn=<MeanBackward0>), tensor(0.2958, grad_fn=<MeanBackward0>), tensor(0.2987, grad_fn=<MeanBackward0>), tensor(0.2933, grad_fn=<MeanBackward0>), tensor(0.3024, grad_fn=<MeanBackward0>), tensor(0.3060, grad_fn=<MeanBackward0>), tensor(0.2993, grad_fn=<MeanBackward0>), tensor(0.2968, grad_fn=<MeanBackward0>), tensor(0.2998, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3022, grad_fn=<MeanBackward0>), tensor(0.3024, grad_fn=<MeanBackward0>), tensor(0.3048, grad_fn=<MeanBackward0>), tensor(0.3009, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0020, grad_fn=<MeanBackward0>), tensor(0.0044, grad_fn=<MeanBackward0>), tensor(0.4402, grad_fn=<MeanBackward0>), tensor(0.5325, grad_fn=<MeanBackward0>), tensor(0.5191, grad_fn=<MeanBackward0>), tensor(0.5348, grad_fn=<MeanBackward0>), tensor(0.5508, grad_fn=<MeanBackward0>), tensor(0.5308, grad_fn=<MeanBackward0>), tensor(0.5373, grad_fn=<MeanBackward0>), tensor(0.5763, grad_fn=<MeanBackward0>), tensor(0.5497, grad_fn=<MeanBackward0>), tensor(0.5700, grad_fn=<MeanBackward0>), tensor(0.5578, grad_fn=<MeanBackward0>), tensor(0.5734, grad_fn=<MeanBackward0>), tensor(0.5504, grad_fn=<MeanBackward0>), tensor(0.5682, grad_fn=<MeanBackward0>), tensor(0.5699, grad_fn=<MeanBackward0>), tensor(0.5607, grad_fn=<MeanBackward0>), tensor(0.5811, grad_fn=<MeanBackward0>), tensor(0.5779, grad_fn=<MeanBackward0>), tensor(0.5733, grad_fn=<MeanBackward0>), tensor(0.5605, grad_fn=<MeanBackward0>), tensor(0.5779, grad_fn=<MeanBackward0>), tensor(0.5730, grad_fn=<MeanBackward0>), tensor(0.5815, grad_fn=<MeanBackward0>), tensor(0.5695, grad_fn=<MeanBackward0>), tensor(0.5699, grad_fn=<MeanBackward0>), tensor(0.5683, grad_fn=<MeanBackward0>), tensor(0.5717, grad_fn=<MeanBackward0>), tensor(0.5505, grad_fn=<MeanBackward0>), tensor(0.5592, grad_fn=<MeanBackward0>), tensor(0.5703, grad_fn=<MeanBackward0>), tensor(0.5780, grad_fn=<MeanBackward0>), tensor(0.5608, grad_fn=<MeanBackward0>), tensor(0.5567, grad_fn=<MeanBackward0>), tensor(0.5532, grad_fn=<MeanBackward0>), tensor(0.5547, grad_fn=<MeanBackward0>), tensor(0.5561, grad_fn=<MeanBackward0>), tensor(0.5495, grad_fn=<MeanBackward0>), tensor(0.5578, grad_fn=<MeanBackward0>), tensor(0.5470, grad_fn=<MeanBackward0>), tensor(0.5454, grad_fn=<MeanBackward0>), tensor(0.5532, grad_fn=<MeanBackward0>), tensor(0.5559, grad_fn=<MeanBackward0>), tensor(0.5532, grad_fn=<MeanBackward0>), tensor(0.5520, grad_fn=<MeanBackward0>), tensor(0.5530, grad_fn=<MeanBackward0>), tensor(0.5525, grad_fn=<MeanBackward0>), tensor(0.5488, grad_fn=<MeanBackward0>), tensor(0.5552, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.007432477781549096, 0.007616111310198903, 0.15035023167729378, 0.27743260003626347, 0.33196355402469635, 0.3518451191484928, 0.3663039058446884, 0.3711462840437889, 0.3756405636668205, 0.37856297194957733, 0.38328251242637634, 0.38648318499326706, 0.38989969342947006, 0.3886025659739971, 0.3938790634274483, 0.3953825682401657, 0.3945397213101387, 0.39492326974868774, 0.3976452127099037, 0.39690595865249634, 0.3985053300857544, 0.39890460669994354, 0.39997973293066025, 0.39935216307640076, 0.4000226557254791, 0.3995169624686241, 0.40120695531368256, 0.40125924348831177, 0.400505967438221, 0.40019381791353226, 0.40144745260477066, 0.39925579726696014, 0.40008214116096497, 0.39980510622262955, 0.3999709002673626, 0.39988719671964645, 0.3994910307228565, 0.3994886018335819, 0.3984619006514549, 0.39804790914058685, 0.39850206300616264, 0.39840542525053024, 0.39792224764823914, 0.3974546156823635, 0.3975033834576607, 0.39780454710125923, 0.3971572630107403, 0.39710136502981186, 0.39673660323023796, 0.3972465954720974]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsO-S8yfp-EP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68625ffb-2bab-4518-e1c6-16c8d9f92a4b"
      },
      "source": [
        "model_factory('Adagrad')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adagrad (\n",
            "Parameter Group 0\n",
            "    eps: 1e-10\n",
            "    initial_accumulator_value: 0\n",
            "    lr: 0.1\n",
            "    lr_decay: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.41819022, Test Loss: 2.32539536, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.32925108, Test Loss: 2.33123923, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.32166392, Test Loss: 2.32314516, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.31882394, Test Loss: 2.32085734, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.31751979, Test Loss: 2.32146750, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.31592446, Test Loss: 2.31645982, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.31481103, Test Loss: 2.31098961, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.31389957, Test Loss: 2.31366414, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.31191634, Test Loss: 2.31590306, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.31286573, Test Loss: 2.30808157, Test Accuracy: 0.08920000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.31184440, Test Loss: 2.32162372, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.31107149, Test Loss: 2.31959062, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.31080975, Test Loss: 2.32261588, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.31069644, Test Loss: 2.31980920, Test Accuracy: 0.08920000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.30973258, Test Loss: 2.30672049, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.30907274, Test Loss: 2.30789469, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.30989215, Test Loss: 2.31089048, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.06967501, Test Loss: 1.79579359, Test Accuracy: 0.30290000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 1.36391798, Test Loss: 0.68855165, Test Accuracy: 0.76890000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.30257350, Test Loss: 0.17812443, Test Accuracy: 0.95210000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.12702743, Test Loss: 0.12733839, Test Accuracy: 0.96650000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.08497407, Test Loss: 0.12222327, Test Accuracy: 0.96680000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.06324314, Test Loss: 0.10979649, Test Accuracy: 0.97100000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.04882699, Test Loss: 0.11148373, Test Accuracy: 0.97320000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.03828752, Test Loss: 0.10659371, Test Accuracy: 0.97300000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.03004818, Test Loss: 0.10611726, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.02447630, Test Loss: 0.11308604, Test Accuracy: 0.97280000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.01982690, Test Loss: 0.11356062, Test Accuracy: 0.97470000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.01734117, Test Loss: 0.11529768, Test Accuracy: 0.97410000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.01404227, Test Loss: 0.12361706, Test Accuracy: 0.97370000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.01252323, Test Loss: 0.12372039, Test Accuracy: 0.97480000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00998399, Test Loss: 0.12418616, Test Accuracy: 0.97480000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00903993, Test Loss: 0.12977210, Test Accuracy: 0.97480000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00758339, Test Loss: 0.13564425, Test Accuracy: 0.97330000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00760029, Test Loss: 0.13117826, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00612100, Test Loss: 0.13309895, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00525761, Test Loss: 0.13545938, Test Accuracy: 0.97490000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00410157, Test Loss: 0.13759412, Test Accuracy: 0.97550000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00373752, Test Loss: 0.14452383, Test Accuracy: 0.97360000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00341219, Test Loss: 0.14853351, Test Accuracy: 0.97380000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00303024, Test Loss: 0.15659665, Test Accuracy: 0.97160000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00265175, Test Loss: 0.14970841, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00241900, Test Loss: 0.15266190, Test Accuracy: 0.97390000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00223889, Test Loss: 0.15512001, Test Accuracy: 0.97440000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00210107, Test Loss: 0.16041171, Test Accuracy: 0.97350000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00196907, Test Loss: 0.15874036, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00179892, Test Loss: 0.16095162, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00165242, Test Loss: 0.16210078, Test Accuracy: 0.97440000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00150407, Test Loss: 0.16595333, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00146326, Test Loss: 0.16547024, Test Accuracy: 0.97420000\n",
            "[tensor(0.2704, grad_fn=<MeanBackward0>), tensor(0.2705, grad_fn=<MeanBackward0>), tensor(0.2705, grad_fn=<MeanBackward0>), tensor(0.2706, grad_fn=<MeanBackward0>), tensor(0.2707, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.3013, grad_fn=<MeanBackward0>), tensor(0.4508, grad_fn=<MeanBackward0>), tensor(0.4567, grad_fn=<MeanBackward0>), tensor(0.4476, grad_fn=<MeanBackward0>), tensor(0.4509, grad_fn=<MeanBackward0>), tensor(0.4498, grad_fn=<MeanBackward0>), tensor(0.4509, grad_fn=<MeanBackward0>), tensor(0.4511, grad_fn=<MeanBackward0>), tensor(0.4531, grad_fn=<MeanBackward0>), tensor(0.4507, grad_fn=<MeanBackward0>), tensor(0.4520, grad_fn=<MeanBackward0>), tensor(0.4505, grad_fn=<MeanBackward0>), tensor(0.4514, grad_fn=<MeanBackward0>), tensor(0.4517, grad_fn=<MeanBackward0>), tensor(0.4521, grad_fn=<MeanBackward0>), tensor(0.4515, grad_fn=<MeanBackward0>), tensor(0.4523, grad_fn=<MeanBackward0>), tensor(0.4532, grad_fn=<MeanBackward0>), tensor(0.4521, grad_fn=<MeanBackward0>), tensor(0.4517, grad_fn=<MeanBackward0>), tensor(0.4520, grad_fn=<MeanBackward0>), tensor(0.4516, grad_fn=<MeanBackward0>), tensor(0.4512, grad_fn=<MeanBackward0>), tensor(0.4516, grad_fn=<MeanBackward0>), tensor(0.4514, grad_fn=<MeanBackward0>), tensor(0.4516, grad_fn=<MeanBackward0>), tensor(0.4513, grad_fn=<MeanBackward0>), tensor(0.4514, grad_fn=<MeanBackward0>), tensor(0.4512, grad_fn=<MeanBackward0>), tensor(0.4511, grad_fn=<MeanBackward0>), tensor(0.4514, grad_fn=<MeanBackward0>), tensor(0.4517, grad_fn=<MeanBackward0>), tensor(0.4516, grad_fn=<MeanBackward0>), tensor(0.4518, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3174, grad_fn=<MeanBackward0>), tensor(0.3175, grad_fn=<MeanBackward0>), tensor(0.3176, grad_fn=<MeanBackward0>), tensor(0.3178, grad_fn=<MeanBackward0>), tensor(0.3178, grad_fn=<MeanBackward0>), tensor(0.3140, grad_fn=<MeanBackward0>), tensor(0.3140, grad_fn=<MeanBackward0>), tensor(0.3141, grad_fn=<MeanBackward0>), tensor(0.3141, grad_fn=<MeanBackward0>), tensor(0.3142, grad_fn=<MeanBackward0>), tensor(0.3142, grad_fn=<MeanBackward0>), tensor(0.3143, grad_fn=<MeanBackward0>), tensor(0.3143, grad_fn=<MeanBackward0>), tensor(0.3144, grad_fn=<MeanBackward0>), tensor(0.3144, grad_fn=<MeanBackward0>), tensor(0.3145, grad_fn=<MeanBackward0>), tensor(0.3164, grad_fn=<MeanBackward0>), tensor(0.4237, grad_fn=<MeanBackward0>), tensor(0.4780, grad_fn=<MeanBackward0>), tensor(0.4874, grad_fn=<MeanBackward0>), tensor(0.4924, grad_fn=<MeanBackward0>), tensor(0.5023, grad_fn=<MeanBackward0>), tensor(0.5021, grad_fn=<MeanBackward0>), tensor(0.5004, grad_fn=<MeanBackward0>), tensor(0.4925, grad_fn=<MeanBackward0>), tensor(0.4954, grad_fn=<MeanBackward0>), tensor(0.4943, grad_fn=<MeanBackward0>), tensor(0.4949, grad_fn=<MeanBackward0>), tensor(0.4973, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.4948, grad_fn=<MeanBackward0>), tensor(0.4949, grad_fn=<MeanBackward0>), tensor(0.4959, grad_fn=<MeanBackward0>), tensor(0.4951, grad_fn=<MeanBackward0>), tensor(0.4957, grad_fn=<MeanBackward0>), tensor(0.4934, grad_fn=<MeanBackward0>), tensor(0.4941, grad_fn=<MeanBackward0>), tensor(0.4864, grad_fn=<MeanBackward0>), tensor(0.4892, grad_fn=<MeanBackward0>), tensor(0.4897, grad_fn=<MeanBackward0>), tensor(0.4905, grad_fn=<MeanBackward0>), tensor(0.4904, grad_fn=<MeanBackward0>), tensor(0.4877, grad_fn=<MeanBackward0>), tensor(0.4869, grad_fn=<MeanBackward0>), tensor(0.4889, grad_fn=<MeanBackward0>), tensor(0.4890, grad_fn=<MeanBackward0>), tensor(0.4893, grad_fn=<MeanBackward0>), tensor(0.4878, grad_fn=<MeanBackward0>), tensor(0.4848, grad_fn=<MeanBackward0>), tensor(0.4853, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3289, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3831, grad_fn=<MeanBackward0>), tensor(0.4928, grad_fn=<MeanBackward0>), tensor(0.5302, grad_fn=<MeanBackward0>), tensor(0.5366, grad_fn=<MeanBackward0>), tensor(0.5262, grad_fn=<MeanBackward0>), tensor(0.5304, grad_fn=<MeanBackward0>), tensor(0.5322, grad_fn=<MeanBackward0>), tensor(0.5325, grad_fn=<MeanBackward0>), tensor(0.5362, grad_fn=<MeanBackward0>), tensor(0.5420, grad_fn=<MeanBackward0>), tensor(0.5400, grad_fn=<MeanBackward0>), tensor(0.5378, grad_fn=<MeanBackward0>), tensor(0.5425, grad_fn=<MeanBackward0>), tensor(0.5392, grad_fn=<MeanBackward0>), tensor(0.5397, grad_fn=<MeanBackward0>), tensor(0.5306, grad_fn=<MeanBackward0>), tensor(0.5405, grad_fn=<MeanBackward0>), tensor(0.5345, grad_fn=<MeanBackward0>), tensor(0.5333, grad_fn=<MeanBackward0>), tensor(0.5354, grad_fn=<MeanBackward0>), tensor(0.5316, grad_fn=<MeanBackward0>), tensor(0.5321, grad_fn=<MeanBackward0>), tensor(0.5317, grad_fn=<MeanBackward0>), tensor(0.5273, grad_fn=<MeanBackward0>), tensor(0.5255, grad_fn=<MeanBackward0>), tensor(0.5208, grad_fn=<MeanBackward0>), tensor(0.5258, grad_fn=<MeanBackward0>), tensor(0.5242, grad_fn=<MeanBackward0>), tensor(0.5220, grad_fn=<MeanBackward0>), tensor(0.5203, grad_fn=<MeanBackward0>), tensor(0.5173, grad_fn=<MeanBackward0>), tensor(0.5183, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.4009, grad_fn=<MeanBackward0>), tensor(0.4158, grad_fn=<MeanBackward0>), tensor(0.4021, grad_fn=<MeanBackward0>), tensor(0.4036, grad_fn=<MeanBackward0>), tensor(0.4064, grad_fn=<MeanBackward0>), tensor(0.4054, grad_fn=<MeanBackward0>), tensor(0.4003, grad_fn=<MeanBackward0>), tensor(0.4077, grad_fn=<MeanBackward0>), tensor(0.4097, grad_fn=<MeanBackward0>), tensor(0.4080, grad_fn=<MeanBackward0>), tensor(0.4072, grad_fn=<MeanBackward0>), tensor(0.4086, grad_fn=<MeanBackward0>), tensor(0.4088, grad_fn=<MeanBackward0>), tensor(0.4082, grad_fn=<MeanBackward0>), tensor(0.4071, grad_fn=<MeanBackward0>), tensor(0.4039, grad_fn=<MeanBackward0>), tensor(0.4099, grad_fn=<MeanBackward0>), tensor(0.4112, grad_fn=<MeanBackward0>), tensor(0.4108, grad_fn=<MeanBackward0>), tensor(0.4103, grad_fn=<MeanBackward0>), tensor(0.4071, grad_fn=<MeanBackward0>), tensor(0.4072, grad_fn=<MeanBackward0>), tensor(0.4047, grad_fn=<MeanBackward0>), tensor(0.4059, grad_fn=<MeanBackward0>), tensor(0.4072, grad_fn=<MeanBackward0>), tensor(0.4071, grad_fn=<MeanBackward0>), tensor(0.4066, grad_fn=<MeanBackward0>), tensor(0.4062, grad_fn=<MeanBackward0>), tensor(0.4085, grad_fn=<MeanBackward0>), tensor(0.4079, grad_fn=<MeanBackward0>), tensor(0.4067, grad_fn=<MeanBackward0>), tensor(0.4075, grad_fn=<MeanBackward0>), tensor(0.4073, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.32059696316719055, 0.3206307664513588, 0.32067976146936417, 0.32073453813791275, 0.3207795023918152, 0.3249920681118965, 0.3250023126602173, 0.3250133842229843, 0.3250245228409767, 0.32503629475831985, 0.32504870742559433, 0.3250611573457718, 0.3250742256641388, 0.3250885531306267, 0.3251046761870384, 0.32512906938791275, 0.3284998759627342, 0.4014400690793991, 0.43339086323976517, 0.4574742540717125, 0.4692729264497757, 0.47377123683691025, 0.47116807103157043, 0.4705396220088005, 0.47138598561286926, 0.4720718339085579, 0.4726187661290169, 0.47365985810756683, 0.4743337258696556, 0.474080391228199, 0.4743865951895714, 0.47319700568914413, 0.47293689101934433, 0.47218628227710724, 0.47487449645996094, 0.4726182594895363, 0.47239939868450165, 0.4701022058725357, 0.46980106085538864, 0.4695197120308876, 0.4698679596185684, 0.4691237509250641, 0.4679178521037102, 0.4664338454604149, 0.46800675243139267, 0.4682137742638588, 0.46765486150979996, 0.466618612408638, 0.46530651301145554, 0.4656742289662361]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t90b1wtp-IL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df9558db-98f4-43a3-a39f-92ebb9f10218"
      },
      "source": [
        "model_factory('Adam')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 0.70349304, Test Loss: 0.24971213, Test Accuracy: 0.92520000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 0.19108419, Test Loss: 0.15407089, Test Accuracy: 0.95410000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 0.13175319, Test Loss: 0.12008267, Test Accuracy: 0.96590000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 0.10181782, Test Loss: 0.11060085, Test Accuracy: 0.96780000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.08089231, Test Loss: 0.10700331, Test Accuracy: 0.96830000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.06627438, Test Loss: 0.08847845, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.05288039, Test Loss: 0.10273121, Test Accuracy: 0.97100000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.04713998, Test Loss: 0.09349366, Test Accuracy: 0.97300000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.03822255, Test Loss: 0.09995507, Test Accuracy: 0.97370000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.03322133, Test Loss: 0.09568298, Test Accuracy: 0.97350000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.02961539, Test Loss: 0.10790298, Test Accuracy: 0.97340000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.02436374, Test Loss: 0.09440881, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.02099391, Test Loss: 0.09452429, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.01969475, Test Loss: 0.09713552, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.01825854, Test Loss: 0.11497826, Test Accuracy: 0.97410000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.01671022, Test Loss: 0.10951894, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.01490426, Test Loss: 0.10493772, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.01329708, Test Loss: 0.09998171, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.01283279, Test Loss: 0.10917581, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.01115321, Test Loss: 0.12180192, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.01102828, Test Loss: 0.11627207, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.00997441, Test Loss: 0.11814355, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.00922417, Test Loss: 0.14756817, Test Accuracy: 0.97080000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.00849225, Test Loss: 0.12215931, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.00859533, Test Loss: 0.13734381, Test Accuracy: 0.97440000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.00826923, Test Loss: 0.11322766, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.00783183, Test Loss: 0.13318586, Test Accuracy: 0.97420000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.00826555, Test Loss: 0.11960444, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00605629, Test Loss: 0.13121920, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00754072, Test Loss: 0.12752886, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00727915, Test Loss: 0.11271087, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00560611, Test Loss: 0.12765694, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00632649, Test Loss: 0.11225688, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00576471, Test Loss: 0.11775057, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00513396, Test Loss: 0.12794133, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00606756, Test Loss: 0.14065759, Test Accuracy: 0.97670000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00405719, Test Loss: 0.12296128, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00648892, Test Loss: 0.12381157, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00592333, Test Loss: 0.11700157, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00375700, Test Loss: 0.14470491, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00607972, Test Loss: 0.12169000, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00308262, Test Loss: 0.13623985, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00596998, Test Loss: 0.12338690, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00449574, Test Loss: 0.12291180, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00371980, Test Loss: 0.14455057, Test Accuracy: 0.97350000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00398769, Test Loss: 0.13136862, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00570741, Test Loss: 0.13846129, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00424031, Test Loss: 0.12801689, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00418253, Test Loss: 0.11849166, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00359042, Test Loss: 0.12666425, Test Accuracy: 0.97900000\n",
            "[tensor(0.2816, grad_fn=<MeanBackward0>), tensor(0.3121, grad_fn=<MeanBackward0>), tensor(0.3332, grad_fn=<MeanBackward0>), tensor(0.3422, grad_fn=<MeanBackward0>), tensor(0.3539, grad_fn=<MeanBackward0>), tensor(0.3557, grad_fn=<MeanBackward0>), tensor(0.3622, grad_fn=<MeanBackward0>), tensor(0.3674, grad_fn=<MeanBackward0>), tensor(0.3701, grad_fn=<MeanBackward0>), tensor(0.3745, grad_fn=<MeanBackward0>), tensor(0.3761, grad_fn=<MeanBackward0>), tensor(0.3816, grad_fn=<MeanBackward0>), tensor(0.3795, grad_fn=<MeanBackward0>), tensor(0.3833, grad_fn=<MeanBackward0>), tensor(0.3888, grad_fn=<MeanBackward0>), tensor(0.3862, grad_fn=<MeanBackward0>), tensor(0.3913, grad_fn=<MeanBackward0>), tensor(0.3915, grad_fn=<MeanBackward0>), tensor(0.3904, grad_fn=<MeanBackward0>), tensor(0.3953, grad_fn=<MeanBackward0>), tensor(0.4004, grad_fn=<MeanBackward0>), tensor(0.3951, grad_fn=<MeanBackward0>), tensor(0.3985, grad_fn=<MeanBackward0>), tensor(0.4027, grad_fn=<MeanBackward0>), tensor(0.4070, grad_fn=<MeanBackward0>), tensor(0.4059, grad_fn=<MeanBackward0>), tensor(0.4060, grad_fn=<MeanBackward0>), tensor(0.4073, grad_fn=<MeanBackward0>), tensor(0.4077, grad_fn=<MeanBackward0>), tensor(0.4108, grad_fn=<MeanBackward0>), tensor(0.4092, grad_fn=<MeanBackward0>), tensor(0.4126, grad_fn=<MeanBackward0>), tensor(0.4108, grad_fn=<MeanBackward0>), tensor(0.4121, grad_fn=<MeanBackward0>), tensor(0.4139, grad_fn=<MeanBackward0>), tensor(0.4162, grad_fn=<MeanBackward0>), tensor(0.4151, grad_fn=<MeanBackward0>), tensor(0.4206, grad_fn=<MeanBackward0>), tensor(0.4185, grad_fn=<MeanBackward0>), tensor(0.4203, grad_fn=<MeanBackward0>), tensor(0.4170, grad_fn=<MeanBackward0>), tensor(0.4130, grad_fn=<MeanBackward0>), tensor(0.4162, grad_fn=<MeanBackward0>), tensor(0.4194, grad_fn=<MeanBackward0>), tensor(0.4186, grad_fn=<MeanBackward0>), tensor(0.4239, grad_fn=<MeanBackward0>), tensor(0.4234, grad_fn=<MeanBackward0>), tensor(0.4183, grad_fn=<MeanBackward0>), tensor(0.4227, grad_fn=<MeanBackward0>), tensor(0.4219, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2432, grad_fn=<MeanBackward0>), tensor(0.2450, grad_fn=<MeanBackward0>), tensor(0.2545, grad_fn=<MeanBackward0>), tensor(0.2650, grad_fn=<MeanBackward0>), tensor(0.2695, grad_fn=<MeanBackward0>), tensor(0.2862, grad_fn=<MeanBackward0>), tensor(0.2956, grad_fn=<MeanBackward0>), tensor(0.3045, grad_fn=<MeanBackward0>), tensor(0.3101, grad_fn=<MeanBackward0>), tensor(0.3218, grad_fn=<MeanBackward0>), tensor(0.3397, grad_fn=<MeanBackward0>), tensor(0.3426, grad_fn=<MeanBackward0>), tensor(0.3443, grad_fn=<MeanBackward0>), tensor(0.3551, grad_fn=<MeanBackward0>), tensor(0.3683, grad_fn=<MeanBackward0>), tensor(0.3697, grad_fn=<MeanBackward0>), tensor(0.3793, grad_fn=<MeanBackward0>), tensor(0.3848, grad_fn=<MeanBackward0>), tensor(0.3948, grad_fn=<MeanBackward0>), tensor(0.3933, grad_fn=<MeanBackward0>), tensor(0.3989, grad_fn=<MeanBackward0>), tensor(0.4101, grad_fn=<MeanBackward0>), tensor(0.4085, grad_fn=<MeanBackward0>), tensor(0.4200, grad_fn=<MeanBackward0>), tensor(0.4245, grad_fn=<MeanBackward0>), tensor(0.4332, grad_fn=<MeanBackward0>), tensor(0.4278, grad_fn=<MeanBackward0>), tensor(0.4392, grad_fn=<MeanBackward0>), tensor(0.4367, grad_fn=<MeanBackward0>), tensor(0.4386, grad_fn=<MeanBackward0>), tensor(0.4432, grad_fn=<MeanBackward0>), tensor(0.4498, grad_fn=<MeanBackward0>), tensor(0.4484, grad_fn=<MeanBackward0>), tensor(0.4518, grad_fn=<MeanBackward0>), tensor(0.4573, grad_fn=<MeanBackward0>), tensor(0.4582, grad_fn=<MeanBackward0>), tensor(0.4580, grad_fn=<MeanBackward0>), tensor(0.4606, grad_fn=<MeanBackward0>), tensor(0.4629, grad_fn=<MeanBackward0>), tensor(0.4643, grad_fn=<MeanBackward0>), tensor(0.4710, grad_fn=<MeanBackward0>), tensor(0.4708, grad_fn=<MeanBackward0>), tensor(0.4641, grad_fn=<MeanBackward0>), tensor(0.4689, grad_fn=<MeanBackward0>), tensor(0.4642, grad_fn=<MeanBackward0>), tensor(0.4702, grad_fn=<MeanBackward0>), tensor(0.4659, grad_fn=<MeanBackward0>), tensor(0.4701, grad_fn=<MeanBackward0>), tensor(0.4719, grad_fn=<MeanBackward0>), tensor(0.4772, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3006, grad_fn=<MeanBackward0>), tensor(0.3095, grad_fn=<MeanBackward0>), tensor(0.3186, grad_fn=<MeanBackward0>), tensor(0.3252, grad_fn=<MeanBackward0>), tensor(0.3395, grad_fn=<MeanBackward0>), tensor(0.3406, grad_fn=<MeanBackward0>), tensor(0.3495, grad_fn=<MeanBackward0>), tensor(0.3494, grad_fn=<MeanBackward0>), tensor(0.3619, grad_fn=<MeanBackward0>), tensor(0.3681, grad_fn=<MeanBackward0>), tensor(0.3617, grad_fn=<MeanBackward0>), tensor(0.3767, grad_fn=<MeanBackward0>), tensor(0.3821, grad_fn=<MeanBackward0>), tensor(0.3810, grad_fn=<MeanBackward0>), tensor(0.3792, grad_fn=<MeanBackward0>), tensor(0.3891, grad_fn=<MeanBackward0>), tensor(0.3866, grad_fn=<MeanBackward0>), tensor(0.3929, grad_fn=<MeanBackward0>), tensor(0.3970, grad_fn=<MeanBackward0>), tensor(0.3925, grad_fn=<MeanBackward0>), tensor(0.3996, grad_fn=<MeanBackward0>), tensor(0.4017, grad_fn=<MeanBackward0>), tensor(0.4004, grad_fn=<MeanBackward0>), tensor(0.4035, grad_fn=<MeanBackward0>), tensor(0.4005, grad_fn=<MeanBackward0>), tensor(0.4062, grad_fn=<MeanBackward0>), tensor(0.4121, grad_fn=<MeanBackward0>), tensor(0.4117, grad_fn=<MeanBackward0>), tensor(0.4163, grad_fn=<MeanBackward0>), tensor(0.4168, grad_fn=<MeanBackward0>), tensor(0.4155, grad_fn=<MeanBackward0>), tensor(0.4180, grad_fn=<MeanBackward0>), tensor(0.4129, grad_fn=<MeanBackward0>), tensor(0.4201, grad_fn=<MeanBackward0>), tensor(0.4212, grad_fn=<MeanBackward0>), tensor(0.4220, grad_fn=<MeanBackward0>), tensor(0.4237, grad_fn=<MeanBackward0>), tensor(0.4257, grad_fn=<MeanBackward0>), tensor(0.4284, grad_fn=<MeanBackward0>), tensor(0.4259, grad_fn=<MeanBackward0>), tensor(0.4239, grad_fn=<MeanBackward0>), tensor(0.4263, grad_fn=<MeanBackward0>), tensor(0.4316, grad_fn=<MeanBackward0>), tensor(0.4305, grad_fn=<MeanBackward0>), tensor(0.4321, grad_fn=<MeanBackward0>), tensor(0.4303, grad_fn=<MeanBackward0>), tensor(0.4349, grad_fn=<MeanBackward0>), tensor(0.4333, grad_fn=<MeanBackward0>), tensor(0.4331, grad_fn=<MeanBackward0>), tensor(0.4345, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2451, grad_fn=<MeanBackward0>), tensor(0.2649, grad_fn=<MeanBackward0>), tensor(0.2825, grad_fn=<MeanBackward0>), tensor(0.2960, grad_fn=<MeanBackward0>), tensor(0.3098, grad_fn=<MeanBackward0>), tensor(0.3232, grad_fn=<MeanBackward0>), tensor(0.3245, grad_fn=<MeanBackward0>), tensor(0.3389, grad_fn=<MeanBackward0>), tensor(0.3425, grad_fn=<MeanBackward0>), tensor(0.3488, grad_fn=<MeanBackward0>), tensor(0.3552, grad_fn=<MeanBackward0>), tensor(0.3521, grad_fn=<MeanBackward0>), tensor(0.3567, grad_fn=<MeanBackward0>), tensor(0.3686, grad_fn=<MeanBackward0>), tensor(0.3736, grad_fn=<MeanBackward0>), tensor(0.3690, grad_fn=<MeanBackward0>), tensor(0.3746, grad_fn=<MeanBackward0>), tensor(0.3786, grad_fn=<MeanBackward0>), tensor(0.3760, grad_fn=<MeanBackward0>), tensor(0.3842, grad_fn=<MeanBackward0>), tensor(0.3845, grad_fn=<MeanBackward0>), tensor(0.3869, grad_fn=<MeanBackward0>), tensor(0.3958, grad_fn=<MeanBackward0>), tensor(0.3896, grad_fn=<MeanBackward0>), tensor(0.4047, grad_fn=<MeanBackward0>), tensor(0.4051, grad_fn=<MeanBackward0>), tensor(0.4116, grad_fn=<MeanBackward0>), tensor(0.4108, grad_fn=<MeanBackward0>), tensor(0.4100, grad_fn=<MeanBackward0>), tensor(0.4148, grad_fn=<MeanBackward0>), tensor(0.4294, grad_fn=<MeanBackward0>), tensor(0.4188, grad_fn=<MeanBackward0>), tensor(0.4316, grad_fn=<MeanBackward0>), tensor(0.4272, grad_fn=<MeanBackward0>), tensor(0.4237, grad_fn=<MeanBackward0>), tensor(0.4347, grad_fn=<MeanBackward0>), tensor(0.4258, grad_fn=<MeanBackward0>), tensor(0.4371, grad_fn=<MeanBackward0>), tensor(0.4438, grad_fn=<MeanBackward0>), tensor(0.4369, grad_fn=<MeanBackward0>), tensor(0.4441, grad_fn=<MeanBackward0>), tensor(0.4359, grad_fn=<MeanBackward0>), tensor(0.4463, grad_fn=<MeanBackward0>), tensor(0.4451, grad_fn=<MeanBackward0>), tensor(0.4496, grad_fn=<MeanBackward0>), tensor(0.4423, grad_fn=<MeanBackward0>), tensor(0.4524, grad_fn=<MeanBackward0>), tensor(0.4553, grad_fn=<MeanBackward0>), tensor(0.4584, grad_fn=<MeanBackward0>), tensor(0.4560, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.267634104937315, 0.2828459218144417, 0.2971903085708618, 0.30710478127002716, 0.3181621879339218, 0.32641564309597015, 0.332942858338356, 0.3400544598698616, 0.3461431935429573, 0.353293314576149, 0.3581799939274788, 0.36326414346694946, 0.36566026508808136, 0.37202855199575424, 0.37744715064764023, 0.37850913405418396, 0.3829544633626938, 0.386959545314312, 0.38956472277641296, 0.3913288861513138, 0.39583589881658554, 0.3984384387731552, 0.40079162269830704, 0.4039528965950012, 0.40917084366083145, 0.4125952422618866, 0.41435714811086655, 0.4172382652759552, 0.41769295185804367, 0.42026493698358536, 0.4243251383304596, 0.42479208111763, 0.4259161278605461, 0.4277781769633293, 0.42904096841812134, 0.43278172612190247, 0.4306294769048691, 0.4360020086169243, 0.43837809562683105, 0.43683530390262604, 0.43899495899677277, 0.43646831065416336, 0.4395561143755913, 0.44095009565353394, 0.4411163926124573, 0.44168999046087265, 0.44417470693588257, 0.4442633390426636, 0.44651738554239273, 0.4473862797021866]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb-4TPM5MGuE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "125a057c-6f2f-41c4-9fe5-9ad0bc4bb934"
      },
      "source": [
        "model_factory('SGD')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.32472255, Test Loss: 2.30778245, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30940653, Test Loss: 2.30538360, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.30504895, Test Loss: 2.30385843, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.30414631, Test Loss: 2.30156305, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.30311251, Test Loss: 2.30271003, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.30280634, Test Loss: 2.30421965, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.30241329, Test Loss: 2.30187196, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.30236180, Test Loss: 2.30162264, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.30226276, Test Loss: 2.30258062, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.30224197, Test Loss: 2.30129197, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.30211327, Test Loss: 2.30147549, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.30208901, Test Loss: 2.30194131, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.30188317, Test Loss: 2.30093815, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.30185788, Test Loss: 2.30169513, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.30183446, Test Loss: 2.30174162, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.30153028, Test Loss: 2.30066975, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.30149255, Test Loss: 2.30069772, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.30110694, Test Loss: 2.30100843, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 2.30059907, Test Loss: 2.29985386, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 2.29995046, Test Loss: 2.29829600, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 2.29775782, Test Loss: 2.29489251, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 2.29045097, Test Loss: 2.27903566, Test Accuracy: 0.16450000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 2.20490847, Test Loss: 2.01635596, Test Accuracy: 0.26470000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 1.90212216, Test Loss: 1.76812775, Test Accuracy: 0.31810000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 1.70639155, Test Loss: 1.61692866, Test Accuracy: 0.37060000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 1.58316823, Test Loss: 1.50658412, Test Accuracy: 0.41430000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 1.49127364, Test Loss: 1.42470402, Test Accuracy: 0.46290000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 1.39367435, Test Loss: 1.31480310, Test Accuracy: 0.50790000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 1.27472008, Test Loss: 1.17779119, Test Accuracy: 0.58130000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 1.12610747, Test Loss: 0.96869375, Test Accuracy: 0.69030000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.84822849, Test Loss: 0.71851771, Test Accuracy: 0.76760000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.68882707, Test Loss: 0.60984892, Test Accuracy: 0.82420000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.58462217, Test Loss: 0.52073304, Test Accuracy: 0.85840000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.50844961, Test Loss: 0.45544165, Test Accuracy: 0.88080000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.45320698, Test Loss: 0.40884850, Test Accuracy: 0.89190000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.41182485, Test Loss: 0.37991261, Test Accuracy: 0.90230000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.37641085, Test Loss: 0.36050134, Test Accuracy: 0.90810000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.33777103, Test Loss: 0.32110027, Test Accuracy: 0.91650000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.29697440, Test Loss: 0.28950200, Test Accuracy: 0.92530000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.26155129, Test Loss: 0.27324196, Test Accuracy: 0.92870000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.23715473, Test Loss: 0.29452112, Test Accuracy: 0.92330000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.21778315, Test Loss: 0.22168362, Test Accuracy: 0.94050000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.20062624, Test Loss: 0.22618359, Test Accuracy: 0.94060000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.18813137, Test Loss: 0.20124995, Test Accuracy: 0.94610000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.17555071, Test Loss: 0.20228650, Test Accuracy: 0.94460000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.16583115, Test Loss: 0.19722298, Test Accuracy: 0.94750000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.15701749, Test Loss: 0.17799301, Test Accuracy: 0.95240000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.14850828, Test Loss: 0.21331062, Test Accuracy: 0.94190000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.13957012, Test Loss: 0.16412113, Test Accuracy: 0.95400000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.13276905, Test Loss: 0.16496014, Test Accuracy: 0.95400000\n",
            "[tensor(0.0049, grad_fn=<MeanBackward0>), tensor(0.0049, grad_fn=<MeanBackward0>), tensor(0.0050, grad_fn=<MeanBackward0>), tensor(0.0050, grad_fn=<MeanBackward0>), tensor(0.0050, grad_fn=<MeanBackward0>), tensor(0.0051, grad_fn=<MeanBackward0>), tensor(0.0051, grad_fn=<MeanBackward0>), tensor(0.0052, grad_fn=<MeanBackward0>), tensor(0.0052, grad_fn=<MeanBackward0>), tensor(0.0053, grad_fn=<MeanBackward0>), tensor(0.0054, grad_fn=<MeanBackward0>), tensor(0.0055, grad_fn=<MeanBackward0>), tensor(0.0056, grad_fn=<MeanBackward0>), tensor(0.0058, grad_fn=<MeanBackward0>), tensor(0.0060, grad_fn=<MeanBackward0>), tensor(0.0063, grad_fn=<MeanBackward0>), tensor(0.0067, grad_fn=<MeanBackward0>), tensor(0.0072, grad_fn=<MeanBackward0>), tensor(0.0081, grad_fn=<MeanBackward0>), tensor(0.0094, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0165, grad_fn=<MeanBackward0>), tensor(0.0394, grad_fn=<MeanBackward0>), tensor(0.0326, grad_fn=<MeanBackward0>), tensor(0.0398, grad_fn=<MeanBackward0>), tensor(0.0507, grad_fn=<MeanBackward0>), tensor(0.0586, grad_fn=<MeanBackward0>), tensor(0.0671, grad_fn=<MeanBackward0>), tensor(0.0772, grad_fn=<MeanBackward0>), tensor(0.0933, grad_fn=<MeanBackward0>), tensor(0.1087, grad_fn=<MeanBackward0>), tensor(0.1234, grad_fn=<MeanBackward0>), tensor(0.1473, grad_fn=<MeanBackward0>), tensor(0.1665, grad_fn=<MeanBackward0>), tensor(0.1805, grad_fn=<MeanBackward0>), tensor(0.1914, grad_fn=<MeanBackward0>), tensor(0.1993, grad_fn=<MeanBackward0>), tensor(0.2039, grad_fn=<MeanBackward0>), tensor(0.2066, grad_fn=<MeanBackward0>), tensor(0.2108, grad_fn=<MeanBackward0>), tensor(0.2137, grad_fn=<MeanBackward0>), tensor(0.2168, grad_fn=<MeanBackward0>), tensor(0.2184, grad_fn=<MeanBackward0>), tensor(0.2211, grad_fn=<MeanBackward0>), tensor(0.2234, grad_fn=<MeanBackward0>), tensor(0.2262, grad_fn=<MeanBackward0>), tensor(0.2292, grad_fn=<MeanBackward0>), tensor(0.2313, grad_fn=<MeanBackward0>), tensor(0.2345, grad_fn=<MeanBackward0>), tensor(0.2370, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0123, grad_fn=<MeanBackward0>), tensor(0.0124, grad_fn=<MeanBackward0>), tensor(0.0125, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0127, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0122, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0096, grad_fn=<MeanBackward0>), tensor(0.0232, grad_fn=<MeanBackward0>), tensor(0.0535, grad_fn=<MeanBackward0>), tensor(0.0601, grad_fn=<MeanBackward0>), tensor(0.0617, grad_fn=<MeanBackward0>), tensor(0.0634, grad_fn=<MeanBackward0>), tensor(0.0644, grad_fn=<MeanBackward0>), tensor(0.0611, grad_fn=<MeanBackward0>), tensor(0.0631, grad_fn=<MeanBackward0>), tensor(0.0785, grad_fn=<MeanBackward0>), tensor(0.0758, grad_fn=<MeanBackward0>), tensor(0.0761, grad_fn=<MeanBackward0>), tensor(0.0761, grad_fn=<MeanBackward0>), tensor(0.0765, grad_fn=<MeanBackward0>), tensor(0.0761, grad_fn=<MeanBackward0>), tensor(0.0796, grad_fn=<MeanBackward0>), tensor(0.0762, grad_fn=<MeanBackward0>), tensor(0.0765, grad_fn=<MeanBackward0>), tensor(0.0789, grad_fn=<MeanBackward0>), tensor(0.0799, grad_fn=<MeanBackward0>), tensor(0.0780, grad_fn=<MeanBackward0>), tensor(0.0802, grad_fn=<MeanBackward0>), tensor(0.0795, grad_fn=<MeanBackward0>), tensor(0.0789, grad_fn=<MeanBackward0>), tensor(0.0814, grad_fn=<MeanBackward0>), tensor(0.0789, grad_fn=<MeanBackward0>), tensor(0.0804, grad_fn=<MeanBackward0>), tensor(0.0791, grad_fn=<MeanBackward0>), tensor(0.0785, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0124, grad_fn=<MeanBackward0>), tensor(0.0133, grad_fn=<MeanBackward0>), tensor(0.0134, grad_fn=<MeanBackward0>), tensor(0.0134, grad_fn=<MeanBackward0>), tensor(0.0132, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0127, grad_fn=<MeanBackward0>), tensor(0.0125, grad_fn=<MeanBackward0>), tensor(0.0123, grad_fn=<MeanBackward0>), tensor(0.0122, grad_fn=<MeanBackward0>), tensor(0.0120, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0110, grad_fn=<MeanBackward0>), tensor(0.0107, grad_fn=<MeanBackward0>), tensor(0.0103, grad_fn=<MeanBackward0>), tensor(0.0095, grad_fn=<MeanBackward0>), tensor(0.0083, grad_fn=<MeanBackward0>), tensor(0.0060, grad_fn=<MeanBackward0>), tensor(0.0050, grad_fn=<MeanBackward0>), tensor(0.0246, grad_fn=<MeanBackward0>), tensor(0.0551, grad_fn=<MeanBackward0>), tensor(0.0882, grad_fn=<MeanBackward0>), tensor(0.1119, grad_fn=<MeanBackward0>), tensor(0.1372, grad_fn=<MeanBackward0>), tensor(0.1387, grad_fn=<MeanBackward0>), tensor(0.1438, grad_fn=<MeanBackward0>), tensor(0.1476, grad_fn=<MeanBackward0>), tensor(0.1507, grad_fn=<MeanBackward0>), tensor(0.1567, grad_fn=<MeanBackward0>), tensor(0.1628, grad_fn=<MeanBackward0>), tensor(0.1616, grad_fn=<MeanBackward0>), tensor(0.1581, grad_fn=<MeanBackward0>), tensor(0.1635, grad_fn=<MeanBackward0>), tensor(0.1496, grad_fn=<MeanBackward0>), tensor(0.1444, grad_fn=<MeanBackward0>), tensor(0.1483, grad_fn=<MeanBackward0>), tensor(0.1521, grad_fn=<MeanBackward0>), tensor(0.1381, grad_fn=<MeanBackward0>), tensor(0.1433, grad_fn=<MeanBackward0>), tensor(0.1359, grad_fn=<MeanBackward0>), tensor(0.1290, grad_fn=<MeanBackward0>), tensor(0.1414, grad_fn=<MeanBackward0>), tensor(0.1343, grad_fn=<MeanBackward0>), tensor(0.1443, grad_fn=<MeanBackward0>), tensor(0.1312, grad_fn=<MeanBackward0>), tensor(0.1324, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0066, grad_fn=<MeanBackward0>), tensor(0.0035, grad_fn=<MeanBackward0>), tensor(0.0022, grad_fn=<MeanBackward0>), tensor(0.0014, grad_fn=<MeanBackward0>), tensor(0.0011, grad_fn=<MeanBackward0>), tensor(0.0012, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0006, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0006, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0009, grad_fn=<MeanBackward0>), tensor(0.0012, grad_fn=<MeanBackward0>), tensor(0.0021, grad_fn=<MeanBackward0>), tensor(0.0033, grad_fn=<MeanBackward0>), tensor(0.0061, grad_fn=<MeanBackward0>), tensor(0.0105, grad_fn=<MeanBackward0>), tensor(0.0078, grad_fn=<MeanBackward0>), tensor(0.0281, grad_fn=<MeanBackward0>), tensor(0.1274, grad_fn=<MeanBackward0>), tensor(0.2471, grad_fn=<MeanBackward0>), tensor(0.3315, grad_fn=<MeanBackward0>), tensor(0.3732, grad_fn=<MeanBackward0>), tensor(0.4338, grad_fn=<MeanBackward0>), tensor(0.3760, grad_fn=<MeanBackward0>), tensor(0.3254, grad_fn=<MeanBackward0>), tensor(0.3448, grad_fn=<MeanBackward0>), tensor(0.3405, grad_fn=<MeanBackward0>), tensor(0.3296, grad_fn=<MeanBackward0>), tensor(0.3334, grad_fn=<MeanBackward0>), tensor(0.3360, grad_fn=<MeanBackward0>), tensor(0.3140, grad_fn=<MeanBackward0>), tensor(0.3322, grad_fn=<MeanBackward0>), tensor(0.3386, grad_fn=<MeanBackward0>), tensor(0.3212, grad_fn=<MeanBackward0>), tensor(0.3070, grad_fn=<MeanBackward0>), tensor(0.3445, grad_fn=<MeanBackward0>), tensor(0.3253, grad_fn=<MeanBackward0>), tensor(0.3445, grad_fn=<MeanBackward0>), tensor(0.3477, grad_fn=<MeanBackward0>), tensor(0.3308, grad_fn=<MeanBackward0>), tensor(0.3432, grad_fn=<MeanBackward0>), tensor(0.3424, grad_fn=<MeanBackward0>), tensor(0.3579, grad_fn=<MeanBackward0>), tensor(0.3558, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.009069899329915643, 0.008529701561201364, 0.008264877018518746, 0.008089612500043586, 0.007983949239132926, 0.007989918231032789, 0.007861950711230747, 0.007802263993653469, 0.007780054525937885, 0.0077620814117835835, 0.007733740254479926, 0.007744447342702188, 0.0077466231159633026, 0.0077579848148161545, 0.007747451716568321, 0.007824410175089724, 0.007895759597886354, 0.008008062228327617, 0.008266671327874064, 0.008619060565251857, 0.009351185406558216, 0.010671804542653263, 0.018850363441742957, 0.03470508800819516, 0.07058975379914045, 0.11192135512828827, 0.14134302455931902, 0.16045393981039524, 0.17770483251661062, 0.16902933456003666, 0.16507581435143948, 0.1736761387437582, 0.18012123554944992, 0.18374722823500633, 0.18799838423728943, 0.1903698593378067, 0.18911243602633476, 0.19049577228724957, 0.19153404794633389, 0.1898093093186617, 0.18817619234323502, 0.1943462211638689, 0.19180361926555634, 0.19527164474129677, 0.19473923929035664, 0.19494229555130005, 0.19641395658254623, 0.19963864423334599, 0.20069399662315845, 0.2009530309587717]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}