{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "all_seeds_each_4HL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/all_seeds_each_4HL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7STrWa0P3z_",
        "outputId": "96f32476-737a-4c86-bbdd-2ec87cc728cb"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYoWaxyTpG6m",
        "outputId": "9bf167c0-711e-4181-f82d-19998284cb05"
      },
      "source": [
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "root_dir = './'\n",
        "torchvision.datasets.MNIST(root=root_dir,download=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-09 12:14:07--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
            "--2021-04-09 12:14:08--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘MNIST.tar.gz’\n",
            "\n",
            "MNIST.tar.gz            [             <=>    ]  33.20M  6.48MB/s    in 14s     \n",
            "\n",
            "2021-04-09 12:14:23 (2.29 MB/s) - ‘MNIST.tar.gz’ saved [34813078]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte\n",
            "MNIST/raw/train-images-idx3-ubyte.gz\n",
            "MNIST/processed/\n",
            "MNIST/processed/training.pt\n",
            "MNIST/processed/test.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./\n",
              "    Split: Train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4j9WoP-UnAm"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTW5TOUnP5XY"
      },
      "source": [
        "mnist_trainset = torchvision.datasets.MNIST(root=root_dir, train=True, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "mnist_testset  = torchvision.datasets.MNIST(root=root_dir, \n",
        "                                train=False, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(mnist_trainset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=True)\n",
        "\n",
        "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXTkEUJ5P6kU"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# Define the model \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # modify this section for later use \n",
        "        self.linear_1 = torch.nn.Linear(784, 256)\n",
        "        self.linear_2 = torch.nn.Linear(256, 256)\n",
        "        self.linear_3 = torch.nn.Linear(256, 256)\n",
        "        self.linear_4 = torch.nn.Linear(256, 256)\n",
        "        self.linear_5 = torch.nn.Linear(256, 10)\n",
        "        self.sigmoid12  = torch.nn.Sigmoid()\n",
        "        self.sigmoid23  = torch.nn.Sigmoid()\n",
        "        self.sigmoid34  = torch.nn.Sigmoid()\n",
        "        self.sigmoid45  = torch.nn.Sigmoid()\n",
        "\n",
        "        self.layer_activations = dict()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # modify this section for later use \n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.sigmoid12(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.sigmoid23(x)\n",
        "        x = self.linear_3(x)\n",
        "        x = self.sigmoid34(x)\n",
        "        x = self.linear_4(x)\n",
        "        x = self.sigmoid45(x)\n",
        "        pred = self.linear_5(x)\n",
        "        return pred\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfgvKH6eP9Ou"
      },
      "source": [
        "def get_activation(model, layer_name):    \n",
        "    def hook(module, input, output):\n",
        "        model.layer_activations[layer_name] = output\n",
        "    return hook"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOAaaXKupOys"
      },
      "source": [
        "def sparsity_calculator(final_spareness):\n",
        "    sparseness_list = list()\n",
        "    for single_epoch_spareness in final_spareness:\n",
        "\n",
        "        hidden_layer_activation_list = single_epoch_spareness\n",
        "        hidden_layer_activation_list = torch.stack(hidden_layer_activation_list)\n",
        "        layer_activations_list = torch.reshape(hidden_layer_activation_list, (10000, 256))\n",
        "\n",
        "        layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
        "        num_neurons = layer_activations_list.shape[1]\n",
        "        population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
        "        mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
        "\n",
        "        sparseness_list.append(mean_sparseness_per_epoch)\n",
        "\n",
        "    return sparseness_list"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvHGO5RSvi6I"
      },
      "source": [
        "def selectivity(hidden_layer_each_neuron):\n",
        "    __selectivity__ = list()\n",
        "    # I will now try to find the average of each class for each neuron.\n",
        "    # check out the next cell \n",
        "    avg_activations = [dict() for x in range(256)]\n",
        "    for i, neuron in enumerate(hidden_layer_each_neuron):\n",
        "        for k, v in neuron.items():\n",
        "            # v is the list of activations for hidden layer's neuron k \n",
        "            avg_activations[i][k] = sum(v) / float(len(v))\n",
        "\n",
        "    # generate 256 lists to get only values in avg_activations\n",
        "    only_activation_vals = [list() for x in range(256)]\n",
        "\n",
        "    # get only values from avg_activations\n",
        "    for i, avg_activation in enumerate(avg_activations):\n",
        "        for value in avg_activation.values():\n",
        "            only_activation_vals[i].append(value)\n",
        "\n",
        "    for activation_val in only_activation_vals:\n",
        "        # find u_max \n",
        "        u_max = np.max(activation_val)\n",
        "\n",
        "        # find u_minus_max \n",
        "        u_minus_max = (np.sum(activation_val) - u_max) / 9\n",
        "\n",
        "        # find selectivity \n",
        "        selectivity = (u_max - u_minus_max) / (u_max + u_minus_max)\n",
        "\n",
        "        # append selectivity value to selectivity\n",
        "        __selectivity__.append(selectivity)\n",
        "\n",
        "    avg_selectivity = np.average(__selectivity__)\n",
        "    std_selectivity = np.std(__selectivity__)\n",
        "                                 \n",
        "    return avg_selectivity, std_selectivity"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcKnp4rEpQ1q"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# add a parameter to the function and calculate avg and std. Do not forget to change division by 2, 3, 4, or 5 \n",
        "def avg_std_calculator(_hidden_layer_each_neuron_12, _hidden_layer_each_neuron_23, _hidden_layer_each_neuron_34, _hidden_layer_each_neuron_45):\n",
        "\n",
        "    avg_selectivity12, std_selectivity12 = selectivity(_hidden_layer_each_neuron_12)\n",
        "    avg_selectivity23, std_selectivity23 = selectivity(_hidden_layer_each_neuron_23)\n",
        "    avg_selectivity34, std_selectivity34 = selectivity(_hidden_layer_each_neuron_34)\n",
        "    avg_selectivity45, std_selectivity45 = selectivity(_hidden_layer_each_neuron_45)\n",
        "\n",
        "    final_selectivity_avg = (avg_selectivity12 + avg_selectivity23 + avg_selectivity34 + avg_selectivity45) / 4\n",
        "    final_selecvitity_std = (std_selectivity12 + std_selectivity23 + std_selectivity34 + std_selectivity45) / 4\n",
        "\n",
        "    return final_selectivity_avg, final_selecvitity_std, avg_selectivity12, std_selectivity12, avg_selectivity23, std_selectivity23, avg_selectivity34, std_selectivity34, avg_selectivity45, std_selectivity45 \n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5PUiBNqUImf"
      },
      "source": [
        "def model_factory(optimizer_name, seed_num):\n",
        "    '''\n",
        "    optimizer_name : choose one of Adagrad, Adadelta, SGD, and Adam \n",
        "\n",
        "    '''\n",
        "    my_model = Model()\n",
        "    print(\"my_model:\", my_model)\n",
        "    my_model.to(device)\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    # chagen sigmoid34 an 's34'\n",
        "    my_model.sigmoid12.register_forward_hook(get_activation(my_model, 's12'))\n",
        "    my_model.sigmoid23.register_forward_hook(get_activation(my_model, 's23'))\n",
        "    my_model.sigmoid34.register_forward_hook(get_activation(my_model, 's34'))\n",
        "    my_model.sigmoid45.register_forward_hook(get_activation(my_model, 's45'))\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        my_optimizer = torch.optim.Adadelta(my_model.parameters(), lr=1.0)\n",
        "\n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        my_optimizer = torch.optim.Adagrad(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        my_optimizer = torch.optim.SGD(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        my_optimizer = torch.optim.Adam(my_model.parameters(), lr=0.001)\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")\n",
        "    \n",
        "    print(\"my_optimizer:\", my_optimizer)\n",
        "    test_acc, sparseness_list, spar12, spar23, spar34, spar45, selectivity_list_avg, selectivity_list_std, selec_12_avg, selec_12_std, selec_23_avg, selec_23_std, selec_34_avg, selec_34_std, selec_45_avg, selec_45_std = selectivity_trainer(optimizer=my_optimizer, model=my_model)\n",
        "    # ************* modify this section for later use *************\n",
        "    # change name of the file \n",
        "    file_saver = open(f\"seed{seed_num}_only_4HL_{optimizer_name}.txt\", \"w\")\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver.write(str(test_acc)+'\\n'+str(sparseness_list)+'\\n'+str(spar12)+'\\n'+str(spar23)+'\\n'+str(spar34)+'\\n'+str(spar45)+'\\n'+str(selectivity_list_avg)+'\\n'+str(selectivity_list_std)+'\\n'+str(selec_12_avg)+'\\n'+str(selec_12_std)+'\\n'+str(selec_23_avg)+'\\n'+str(selec_23_std)+'\\n'+str(selec_34_avg)+'\\n'+str(selec_34_std)+'\\n'+str(selec_45_avg)+'\\n'+str(selec_45_std)+'\\n\\n')\n",
        "    file_saver.close()\n",
        "\n",
        "    if seed_num == 1:\n",
        "        # ************* modify this section for later use *************\n",
        "        if optimizer_name == 'Adadelta':\n",
        "            !cp seed1_only_4HL_Adadelta.txt /content/drive/MyDrive\n",
        "        \n",
        "        elif optimizer_name == 'Adagrad':\n",
        "            !cp seed1_only_4HL_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "        elif optimizer_name == 'SGD':\n",
        "            !cp seed1_only_4HL_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "        elif optimizer_name == 'Adam':\n",
        "            !cp seed1_only_4HL_Adam.txt /content/drive/MyDrive\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "    elif seed_num == 100:\n",
        "        # ************* modify this section for later use *************\n",
        "        if optimizer_name == 'Adadelta':\n",
        "            !cp seed100_only_4HL_Adadelta.txt /content/drive/MyDrive\n",
        "        \n",
        "        elif optimizer_name == 'Adagrad':\n",
        "            !cp seed100_only_4HL_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "        elif optimizer_name == 'SGD':\n",
        "            !cp seed100_only_4HL_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "        elif optimizer_name == 'Adam':\n",
        "            !cp seed100_only_4HL_Adam.txt /content/drive/MyDrive\n",
        "    \n",
        "    elif seed_num == 1234:\n",
        "        # ************* modify this section for later use *************\n",
        "        if optimizer_name == 'Adadelta':\n",
        "            !cp seed1234_only_4HL_Adadelta.txt /content/drive/MyDrive\n",
        "        \n",
        "        elif optimizer_name == 'Adagrad':\n",
        "            !cp seed1234_only_4HL_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "        elif optimizer_name == 'SGD':\n",
        "            !cp seed1234_only_4HL_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "        elif optimizer_name == 'Adam':\n",
        "            !cp seed1234_only_4HL_Adam.txt /content/drive/MyDrive\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXOpwTXEQFKY"
      },
      "source": [
        "no_epochs = 50\n",
        "def selectivity_trainer(optimizer, model):\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    train_loss = list()\n",
        "    test_loss  = list()\n",
        "    test_acc   = list()\n",
        "\n",
        "    best_test_loss = 1\n",
        "\n",
        "    selectivity_avg_list = list()\n",
        "    selectivity_std_list = list()\n",
        "\n",
        "    selec_12_avg = list()\n",
        "    selec_12_std = list()\n",
        "    selec_23_avg = list()\n",
        "    selec_23_std = list()\n",
        "    selec_34_avg = list()\n",
        "    selec_34_std = list()\n",
        "    selec_45_avg = list()\n",
        "    selec_45_std = list()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    final_spareness_12 = list()\n",
        "    final_spareness_23 = list()\n",
        "    final_spareness_34 = list()\n",
        "    final_spareness_45 = list()\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    for epoch in range(no_epochs):\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_each_neuron_12 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_12 = np.array(hidden_layer_each_neuron_12)\n",
        "\n",
        "        hidden_layer_each_neuron_23 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_23 = np.array(hidden_layer_each_neuron_23)\n",
        "\n",
        "        hidden_layer_each_neuron_34 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_34 = np.array(hidden_layer_each_neuron_34)\n",
        "\n",
        "        hidden_layer_each_neuron_45 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_45 = np.array(hidden_layer_each_neuron_45)\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_activation_list_12 = list()\n",
        "        hidden_layer_activation_list_23 = list()\n",
        "        hidden_layer_activation_list_34 = list()\n",
        "        hidden_layer_activation_list_45 = list()\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        total_train_loss = 0\n",
        "        total_test_loss = 0\n",
        "\n",
        "        # training\n",
        "        # set up training mode \n",
        "        model.train()\n",
        "\n",
        "        for itr, (images, labels) in enumerate(train_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_train_loss = total_train_loss / (itr + 1)\n",
        "        train_loss.append(total_train_loss)\n",
        "\n",
        "        # testing \n",
        "        # change to evaluation mode \n",
        "        model.eval()\n",
        "        total = 0\n",
        "        for itr, (images, labels) in enumerate(test_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # we now need softmax because we are testing.\n",
        "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "            for i, p in enumerate(pred):\n",
        "                if labels[i] == torch.max(p.data, 0)[1]:\n",
        "                    total = total + 1\n",
        "\n",
        "            # ***************** sparsity calculation ***************** #\n",
        "            hidden_layer_activation_list_12.append(model.layer_activations['s12'])\n",
        "            hidden_layer_activation_list_23.append(model.layer_activations['s23'])\n",
        "            hidden_layer_activation_list_34.append(model.layer_activations['s34'])\n",
        "            hidden_layer_activation_list_45.append(model.layer_activations['s45'])\n",
        "\n",
        "            # ************* modify this section for later use *************\n",
        "            # Do not forget to change hidden_layer_each_neuron_12 name \n",
        "            for activation, label in zip(model.layer_activations['s12'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_12[i][label].append(activation[i])\n",
        "\n",
        "            for activation, label in zip(model.layer_activations['s23'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_23[i][label].append(activation[i])\n",
        "            \n",
        "            for activation, label in zip(model.layer_activations['s34'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_34[i][label].append(activation[i])\n",
        "\n",
        "            for activation, label in zip(model.layer_activations['s45'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_45[i][label].append(activation[i])\n",
        "\n",
        "        # Add one more parameter \n",
        "        selectivity_avg, selectivity_std, avg_selectivity12, std_selectivity12, avg_selectivity23, std_selectivity23, avg_selectivity34, std_selectivity34, avg_selectivity45, std_selectivity45 = avg_std_calculator(hidden_layer_each_neuron_12, hidden_layer_each_neuron_23, hidden_layer_each_neuron_34, hidden_layer_each_neuron_45)\n",
        "        # ************* modify this section for later use *************\n",
        "        \n",
        "        selec_12_avg.append(avg_selectivity12)\n",
        "        selec_12_std.append(std_selectivity12)\n",
        "        selec_23_avg.append(avg_selectivity23)\n",
        "        selec_23_std.append(std_selectivity23)\n",
        "        selec_34_avg.append(avg_selectivity34)\n",
        "        selec_34_std.append(std_selectivity34)\n",
        "        selec_45_avg.append(avg_selectivity45)\n",
        "        selec_45_std.append(std_selectivity45)\n",
        "\n",
        "        selectivity_avg_list.append(selectivity_avg)\n",
        "        selectivity_std_list.append(selectivity_std)\n",
        "\n",
        "        final_spareness_12.append(hidden_layer_activation_list_12)\n",
        "        final_spareness_23.append(hidden_layer_activation_list_23)\n",
        "        final_spareness_34.append(hidden_layer_activation_list_34)\n",
        "        final_spareness_45.append(hidden_layer_activation_list_45)\n",
        "        # ***************** sparsity calculation ***************** #\n",
        "\n",
        "        # caculate accuracy \n",
        "        accuracy = total / len(mnist_testset)\n",
        "\n",
        "        # append accuracy here\n",
        "        test_acc.append(accuracy)\n",
        "\n",
        "        # append test loss here \n",
        "        total_test_loss = total_test_loss / (itr + 1)\n",
        "        test_loss.append(total_test_loss)\n",
        "\n",
        "        print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Test Loss: {:.8f}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, total_train_loss, total_test_loss, accuracy))\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "    sparsity_list12 = sparsity_calculator(final_spareness_12)\n",
        "    sparsity_list23 = sparsity_calculator(final_spareness_23)\n",
        "    sparsity_list34 = sparsity_calculator(final_spareness_34)\n",
        "    sparsity_list45 = sparsity_calculator(final_spareness_45)\n",
        "\n",
        "    print(sparsity_list12)\n",
        "    print(sparsity_list23)\n",
        "    print(sparsity_list34)\n",
        "    print(sparsity_list45)\n",
        "\n",
        "    average_sparsity = list()\n",
        "    sparsity12 = list()\n",
        "    sparsity23 = list()\n",
        "    sparsity34 = list()\n",
        "    sparsity45 = list()\n",
        "\n",
        "    for i in range(no_epochs):\n",
        "        sparsity12.append(sparsity_list12[i].item())\n",
        "        sparsity23.append(sparsity_list23[i].item())\n",
        "        sparsity34.append(sparsity_list34[i].item())\n",
        "        sparsity45.append(sparsity_list45[i].item())\n",
        "        average_sparsity.append( (sparsity_list12[i].item() + sparsity_list23[i].item() + sparsity_list34[i].item() + sparsity_list45[i].item()) / 4 )\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "\n",
        "    print(\"average_sparsity:\", average_sparsity)\n",
        "\n",
        "    return test_acc, average_sparsity, sparsity12, sparsity23, sparsity34, sparsity45, selectivity_avg_list, selectivity_std_list, selec_12_avg, selec_12_std, selec_23_avg, selec_23_std, selec_34_avg, selec_34_std, selec_45_avg, selec_45_std"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmLiu0sSp-CJ",
        "outputId": "a386e71c-a229-4c6f-da60-7a2f4ff19b85"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "model_factory('Adadelta', 1)\n",
        "model_factory('Adagrad', 1)\n",
        "model_factory('Adam', 1)\n",
        "model_factory('SGD', 1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-06\n",
            "    lr: 1.0\n",
            "    rho: 0.9\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.30728241, Test Loss: 2.30133314, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30123473, Test Loss: 2.29928559, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.17011523, Test Loss: 1.80862197, Test Accuracy: 0.36920000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 1.41086266, Test Loss: 1.03124689, Test Accuracy: 0.60970000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.75379442, Test Loss: 0.47830317, Test Accuracy: 0.86060000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.38202769, Test Loss: 0.31924961, Test Accuracy: 0.90620000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.24944633, Test Loss: 0.19842744, Test Accuracy: 0.94560000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.18402175, Test Loss: 0.20962805, Test Accuracy: 0.93710000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.14514048, Test Loss: 0.14161652, Test Accuracy: 0.96120000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.12119154, Test Loss: 0.13142935, Test Accuracy: 0.96200000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.10203985, Test Loss: 0.14951582, Test Accuracy: 0.95650000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.09000254, Test Loss: 0.17859021, Test Accuracy: 0.94990000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.07842495, Test Loss: 0.10787316, Test Accuracy: 0.96820000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.07029801, Test Loss: 0.10384943, Test Accuracy: 0.97070000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.06052540, Test Loss: 0.10108096, Test Accuracy: 0.97140000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.05423312, Test Loss: 0.09733289, Test Accuracy: 0.97330000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.04849143, Test Loss: 0.10479232, Test Accuracy: 0.97260000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.04333516, Test Loss: 0.10041861, Test Accuracy: 0.97410000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.03717152, Test Loss: 0.10028889, Test Accuracy: 0.97380000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.03431387, Test Loss: 0.14081646, Test Accuracy: 0.96090000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.03022327, Test Loss: 0.11118784, Test Accuracy: 0.97230000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.02659577, Test Loss: 0.12500803, Test Accuracy: 0.97000000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.02393013, Test Loss: 0.11626472, Test Accuracy: 0.97270000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.02087015, Test Loss: 0.09923693, Test Accuracy: 0.97550000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.01978010, Test Loss: 0.11147179, Test Accuracy: 0.97390000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.01763351, Test Loss: 0.10459694, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.01444519, Test Loss: 0.11202822, Test Accuracy: 0.97620000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.01220594, Test Loss: 0.12451969, Test Accuracy: 0.97540000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.01194446, Test Loss: 0.10691379, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.01067564, Test Loss: 0.11750542, Test Accuracy: 0.97660000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00883747, Test Loss: 0.12464424, Test Accuracy: 0.97570000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00793621, Test Loss: 0.16269972, Test Accuracy: 0.97090000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00708513, Test Loss: 0.11839673, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00572714, Test Loss: 0.11934497, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00551724, Test Loss: 0.11826491, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00405678, Test Loss: 0.12047445, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00339900, Test Loss: 0.12771686, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00303057, Test Loss: 0.11673111, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00282400, Test Loss: 0.12293258, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00260147, Test Loss: 0.13032745, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00202667, Test Loss: 0.15064391, Test Accuracy: 0.97390000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00213845, Test Loss: 0.12999282, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00148443, Test Loss: 0.12487847, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00153169, Test Loss: 0.13162142, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00108885, Test Loss: 0.13171464, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00100106, Test Loss: 0.13181068, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00087281, Test Loss: 0.13274503, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00080971, Test Loss: 0.13038506, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00071164, Test Loss: 0.13985493, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00070801, Test Loss: 0.13002869, Test Accuracy: 0.97820000\n",
            "[tensor(0.0063, grad_fn=<MeanBackward0>), tensor(0.0108, grad_fn=<MeanBackward0>), tensor(0.0621, grad_fn=<MeanBackward0>), tensor(0.1820, grad_fn=<MeanBackward0>), tensor(0.2970, grad_fn=<MeanBackward0>), tensor(0.3329, grad_fn=<MeanBackward0>), tensor(0.3416, grad_fn=<MeanBackward0>), tensor(0.3555, grad_fn=<MeanBackward0>), tensor(0.3646, grad_fn=<MeanBackward0>), tensor(0.3677, grad_fn=<MeanBackward0>), tensor(0.3729, grad_fn=<MeanBackward0>), tensor(0.3756, grad_fn=<MeanBackward0>), tensor(0.3740, grad_fn=<MeanBackward0>), tensor(0.3785, grad_fn=<MeanBackward0>), tensor(0.3797, grad_fn=<MeanBackward0>), tensor(0.3826, grad_fn=<MeanBackward0>), tensor(0.3864, grad_fn=<MeanBackward0>), tensor(0.3860, grad_fn=<MeanBackward0>), tensor(0.3868, grad_fn=<MeanBackward0>), tensor(0.3879, grad_fn=<MeanBackward0>), tensor(0.3893, grad_fn=<MeanBackward0>), tensor(0.3880, grad_fn=<MeanBackward0>), tensor(0.3884, grad_fn=<MeanBackward0>), tensor(0.3898, grad_fn=<MeanBackward0>), tensor(0.3916, grad_fn=<MeanBackward0>), tensor(0.3901, grad_fn=<MeanBackward0>), tensor(0.3910, grad_fn=<MeanBackward0>), tensor(0.3919, grad_fn=<MeanBackward0>), tensor(0.3915, grad_fn=<MeanBackward0>), tensor(0.3894, grad_fn=<MeanBackward0>), tensor(0.3937, grad_fn=<MeanBackward0>), tensor(0.3913, grad_fn=<MeanBackward0>), tensor(0.3934, grad_fn=<MeanBackward0>), tensor(0.3915, grad_fn=<MeanBackward0>), tensor(0.3936, grad_fn=<MeanBackward0>), tensor(0.3927, grad_fn=<MeanBackward0>), tensor(0.3935, grad_fn=<MeanBackward0>), tensor(0.3920, grad_fn=<MeanBackward0>), tensor(0.3921, grad_fn=<MeanBackward0>), tensor(0.3924, grad_fn=<MeanBackward0>), tensor(0.3917, grad_fn=<MeanBackward0>), tensor(0.3902, grad_fn=<MeanBackward0>), tensor(0.3910, grad_fn=<MeanBackward0>), tensor(0.3909, grad_fn=<MeanBackward0>), tensor(0.3908, grad_fn=<MeanBackward0>), tensor(0.3911, grad_fn=<MeanBackward0>), tensor(0.3916, grad_fn=<MeanBackward0>), tensor(0.3919, grad_fn=<MeanBackward0>), tensor(0.3910, grad_fn=<MeanBackward0>), tensor(0.3907, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0132, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0532, grad_fn=<MeanBackward0>), tensor(0.1400, grad_fn=<MeanBackward0>), tensor(0.1924, grad_fn=<MeanBackward0>), tensor(0.2283, grad_fn=<MeanBackward0>), tensor(0.2375, grad_fn=<MeanBackward0>), tensor(0.2270, grad_fn=<MeanBackward0>), tensor(0.2309, grad_fn=<MeanBackward0>), tensor(0.2487, grad_fn=<MeanBackward0>), tensor(0.2436, grad_fn=<MeanBackward0>), tensor(0.2420, grad_fn=<MeanBackward0>), tensor(0.2536, grad_fn=<MeanBackward0>), tensor(0.2548, grad_fn=<MeanBackward0>), tensor(0.2611, grad_fn=<MeanBackward0>), tensor(0.2577, grad_fn=<MeanBackward0>), tensor(0.2552, grad_fn=<MeanBackward0>), tensor(0.2598, grad_fn=<MeanBackward0>), tensor(0.2617, grad_fn=<MeanBackward0>), tensor(0.2550, grad_fn=<MeanBackward0>), tensor(0.2611, grad_fn=<MeanBackward0>), tensor(0.2604, grad_fn=<MeanBackward0>), tensor(0.2551, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2552, grad_fn=<MeanBackward0>), tensor(0.2638, grad_fn=<MeanBackward0>), tensor(0.2617, grad_fn=<MeanBackward0>), tensor(0.2645, grad_fn=<MeanBackward0>), tensor(0.2568, grad_fn=<MeanBackward0>), tensor(0.2629, grad_fn=<MeanBackward0>), tensor(0.2652, grad_fn=<MeanBackward0>), tensor(0.2602, grad_fn=<MeanBackward0>), tensor(0.2580, grad_fn=<MeanBackward0>), tensor(0.2591, grad_fn=<MeanBackward0>), tensor(0.2590, grad_fn=<MeanBackward0>), tensor(0.2538, grad_fn=<MeanBackward0>), tensor(0.2532, grad_fn=<MeanBackward0>), tensor(0.2525, grad_fn=<MeanBackward0>), tensor(0.2532, grad_fn=<MeanBackward0>), tensor(0.2523, grad_fn=<MeanBackward0>), tensor(0.2509, grad_fn=<MeanBackward0>), tensor(0.2467, grad_fn=<MeanBackward0>), tensor(0.2464, grad_fn=<MeanBackward0>), tensor(0.2459, grad_fn=<MeanBackward0>), tensor(0.2435, grad_fn=<MeanBackward0>), tensor(0.2427, grad_fn=<MeanBackward0>), tensor(0.2415, grad_fn=<MeanBackward0>), tensor(0.2397, grad_fn=<MeanBackward0>), tensor(0.2384, grad_fn=<MeanBackward0>), tensor(0.2368, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0087, grad_fn=<MeanBackward0>), tensor(0.0060, grad_fn=<MeanBackward0>), tensor(0.0320, grad_fn=<MeanBackward0>), tensor(0.2066, grad_fn=<MeanBackward0>), tensor(0.2402, grad_fn=<MeanBackward0>), tensor(0.2692, grad_fn=<MeanBackward0>), tensor(0.2650, grad_fn=<MeanBackward0>), tensor(0.2444, grad_fn=<MeanBackward0>), tensor(0.2517, grad_fn=<MeanBackward0>), tensor(0.2613, grad_fn=<MeanBackward0>), tensor(0.2681, grad_fn=<MeanBackward0>), tensor(0.2572, grad_fn=<MeanBackward0>), tensor(0.2470, grad_fn=<MeanBackward0>), tensor(0.2550, grad_fn=<MeanBackward0>), tensor(0.2507, grad_fn=<MeanBackward0>), tensor(0.2479, grad_fn=<MeanBackward0>), tensor(0.2803, grad_fn=<MeanBackward0>), tensor(0.2656, grad_fn=<MeanBackward0>), tensor(0.2610, grad_fn=<MeanBackward0>), tensor(0.2330, grad_fn=<MeanBackward0>), tensor(0.2870, grad_fn=<MeanBackward0>), tensor(0.2732, grad_fn=<MeanBackward0>), tensor(0.2675, grad_fn=<MeanBackward0>), tensor(0.2675, grad_fn=<MeanBackward0>), tensor(0.2707, grad_fn=<MeanBackward0>), tensor(0.2639, grad_fn=<MeanBackward0>), tensor(0.2690, grad_fn=<MeanBackward0>), tensor(0.2944, grad_fn=<MeanBackward0>), tensor(0.2768, grad_fn=<MeanBackward0>), tensor(0.2850, grad_fn=<MeanBackward0>), tensor(0.2964, grad_fn=<MeanBackward0>), tensor(0.3133, grad_fn=<MeanBackward0>), tensor(0.2886, grad_fn=<MeanBackward0>), tensor(0.2843, grad_fn=<MeanBackward0>), tensor(0.2890, grad_fn=<MeanBackward0>), tensor(0.2811, grad_fn=<MeanBackward0>), tensor(0.2865, grad_fn=<MeanBackward0>), tensor(0.2896, grad_fn=<MeanBackward0>), tensor(0.2922, grad_fn=<MeanBackward0>), tensor(0.2947, grad_fn=<MeanBackward0>), tensor(0.2928, grad_fn=<MeanBackward0>), tensor(0.3010, grad_fn=<MeanBackward0>), tensor(0.2960, grad_fn=<MeanBackward0>), tensor(0.2972, grad_fn=<MeanBackward0>), tensor(0.2996, grad_fn=<MeanBackward0>), tensor(0.2987, grad_fn=<MeanBackward0>), tensor(0.2992, grad_fn=<MeanBackward0>), tensor(0.3001, grad_fn=<MeanBackward0>), tensor(0.2951, grad_fn=<MeanBackward0>), tensor(0.2911, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0048, grad_fn=<MeanBackward0>), tensor(0.0068, grad_fn=<MeanBackward0>), tensor(0.4298, grad_fn=<MeanBackward0>), tensor(0.5309, grad_fn=<MeanBackward0>), tensor(0.5748, grad_fn=<MeanBackward0>), tensor(0.5578, grad_fn=<MeanBackward0>), tensor(0.5609, grad_fn=<MeanBackward0>), tensor(0.5860, grad_fn=<MeanBackward0>), tensor(0.5778, grad_fn=<MeanBackward0>), tensor(0.5602, grad_fn=<MeanBackward0>), tensor(0.5676, grad_fn=<MeanBackward0>), tensor(0.5932, grad_fn=<MeanBackward0>), tensor(0.5989, grad_fn=<MeanBackward0>), tensor(0.5971, grad_fn=<MeanBackward0>), tensor(0.5935, grad_fn=<MeanBackward0>), tensor(0.5978, grad_fn=<MeanBackward0>), tensor(0.5722, grad_fn=<MeanBackward0>), tensor(0.5914, grad_fn=<MeanBackward0>), tensor(0.5948, grad_fn=<MeanBackward0>), tensor(0.6343, grad_fn=<MeanBackward0>), tensor(0.5770, grad_fn=<MeanBackward0>), tensor(0.5917, grad_fn=<MeanBackward0>), tensor(0.6028, grad_fn=<MeanBackward0>), tensor(0.5934, grad_fn=<MeanBackward0>), tensor(0.5971, grad_fn=<MeanBackward0>), tensor(0.5981, grad_fn=<MeanBackward0>), tensor(0.5929, grad_fn=<MeanBackward0>), tensor(0.5632, grad_fn=<MeanBackward0>), tensor(0.5881, grad_fn=<MeanBackward0>), tensor(0.5780, grad_fn=<MeanBackward0>), tensor(0.5667, grad_fn=<MeanBackward0>), tensor(0.5493, grad_fn=<MeanBackward0>), tensor(0.5732, grad_fn=<MeanBackward0>), tensor(0.5753, grad_fn=<MeanBackward0>), tensor(0.5631, grad_fn=<MeanBackward0>), tensor(0.5780, grad_fn=<MeanBackward0>), tensor(0.5724, grad_fn=<MeanBackward0>), tensor(0.5649, grad_fn=<MeanBackward0>), tensor(0.5646, grad_fn=<MeanBackward0>), tensor(0.5604, grad_fn=<MeanBackward0>), tensor(0.5635, grad_fn=<MeanBackward0>), tensor(0.5656, grad_fn=<MeanBackward0>), tensor(0.5669, grad_fn=<MeanBackward0>), tensor(0.5659, grad_fn=<MeanBackward0>), tensor(0.5632, grad_fn=<MeanBackward0>), tensor(0.5649, grad_fn=<MeanBackward0>), tensor(0.5645, grad_fn=<MeanBackward0>), tensor(0.5643, grad_fn=<MeanBackward0>), tensor(0.5680, grad_fn=<MeanBackward0>), tensor(0.5744, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.00823333510197699, 0.008862340939231217, 0.1442822590470314, 0.2648982107639313, 0.3261108361184597, 0.3470563068985939, 0.3512256555259228, 0.35323186218738556, 0.35624779388308525, 0.3594815880060196, 0.3630429022014141, 0.3669774681329727, 0.3683433085680008, 0.37134949862957, 0.37124648690223694, 0.3715174086391926, 0.3735335096716881, 0.37569045275449753, 0.37607474625110626, 0.3775494657456875, 0.378592848777771, 0.37831003218889236, 0.37846729159355164, 0.3770840987563133, 0.378647156059742, 0.3789902999997139, 0.3786272704601288, 0.3785126432776451, 0.37828194350004196, 0.378842368721962, 0.38051367551088333, 0.3785300478339195, 0.3783027455210686, 0.3775408864021301, 0.37616468220949173, 0.37639467418193817, 0.3763869181275368, 0.3747551590204239, 0.3755117282271385, 0.3749714568257332, 0.3747447207570076, 0.3758745305240154, 0.3750995509326458, 0.3749603070318699, 0.374256681650877, 0.37435179203748703, 0.374188669025898, 0.37398842722177505, 0.3731505833566189, 0.3732851482927799]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adagrad (\n",
            "Parameter Group 0\n",
            "    eps: 1e-10\n",
            "    initial_accumulator_value: 0\n",
            "    lr: 0.1\n",
            "    lr_decay: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.44979455, Test Loss: 2.30983172, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.33305227, Test Loss: 2.33419909, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.32642089, Test Loss: 2.31778644, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.32260394, Test Loss: 2.32555158, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.31936882, Test Loss: 2.31134513, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.31845552, Test Loss: 2.31996165, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.31728969, Test Loss: 2.31143374, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.31646198, Test Loss: 2.30857979, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.31507596, Test Loss: 2.31294749, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.31430206, Test Loss: 2.31249977, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.31330357, Test Loss: 2.31158349, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.31265659, Test Loss: 2.30796239, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.31248808, Test Loss: 2.32036137, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.31135246, Test Loss: 2.31347032, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.31168978, Test Loss: 2.31335057, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.31149726, Test Loss: 2.31289987, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.31101911, Test Loss: 2.31108292, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.31100129, Test Loss: 2.31291881, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 2.31065667, Test Loss: 2.31275089, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 2.31061739, Test Loss: 2.31968040, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 2.30974718, Test Loss: 2.31080300, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 2.30934652, Test Loss: 2.30873585, Test Accuracy: 0.08920000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 2.30930243, Test Loss: 2.30976390, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 2.30945997, Test Loss: 2.31220591, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 2.30973429, Test Loss: 2.30816215, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 2.30898187, Test Loss: 2.30950647, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 2.30880726, Test Loss: 2.30376330, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 2.30859065, Test Loss: 2.30668202, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 2.30863066, Test Loss: 2.30681895, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 2.30888246, Test Loss: 2.31022474, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 2.30817301, Test Loss: 2.30878559, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 2.30782572, Test Loss: 2.30739538, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 2.30846529, Test Loss: 2.30629158, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 2.30829779, Test Loss: 2.30961096, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 2.30786212, Test Loss: 2.30387751, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 2.30720213, Test Loss: 2.30577651, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 2.30772048, Test Loss: 2.30480266, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 2.30739869, Test Loss: 2.30562956, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 2.30718175, Test Loss: 2.31127932, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 2.30763262, Test Loss: 2.30588408, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 2.30781747, Test Loss: 2.30717658, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 2.30716651, Test Loss: 2.30728855, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 2.30717113, Test Loss: 2.30327155, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 2.30711301, Test Loss: 2.30434975, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 2.30687389, Test Loss: 2.30621530, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 2.30735111, Test Loss: 2.30939955, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 2.30673084, Test Loss: 2.30282617, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 2.30664046, Test Loss: 2.30251529, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 1.89836690, Test Loss: 1.74651534, Test Accuracy: 0.20760000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 1.55140960, Test Loss: 1.40094747, Test Accuracy: 0.38930000\n",
            "[tensor(0.2576, grad_fn=<MeanBackward0>), tensor(0.2576, grad_fn=<MeanBackward0>), tensor(0.2576, grad_fn=<MeanBackward0>), tensor(0.2576, grad_fn=<MeanBackward0>), tensor(0.2576, grad_fn=<MeanBackward0>), tensor(0.2576, grad_fn=<MeanBackward0>), tensor(0.2576, grad_fn=<MeanBackward0>), tensor(0.2576, grad_fn=<MeanBackward0>), tensor(0.2576, grad_fn=<MeanBackward0>), tensor(0.2576, grad_fn=<MeanBackward0>), tensor(0.2576, grad_fn=<MeanBackward0>), tensor(0.2576, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2578, grad_fn=<MeanBackward0>), tensor(0.2704, grad_fn=<MeanBackward0>), tensor(0.2705, grad_fn=<MeanBackward0>), tensor(0.2705, grad_fn=<MeanBackward0>), tensor(0.2707, grad_fn=<MeanBackward0>), tensor(0.2708, grad_fn=<MeanBackward0>), tensor(0.2711, grad_fn=<MeanBackward0>), tensor(0.3286, grad_fn=<MeanBackward0>), tensor(0.4701, grad_fn=<MeanBackward0>), tensor(0.4729, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3468, grad_fn=<MeanBackward0>), tensor(0.3468, grad_fn=<MeanBackward0>), tensor(0.3468, grad_fn=<MeanBackward0>), tensor(0.3468, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3470, grad_fn=<MeanBackward0>), tensor(0.3470, grad_fn=<MeanBackward0>), tensor(0.3470, grad_fn=<MeanBackward0>), tensor(0.3470, grad_fn=<MeanBackward0>), tensor(0.3470, grad_fn=<MeanBackward0>), tensor(0.3470, grad_fn=<MeanBackward0>), tensor(0.3470, grad_fn=<MeanBackward0>), tensor(0.3470, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3469, grad_fn=<MeanBackward0>), tensor(0.3468, grad_fn=<MeanBackward0>), tensor(0.3467, grad_fn=<MeanBackward0>), tensor(0.3466, grad_fn=<MeanBackward0>), tensor(0.3464, grad_fn=<MeanBackward0>), tensor(0.3463, grad_fn=<MeanBackward0>), tensor(0.3461, grad_fn=<MeanBackward0>), tensor(0.3460, grad_fn=<MeanBackward0>), tensor(0.3458, grad_fn=<MeanBackward0>), tensor(0.3456, grad_fn=<MeanBackward0>), tensor(0.3454, grad_fn=<MeanBackward0>), tensor(0.3451, grad_fn=<MeanBackward0>), tensor(0.3449, grad_fn=<MeanBackward0>), tensor(0.3447, grad_fn=<MeanBackward0>), tensor(0.3425, grad_fn=<MeanBackward0>), tensor(0.3394, grad_fn=<MeanBackward0>), tensor(0.3394, grad_fn=<MeanBackward0>), tensor(0.3395, grad_fn=<MeanBackward0>), tensor(0.3395, grad_fn=<MeanBackward0>), tensor(0.3396, grad_fn=<MeanBackward0>), tensor(0.3398, grad_fn=<MeanBackward0>), tensor(0.3538, grad_fn=<MeanBackward0>), tensor(0.3707, grad_fn=<MeanBackward0>), tensor(0.3833, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3026, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3015, grad_fn=<MeanBackward0>), tensor(0.2889, grad_fn=<MeanBackward0>), tensor(0.2848, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3124, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3188, grad_fn=<MeanBackward0>), tensor(0.3128, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.3051053136587143, 0.3051139861345291, 0.3051183968782425, 0.3051202893257141, 0.30512019246816635, 0.3051188811659813, 0.30511702597141266, 0.3051159009337425, 0.3051157370209694, 0.3051157593727112, 0.30511657148599625, 0.30511802434921265, 0.30512017011642456, 0.30512261390686035, 0.30512578040361404, 0.30512794852256775, 0.30512960255146027, 0.30513131618499756, 0.3051322177052498, 0.30513209104537964, 0.3051307797431946, 0.30512895435094833, 0.30512672662734985, 0.30512308329343796, 0.3051176518201828, 0.3051076978445053, 0.30509278178215027, 0.30507201701402664, 0.305048868060112, 0.3050210401415825, 0.3049897104501724, 0.3049549385905266, 0.30491556227207184, 0.30487336963415146, 0.3048274368047714, 0.3047795966267586, 0.3047291114926338, 0.3046789914369583, 0.30462872982025146, 0.3045786917209625, 0.3038153424859047, 0.3071972504258156, 0.3072190210223198, 0.30724985897541046, 0.3072963356971741, 0.3073723167181015, 0.3074885606765747, 0.3277885839343071, 0.36214039474725723, 0.36346516013145447]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 0.74083274, Test Loss: 0.27366460, Test Accuracy: 0.92240000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 0.20696766, Test Loss: 0.16214297, Test Accuracy: 0.95280000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 0.13833021, Test Loss: 0.15494383, Test Accuracy: 0.95410000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 0.10658678, Test Loss: 0.11438954, Test Accuracy: 0.96520000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.08431046, Test Loss: 0.11124561, Test Accuracy: 0.96860000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.07065431, Test Loss: 0.09577204, Test Accuracy: 0.97270000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.05658659, Test Loss: 0.11394703, Test Accuracy: 0.96770000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.04688496, Test Loss: 0.09626687, Test Accuracy: 0.97290000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.04110096, Test Loss: 0.08827197, Test Accuracy: 0.97490000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.03566707, Test Loss: 0.10030927, Test Accuracy: 0.97140000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.03145766, Test Loss: 0.09391242, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.02608346, Test Loss: 0.09485634, Test Accuracy: 0.97640000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.02240905, Test Loss: 0.09498489, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.02052783, Test Loss: 0.11280183, Test Accuracy: 0.97480000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.01836193, Test Loss: 0.11328903, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.01700143, Test Loss: 0.10119163, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.01523177, Test Loss: 0.11293147, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.01506850, Test Loss: 0.10821046, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.01256955, Test Loss: 0.11064329, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.01168610, Test Loss: 0.11751609, Test Accuracy: 0.97600000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.01143527, Test Loss: 0.13358089, Test Accuracy: 0.97290000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.01045469, Test Loss: 0.10802264, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.01019437, Test Loss: 0.10645622, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.00880150, Test Loss: 0.13013354, Test Accuracy: 0.97650000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.00853657, Test Loss: 0.12627110, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.00953820, Test Loss: 0.11709766, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.00874483, Test Loss: 0.12166576, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.00655285, Test Loss: 0.14693146, Test Accuracy: 0.97400000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00722527, Test Loss: 0.12761038, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00728301, Test Loss: 0.13114985, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00728377, Test Loss: 0.13190910, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00712015, Test Loss: 0.13620317, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00511692, Test Loss: 0.12261212, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00642080, Test Loss: 0.14584105, Test Accuracy: 0.97610000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00671135, Test Loss: 0.12912386, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00473969, Test Loss: 0.12426794, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00409365, Test Loss: 0.13106085, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00594908, Test Loss: 0.14051398, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00463282, Test Loss: 0.13718757, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00436462, Test Loss: 0.13248294, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00617694, Test Loss: 0.12525073, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00451425, Test Loss: 0.14052188, Test Accuracy: 0.97610000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00399713, Test Loss: 0.12720636, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00364545, Test Loss: 0.12660538, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00531079, Test Loss: 0.15294011, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00420020, Test Loss: 0.14681811, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00583353, Test Loss: 0.12681012, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00426878, Test Loss: 0.15175005, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00416887, Test Loss: 0.13863895, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00246156, Test Loss: 0.14576069, Test Accuracy: 0.97990000\n",
            "[tensor(0.2934, grad_fn=<MeanBackward0>), tensor(0.3236, grad_fn=<MeanBackward0>), tensor(0.3408, grad_fn=<MeanBackward0>), tensor(0.3529, grad_fn=<MeanBackward0>), tensor(0.3630, grad_fn=<MeanBackward0>), tensor(0.3664, grad_fn=<MeanBackward0>), tensor(0.3737, grad_fn=<MeanBackward0>), tensor(0.3833, grad_fn=<MeanBackward0>), tensor(0.3829, grad_fn=<MeanBackward0>), tensor(0.3848, grad_fn=<MeanBackward0>), tensor(0.3924, grad_fn=<MeanBackward0>), tensor(0.3950, grad_fn=<MeanBackward0>), tensor(0.3949, grad_fn=<MeanBackward0>), tensor(0.4000, grad_fn=<MeanBackward0>), tensor(0.3998, grad_fn=<MeanBackward0>), tensor(0.4002, grad_fn=<MeanBackward0>), tensor(0.4048, grad_fn=<MeanBackward0>), tensor(0.4059, grad_fn=<MeanBackward0>), tensor(0.4081, grad_fn=<MeanBackward0>), tensor(0.4080, grad_fn=<MeanBackward0>), tensor(0.4118, grad_fn=<MeanBackward0>), tensor(0.4120, grad_fn=<MeanBackward0>), tensor(0.4084, grad_fn=<MeanBackward0>), tensor(0.4179, grad_fn=<MeanBackward0>), tensor(0.4151, grad_fn=<MeanBackward0>), tensor(0.4151, grad_fn=<MeanBackward0>), tensor(0.4177, grad_fn=<MeanBackward0>), tensor(0.4164, grad_fn=<MeanBackward0>), tensor(0.4215, grad_fn=<MeanBackward0>), tensor(0.4201, grad_fn=<MeanBackward0>), tensor(0.4226, grad_fn=<MeanBackward0>), tensor(0.4225, grad_fn=<MeanBackward0>), tensor(0.4227, grad_fn=<MeanBackward0>), tensor(0.4224, grad_fn=<MeanBackward0>), tensor(0.4253, grad_fn=<MeanBackward0>), tensor(0.4266, grad_fn=<MeanBackward0>), tensor(0.4261, grad_fn=<MeanBackward0>), tensor(0.4337, grad_fn=<MeanBackward0>), tensor(0.4362, grad_fn=<MeanBackward0>), tensor(0.4309, grad_fn=<MeanBackward0>), tensor(0.4359, grad_fn=<MeanBackward0>), tensor(0.4361, grad_fn=<MeanBackward0>), tensor(0.4343, grad_fn=<MeanBackward0>), tensor(0.4373, grad_fn=<MeanBackward0>), tensor(0.4323, grad_fn=<MeanBackward0>), tensor(0.4332, grad_fn=<MeanBackward0>), tensor(0.4356, grad_fn=<MeanBackward0>), tensor(0.4396, grad_fn=<MeanBackward0>), tensor(0.4407, grad_fn=<MeanBackward0>), tensor(0.4462, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2191, grad_fn=<MeanBackward0>), tensor(0.2200, grad_fn=<MeanBackward0>), tensor(0.2325, grad_fn=<MeanBackward0>), tensor(0.2454, grad_fn=<MeanBackward0>), tensor(0.2495, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2740, grad_fn=<MeanBackward0>), tensor(0.2825, grad_fn=<MeanBackward0>), tensor(0.2920, grad_fn=<MeanBackward0>), tensor(0.3032, grad_fn=<MeanBackward0>), tensor(0.3140, grad_fn=<MeanBackward0>), tensor(0.3170, grad_fn=<MeanBackward0>), tensor(0.3334, grad_fn=<MeanBackward0>), tensor(0.3344, grad_fn=<MeanBackward0>), tensor(0.3507, grad_fn=<MeanBackward0>), tensor(0.3696, grad_fn=<MeanBackward0>), tensor(0.3783, grad_fn=<MeanBackward0>), tensor(0.3872, grad_fn=<MeanBackward0>), tensor(0.3808, grad_fn=<MeanBackward0>), tensor(0.3931, grad_fn=<MeanBackward0>), tensor(0.3937, grad_fn=<MeanBackward0>), tensor(0.4114, grad_fn=<MeanBackward0>), tensor(0.4151, grad_fn=<MeanBackward0>), tensor(0.4214, grad_fn=<MeanBackward0>), tensor(0.4388, grad_fn=<MeanBackward0>), tensor(0.4377, grad_fn=<MeanBackward0>), tensor(0.4439, grad_fn=<MeanBackward0>), tensor(0.4481, grad_fn=<MeanBackward0>), tensor(0.4472, grad_fn=<MeanBackward0>), tensor(0.4542, grad_fn=<MeanBackward0>), tensor(0.4413, grad_fn=<MeanBackward0>), tensor(0.4521, grad_fn=<MeanBackward0>), tensor(0.4602, grad_fn=<MeanBackward0>), tensor(0.4614, grad_fn=<MeanBackward0>), tensor(0.4652, grad_fn=<MeanBackward0>), tensor(0.4708, grad_fn=<MeanBackward0>), tensor(0.4790, grad_fn=<MeanBackward0>), tensor(0.4740, grad_fn=<MeanBackward0>), tensor(0.4714, grad_fn=<MeanBackward0>), tensor(0.4820, grad_fn=<MeanBackward0>), tensor(0.4840, grad_fn=<MeanBackward0>), tensor(0.4764, grad_fn=<MeanBackward0>), tensor(0.4876, grad_fn=<MeanBackward0>), tensor(0.4862, grad_fn=<MeanBackward0>), tensor(0.4914, grad_fn=<MeanBackward0>), tensor(0.4930, grad_fn=<MeanBackward0>), tensor(0.4982, grad_fn=<MeanBackward0>), tensor(0.4957, grad_fn=<MeanBackward0>), tensor(0.4957, grad_fn=<MeanBackward0>), tensor(0.4966, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2554, grad_fn=<MeanBackward0>), tensor(0.2652, grad_fn=<MeanBackward0>), tensor(0.2811, grad_fn=<MeanBackward0>), tensor(0.2885, grad_fn=<MeanBackward0>), tensor(0.3025, grad_fn=<MeanBackward0>), tensor(0.3094, grad_fn=<MeanBackward0>), tensor(0.3134, grad_fn=<MeanBackward0>), tensor(0.3274, grad_fn=<MeanBackward0>), tensor(0.3292, grad_fn=<MeanBackward0>), tensor(0.3254, grad_fn=<MeanBackward0>), tensor(0.3417, grad_fn=<MeanBackward0>), tensor(0.3450, grad_fn=<MeanBackward0>), tensor(0.3441, grad_fn=<MeanBackward0>), tensor(0.3505, grad_fn=<MeanBackward0>), tensor(0.3612, grad_fn=<MeanBackward0>), tensor(0.3487, grad_fn=<MeanBackward0>), tensor(0.3542, grad_fn=<MeanBackward0>), tensor(0.3646, grad_fn=<MeanBackward0>), tensor(0.3686, grad_fn=<MeanBackward0>), tensor(0.3620, grad_fn=<MeanBackward0>), tensor(0.3744, grad_fn=<MeanBackward0>), tensor(0.3741, grad_fn=<MeanBackward0>), tensor(0.3712, grad_fn=<MeanBackward0>), tensor(0.3691, grad_fn=<MeanBackward0>), tensor(0.3759, grad_fn=<MeanBackward0>), tensor(0.3789, grad_fn=<MeanBackward0>), tensor(0.3770, grad_fn=<MeanBackward0>), tensor(0.3812, grad_fn=<MeanBackward0>), tensor(0.3815, grad_fn=<MeanBackward0>), tensor(0.3900, grad_fn=<MeanBackward0>), tensor(0.3914, grad_fn=<MeanBackward0>), tensor(0.3936, grad_fn=<MeanBackward0>), tensor(0.3915, grad_fn=<MeanBackward0>), tensor(0.3966, grad_fn=<MeanBackward0>), tensor(0.3969, grad_fn=<MeanBackward0>), tensor(0.3944, grad_fn=<MeanBackward0>), tensor(0.3954, grad_fn=<MeanBackward0>), tensor(0.3984, grad_fn=<MeanBackward0>), tensor(0.4004, grad_fn=<MeanBackward0>), tensor(0.3976, grad_fn=<MeanBackward0>), tensor(0.3977, grad_fn=<MeanBackward0>), tensor(0.4038, grad_fn=<MeanBackward0>), tensor(0.4028, grad_fn=<MeanBackward0>), tensor(0.4037, grad_fn=<MeanBackward0>), tensor(0.4089, grad_fn=<MeanBackward0>), tensor(0.4044, grad_fn=<MeanBackward0>), tensor(0.4080, grad_fn=<MeanBackward0>), tensor(0.4053, grad_fn=<MeanBackward0>), tensor(0.4065, grad_fn=<MeanBackward0>), tensor(0.4103, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2498, grad_fn=<MeanBackward0>), tensor(0.2637, grad_fn=<MeanBackward0>), tensor(0.2754, grad_fn=<MeanBackward0>), tensor(0.2918, grad_fn=<MeanBackward0>), tensor(0.2985, grad_fn=<MeanBackward0>), tensor(0.3069, grad_fn=<MeanBackward0>), tensor(0.3142, grad_fn=<MeanBackward0>), tensor(0.3180, grad_fn=<MeanBackward0>), tensor(0.3302, grad_fn=<MeanBackward0>), tensor(0.3379, grad_fn=<MeanBackward0>), tensor(0.3379, grad_fn=<MeanBackward0>), tensor(0.3410, grad_fn=<MeanBackward0>), tensor(0.3443, grad_fn=<MeanBackward0>), tensor(0.3458, grad_fn=<MeanBackward0>), tensor(0.3480, grad_fn=<MeanBackward0>), tensor(0.3603, grad_fn=<MeanBackward0>), tensor(0.3584, grad_fn=<MeanBackward0>), tensor(0.3597, grad_fn=<MeanBackward0>), tensor(0.3665, grad_fn=<MeanBackward0>), tensor(0.3666, grad_fn=<MeanBackward0>), tensor(0.3656, grad_fn=<MeanBackward0>), tensor(0.3702, grad_fn=<MeanBackward0>), tensor(0.3729, grad_fn=<MeanBackward0>), tensor(0.3756, grad_fn=<MeanBackward0>), tensor(0.3730, grad_fn=<MeanBackward0>), tensor(0.3799, grad_fn=<MeanBackward0>), tensor(0.3873, grad_fn=<MeanBackward0>), tensor(0.3785, grad_fn=<MeanBackward0>), tensor(0.3795, grad_fn=<MeanBackward0>), tensor(0.3861, grad_fn=<MeanBackward0>), tensor(0.3903, grad_fn=<MeanBackward0>), tensor(0.3960, grad_fn=<MeanBackward0>), tensor(0.3919, grad_fn=<MeanBackward0>), tensor(0.3927, grad_fn=<MeanBackward0>), tensor(0.3995, grad_fn=<MeanBackward0>), tensor(0.3964, grad_fn=<MeanBackward0>), tensor(0.3958, grad_fn=<MeanBackward0>), tensor(0.3971, grad_fn=<MeanBackward0>), tensor(0.3997, grad_fn=<MeanBackward0>), tensor(0.4031, grad_fn=<MeanBackward0>), tensor(0.4096, grad_fn=<MeanBackward0>), tensor(0.4115, grad_fn=<MeanBackward0>), tensor(0.4051, grad_fn=<MeanBackward0>), tensor(0.4086, grad_fn=<MeanBackward0>), tensor(0.4088, grad_fn=<MeanBackward0>), tensor(0.4092, grad_fn=<MeanBackward0>), tensor(0.4179, grad_fn=<MeanBackward0>), tensor(0.4213, grad_fn=<MeanBackward0>), tensor(0.4204, grad_fn=<MeanBackward0>), tensor(0.4218, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.25440165400505066, 0.2681394927203655, 0.2824394293129444, 0.2946499213576317, 0.3033596873283386, 0.31006836146116257, 0.3188430964946747, 0.3277875855565071, 0.33358605206012726, 0.33781126141548157, 0.34648547321558, 0.3494623973965645, 0.35417552292346954, 0.357686884701252, 0.3649352341890335, 0.36970164626836777, 0.37392598390579224, 0.3793465793132782, 0.3810003101825714, 0.3824203461408615, 0.38637854158878326, 0.39194177836179733, 0.39190371334552765, 0.39599256962537766, 0.40071403980255127, 0.4028986617922783, 0.4064629599452019, 0.4060535877943039, 0.40742699056863785, 0.4126215726137161, 0.4113771766424179, 0.41607727855443954, 0.41656167805194855, 0.41829051077365875, 0.4217016100883484, 0.42205070704221725, 0.42409490048885345, 0.42581699043512344, 0.4269360676407814, 0.4284050315618515, 0.43180207163095474, 0.4319478049874306, 0.43246880173683167, 0.43396472185850143, 0.43534617125988007, 0.43496283888816833, 0.43994224816560745, 0.4405015707015991, 0.44085172563791275, 0.44369396567344666]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.32647296, Test Loss: 2.30320653, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30898025, Test Loss: 2.30546889, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.30541598, Test Loss: 2.30698907, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.30400411, Test Loss: 2.30339177, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.30317193, Test Loss: 2.30155510, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.30278736, Test Loss: 2.30164361, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.30271599, Test Loss: 2.30123398, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.30217526, Test Loss: 2.30153566, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.30236674, Test Loss: 2.30198787, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.30224591, Test Loss: 2.30156436, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.30213691, Test Loss: 2.30188901, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.30205608, Test Loss: 2.30178010, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.30172829, Test Loss: 2.30113750, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.30177970, Test Loss: 2.30195805, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.30170218, Test Loss: 2.30136621, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.30166336, Test Loss: 2.30143015, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.30151429, Test Loss: 2.30096547, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.30137988, Test Loss: 2.30040289, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 2.30074898, Test Loss: 2.30002708, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 2.30007394, Test Loss: 2.29924981, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 2.29851684, Test Loss: 2.29702772, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 2.29421128, Test Loss: 2.28823511, Test Accuracy: 0.18370000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 2.26724203, Test Loss: 2.20671413, Test Accuracy: 0.22200000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 1.98336970, Test Loss: 1.78385234, Test Accuracy: 0.30450000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 1.71249178, Test Loss: 1.63691515, Test Accuracy: 0.35080000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 1.58146378, Test Loss: 1.50251546, Test Accuracy: 0.40370000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 1.45804784, Test Loss: 1.37996081, Test Accuracy: 0.45910000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 1.35125608, Test Loss: 1.28388827, Test Accuracy: 0.51150000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 1.24847750, Test Loss: 1.15907775, Test Accuracy: 0.57650000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 1.12610913, Test Loss: 1.03637853, Test Accuracy: 0.62350000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.97081711, Test Loss: 0.85967486, Test Accuracy: 0.73130000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.78422849, Test Loss: 0.73829297, Test Accuracy: 0.75250000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.60883161, Test Loss: 0.58131220, Test Accuracy: 0.83320000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.51859185, Test Loss: 0.47750344, Test Accuracy: 0.86860000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.45335431, Test Loss: 0.41198989, Test Accuracy: 0.89040000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.39264779, Test Loss: 0.38744814, Test Accuracy: 0.89260000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.34258194, Test Loss: 0.34703522, Test Accuracy: 0.91030000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.30420756, Test Loss: 0.29267876, Test Accuracy: 0.92220000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.27489017, Test Loss: 0.26456219, Test Accuracy: 0.92900000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.24859731, Test Loss: 0.24222408, Test Accuracy: 0.93560000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.22672054, Test Loss: 0.23085084, Test Accuracy: 0.93650000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.20946911, Test Loss: 0.22053909, Test Accuracy: 0.93890000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.19512775, Test Loss: 0.20243480, Test Accuracy: 0.94360000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.18223747, Test Loss: 0.21499279, Test Accuracy: 0.93820000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.17217478, Test Loss: 0.18561745, Test Accuracy: 0.94790000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.16190069, Test Loss: 0.17851024, Test Accuracy: 0.95070000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.15221237, Test Loss: 0.19687150, Test Accuracy: 0.94440000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.14179598, Test Loss: 0.18526426, Test Accuracy: 0.94730000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.13448367, Test Loss: 0.17377085, Test Accuracy: 0.95180000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.12659389, Test Loss: 0.16043496, Test Accuracy: 0.95510000\n",
            "[tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0048, grad_fn=<MeanBackward0>), tensor(0.0048, grad_fn=<MeanBackward0>), tensor(0.0049, grad_fn=<MeanBackward0>), tensor(0.0049, grad_fn=<MeanBackward0>), tensor(0.0050, grad_fn=<MeanBackward0>), tensor(0.0051, grad_fn=<MeanBackward0>), tensor(0.0051, grad_fn=<MeanBackward0>), tensor(0.0052, grad_fn=<MeanBackward0>), tensor(0.0053, grad_fn=<MeanBackward0>), tensor(0.0054, grad_fn=<MeanBackward0>), tensor(0.0055, grad_fn=<MeanBackward0>), tensor(0.0057, grad_fn=<MeanBackward0>), tensor(0.0060, grad_fn=<MeanBackward0>), tensor(0.0063, grad_fn=<MeanBackward0>), tensor(0.0067, grad_fn=<MeanBackward0>), tensor(0.0074, grad_fn=<MeanBackward0>), tensor(0.0083, grad_fn=<MeanBackward0>), tensor(0.0099, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0225, grad_fn=<MeanBackward0>), tensor(0.0399, grad_fn=<MeanBackward0>), tensor(0.0438, grad_fn=<MeanBackward0>), tensor(0.0663, grad_fn=<MeanBackward0>), tensor(0.0809, grad_fn=<MeanBackward0>), tensor(0.0913, grad_fn=<MeanBackward0>), tensor(0.0999, grad_fn=<MeanBackward0>), tensor(0.1076, grad_fn=<MeanBackward0>), tensor(0.1189, grad_fn=<MeanBackward0>), tensor(0.1379, grad_fn=<MeanBackward0>), tensor(0.1591, grad_fn=<MeanBackward0>), tensor(0.1776, grad_fn=<MeanBackward0>), tensor(0.1906, grad_fn=<MeanBackward0>), tensor(0.1991, grad_fn=<MeanBackward0>), tensor(0.2062, grad_fn=<MeanBackward0>), tensor(0.2124, grad_fn=<MeanBackward0>), tensor(0.2191, grad_fn=<MeanBackward0>), tensor(0.2254, grad_fn=<MeanBackward0>), tensor(0.2286, grad_fn=<MeanBackward0>), tensor(0.2339, grad_fn=<MeanBackward0>), tensor(0.2375, grad_fn=<MeanBackward0>), tensor(0.2420, grad_fn=<MeanBackward0>), tensor(0.2461, grad_fn=<MeanBackward0>), tensor(0.2498, grad_fn=<MeanBackward0>), tensor(0.2540, grad_fn=<MeanBackward0>), tensor(0.2573, grad_fn=<MeanBackward0>), tensor(0.2609, grad_fn=<MeanBackward0>), tensor(0.2637, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0106, grad_fn=<MeanBackward0>), tensor(0.0107, grad_fn=<MeanBackward0>), tensor(0.0108, grad_fn=<MeanBackward0>), tensor(0.0109, grad_fn=<MeanBackward0>), tensor(0.0110, grad_fn=<MeanBackward0>), tensor(0.0110, grad_fn=<MeanBackward0>), tensor(0.0111, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0110, grad_fn=<MeanBackward0>), tensor(0.0105, grad_fn=<MeanBackward0>), tensor(0.0096, grad_fn=<MeanBackward0>), tensor(0.0101, grad_fn=<MeanBackward0>), tensor(0.0466, grad_fn=<MeanBackward0>), tensor(0.0565, grad_fn=<MeanBackward0>), tensor(0.0579, grad_fn=<MeanBackward0>), tensor(0.0577, grad_fn=<MeanBackward0>), tensor(0.0570, grad_fn=<MeanBackward0>), tensor(0.0594, grad_fn=<MeanBackward0>), tensor(0.0572, grad_fn=<MeanBackward0>), tensor(0.0552, grad_fn=<MeanBackward0>), tensor(0.0633, grad_fn=<MeanBackward0>), tensor(0.0692, grad_fn=<MeanBackward0>), tensor(0.0641, grad_fn=<MeanBackward0>), tensor(0.0642, grad_fn=<MeanBackward0>), tensor(0.0627, grad_fn=<MeanBackward0>), tensor(0.0675, grad_fn=<MeanBackward0>), tensor(0.0702, grad_fn=<MeanBackward0>), tensor(0.0716, grad_fn=<MeanBackward0>), tensor(0.0712, grad_fn=<MeanBackward0>), tensor(0.0718, grad_fn=<MeanBackward0>), tensor(0.0737, grad_fn=<MeanBackward0>), tensor(0.0740, grad_fn=<MeanBackward0>), tensor(0.0715, grad_fn=<MeanBackward0>), tensor(0.0723, grad_fn=<MeanBackward0>), tensor(0.0734, grad_fn=<MeanBackward0>), tensor(0.0733, grad_fn=<MeanBackward0>), tensor(0.0685, grad_fn=<MeanBackward0>), tensor(0.0704, grad_fn=<MeanBackward0>), tensor(0.0700, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0123, grad_fn=<MeanBackward0>), tensor(0.0131, grad_fn=<MeanBackward0>), tensor(0.0132, grad_fn=<MeanBackward0>), tensor(0.0131, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0127, grad_fn=<MeanBackward0>), tensor(0.0124, grad_fn=<MeanBackward0>), tensor(0.0122, grad_fn=<MeanBackward0>), tensor(0.0120, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0111, grad_fn=<MeanBackward0>), tensor(0.0109, grad_fn=<MeanBackward0>), tensor(0.0107, grad_fn=<MeanBackward0>), tensor(0.0104, grad_fn=<MeanBackward0>), tensor(0.0100, grad_fn=<MeanBackward0>), tensor(0.0096, grad_fn=<MeanBackward0>), tensor(0.0089, grad_fn=<MeanBackward0>), tensor(0.0078, grad_fn=<MeanBackward0>), tensor(0.0059, grad_fn=<MeanBackward0>), tensor(0.0030, grad_fn=<MeanBackward0>), tensor(0.0155, grad_fn=<MeanBackward0>), tensor(0.0410, grad_fn=<MeanBackward0>), tensor(0.0877, grad_fn=<MeanBackward0>), tensor(0.1203, grad_fn=<MeanBackward0>), tensor(0.1321, grad_fn=<MeanBackward0>), tensor(0.1527, grad_fn=<MeanBackward0>), tensor(0.1561, grad_fn=<MeanBackward0>), tensor(0.1499, grad_fn=<MeanBackward0>), tensor(0.1583, grad_fn=<MeanBackward0>), tensor(0.1792, grad_fn=<MeanBackward0>), tensor(0.1697, grad_fn=<MeanBackward0>), tensor(0.1737, grad_fn=<MeanBackward0>), tensor(0.1470, grad_fn=<MeanBackward0>), tensor(0.1604, grad_fn=<MeanBackward0>), tensor(0.1496, grad_fn=<MeanBackward0>), tensor(0.1409, grad_fn=<MeanBackward0>), tensor(0.1359, grad_fn=<MeanBackward0>), tensor(0.1273, grad_fn=<MeanBackward0>), tensor(0.1347, grad_fn=<MeanBackward0>), tensor(0.1369, grad_fn=<MeanBackward0>), tensor(0.1320, grad_fn=<MeanBackward0>), tensor(0.1305, grad_fn=<MeanBackward0>), tensor(0.1351, grad_fn=<MeanBackward0>), tensor(0.1357, grad_fn=<MeanBackward0>), tensor(0.1296, grad_fn=<MeanBackward0>), tensor(0.1399, grad_fn=<MeanBackward0>), tensor(0.1364, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0063, grad_fn=<MeanBackward0>), tensor(0.0033, grad_fn=<MeanBackward0>), tensor(0.0021, grad_fn=<MeanBackward0>), tensor(0.0014, grad_fn=<MeanBackward0>), tensor(0.0011, grad_fn=<MeanBackward0>), tensor(0.0010, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0006, grad_fn=<MeanBackward0>), tensor(0.0006, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0009, grad_fn=<MeanBackward0>), tensor(0.0010, grad_fn=<MeanBackward0>), tensor(0.0011, grad_fn=<MeanBackward0>), tensor(0.0020, grad_fn=<MeanBackward0>), tensor(0.0031, grad_fn=<MeanBackward0>), tensor(0.0055, grad_fn=<MeanBackward0>), tensor(0.0085, grad_fn=<MeanBackward0>), tensor(0.0095, grad_fn=<MeanBackward0>), tensor(0.0163, grad_fn=<MeanBackward0>), tensor(0.1065, grad_fn=<MeanBackward0>), tensor(0.2289, grad_fn=<MeanBackward0>), tensor(0.3375, grad_fn=<MeanBackward0>), tensor(0.4073, grad_fn=<MeanBackward0>), tensor(0.4254, grad_fn=<MeanBackward0>), tensor(0.4352, grad_fn=<MeanBackward0>), tensor(0.4097, grad_fn=<MeanBackward0>), tensor(0.3723, grad_fn=<MeanBackward0>), tensor(0.3472, grad_fn=<MeanBackward0>), tensor(0.3835, grad_fn=<MeanBackward0>), tensor(0.3655, grad_fn=<MeanBackward0>), tensor(0.3782, grad_fn=<MeanBackward0>), tensor(0.3494, grad_fn=<MeanBackward0>), tensor(0.3492, grad_fn=<MeanBackward0>), tensor(0.3589, grad_fn=<MeanBackward0>), tensor(0.3625, grad_fn=<MeanBackward0>), tensor(0.3746, grad_fn=<MeanBackward0>), tensor(0.3596, grad_fn=<MeanBackward0>), tensor(0.3667, grad_fn=<MeanBackward0>), tensor(0.3773, grad_fn=<MeanBackward0>), tensor(0.3771, grad_fn=<MeanBackward0>), tensor(0.3664, grad_fn=<MeanBackward0>), tensor(0.3596, grad_fn=<MeanBackward0>), tensor(0.3896, grad_fn=<MeanBackward0>), tensor(0.3639, grad_fn=<MeanBackward0>), tensor(0.3678, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.008444311213679612, 0.00795461080269888, 0.007709801662713289, 0.0075403814553283155, 0.007445583876688033, 0.007388019992504269, 0.007302492318558507, 0.0072555694641778246, 0.007227471025544219, 0.007210023250081576, 0.007162021458498202, 0.0071589119179407135, 0.007123996183509007, 0.007123996809241362, 0.007157711399486288, 0.007237965794047341, 0.007254215044667944, 0.007289603119716048, 0.007539276382885873, 0.007835502037778497, 0.008441394777037203, 0.009194928919896483, 0.01126961357658729, 0.029550060629844666, 0.06196377892047167, 0.11021000798791647, 0.14911472517997026, 0.17188785318285227, 0.18434772547334433, 0.18902782443910837, 0.1834421530365944, 0.18296268582344055, 0.18867453932762146, 0.19872639514505863, 0.1984885260462761, 0.1967579498887062, 0.19587615504860878, 0.1953859142959118, 0.19764156825840473, 0.19872578419744968, 0.2005766797810793, 0.2004352416843176, 0.2037817183881998, 0.20569603703916073, 0.2065149862319231, 0.2061896827071905, 0.20565867237746716, 0.21124069578945637, 0.20878545194864273, 0.20947911590337753]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvVFXsC7jLkC",
        "outputId": "b175f2b5-8df0-4547-d49d-ce945d12e28f"
      },
      "source": [
        "torch.manual_seed(100)\n",
        "np.random.seed(100)\n",
        "\n",
        "model_factory('Adadelta', 100)\n",
        "model_factory('Adagrad', 100)\n",
        "model_factory('Adam', 100)\n",
        "model_factory('SGD', 100)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-06\n",
            "    lr: 1.0\n",
            "    rho: 0.9\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.30741241, Test Loss: 2.30201490, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30162291, Test Loss: 2.30123923, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.29944219, Test Loss: 2.28878242, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 1.88691054, Test Loss: 1.66493252, Test Accuracy: 0.31360000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 1.26352911, Test Loss: 0.71505122, Test Accuracy: 0.80870000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.48219014, Test Loss: 0.33749985, Test Accuracy: 0.90230000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.27498378, Test Loss: 0.21221343, Test Accuracy: 0.94110000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.19264627, Test Loss: 0.16557703, Test Accuracy: 0.95190000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.14983726, Test Loss: 0.14614338, Test Accuracy: 0.95770000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.12232674, Test Loss: 0.11910416, Test Accuracy: 0.96410000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.10413701, Test Loss: 0.12248394, Test Accuracy: 0.96420000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.08863543, Test Loss: 0.10077042, Test Accuracy: 0.97020000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.07760290, Test Loss: 0.10632458, Test Accuracy: 0.97010000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.06889146, Test Loss: 0.09283234, Test Accuracy: 0.97400000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.06062922, Test Loss: 0.09367127, Test Accuracy: 0.97310000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.05302781, Test Loss: 0.09743297, Test Accuracy: 0.97310000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.04642951, Test Loss: 0.09579489, Test Accuracy: 0.97340000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.04161406, Test Loss: 0.10063842, Test Accuracy: 0.97310000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.03816646, Test Loss: 0.12526976, Test Accuracy: 0.96850000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.03373059, Test Loss: 0.09801137, Test Accuracy: 0.97390000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.03013606, Test Loss: 0.12714676, Test Accuracy: 0.97010000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.02630665, Test Loss: 0.09998583, Test Accuracy: 0.97550000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.02344878, Test Loss: 0.09433635, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.02075069, Test Loss: 0.14788091, Test Accuracy: 0.96580000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.01870166, Test Loss: 0.10490179, Test Accuracy: 0.97560000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.01618644, Test Loss: 0.10282663, Test Accuracy: 0.97610000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.01559646, Test Loss: 0.10825716, Test Accuracy: 0.97650000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.01331202, Test Loss: 0.11084707, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.01068016, Test Loss: 0.11208189, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00989511, Test Loss: 0.10749095, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00813744, Test Loss: 0.11479819, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00673175, Test Loss: 0.10496204, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00611818, Test Loss: 0.11016926, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00542750, Test Loss: 0.11703173, Test Accuracy: 0.97600000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00407810, Test Loss: 0.11833709, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00337287, Test Loss: 0.11642531, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00308040, Test Loss: 0.11883133, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00290544, Test Loss: 0.11793873, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00253037, Test Loss: 0.11741727, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00208047, Test Loss: 0.12016554, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00234951, Test Loss: 0.12179821, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00173569, Test Loss: 0.12200125, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00146968, Test Loss: 0.11892515, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00128068, Test Loss: 0.12309764, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00120501, Test Loss: 0.12446270, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00113985, Test Loss: 0.12452874, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00104960, Test Loss: 0.12672085, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00094648, Test Loss: 0.12737097, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00090876, Test Loss: 0.12521051, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00077405, Test Loss: 0.12444321, Test Accuracy: 0.97840000\n",
            "[tensor(0.0052, grad_fn=<MeanBackward0>), tensor(0.0063, grad_fn=<MeanBackward0>), tensor(0.0144, grad_fn=<MeanBackward0>), tensor(0.1099, grad_fn=<MeanBackward0>), tensor(0.2608, grad_fn=<MeanBackward0>), tensor(0.3526, grad_fn=<MeanBackward0>), tensor(0.3899, grad_fn=<MeanBackward0>), tensor(0.4093, grad_fn=<MeanBackward0>), tensor(0.4174, grad_fn=<MeanBackward0>), tensor(0.4182, grad_fn=<MeanBackward0>), tensor(0.4246, grad_fn=<MeanBackward0>), tensor(0.4265, grad_fn=<MeanBackward0>), tensor(0.4300, grad_fn=<MeanBackward0>), tensor(0.4301, grad_fn=<MeanBackward0>), tensor(0.4340, grad_fn=<MeanBackward0>), tensor(0.4366, grad_fn=<MeanBackward0>), tensor(0.4363, grad_fn=<MeanBackward0>), tensor(0.4357, grad_fn=<MeanBackward0>), tensor(0.4394, grad_fn=<MeanBackward0>), tensor(0.4404, grad_fn=<MeanBackward0>), tensor(0.4411, grad_fn=<MeanBackward0>), tensor(0.4428, grad_fn=<MeanBackward0>), tensor(0.4459, grad_fn=<MeanBackward0>), tensor(0.4462, grad_fn=<MeanBackward0>), tensor(0.4439, grad_fn=<MeanBackward0>), tensor(0.4431, grad_fn=<MeanBackward0>), tensor(0.4482, grad_fn=<MeanBackward0>), tensor(0.4478, grad_fn=<MeanBackward0>), tensor(0.4485, grad_fn=<MeanBackward0>), tensor(0.4478, grad_fn=<MeanBackward0>), tensor(0.4489, grad_fn=<MeanBackward0>), tensor(0.4446, grad_fn=<MeanBackward0>), tensor(0.4470, grad_fn=<MeanBackward0>), tensor(0.4465, grad_fn=<MeanBackward0>), tensor(0.4445, grad_fn=<MeanBackward0>), tensor(0.4450, grad_fn=<MeanBackward0>), tensor(0.4455, grad_fn=<MeanBackward0>), tensor(0.4443, grad_fn=<MeanBackward0>), tensor(0.4450, grad_fn=<MeanBackward0>), tensor(0.4438, grad_fn=<MeanBackward0>), tensor(0.4463, grad_fn=<MeanBackward0>), tensor(0.4449, grad_fn=<MeanBackward0>), tensor(0.4465, grad_fn=<MeanBackward0>), tensor(0.4458, grad_fn=<MeanBackward0>), tensor(0.4462, grad_fn=<MeanBackward0>), tensor(0.4456, grad_fn=<MeanBackward0>), tensor(0.4452, grad_fn=<MeanBackward0>), tensor(0.4447, grad_fn=<MeanBackward0>), tensor(0.4449, grad_fn=<MeanBackward0>), tensor(0.4443, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0133, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0085, grad_fn=<MeanBackward0>), tensor(0.0774, grad_fn=<MeanBackward0>), tensor(0.1738, grad_fn=<MeanBackward0>), tensor(0.2058, grad_fn=<MeanBackward0>), tensor(0.2246, grad_fn=<MeanBackward0>), tensor(0.2417, grad_fn=<MeanBackward0>), tensor(0.2551, grad_fn=<MeanBackward0>), tensor(0.2577, grad_fn=<MeanBackward0>), tensor(0.2603, grad_fn=<MeanBackward0>), tensor(0.2618, grad_fn=<MeanBackward0>), tensor(0.2656, grad_fn=<MeanBackward0>), tensor(0.2652, grad_fn=<MeanBackward0>), tensor(0.2765, grad_fn=<MeanBackward0>), tensor(0.2713, grad_fn=<MeanBackward0>), tensor(0.2736, grad_fn=<MeanBackward0>), tensor(0.2729, grad_fn=<MeanBackward0>), tensor(0.2726, grad_fn=<MeanBackward0>), tensor(0.2703, grad_fn=<MeanBackward0>), tensor(0.2699, grad_fn=<MeanBackward0>), tensor(0.2748, grad_fn=<MeanBackward0>), tensor(0.2702, grad_fn=<MeanBackward0>), tensor(0.2676, grad_fn=<MeanBackward0>), tensor(0.2771, grad_fn=<MeanBackward0>), tensor(0.2748, grad_fn=<MeanBackward0>), tensor(0.2714, grad_fn=<MeanBackward0>), tensor(0.2669, grad_fn=<MeanBackward0>), tensor(0.2670, grad_fn=<MeanBackward0>), tensor(0.2653, grad_fn=<MeanBackward0>), tensor(0.2622, grad_fn=<MeanBackward0>), tensor(0.2678, grad_fn=<MeanBackward0>), tensor(0.2619, grad_fn=<MeanBackward0>), tensor(0.2596, grad_fn=<MeanBackward0>), tensor(0.2595, grad_fn=<MeanBackward0>), tensor(0.2600, grad_fn=<MeanBackward0>), tensor(0.2579, grad_fn=<MeanBackward0>), tensor(0.2541, grad_fn=<MeanBackward0>), tensor(0.2536, grad_fn=<MeanBackward0>), tensor(0.2526, grad_fn=<MeanBackward0>), tensor(0.2497, grad_fn=<MeanBackward0>), tensor(0.2517, grad_fn=<MeanBackward0>), tensor(0.2470, grad_fn=<MeanBackward0>), tensor(0.2466, grad_fn=<MeanBackward0>), tensor(0.2454, grad_fn=<MeanBackward0>), tensor(0.2457, grad_fn=<MeanBackward0>), tensor(0.2442, grad_fn=<MeanBackward0>), tensor(0.2463, grad_fn=<MeanBackward0>), tensor(0.2450, grad_fn=<MeanBackward0>), tensor(0.2451, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0083, grad_fn=<MeanBackward0>), tensor(0.0064, grad_fn=<MeanBackward0>), tensor(0.0028, grad_fn=<MeanBackward0>), tensor(0.1190, grad_fn=<MeanBackward0>), tensor(0.1873, grad_fn=<MeanBackward0>), tensor(0.2381, grad_fn=<MeanBackward0>), tensor(0.2446, grad_fn=<MeanBackward0>), tensor(0.2606, grad_fn=<MeanBackward0>), tensor(0.2632, grad_fn=<MeanBackward0>), tensor(0.2673, grad_fn=<MeanBackward0>), tensor(0.2729, grad_fn=<MeanBackward0>), tensor(0.2823, grad_fn=<MeanBackward0>), tensor(0.2885, grad_fn=<MeanBackward0>), tensor(0.2705, grad_fn=<MeanBackward0>), tensor(0.2897, grad_fn=<MeanBackward0>), tensor(0.2822, grad_fn=<MeanBackward0>), tensor(0.2872, grad_fn=<MeanBackward0>), tensor(0.2913, grad_fn=<MeanBackward0>), tensor(0.2923, grad_fn=<MeanBackward0>), tensor(0.2875, grad_fn=<MeanBackward0>), tensor(0.2616, grad_fn=<MeanBackward0>), tensor(0.2711, grad_fn=<MeanBackward0>), tensor(0.2925, grad_fn=<MeanBackward0>), tensor(0.2792, grad_fn=<MeanBackward0>), tensor(0.2929, grad_fn=<MeanBackward0>), tensor(0.2747, grad_fn=<MeanBackward0>), tensor(0.2851, grad_fn=<MeanBackward0>), tensor(0.2822, grad_fn=<MeanBackward0>), tensor(0.3039, grad_fn=<MeanBackward0>), tensor(0.2983, grad_fn=<MeanBackward0>), tensor(0.2894, grad_fn=<MeanBackward0>), tensor(0.2947, grad_fn=<MeanBackward0>), tensor(0.3003, grad_fn=<MeanBackward0>), tensor(0.2879, grad_fn=<MeanBackward0>), tensor(0.2981, grad_fn=<MeanBackward0>), tensor(0.3085, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3048, grad_fn=<MeanBackward0>), tensor(0.3023, grad_fn=<MeanBackward0>), tensor(0.3080, grad_fn=<MeanBackward0>), tensor(0.3069, grad_fn=<MeanBackward0>), tensor(0.3131, grad_fn=<MeanBackward0>), tensor(0.3084, grad_fn=<MeanBackward0>), tensor(0.3062, grad_fn=<MeanBackward0>), tensor(0.3064, grad_fn=<MeanBackward0>), tensor(0.3069, grad_fn=<MeanBackward0>), tensor(0.2970, grad_fn=<MeanBackward0>), tensor(0.3079, grad_fn=<MeanBackward0>), tensor(0.3063, grad_fn=<MeanBackward0>), tensor(0.3070, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0094, grad_fn=<MeanBackward0>), tensor(0.0074, grad_fn=<MeanBackward0>), tensor(0.0167, grad_fn=<MeanBackward0>), tensor(0.6537, grad_fn=<MeanBackward0>), tensor(0.5709, grad_fn=<MeanBackward0>), tensor(0.5720, grad_fn=<MeanBackward0>), tensor(0.5759, grad_fn=<MeanBackward0>), tensor(0.5453, grad_fn=<MeanBackward0>), tensor(0.5359, grad_fn=<MeanBackward0>), tensor(0.5390, grad_fn=<MeanBackward0>), tensor(0.5447, grad_fn=<MeanBackward0>), tensor(0.5388, grad_fn=<MeanBackward0>), tensor(0.5363, grad_fn=<MeanBackward0>), tensor(0.5634, grad_fn=<MeanBackward0>), tensor(0.5291, grad_fn=<MeanBackward0>), tensor(0.5430, grad_fn=<MeanBackward0>), tensor(0.5432, grad_fn=<MeanBackward0>), tensor(0.5480, grad_fn=<MeanBackward0>), tensor(0.5524, grad_fn=<MeanBackward0>), tensor(0.5705, grad_fn=<MeanBackward0>), tensor(0.5800, grad_fn=<MeanBackward0>), tensor(0.5827, grad_fn=<MeanBackward0>), tensor(0.5693, grad_fn=<MeanBackward0>), tensor(0.5814, grad_fn=<MeanBackward0>), tensor(0.5608, grad_fn=<MeanBackward0>), tensor(0.5780, grad_fn=<MeanBackward0>), tensor(0.5774, grad_fn=<MeanBackward0>), tensor(0.5873, grad_fn=<MeanBackward0>), tensor(0.5648, grad_fn=<MeanBackward0>), tensor(0.5681, grad_fn=<MeanBackward0>), tensor(0.5778, grad_fn=<MeanBackward0>), tensor(0.5729, grad_fn=<MeanBackward0>), tensor(0.5730, grad_fn=<MeanBackward0>), tensor(0.5728, grad_fn=<MeanBackward0>), tensor(0.5716, grad_fn=<MeanBackward0>), tensor(0.5604, grad_fn=<MeanBackward0>), tensor(0.5612, grad_fn=<MeanBackward0>), tensor(0.5675, grad_fn=<MeanBackward0>), tensor(0.5697, grad_fn=<MeanBackward0>), tensor(0.5629, grad_fn=<MeanBackward0>), tensor(0.5652, grad_fn=<MeanBackward0>), tensor(0.5564, grad_fn=<MeanBackward0>), tensor(0.5596, grad_fn=<MeanBackward0>), tensor(0.5604, grad_fn=<MeanBackward0>), tensor(0.5648, grad_fn=<MeanBackward0>), tensor(0.5596, grad_fn=<MeanBackward0>), tensor(0.5735, grad_fn=<MeanBackward0>), tensor(0.5618, grad_fn=<MeanBackward0>), tensor(0.5625, grad_fn=<MeanBackward0>), tensor(0.5640, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.009051624336279929, 0.008240504655987024, 0.010602280963212252, 0.24000471085309982, 0.29820074886083603, 0.3421092852950096, 0.3587622754275799, 0.3642297498881817, 0.36789438873529434, 0.370538592338562, 0.37562815099954605, 0.37737368047237396, 0.3800833746790886, 0.3823147192597389, 0.3823230713605881, 0.38326673209667206, 0.3850577175617218, 0.38697699457407, 0.3891899213194847, 0.3921632170677185, 0.388137124478817, 0.3928566351532936, 0.39446480572223663, 0.3935963660478592, 0.3936666399240494, 0.39265603572130203, 0.3955387994647026, 0.396037295460701, 0.3960510268807411, 0.39486998319625854, 0.3945489302277565, 0.39499515295028687, 0.39555474370718, 0.3916924148797989, 0.3934198170900345, 0.3934665098786354, 0.3948361724615097, 0.39266323298215866, 0.39265352487564087, 0.3918222263455391, 0.3920317254960537, 0.3915278911590576, 0.390364870429039, 0.3897448964416981, 0.3907136246562004, 0.38946113362908363, 0.3899542987346649, 0.39017949253320694, 0.3896633982658386, 0.3900853395462036]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adagrad (\n",
            "Parameter Group 0\n",
            "    eps: 1e-10\n",
            "    initial_accumulator_value: 0\n",
            "    lr: 0.1\n",
            "    lr_decay: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.44826035, Test Loss: 2.32040839, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.33323578, Test Loss: 2.31868608, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.32574070, Test Loss: 2.31181767, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.32222157, Test Loss: 2.32265028, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.31839305, Test Loss: 2.31226132, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.31817641, Test Loss: 2.31909922, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.31686433, Test Loss: 2.30656602, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.31621856, Test Loss: 2.30877484, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.31479976, Test Loss: 2.30557861, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.31392092, Test Loss: 2.31500641, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.31318187, Test Loss: 2.31370846, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.31244862, Test Loss: 2.31191309, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.31195119, Test Loss: 2.31050504, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.31210343, Test Loss: 2.31723591, Test Accuracy: 0.08920000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.31100083, Test Loss: 2.30570880, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.31103665, Test Loss: 2.30912531, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.31100477, Test Loss: 2.31344296, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.31073687, Test Loss: 2.31262932, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 2.31056917, Test Loss: 2.30818274, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 2.31041571, Test Loss: 2.30978706, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 2.30982578, Test Loss: 2.31129679, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 2.31011918, Test Loss: 2.30852944, Test Accuracy: 0.08920000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 2.30925003, Test Loss: 2.30506796, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 2.30943036, Test Loss: 2.31007943, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 2.30984910, Test Loss: 2.31034604, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 2.30893674, Test Loss: 2.30536987, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 2.30824466, Test Loss: 2.30357355, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 2.30863134, Test Loss: 2.30795430, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 2.30860052, Test Loss: 2.31171233, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 2.30801860, Test Loss: 2.30439942, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 2.30847722, Test Loss: 2.30929485, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 2.30862363, Test Loss: 2.30731819, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 2.30796133, Test Loss: 2.30726622, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 2.30798878, Test Loss: 2.30388076, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 2.30753334, Test Loss: 2.31127131, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 2.30775161, Test Loss: 2.30802438, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 2.30744532, Test Loss: 2.30528811, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 2.30746528, Test Loss: 2.31017886, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 2.30753740, Test Loss: 2.30657174, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 2.30775788, Test Loss: 2.30635290, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 2.30752829, Test Loss: 2.30871112, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 2.30787433, Test Loss: 2.30487000, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 2.30690393, Test Loss: 2.30793014, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 2.30706057, Test Loss: 2.30900697, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 2.30722827, Test Loss: 2.30691617, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 2.30732534, Test Loss: 2.31213381, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 2.30681391, Test Loss: 2.30385978, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 2.30672596, Test Loss: 2.30553020, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 2.30697215, Test Loss: 2.30824526, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 2.30629209, Test Loss: 2.31084276, Test Accuracy: 0.10320000\n",
            "[tensor(0.2189, grad_fn=<MeanBackward0>), tensor(0.2189, grad_fn=<MeanBackward0>), tensor(0.2189, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>), tensor(0.2465, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3040, grad_fn=<MeanBackward0>), tensor(0.3037, grad_fn=<MeanBackward0>), tensor(0.3035, grad_fn=<MeanBackward0>), tensor(0.2981, grad_fn=<MeanBackward0>), tensor(0.2981, grad_fn=<MeanBackward0>), tensor(0.2981, grad_fn=<MeanBackward0>), tensor(0.2981, grad_fn=<MeanBackward0>), tensor(0.2981, grad_fn=<MeanBackward0>), tensor(0.2982, grad_fn=<MeanBackward0>), tensor(0.2982, grad_fn=<MeanBackward0>), tensor(0.2982, grad_fn=<MeanBackward0>), tensor(0.2982, grad_fn=<MeanBackward0>), tensor(0.2983, grad_fn=<MeanBackward0>), tensor(0.2983, grad_fn=<MeanBackward0>), tensor(0.2983, grad_fn=<MeanBackward0>), tensor(0.2983, grad_fn=<MeanBackward0>), tensor(0.2984, grad_fn=<MeanBackward0>), tensor(0.2984, grad_fn=<MeanBackward0>), tensor(0.2984, grad_fn=<MeanBackward0>), tensor(0.2984, grad_fn=<MeanBackward0>), tensor(0.2984, grad_fn=<MeanBackward0>), tensor(0.2984, grad_fn=<MeanBackward0>), tensor(0.2984, grad_fn=<MeanBackward0>), tensor(0.2985, grad_fn=<MeanBackward0>), tensor(0.2985, grad_fn=<MeanBackward0>), tensor(0.2985, grad_fn=<MeanBackward0>), tensor(0.2985, grad_fn=<MeanBackward0>), tensor(0.2985, grad_fn=<MeanBackward0>), tensor(0.2985, grad_fn=<MeanBackward0>), tensor(0.2985, grad_fn=<MeanBackward0>), tensor(0.2985, grad_fn=<MeanBackward0>), tensor(0.2985, grad_fn=<MeanBackward0>), tensor(0.2985, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2949, grad_fn=<MeanBackward0>), tensor(0.2949, grad_fn=<MeanBackward0>), tensor(0.2949, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>), tensor(0.3154, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.28329430148005486, 0.2832220755517483, 0.2831467129290104, 0.2879938334226608, 0.2879971116781235, 0.2880002111196518, 0.28800297528505325, 0.28800608590245247, 0.28801126405596733, 0.28801969438791275, 0.28802745416760445, 0.28803444653749466, 0.2880410924553871, 0.28804681450128555, 0.2880524657666683, 0.2880575880408287, 0.2880623787641525, 0.28806669637560844, 0.2880709543824196, 0.2880747728049755, 0.28807828947901726, 0.2880818769335747, 0.28808486461639404, 0.2880878858268261, 0.2880907282233238, 0.28809356316924095, 0.28809618577361107, 0.28809887543320656, 0.2881012633442879, 0.288103386759758, 0.28810541331768036, 0.28810735419392586, 0.2881091758608818, 0.28811081498861313, 0.28811239823699, 0.2881140075623989, 0.288115493953228, 0.28811682760715485, 0.288118127733469, 0.28811925277113914, 0.28812041506171227, 0.2881215028464794, 0.2881224974989891, 0.28812338784337044, 0.2881241589784622, 0.28812485933303833, 0.2881254516541958, 0.28812598064541817, 0.28812646120786667, 0.28812678158283234]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 0.65697816, Test Loss: 0.28428689, Test Accuracy: 0.92230000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 0.21632076, Test Loss: 0.18385929, Test Accuracy: 0.94760000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 0.14272761, Test Loss: 0.15748303, Test Accuracy: 0.95350000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 0.10698658, Test Loss: 0.11238963, Test Accuracy: 0.96790000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.08644056, Test Loss: 0.10776763, Test Accuracy: 0.96680000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.06996219, Test Loss: 0.11100169, Test Accuracy: 0.96860000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.05804158, Test Loss: 0.08447942, Test Accuracy: 0.97640000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.04756188, Test Loss: 0.09373774, Test Accuracy: 0.97270000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.04096874, Test Loss: 0.10649630, Test Accuracy: 0.97230000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.03367784, Test Loss: 0.08688557, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.03140000, Test Loss: 0.08914334, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.02616241, Test Loss: 0.10441201, Test Accuracy: 0.97400000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.02386507, Test Loss: 0.10453352, Test Accuracy: 0.97500000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.02223626, Test Loss: 0.09965132, Test Accuracy: 0.97670000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.01623901, Test Loss: 0.09961726, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.01493905, Test Loss: 0.10694100, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.01628979, Test Loss: 0.10328705, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.01549254, Test Loss: 0.09765489, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.01221247, Test Loss: 0.11523367, Test Accuracy: 0.97550000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.01215005, Test Loss: 0.10382759, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.00992177, Test Loss: 0.10351904, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.01002474, Test Loss: 0.11513205, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.01127725, Test Loss: 0.11938290, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.00722673, Test Loss: 0.13326748, Test Accuracy: 0.97550000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.00926952, Test Loss: 0.10724750, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.00848145, Test Loss: 0.12525359, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.00850538, Test Loss: 0.10396295, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.00736271, Test Loss: 0.11690963, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00857008, Test Loss: 0.12127572, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00543956, Test Loss: 0.11343727, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00770273, Test Loss: 0.12183715, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00526561, Test Loss: 0.11059273, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00622100, Test Loss: 0.13289022, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00811987, Test Loss: 0.11453726, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00385260, Test Loss: 0.11430452, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00523552, Test Loss: 0.12485433, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00466560, Test Loss: 0.14206052, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00553115, Test Loss: 0.14886263, Test Accuracy: 0.97660000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00632579, Test Loss: 0.12623926, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00502727, Test Loss: 0.12174207, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00239355, Test Loss: 0.13466802, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00675447, Test Loss: 0.10998790, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00414114, Test Loss: 0.12515032, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00452586, Test Loss: 0.12185750, Test Accuracy: 0.98300000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00512297, Test Loss: 0.13088301, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00487880, Test Loss: 0.12382734, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00279969, Test Loss: 0.15041371, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00582292, Test Loss: 0.13067319, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00426223, Test Loss: 0.13278125, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00397726, Test Loss: 0.15179911, Test Accuracy: 0.97860000\n",
            "[tensor(0.2627, grad_fn=<MeanBackward0>), tensor(0.2873, grad_fn=<MeanBackward0>), tensor(0.3031, grad_fn=<MeanBackward0>), tensor(0.3161, grad_fn=<MeanBackward0>), tensor(0.3218, grad_fn=<MeanBackward0>), tensor(0.3357, grad_fn=<MeanBackward0>), tensor(0.3355, grad_fn=<MeanBackward0>), tensor(0.3400, grad_fn=<MeanBackward0>), tensor(0.3459, grad_fn=<MeanBackward0>), tensor(0.3475, grad_fn=<MeanBackward0>), tensor(0.3543, grad_fn=<MeanBackward0>), tensor(0.3553, grad_fn=<MeanBackward0>), tensor(0.3601, grad_fn=<MeanBackward0>), tensor(0.3586, grad_fn=<MeanBackward0>), tensor(0.3585, grad_fn=<MeanBackward0>), tensor(0.3656, grad_fn=<MeanBackward0>), tensor(0.3631, grad_fn=<MeanBackward0>), tensor(0.3665, grad_fn=<MeanBackward0>), tensor(0.3671, grad_fn=<MeanBackward0>), tensor(0.3669, grad_fn=<MeanBackward0>), tensor(0.3720, grad_fn=<MeanBackward0>), tensor(0.3742, grad_fn=<MeanBackward0>), tensor(0.3710, grad_fn=<MeanBackward0>), tensor(0.3750, grad_fn=<MeanBackward0>), tensor(0.3771, grad_fn=<MeanBackward0>), tensor(0.3755, grad_fn=<MeanBackward0>), tensor(0.3784, grad_fn=<MeanBackward0>), tensor(0.3765, grad_fn=<MeanBackward0>), tensor(0.3795, grad_fn=<MeanBackward0>), tensor(0.3789, grad_fn=<MeanBackward0>), tensor(0.3786, grad_fn=<MeanBackward0>), tensor(0.3798, grad_fn=<MeanBackward0>), tensor(0.3879, grad_fn=<MeanBackward0>), tensor(0.3854, grad_fn=<MeanBackward0>), tensor(0.3828, grad_fn=<MeanBackward0>), tensor(0.3827, grad_fn=<MeanBackward0>), tensor(0.3839, grad_fn=<MeanBackward0>), tensor(0.3805, grad_fn=<MeanBackward0>), tensor(0.3875, grad_fn=<MeanBackward0>), tensor(0.3840, grad_fn=<MeanBackward0>), tensor(0.3856, grad_fn=<MeanBackward0>), tensor(0.3850, grad_fn=<MeanBackward0>), tensor(0.3848, grad_fn=<MeanBackward0>), tensor(0.3887, grad_fn=<MeanBackward0>), tensor(0.3842, grad_fn=<MeanBackward0>), tensor(0.3865, grad_fn=<MeanBackward0>), tensor(0.3905, grad_fn=<MeanBackward0>), tensor(0.3873, grad_fn=<MeanBackward0>), tensor(0.3884, grad_fn=<MeanBackward0>), tensor(0.3921, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2251, grad_fn=<MeanBackward0>), tensor(0.2205, grad_fn=<MeanBackward0>), tensor(0.2388, grad_fn=<MeanBackward0>), tensor(0.2446, grad_fn=<MeanBackward0>), tensor(0.2589, grad_fn=<MeanBackward0>), tensor(0.2671, grad_fn=<MeanBackward0>), tensor(0.2737, grad_fn=<MeanBackward0>), tensor(0.2877, grad_fn=<MeanBackward0>), tensor(0.2931, grad_fn=<MeanBackward0>), tensor(0.3037, grad_fn=<MeanBackward0>), tensor(0.3173, grad_fn=<MeanBackward0>), tensor(0.3239, grad_fn=<MeanBackward0>), tensor(0.3287, grad_fn=<MeanBackward0>), tensor(0.3470, grad_fn=<MeanBackward0>), tensor(0.3512, grad_fn=<MeanBackward0>), tensor(0.3466, grad_fn=<MeanBackward0>), tensor(0.3600, grad_fn=<MeanBackward0>), tensor(0.3612, grad_fn=<MeanBackward0>), tensor(0.3712, grad_fn=<MeanBackward0>), tensor(0.3879, grad_fn=<MeanBackward0>), tensor(0.3755, grad_fn=<MeanBackward0>), tensor(0.3950, grad_fn=<MeanBackward0>), tensor(0.3929, grad_fn=<MeanBackward0>), tensor(0.3834, grad_fn=<MeanBackward0>), tensor(0.4007, grad_fn=<MeanBackward0>), tensor(0.4043, grad_fn=<MeanBackward0>), tensor(0.4085, grad_fn=<MeanBackward0>), tensor(0.4138, grad_fn=<MeanBackward0>), tensor(0.4114, grad_fn=<MeanBackward0>), tensor(0.4172, grad_fn=<MeanBackward0>), tensor(0.4188, grad_fn=<MeanBackward0>), tensor(0.4263, grad_fn=<MeanBackward0>), tensor(0.4163, grad_fn=<MeanBackward0>), tensor(0.4234, grad_fn=<MeanBackward0>), tensor(0.4265, grad_fn=<MeanBackward0>), tensor(0.4303, grad_fn=<MeanBackward0>), tensor(0.4363, grad_fn=<MeanBackward0>), tensor(0.4356, grad_fn=<MeanBackward0>), tensor(0.4393, grad_fn=<MeanBackward0>), tensor(0.4384, grad_fn=<MeanBackward0>), tensor(0.4363, grad_fn=<MeanBackward0>), tensor(0.4523, grad_fn=<MeanBackward0>), tensor(0.4485, grad_fn=<MeanBackward0>), tensor(0.4489, grad_fn=<MeanBackward0>), tensor(0.4507, grad_fn=<MeanBackward0>), tensor(0.4554, grad_fn=<MeanBackward0>), tensor(0.4540, grad_fn=<MeanBackward0>), tensor(0.4619, grad_fn=<MeanBackward0>), tensor(0.4567, grad_fn=<MeanBackward0>), tensor(0.4509, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2410, grad_fn=<MeanBackward0>), tensor(0.2499, grad_fn=<MeanBackward0>), tensor(0.2674, grad_fn=<MeanBackward0>), tensor(0.2816, grad_fn=<MeanBackward0>), tensor(0.2843, grad_fn=<MeanBackward0>), tensor(0.2946, grad_fn=<MeanBackward0>), tensor(0.3045, grad_fn=<MeanBackward0>), tensor(0.3156, grad_fn=<MeanBackward0>), tensor(0.3253, grad_fn=<MeanBackward0>), tensor(0.3264, grad_fn=<MeanBackward0>), tensor(0.3367, grad_fn=<MeanBackward0>), tensor(0.3380, grad_fn=<MeanBackward0>), tensor(0.3437, grad_fn=<MeanBackward0>), tensor(0.3498, grad_fn=<MeanBackward0>), tensor(0.3554, grad_fn=<MeanBackward0>), tensor(0.3591, grad_fn=<MeanBackward0>), tensor(0.3625, grad_fn=<MeanBackward0>), tensor(0.3620, grad_fn=<MeanBackward0>), tensor(0.3692, grad_fn=<MeanBackward0>), tensor(0.3672, grad_fn=<MeanBackward0>), tensor(0.3777, grad_fn=<MeanBackward0>), tensor(0.3755, grad_fn=<MeanBackward0>), tensor(0.3852, grad_fn=<MeanBackward0>), tensor(0.3864, grad_fn=<MeanBackward0>), tensor(0.3895, grad_fn=<MeanBackward0>), tensor(0.3897, grad_fn=<MeanBackward0>), tensor(0.3909, grad_fn=<MeanBackward0>), tensor(0.3941, grad_fn=<MeanBackward0>), tensor(0.3971, grad_fn=<MeanBackward0>), tensor(0.3983, grad_fn=<MeanBackward0>), tensor(0.4046, grad_fn=<MeanBackward0>), tensor(0.4005, grad_fn=<MeanBackward0>), tensor(0.4015, grad_fn=<MeanBackward0>), tensor(0.4035, grad_fn=<MeanBackward0>), tensor(0.4008, grad_fn=<MeanBackward0>), tensor(0.4042, grad_fn=<MeanBackward0>), tensor(0.4071, grad_fn=<MeanBackward0>), tensor(0.4035, grad_fn=<MeanBackward0>), tensor(0.4026, grad_fn=<MeanBackward0>), tensor(0.4046, grad_fn=<MeanBackward0>), tensor(0.4097, grad_fn=<MeanBackward0>), tensor(0.4078, grad_fn=<MeanBackward0>), tensor(0.4090, grad_fn=<MeanBackward0>), tensor(0.4134, grad_fn=<MeanBackward0>), tensor(0.4136, grad_fn=<MeanBackward0>), tensor(0.4130, grad_fn=<MeanBackward0>), tensor(0.4142, grad_fn=<MeanBackward0>), tensor(0.4134, grad_fn=<MeanBackward0>), tensor(0.4126, grad_fn=<MeanBackward0>), tensor(0.4127, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2562, grad_fn=<MeanBackward0>), tensor(0.2692, grad_fn=<MeanBackward0>), tensor(0.2868, grad_fn=<MeanBackward0>), tensor(0.2999, grad_fn=<MeanBackward0>), tensor(0.3092, grad_fn=<MeanBackward0>), tensor(0.3183, grad_fn=<MeanBackward0>), tensor(0.3309, grad_fn=<MeanBackward0>), tensor(0.3354, grad_fn=<MeanBackward0>), tensor(0.3361, grad_fn=<MeanBackward0>), tensor(0.3455, grad_fn=<MeanBackward0>), tensor(0.3520, grad_fn=<MeanBackward0>), tensor(0.3501, grad_fn=<MeanBackward0>), tensor(0.3561, grad_fn=<MeanBackward0>), tensor(0.3656, grad_fn=<MeanBackward0>), tensor(0.3663, grad_fn=<MeanBackward0>), tensor(0.3646, grad_fn=<MeanBackward0>), tensor(0.3741, grad_fn=<MeanBackward0>), tensor(0.3796, grad_fn=<MeanBackward0>), tensor(0.3824, grad_fn=<MeanBackward0>), tensor(0.3864, grad_fn=<MeanBackward0>), tensor(0.3833, grad_fn=<MeanBackward0>), tensor(0.3826, grad_fn=<MeanBackward0>), tensor(0.3917, grad_fn=<MeanBackward0>), tensor(0.3872, grad_fn=<MeanBackward0>), tensor(0.3912, grad_fn=<MeanBackward0>), tensor(0.3955, grad_fn=<MeanBackward0>), tensor(0.4024, grad_fn=<MeanBackward0>), tensor(0.4045, grad_fn=<MeanBackward0>), tensor(0.4067, grad_fn=<MeanBackward0>), tensor(0.4000, grad_fn=<MeanBackward0>), tensor(0.4154, grad_fn=<MeanBackward0>), tensor(0.4039, grad_fn=<MeanBackward0>), tensor(0.4045, grad_fn=<MeanBackward0>), tensor(0.4150, grad_fn=<MeanBackward0>), tensor(0.4109, grad_fn=<MeanBackward0>), tensor(0.4184, grad_fn=<MeanBackward0>), tensor(0.4152, grad_fn=<MeanBackward0>), tensor(0.4148, grad_fn=<MeanBackward0>), tensor(0.4271, grad_fn=<MeanBackward0>), tensor(0.4271, grad_fn=<MeanBackward0>), tensor(0.4203, grad_fn=<MeanBackward0>), tensor(0.4341, grad_fn=<MeanBackward0>), tensor(0.4253, grad_fn=<MeanBackward0>), tensor(0.4270, grad_fn=<MeanBackward0>), tensor(0.4313, grad_fn=<MeanBackward0>), tensor(0.4359, grad_fn=<MeanBackward0>), tensor(0.4327, grad_fn=<MeanBackward0>), tensor(0.4392, grad_fn=<MeanBackward0>), tensor(0.4442, grad_fn=<MeanBackward0>), tensor(0.4423, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.2462431862950325, 0.2567266635596752, 0.27401895076036453, 0.2855793200433254, 0.29355619847774506, 0.3039194196462631, 0.3111407458782196, 0.3197016716003418, 0.32510294765233994, 0.33076223731040955, 0.3400716781616211, 0.34181472659111023, 0.3471595495939255, 0.355265773832798, 0.3578280657529831, 0.3589491471648216, 0.36489158868789673, 0.3673403933644295, 0.3724443167448044, 0.3770943209528923, 0.37711920589208603, 0.38183897733688354, 0.3851880431175232, 0.3829905539751053, 0.389627106487751, 0.39125996828079224, 0.39506784081459045, 0.3972015529870987, 0.3986791595816612, 0.3986085206270218, 0.4043565019965172, 0.4026097357273102, 0.40254922956228256, 0.40681666135787964, 0.4052506387233734, 0.40888572484254837, 0.4106265753507614, 0.4085840508341789, 0.4141344502568245, 0.4135209321975708, 0.41298989206552505, 0.4197884649038315, 0.41689523309469223, 0.4195195436477661, 0.41994720697402954, 0.4227055683732033, 0.42284731566905975, 0.42547356337308884, 0.4254530295729637, 0.4245155453681946]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.32590480, Test Loss: 2.31206582, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30867945, Test Loss: 2.30727725, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.30516497, Test Loss: 2.30419457, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.30400102, Test Loss: 2.30308807, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.30338692, Test Loss: 2.30124847, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.30265725, Test Loss: 2.30148300, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.30242955, Test Loss: 2.30191276, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.30231748, Test Loss: 2.30128313, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.30227002, Test Loss: 2.30230906, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.30220369, Test Loss: 2.30115642, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.30213529, Test Loss: 2.30204325, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.30208975, Test Loss: 2.30095731, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.30189304, Test Loss: 2.30097571, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.30169844, Test Loss: 2.30115510, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.30141451, Test Loss: 2.30221793, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.30125756, Test Loss: 2.30152938, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.30103796, Test Loss: 2.30057229, Test Accuracy: 0.14930000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.30057189, Test Loss: 2.30035986, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 2.29925543, Test Loss: 2.29763887, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 2.29590507, Test Loss: 2.29181530, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 2.28099924, Test Loss: 2.25235505, Test Accuracy: 0.17400000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 2.05723949, Test Loss: 1.79928446, Test Accuracy: 0.28820000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 1.73116386, Test Loss: 1.67110975, Test Accuracy: 0.31510000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 1.62480579, Test Loss: 1.54644663, Test Accuracy: 0.38480000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 1.49680127, Test Loss: 1.41666598, Test Accuracy: 0.44940000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 1.38229558, Test Loss: 1.34041520, Test Accuracy: 0.48570000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 1.27231856, Test Loss: 1.18536147, Test Accuracy: 0.54650000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 1.12845944, Test Loss: 1.04432741, Test Accuracy: 0.61860000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.92613647, Test Loss: 0.78865097, Test Accuracy: 0.76340000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.70887836, Test Loss: 0.60650604, Test Accuracy: 0.82920000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.58754870, Test Loss: 0.53709662, Test Accuracy: 0.85050000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.51623122, Test Loss: 0.48635009, Test Accuracy: 0.86500000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.45383111, Test Loss: 0.54154836, Test Accuracy: 0.83360000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.39607219, Test Loss: 0.37452715, Test Accuracy: 0.89520000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.34047200, Test Loss: 0.31361272, Test Accuracy: 0.91650000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.30281915, Test Loss: 0.30121590, Test Accuracy: 0.91820000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.27010451, Test Loss: 0.26032967, Test Accuracy: 0.92780000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.24054277, Test Loss: 0.24968659, Test Accuracy: 0.93050000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.22083822, Test Loss: 0.22872889, Test Accuracy: 0.93700000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.20345482, Test Loss: 0.21476998, Test Accuracy: 0.93940000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.18908673, Test Loss: 0.20554118, Test Accuracy: 0.94310000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.17740438, Test Loss: 0.20658506, Test Accuracy: 0.93980000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.16679730, Test Loss: 0.19047465, Test Accuracy: 0.94520000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.15736905, Test Loss: 0.17665622, Test Accuracy: 0.95180000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.14882831, Test Loss: 0.17563316, Test Accuracy: 0.95010000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.14001940, Test Loss: 0.15950197, Test Accuracy: 0.95670000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.13255453, Test Loss: 0.16955120, Test Accuracy: 0.95250000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.12548551, Test Loss: 0.15621166, Test Accuracy: 0.95420000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.11748240, Test Loss: 0.14962639, Test Accuracy: 0.95820000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.11096836, Test Loss: 0.15869806, Test Accuracy: 0.95390000\n",
            "[tensor(0.0054, grad_fn=<MeanBackward0>), tensor(0.0054, grad_fn=<MeanBackward0>), tensor(0.0055, grad_fn=<MeanBackward0>), tensor(0.0055, grad_fn=<MeanBackward0>), tensor(0.0056, grad_fn=<MeanBackward0>), tensor(0.0056, grad_fn=<MeanBackward0>), tensor(0.0057, grad_fn=<MeanBackward0>), tensor(0.0058, grad_fn=<MeanBackward0>), tensor(0.0059, grad_fn=<MeanBackward0>), tensor(0.0060, grad_fn=<MeanBackward0>), tensor(0.0061, grad_fn=<MeanBackward0>), tensor(0.0062, grad_fn=<MeanBackward0>), tensor(0.0064, grad_fn=<MeanBackward0>), tensor(0.0067, grad_fn=<MeanBackward0>), tensor(0.0071, grad_fn=<MeanBackward0>), tensor(0.0075, grad_fn=<MeanBackward0>), tensor(0.0082, grad_fn=<MeanBackward0>), tensor(0.0093, grad_fn=<MeanBackward0>), tensor(0.0108, grad_fn=<MeanBackward0>), tensor(0.0136, grad_fn=<MeanBackward0>), tensor(0.0199, grad_fn=<MeanBackward0>), tensor(0.0451, grad_fn=<MeanBackward0>), tensor(0.0471, grad_fn=<MeanBackward0>), tensor(0.0636, grad_fn=<MeanBackward0>), tensor(0.0832, grad_fn=<MeanBackward0>), tensor(0.0953, grad_fn=<MeanBackward0>), tensor(0.1045, grad_fn=<MeanBackward0>), tensor(0.1121, grad_fn=<MeanBackward0>), tensor(0.1233, grad_fn=<MeanBackward0>), tensor(0.1424, grad_fn=<MeanBackward0>), tensor(0.1597, grad_fn=<MeanBackward0>), tensor(0.1755, grad_fn=<MeanBackward0>), tensor(0.1870, grad_fn=<MeanBackward0>), tensor(0.1985, grad_fn=<MeanBackward0>), tensor(0.2075, grad_fn=<MeanBackward0>), tensor(0.2165, grad_fn=<MeanBackward0>), tensor(0.2234, grad_fn=<MeanBackward0>), tensor(0.2291, grad_fn=<MeanBackward0>), tensor(0.2328, grad_fn=<MeanBackward0>), tensor(0.2383, grad_fn=<MeanBackward0>), tensor(0.2419, grad_fn=<MeanBackward0>), tensor(0.2453, grad_fn=<MeanBackward0>), tensor(0.2499, grad_fn=<MeanBackward0>), tensor(0.2530, grad_fn=<MeanBackward0>), tensor(0.2565, grad_fn=<MeanBackward0>), tensor(0.2591, grad_fn=<MeanBackward0>), tensor(0.2624, grad_fn=<MeanBackward0>), tensor(0.2658, grad_fn=<MeanBackward0>), tensor(0.2681, grad_fn=<MeanBackward0>), tensor(0.2705, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0110, grad_fn=<MeanBackward0>), tensor(0.0111, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0109, grad_fn=<MeanBackward0>), tensor(0.0099, grad_fn=<MeanBackward0>), tensor(0.0085, grad_fn=<MeanBackward0>), tensor(0.0508, grad_fn=<MeanBackward0>), tensor(0.0590, grad_fn=<MeanBackward0>), tensor(0.0592, grad_fn=<MeanBackward0>), tensor(0.0595, grad_fn=<MeanBackward0>), tensor(0.0571, grad_fn=<MeanBackward0>), tensor(0.0579, grad_fn=<MeanBackward0>), tensor(0.0531, grad_fn=<MeanBackward0>), tensor(0.0653, grad_fn=<MeanBackward0>), tensor(0.0700, grad_fn=<MeanBackward0>), tensor(0.0697, grad_fn=<MeanBackward0>), tensor(0.0696, grad_fn=<MeanBackward0>), tensor(0.0716, grad_fn=<MeanBackward0>), tensor(0.0638, grad_fn=<MeanBackward0>), tensor(0.0678, grad_fn=<MeanBackward0>), tensor(0.0666, grad_fn=<MeanBackward0>), tensor(0.0699, grad_fn=<MeanBackward0>), tensor(0.0689, grad_fn=<MeanBackward0>), tensor(0.0718, grad_fn=<MeanBackward0>), tensor(0.0720, grad_fn=<MeanBackward0>), tensor(0.0737, grad_fn=<MeanBackward0>), tensor(0.0725, grad_fn=<MeanBackward0>), tensor(0.0723, grad_fn=<MeanBackward0>), tensor(0.0741, grad_fn=<MeanBackward0>), tensor(0.0716, grad_fn=<MeanBackward0>), tensor(0.0747, grad_fn=<MeanBackward0>), tensor(0.0723, grad_fn=<MeanBackward0>), tensor(0.0712, grad_fn=<MeanBackward0>), tensor(0.0723, grad_fn=<MeanBackward0>), tensor(0.0693, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0125, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0124, grad_fn=<MeanBackward0>), tensor(0.0121, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0111, grad_fn=<MeanBackward0>), tensor(0.0109, grad_fn=<MeanBackward0>), tensor(0.0107, grad_fn=<MeanBackward0>), tensor(0.0105, grad_fn=<MeanBackward0>), tensor(0.0103, grad_fn=<MeanBackward0>), tensor(0.0101, grad_fn=<MeanBackward0>), tensor(0.0099, grad_fn=<MeanBackward0>), tensor(0.0096, grad_fn=<MeanBackward0>), tensor(0.0092, grad_fn=<MeanBackward0>), tensor(0.0087, grad_fn=<MeanBackward0>), tensor(0.0078, grad_fn=<MeanBackward0>), tensor(0.0063, grad_fn=<MeanBackward0>), tensor(0.0036, grad_fn=<MeanBackward0>), tensor(0.0146, grad_fn=<MeanBackward0>), tensor(0.0344, grad_fn=<MeanBackward0>), tensor(0.0724, grad_fn=<MeanBackward0>), tensor(0.1136, grad_fn=<MeanBackward0>), tensor(0.1242, grad_fn=<MeanBackward0>), tensor(0.1426, grad_fn=<MeanBackward0>), tensor(0.1377, grad_fn=<MeanBackward0>), tensor(0.1588, grad_fn=<MeanBackward0>), tensor(0.1790, grad_fn=<MeanBackward0>), tensor(0.1829, grad_fn=<MeanBackward0>), tensor(0.1842, grad_fn=<MeanBackward0>), tensor(0.1875, grad_fn=<MeanBackward0>), tensor(0.1463, grad_fn=<MeanBackward0>), tensor(0.1432, grad_fn=<MeanBackward0>), tensor(0.1400, grad_fn=<MeanBackward0>), tensor(0.1358, grad_fn=<MeanBackward0>), tensor(0.1322, grad_fn=<MeanBackward0>), tensor(0.1351, grad_fn=<MeanBackward0>), tensor(0.1299, grad_fn=<MeanBackward0>), tensor(0.1389, grad_fn=<MeanBackward0>), tensor(0.1250, grad_fn=<MeanBackward0>), tensor(0.1315, grad_fn=<MeanBackward0>), tensor(0.1344, grad_fn=<MeanBackward0>), tensor(0.1285, grad_fn=<MeanBackward0>), tensor(0.1326, grad_fn=<MeanBackward0>), tensor(0.1272, grad_fn=<MeanBackward0>), tensor(0.1246, grad_fn=<MeanBackward0>), tensor(0.1340, grad_fn=<MeanBackward0>), tensor(0.1256, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0064, grad_fn=<MeanBackward0>), tensor(0.0033, grad_fn=<MeanBackward0>), tensor(0.0020, grad_fn=<MeanBackward0>), tensor(0.0014, grad_fn=<MeanBackward0>), tensor(0.0010, grad_fn=<MeanBackward0>), tensor(0.0008, grad_fn=<MeanBackward0>), tensor(0.0008, grad_fn=<MeanBackward0>), tensor(0.0006, grad_fn=<MeanBackward0>), tensor(0.0006, grad_fn=<MeanBackward0>), tensor(0.0004, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0004, grad_fn=<MeanBackward0>), tensor(0.0004, grad_fn=<MeanBackward0>), tensor(0.0004, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0009, grad_fn=<MeanBackward0>), tensor(0.0012, grad_fn=<MeanBackward0>), tensor(0.0019, grad_fn=<MeanBackward0>), tensor(0.0040, grad_fn=<MeanBackward0>), tensor(0.0065, grad_fn=<MeanBackward0>), tensor(0.0097, grad_fn=<MeanBackward0>), tensor(0.0102, grad_fn=<MeanBackward0>), tensor(0.1064, grad_fn=<MeanBackward0>), tensor(0.2139, grad_fn=<MeanBackward0>), tensor(0.3182, grad_fn=<MeanBackward0>), tensor(0.4068, grad_fn=<MeanBackward0>), tensor(0.4193, grad_fn=<MeanBackward0>), tensor(0.4452, grad_fn=<MeanBackward0>), tensor(0.3330, grad_fn=<MeanBackward0>), tensor(0.3371, grad_fn=<MeanBackward0>), tensor(0.3408, grad_fn=<MeanBackward0>), tensor(0.3434, grad_fn=<MeanBackward0>), tensor(0.3202, grad_fn=<MeanBackward0>), tensor(0.3767, grad_fn=<MeanBackward0>), tensor(0.3604, grad_fn=<MeanBackward0>), tensor(0.3711, grad_fn=<MeanBackward0>), tensor(0.3609, grad_fn=<MeanBackward0>), tensor(0.3680, grad_fn=<MeanBackward0>), tensor(0.3547, grad_fn=<MeanBackward0>), tensor(0.3583, grad_fn=<MeanBackward0>), tensor(0.3418, grad_fn=<MeanBackward0>), tensor(0.3690, grad_fn=<MeanBackward0>), tensor(0.3684, grad_fn=<MeanBackward0>), tensor(0.3426, grad_fn=<MeanBackward0>), tensor(0.3641, grad_fn=<MeanBackward0>), tensor(0.3463, grad_fn=<MeanBackward0>), tensor(0.3597, grad_fn=<MeanBackward0>), tensor(0.3665, grad_fn=<MeanBackward0>), tensor(0.3467, grad_fn=<MeanBackward0>), tensor(0.3676, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.008809629711322486, 0.00813708605710417, 0.007813853560946882, 0.007643207849469036, 0.00749567654565908, 0.0074205082055414096, 0.0073802861297735944, 0.007311620109248906, 0.007305495979380794, 0.007241027837153524, 0.007243482607009355, 0.007217042504635174, 0.007242104482429568, 0.007261531944095623, 0.007308621017728001, 0.0074393555405549705, 0.007573831040645018, 0.00782074625021778, 0.008413487812504172, 0.009080886957235634, 0.01043242443120107, 0.03017943841405213, 0.06173452362418175, 0.10226167179644108, 0.14360225852578878, 0.17085203435271978, 0.18108283635228872, 0.1870250143110752, 0.1700807511806488, 0.18211162462830544, 0.188268905505538, 0.19318612478673458, 0.19160737469792366, 0.19633300229907036, 0.1947253067046404, 0.19854063354432583, 0.19746903516352177, 0.1995457112789154, 0.1985893938690424, 0.19962790794670582, 0.19906159676611423, 0.202935341745615, 0.2055385485291481, 0.20100857503712177, 0.20518960244953632, 0.20317714661359787, 0.2053891345858574, 0.2070107776671648, 0.20528115332126617, 0.2082609236240387]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs06MNq_jLqe",
        "outputId": "11cf5d8b-a003-4e20-e3c6-75a81f887793"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "model_factory('Adadelta', 1234)\n",
        "model_factory('Adagrad', 1234)\n",
        "model_factory('Adam', 1234)\n",
        "model_factory('SGD', 1234)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-06\n",
            "    lr: 1.0\n",
            "    rho: 0.9\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.30746806, Test Loss: 2.30202827, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30127321, Test Loss: 2.29988618, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.18322148, Test Loss: 1.78346998, Test Accuracy: 0.35970000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 1.38807317, Test Loss: 1.05196509, Test Accuracy: 0.58940000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.73925288, Test Loss: 0.40720915, Test Accuracy: 0.89020000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.35055134, Test Loss: 0.30235354, Test Accuracy: 0.91540000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.22971163, Test Loss: 0.18080255, Test Accuracy: 0.94850000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.16830976, Test Loss: 0.16070983, Test Accuracy: 0.95200000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.13468076, Test Loss: 0.14689419, Test Accuracy: 0.95620000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.11125832, Test Loss: 0.13769227, Test Accuracy: 0.96050000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.09575471, Test Loss: 0.12158668, Test Accuracy: 0.96510000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.08345701, Test Loss: 0.10658812, Test Accuracy: 0.96770000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.07242749, Test Loss: 0.10226166, Test Accuracy: 0.97110000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.06346309, Test Loss: 0.12266047, Test Accuracy: 0.96430000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.05714226, Test Loss: 0.11184846, Test Accuracy: 0.97000000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.04942244, Test Loss: 0.09444458, Test Accuracy: 0.97360000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.04504926, Test Loss: 0.08744071, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.03980238, Test Loss: 0.09039968, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.03666457, Test Loss: 0.09473740, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.03226412, Test Loss: 0.08644524, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.02791062, Test Loss: 0.09336870, Test Accuracy: 0.97580000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.02439865, Test Loss: 0.10836605, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.02249776, Test Loss: 0.09321603, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.01903939, Test Loss: 0.09567913, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.01696302, Test Loss: 0.09537915, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.01522317, Test Loss: 0.09634081, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.01188203, Test Loss: 0.10121603, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.01161051, Test Loss: 0.09870636, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00893058, Test Loss: 0.09959495, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00833516, Test Loss: 0.12736073, Test Accuracy: 0.97410000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00774268, Test Loss: 0.11226941, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00714412, Test Loss: 0.10984368, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00553139, Test Loss: 0.11530146, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00521357, Test Loss: 0.10872274, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00435523, Test Loss: 0.11609443, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00414847, Test Loss: 0.11266749, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00290722, Test Loss: 0.11466608, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00279187, Test Loss: 0.11495049, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00241140, Test Loss: 0.11872076, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00187569, Test Loss: 0.12077873, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00167831, Test Loss: 0.12387241, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00149886, Test Loss: 0.13416952, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00119602, Test Loss: 0.12152271, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00110594, Test Loss: 0.12464444, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00093754, Test Loss: 0.12232344, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00115545, Test Loss: 0.12480746, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00093265, Test Loss: 0.13523230, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00087843, Test Loss: 0.12731616, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00091637, Test Loss: 0.12694788, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00066378, Test Loss: 0.12831847, Test Accuracy: 0.97920000\n",
            "[tensor(0.0054, grad_fn=<MeanBackward0>), tensor(0.0078, grad_fn=<MeanBackward0>), tensor(0.0597, grad_fn=<MeanBackward0>), tensor(0.2470, grad_fn=<MeanBackward0>), tensor(0.3866, grad_fn=<MeanBackward0>), tensor(0.4355, grad_fn=<MeanBackward0>), tensor(0.4512, grad_fn=<MeanBackward0>), tensor(0.4639, grad_fn=<MeanBackward0>), tensor(0.4669, grad_fn=<MeanBackward0>), tensor(0.4713, grad_fn=<MeanBackward0>), tensor(0.4783, grad_fn=<MeanBackward0>), tensor(0.4782, grad_fn=<MeanBackward0>), tensor(0.4864, grad_fn=<MeanBackward0>), tensor(0.4855, grad_fn=<MeanBackward0>), tensor(0.4901, grad_fn=<MeanBackward0>), tensor(0.4931, grad_fn=<MeanBackward0>), tensor(0.4926, grad_fn=<MeanBackward0>), tensor(0.4947, grad_fn=<MeanBackward0>), tensor(0.4964, grad_fn=<MeanBackward0>), tensor(0.4953, grad_fn=<MeanBackward0>), tensor(0.4937, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.4984, grad_fn=<MeanBackward0>), tensor(0.4980, grad_fn=<MeanBackward0>), tensor(0.4993, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.5000, grad_fn=<MeanBackward0>), tensor(0.5003, grad_fn=<MeanBackward0>), tensor(0.4990, grad_fn=<MeanBackward0>), tensor(0.5004, grad_fn=<MeanBackward0>), tensor(0.4991, grad_fn=<MeanBackward0>), tensor(0.4988, grad_fn=<MeanBackward0>), tensor(0.4986, grad_fn=<MeanBackward0>), tensor(0.5010, grad_fn=<MeanBackward0>), tensor(0.5001, grad_fn=<MeanBackward0>), tensor(0.4996, grad_fn=<MeanBackward0>), tensor(0.4997, grad_fn=<MeanBackward0>), tensor(0.5002, grad_fn=<MeanBackward0>), tensor(0.4990, grad_fn=<MeanBackward0>), tensor(0.4987, grad_fn=<MeanBackward0>), tensor(0.4990, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.4974, grad_fn=<MeanBackward0>), tensor(0.4971, grad_fn=<MeanBackward0>), tensor(0.4963, grad_fn=<MeanBackward0>), tensor(0.4968, grad_fn=<MeanBackward0>), tensor(0.4954, grad_fn=<MeanBackward0>), tensor(0.4957, grad_fn=<MeanBackward0>), tensor(0.4947, grad_fn=<MeanBackward0>), tensor(0.4959, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0556, grad_fn=<MeanBackward0>), tensor(0.1241, grad_fn=<MeanBackward0>), tensor(0.1722, grad_fn=<MeanBackward0>), tensor(0.2126, grad_fn=<MeanBackward0>), tensor(0.2321, grad_fn=<MeanBackward0>), tensor(0.2425, grad_fn=<MeanBackward0>), tensor(0.2504, grad_fn=<MeanBackward0>), tensor(0.2448, grad_fn=<MeanBackward0>), tensor(0.2561, grad_fn=<MeanBackward0>), tensor(0.2542, grad_fn=<MeanBackward0>), tensor(0.2559, grad_fn=<MeanBackward0>), tensor(0.2572, grad_fn=<MeanBackward0>), tensor(0.2665, grad_fn=<MeanBackward0>), tensor(0.2631, grad_fn=<MeanBackward0>), tensor(0.2645, grad_fn=<MeanBackward0>), tensor(0.2626, grad_fn=<MeanBackward0>), tensor(0.2641, grad_fn=<MeanBackward0>), tensor(0.2623, grad_fn=<MeanBackward0>), tensor(0.2689, grad_fn=<MeanBackward0>), tensor(0.2643, grad_fn=<MeanBackward0>), tensor(0.2626, grad_fn=<MeanBackward0>), tensor(0.2623, grad_fn=<MeanBackward0>), tensor(0.2604, grad_fn=<MeanBackward0>), tensor(0.2614, grad_fn=<MeanBackward0>), tensor(0.2594, grad_fn=<MeanBackward0>), tensor(0.2573, grad_fn=<MeanBackward0>), tensor(0.2528, grad_fn=<MeanBackward0>), tensor(0.2539, grad_fn=<MeanBackward0>), tensor(0.2568, grad_fn=<MeanBackward0>), tensor(0.2515, grad_fn=<MeanBackward0>), tensor(0.2485, grad_fn=<MeanBackward0>), tensor(0.2485, grad_fn=<MeanBackward0>), tensor(0.2487, grad_fn=<MeanBackward0>), tensor(0.2503, grad_fn=<MeanBackward0>), tensor(0.2485, grad_fn=<MeanBackward0>), tensor(0.2459, grad_fn=<MeanBackward0>), tensor(0.2466, grad_fn=<MeanBackward0>), tensor(0.2423, grad_fn=<MeanBackward0>), tensor(0.2456, grad_fn=<MeanBackward0>), tensor(0.2442, grad_fn=<MeanBackward0>), tensor(0.2417, grad_fn=<MeanBackward0>), tensor(0.2400, grad_fn=<MeanBackward0>), tensor(0.2407, grad_fn=<MeanBackward0>), tensor(0.2397, grad_fn=<MeanBackward0>), tensor(0.2380, grad_fn=<MeanBackward0>), tensor(0.2379, grad_fn=<MeanBackward0>), tensor(0.2387, grad_fn=<MeanBackward0>), tensor(0.2369, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0097, grad_fn=<MeanBackward0>), tensor(0.0070, grad_fn=<MeanBackward0>), tensor(0.0459, grad_fn=<MeanBackward0>), tensor(0.2062, grad_fn=<MeanBackward0>), tensor(0.2501, grad_fn=<MeanBackward0>), tensor(0.2244, grad_fn=<MeanBackward0>), tensor(0.2312, grad_fn=<MeanBackward0>), tensor(0.2474, grad_fn=<MeanBackward0>), tensor(0.2480, grad_fn=<MeanBackward0>), tensor(0.2218, grad_fn=<MeanBackward0>), tensor(0.2491, grad_fn=<MeanBackward0>), tensor(0.2435, grad_fn=<MeanBackward0>), tensor(0.2594, grad_fn=<MeanBackward0>), tensor(0.2383, grad_fn=<MeanBackward0>), tensor(0.2686, grad_fn=<MeanBackward0>), tensor(0.2571, grad_fn=<MeanBackward0>), tensor(0.2511, grad_fn=<MeanBackward0>), tensor(0.2618, grad_fn=<MeanBackward0>), tensor(0.2489, grad_fn=<MeanBackward0>), tensor(0.2520, grad_fn=<MeanBackward0>), tensor(0.2580, grad_fn=<MeanBackward0>), tensor(0.2727, grad_fn=<MeanBackward0>), tensor(0.2611, grad_fn=<MeanBackward0>), tensor(0.2641, grad_fn=<MeanBackward0>), tensor(0.2590, grad_fn=<MeanBackward0>), tensor(0.2691, grad_fn=<MeanBackward0>), tensor(0.2755, grad_fn=<MeanBackward0>), tensor(0.2791, grad_fn=<MeanBackward0>), tensor(0.2784, grad_fn=<MeanBackward0>), tensor(0.2960, grad_fn=<MeanBackward0>), tensor(0.2906, grad_fn=<MeanBackward0>), tensor(0.2764, grad_fn=<MeanBackward0>), tensor(0.2752, grad_fn=<MeanBackward0>), tensor(0.2889, grad_fn=<MeanBackward0>), tensor(0.2943, grad_fn=<MeanBackward0>), tensor(0.2964, grad_fn=<MeanBackward0>), tensor(0.2951, grad_fn=<MeanBackward0>), tensor(0.2958, grad_fn=<MeanBackward0>), tensor(0.2987, grad_fn=<MeanBackward0>), tensor(0.2933, grad_fn=<MeanBackward0>), tensor(0.3024, grad_fn=<MeanBackward0>), tensor(0.3060, grad_fn=<MeanBackward0>), tensor(0.2993, grad_fn=<MeanBackward0>), tensor(0.2968, grad_fn=<MeanBackward0>), tensor(0.2998, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3022, grad_fn=<MeanBackward0>), tensor(0.3024, grad_fn=<MeanBackward0>), tensor(0.3048, grad_fn=<MeanBackward0>), tensor(0.3009, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0020, grad_fn=<MeanBackward0>), tensor(0.0044, grad_fn=<MeanBackward0>), tensor(0.4402, grad_fn=<MeanBackward0>), tensor(0.5325, grad_fn=<MeanBackward0>), tensor(0.5191, grad_fn=<MeanBackward0>), tensor(0.5348, grad_fn=<MeanBackward0>), tensor(0.5508, grad_fn=<MeanBackward0>), tensor(0.5308, grad_fn=<MeanBackward0>), tensor(0.5373, grad_fn=<MeanBackward0>), tensor(0.5763, grad_fn=<MeanBackward0>), tensor(0.5497, grad_fn=<MeanBackward0>), tensor(0.5700, grad_fn=<MeanBackward0>), tensor(0.5578, grad_fn=<MeanBackward0>), tensor(0.5734, grad_fn=<MeanBackward0>), tensor(0.5504, grad_fn=<MeanBackward0>), tensor(0.5682, grad_fn=<MeanBackward0>), tensor(0.5699, grad_fn=<MeanBackward0>), tensor(0.5607, grad_fn=<MeanBackward0>), tensor(0.5811, grad_fn=<MeanBackward0>), tensor(0.5779, grad_fn=<MeanBackward0>), tensor(0.5733, grad_fn=<MeanBackward0>), tensor(0.5605, grad_fn=<MeanBackward0>), tensor(0.5779, grad_fn=<MeanBackward0>), tensor(0.5730, grad_fn=<MeanBackward0>), tensor(0.5815, grad_fn=<MeanBackward0>), tensor(0.5695, grad_fn=<MeanBackward0>), tensor(0.5699, grad_fn=<MeanBackward0>), tensor(0.5683, grad_fn=<MeanBackward0>), tensor(0.5717, grad_fn=<MeanBackward0>), tensor(0.5505, grad_fn=<MeanBackward0>), tensor(0.5592, grad_fn=<MeanBackward0>), tensor(0.5703, grad_fn=<MeanBackward0>), tensor(0.5780, grad_fn=<MeanBackward0>), tensor(0.5608, grad_fn=<MeanBackward0>), tensor(0.5567, grad_fn=<MeanBackward0>), tensor(0.5532, grad_fn=<MeanBackward0>), tensor(0.5547, grad_fn=<MeanBackward0>), tensor(0.5561, grad_fn=<MeanBackward0>), tensor(0.5495, grad_fn=<MeanBackward0>), tensor(0.5578, grad_fn=<MeanBackward0>), tensor(0.5470, grad_fn=<MeanBackward0>), tensor(0.5454, grad_fn=<MeanBackward0>), tensor(0.5532, grad_fn=<MeanBackward0>), tensor(0.5559, grad_fn=<MeanBackward0>), tensor(0.5532, grad_fn=<MeanBackward0>), tensor(0.5520, grad_fn=<MeanBackward0>), tensor(0.5530, grad_fn=<MeanBackward0>), tensor(0.5525, grad_fn=<MeanBackward0>), tensor(0.5488, grad_fn=<MeanBackward0>), tensor(0.5552, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.007432477781549096, 0.007616111310198903, 0.15035023167729378, 0.27743260003626347, 0.33196355402469635, 0.3518451191484928, 0.3663039058446884, 0.3711462840437889, 0.3756405636668205, 0.37856297194957733, 0.38328251242637634, 0.38648318499326706, 0.38989969342947006, 0.3886025659739971, 0.3938790634274483, 0.3953825682401657, 0.3945397213101387, 0.39492326974868774, 0.3976452127099037, 0.39690595865249634, 0.3985053300857544, 0.39890460669994354, 0.39997973293066025, 0.39935216307640076, 0.4000226557254791, 0.3995169624686241, 0.40120695531368256, 0.40125924348831177, 0.400505967438221, 0.40019381791353226, 0.40144745260477066, 0.39925579726696014, 0.40008214116096497, 0.39980510622262955, 0.3999709002673626, 0.39988719671964645, 0.3994910307228565, 0.3994886018335819, 0.3984619006514549, 0.39804790914058685, 0.39850206300616264, 0.39840542525053024, 0.39792224764823914, 0.3974546156823635, 0.3975033834576607, 0.39780454710125923, 0.3971572630107403, 0.39710136502981186, 0.39673660323023796, 0.3972465954720974]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adagrad (\n",
            "Parameter Group 0\n",
            "    eps: 1e-10\n",
            "    initial_accumulator_value: 0\n",
            "    lr: 0.1\n",
            "    lr_decay: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.41819022, Test Loss: 2.32539536, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.32925108, Test Loss: 2.33123923, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.32166392, Test Loss: 2.32314516, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.31882394, Test Loss: 2.32085734, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.31751979, Test Loss: 2.32146750, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.31592446, Test Loss: 2.31645982, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.31481103, Test Loss: 2.31098961, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.31389957, Test Loss: 2.31366414, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.31191634, Test Loss: 2.31590306, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.31286573, Test Loss: 2.30808157, Test Accuracy: 0.08920000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.31184440, Test Loss: 2.32162372, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.31107149, Test Loss: 2.31959062, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.31080975, Test Loss: 2.32261588, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.31069644, Test Loss: 2.31980920, Test Accuracy: 0.08920000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.30973258, Test Loss: 2.30672049, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.30907274, Test Loss: 2.30789469, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.30989215, Test Loss: 2.31089048, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.06967501, Test Loss: 1.79579359, Test Accuracy: 0.30290000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 1.36391798, Test Loss: 0.68855165, Test Accuracy: 0.76890000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.30257350, Test Loss: 0.17812443, Test Accuracy: 0.95210000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.12702743, Test Loss: 0.12733839, Test Accuracy: 0.96650000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.08497407, Test Loss: 0.12222327, Test Accuracy: 0.96680000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.06324314, Test Loss: 0.10979649, Test Accuracy: 0.97100000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.04882699, Test Loss: 0.11148373, Test Accuracy: 0.97320000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.03828752, Test Loss: 0.10659371, Test Accuracy: 0.97300000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.03004818, Test Loss: 0.10611726, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.02447630, Test Loss: 0.11308604, Test Accuracy: 0.97280000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.01982690, Test Loss: 0.11356062, Test Accuracy: 0.97470000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.01734117, Test Loss: 0.11529768, Test Accuracy: 0.97410000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.01404227, Test Loss: 0.12361706, Test Accuracy: 0.97370000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.01252323, Test Loss: 0.12372039, Test Accuracy: 0.97480000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00998399, Test Loss: 0.12418616, Test Accuracy: 0.97480000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00903993, Test Loss: 0.12977210, Test Accuracy: 0.97480000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00758339, Test Loss: 0.13564425, Test Accuracy: 0.97330000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00760029, Test Loss: 0.13117826, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00612100, Test Loss: 0.13309895, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00525761, Test Loss: 0.13545938, Test Accuracy: 0.97490000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00410157, Test Loss: 0.13759412, Test Accuracy: 0.97550000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00373752, Test Loss: 0.14452383, Test Accuracy: 0.97360000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00341219, Test Loss: 0.14853351, Test Accuracy: 0.97380000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00303024, Test Loss: 0.15659665, Test Accuracy: 0.97160000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00265175, Test Loss: 0.14970841, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00241900, Test Loss: 0.15266190, Test Accuracy: 0.97390000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00223889, Test Loss: 0.15512001, Test Accuracy: 0.97440000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00210107, Test Loss: 0.16041171, Test Accuracy: 0.97350000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00196907, Test Loss: 0.15874036, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00179892, Test Loss: 0.16095162, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00165242, Test Loss: 0.16210078, Test Accuracy: 0.97440000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00150407, Test Loss: 0.16595333, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00146326, Test Loss: 0.16547024, Test Accuracy: 0.97420000\n",
            "[tensor(0.2704, grad_fn=<MeanBackward0>), tensor(0.2705, grad_fn=<MeanBackward0>), tensor(0.2705, grad_fn=<MeanBackward0>), tensor(0.2706, grad_fn=<MeanBackward0>), tensor(0.2707, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.3013, grad_fn=<MeanBackward0>), tensor(0.4508, grad_fn=<MeanBackward0>), tensor(0.4567, grad_fn=<MeanBackward0>), tensor(0.4476, grad_fn=<MeanBackward0>), tensor(0.4509, grad_fn=<MeanBackward0>), tensor(0.4498, grad_fn=<MeanBackward0>), tensor(0.4509, grad_fn=<MeanBackward0>), tensor(0.4511, grad_fn=<MeanBackward0>), tensor(0.4531, grad_fn=<MeanBackward0>), tensor(0.4507, grad_fn=<MeanBackward0>), tensor(0.4520, grad_fn=<MeanBackward0>), tensor(0.4505, grad_fn=<MeanBackward0>), tensor(0.4514, grad_fn=<MeanBackward0>), tensor(0.4517, grad_fn=<MeanBackward0>), tensor(0.4521, grad_fn=<MeanBackward0>), tensor(0.4515, grad_fn=<MeanBackward0>), tensor(0.4523, grad_fn=<MeanBackward0>), tensor(0.4532, grad_fn=<MeanBackward0>), tensor(0.4521, grad_fn=<MeanBackward0>), tensor(0.4517, grad_fn=<MeanBackward0>), tensor(0.4520, grad_fn=<MeanBackward0>), tensor(0.4516, grad_fn=<MeanBackward0>), tensor(0.4512, grad_fn=<MeanBackward0>), tensor(0.4516, grad_fn=<MeanBackward0>), tensor(0.4514, grad_fn=<MeanBackward0>), tensor(0.4516, grad_fn=<MeanBackward0>), tensor(0.4513, grad_fn=<MeanBackward0>), tensor(0.4514, grad_fn=<MeanBackward0>), tensor(0.4512, grad_fn=<MeanBackward0>), tensor(0.4511, grad_fn=<MeanBackward0>), tensor(0.4514, grad_fn=<MeanBackward0>), tensor(0.4517, grad_fn=<MeanBackward0>), tensor(0.4516, grad_fn=<MeanBackward0>), tensor(0.4518, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3174, grad_fn=<MeanBackward0>), tensor(0.3175, grad_fn=<MeanBackward0>), tensor(0.3176, grad_fn=<MeanBackward0>), tensor(0.3178, grad_fn=<MeanBackward0>), tensor(0.3178, grad_fn=<MeanBackward0>), tensor(0.3140, grad_fn=<MeanBackward0>), tensor(0.3140, grad_fn=<MeanBackward0>), tensor(0.3141, grad_fn=<MeanBackward0>), tensor(0.3141, grad_fn=<MeanBackward0>), tensor(0.3142, grad_fn=<MeanBackward0>), tensor(0.3142, grad_fn=<MeanBackward0>), tensor(0.3143, grad_fn=<MeanBackward0>), tensor(0.3143, grad_fn=<MeanBackward0>), tensor(0.3144, grad_fn=<MeanBackward0>), tensor(0.3144, grad_fn=<MeanBackward0>), tensor(0.3145, grad_fn=<MeanBackward0>), tensor(0.3164, grad_fn=<MeanBackward0>), tensor(0.4237, grad_fn=<MeanBackward0>), tensor(0.4780, grad_fn=<MeanBackward0>), tensor(0.4874, grad_fn=<MeanBackward0>), tensor(0.4924, grad_fn=<MeanBackward0>), tensor(0.5023, grad_fn=<MeanBackward0>), tensor(0.5021, grad_fn=<MeanBackward0>), tensor(0.5004, grad_fn=<MeanBackward0>), tensor(0.4925, grad_fn=<MeanBackward0>), tensor(0.4954, grad_fn=<MeanBackward0>), tensor(0.4943, grad_fn=<MeanBackward0>), tensor(0.4949, grad_fn=<MeanBackward0>), tensor(0.4973, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.4948, grad_fn=<MeanBackward0>), tensor(0.4949, grad_fn=<MeanBackward0>), tensor(0.4959, grad_fn=<MeanBackward0>), tensor(0.4951, grad_fn=<MeanBackward0>), tensor(0.4957, grad_fn=<MeanBackward0>), tensor(0.4934, grad_fn=<MeanBackward0>), tensor(0.4941, grad_fn=<MeanBackward0>), tensor(0.4864, grad_fn=<MeanBackward0>), tensor(0.4892, grad_fn=<MeanBackward0>), tensor(0.4897, grad_fn=<MeanBackward0>), tensor(0.4905, grad_fn=<MeanBackward0>), tensor(0.4904, grad_fn=<MeanBackward0>), tensor(0.4877, grad_fn=<MeanBackward0>), tensor(0.4869, grad_fn=<MeanBackward0>), tensor(0.4889, grad_fn=<MeanBackward0>), tensor(0.4890, grad_fn=<MeanBackward0>), tensor(0.4893, grad_fn=<MeanBackward0>), tensor(0.4878, grad_fn=<MeanBackward0>), tensor(0.4848, grad_fn=<MeanBackward0>), tensor(0.4853, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3289, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3831, grad_fn=<MeanBackward0>), tensor(0.4928, grad_fn=<MeanBackward0>), tensor(0.5302, grad_fn=<MeanBackward0>), tensor(0.5366, grad_fn=<MeanBackward0>), tensor(0.5262, grad_fn=<MeanBackward0>), tensor(0.5304, grad_fn=<MeanBackward0>), tensor(0.5322, grad_fn=<MeanBackward0>), tensor(0.5325, grad_fn=<MeanBackward0>), tensor(0.5362, grad_fn=<MeanBackward0>), tensor(0.5420, grad_fn=<MeanBackward0>), tensor(0.5400, grad_fn=<MeanBackward0>), tensor(0.5378, grad_fn=<MeanBackward0>), tensor(0.5425, grad_fn=<MeanBackward0>), tensor(0.5392, grad_fn=<MeanBackward0>), tensor(0.5397, grad_fn=<MeanBackward0>), tensor(0.5306, grad_fn=<MeanBackward0>), tensor(0.5405, grad_fn=<MeanBackward0>), tensor(0.5345, grad_fn=<MeanBackward0>), tensor(0.5333, grad_fn=<MeanBackward0>), tensor(0.5354, grad_fn=<MeanBackward0>), tensor(0.5316, grad_fn=<MeanBackward0>), tensor(0.5321, grad_fn=<MeanBackward0>), tensor(0.5317, grad_fn=<MeanBackward0>), tensor(0.5273, grad_fn=<MeanBackward0>), tensor(0.5255, grad_fn=<MeanBackward0>), tensor(0.5208, grad_fn=<MeanBackward0>), tensor(0.5258, grad_fn=<MeanBackward0>), tensor(0.5242, grad_fn=<MeanBackward0>), tensor(0.5220, grad_fn=<MeanBackward0>), tensor(0.5203, grad_fn=<MeanBackward0>), tensor(0.5173, grad_fn=<MeanBackward0>), tensor(0.5183, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.4009, grad_fn=<MeanBackward0>), tensor(0.4158, grad_fn=<MeanBackward0>), tensor(0.4021, grad_fn=<MeanBackward0>), tensor(0.4036, grad_fn=<MeanBackward0>), tensor(0.4064, grad_fn=<MeanBackward0>), tensor(0.4054, grad_fn=<MeanBackward0>), tensor(0.4003, grad_fn=<MeanBackward0>), tensor(0.4077, grad_fn=<MeanBackward0>), tensor(0.4097, grad_fn=<MeanBackward0>), tensor(0.4080, grad_fn=<MeanBackward0>), tensor(0.4072, grad_fn=<MeanBackward0>), tensor(0.4086, grad_fn=<MeanBackward0>), tensor(0.4088, grad_fn=<MeanBackward0>), tensor(0.4082, grad_fn=<MeanBackward0>), tensor(0.4071, grad_fn=<MeanBackward0>), tensor(0.4039, grad_fn=<MeanBackward0>), tensor(0.4099, grad_fn=<MeanBackward0>), tensor(0.4112, grad_fn=<MeanBackward0>), tensor(0.4108, grad_fn=<MeanBackward0>), tensor(0.4103, grad_fn=<MeanBackward0>), tensor(0.4071, grad_fn=<MeanBackward0>), tensor(0.4072, grad_fn=<MeanBackward0>), tensor(0.4047, grad_fn=<MeanBackward0>), tensor(0.4059, grad_fn=<MeanBackward0>), tensor(0.4072, grad_fn=<MeanBackward0>), tensor(0.4071, grad_fn=<MeanBackward0>), tensor(0.4066, grad_fn=<MeanBackward0>), tensor(0.4062, grad_fn=<MeanBackward0>), tensor(0.4085, grad_fn=<MeanBackward0>), tensor(0.4079, grad_fn=<MeanBackward0>), tensor(0.4067, grad_fn=<MeanBackward0>), tensor(0.4075, grad_fn=<MeanBackward0>), tensor(0.4073, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.32059696316719055, 0.3206307664513588, 0.32067976146936417, 0.32073453813791275, 0.3207795023918152, 0.3249920681118965, 0.3250023126602173, 0.3250133842229843, 0.3250245228409767, 0.32503629475831985, 0.32504870742559433, 0.3250611573457718, 0.3250742256641388, 0.3250885531306267, 0.3251046761870384, 0.32512906938791275, 0.3284998759627342, 0.4014400690793991, 0.43339086323976517, 0.4574742540717125, 0.4692729264497757, 0.47377123683691025, 0.47116807103157043, 0.4705396220088005, 0.47138598561286926, 0.4720718339085579, 0.4726187661290169, 0.47365985810756683, 0.4743337258696556, 0.474080391228199, 0.4743865951895714, 0.47319700568914413, 0.47293689101934433, 0.47218628227710724, 0.47487449645996094, 0.4726182594895363, 0.47239939868450165, 0.4701022058725357, 0.46980106085538864, 0.4695197120308876, 0.4698679596185684, 0.4691237509250641, 0.4679178521037102, 0.4664338454604149, 0.46800675243139267, 0.4682137742638588, 0.46765486150979996, 0.466618612408638, 0.46530651301145554, 0.4656742289662361]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 0.70349304, Test Loss: 0.24971213, Test Accuracy: 0.92520000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 0.19108419, Test Loss: 0.15407089, Test Accuracy: 0.95410000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 0.13175319, Test Loss: 0.12008267, Test Accuracy: 0.96590000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 0.10181782, Test Loss: 0.11060085, Test Accuracy: 0.96780000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.08089231, Test Loss: 0.10700331, Test Accuracy: 0.96830000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.06627438, Test Loss: 0.08847845, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.05288039, Test Loss: 0.10273121, Test Accuracy: 0.97100000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.04713998, Test Loss: 0.09349366, Test Accuracy: 0.97300000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.03822255, Test Loss: 0.09995507, Test Accuracy: 0.97370000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.03322133, Test Loss: 0.09568298, Test Accuracy: 0.97350000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.02961539, Test Loss: 0.10790298, Test Accuracy: 0.97340000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.02436374, Test Loss: 0.09440881, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.02099391, Test Loss: 0.09452429, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.01969475, Test Loss: 0.09713552, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.01825854, Test Loss: 0.11497826, Test Accuracy: 0.97410000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.01671022, Test Loss: 0.10951894, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.01490426, Test Loss: 0.10493772, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.01329708, Test Loss: 0.09998171, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.01283279, Test Loss: 0.10917581, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.01115321, Test Loss: 0.12180192, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.01102828, Test Loss: 0.11627207, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.00997441, Test Loss: 0.11814355, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.00922417, Test Loss: 0.14756817, Test Accuracy: 0.97080000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.00849225, Test Loss: 0.12215931, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.00859533, Test Loss: 0.13734381, Test Accuracy: 0.97440000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.00826923, Test Loss: 0.11322766, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.00783183, Test Loss: 0.13318586, Test Accuracy: 0.97420000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.00826555, Test Loss: 0.11960444, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00605629, Test Loss: 0.13121920, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00754072, Test Loss: 0.12752886, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00727915, Test Loss: 0.11271087, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00560611, Test Loss: 0.12765694, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00632649, Test Loss: 0.11225688, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00576471, Test Loss: 0.11775057, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00513396, Test Loss: 0.12794133, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00606756, Test Loss: 0.14065759, Test Accuracy: 0.97670000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00405719, Test Loss: 0.12296128, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00648892, Test Loss: 0.12381157, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00592333, Test Loss: 0.11700157, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00375700, Test Loss: 0.14470491, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00607972, Test Loss: 0.12169000, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00308262, Test Loss: 0.13623985, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00596998, Test Loss: 0.12338690, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00449574, Test Loss: 0.12291180, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00371980, Test Loss: 0.14455057, Test Accuracy: 0.97350000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00398769, Test Loss: 0.13136862, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00570741, Test Loss: 0.13846129, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00424031, Test Loss: 0.12801689, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00418253, Test Loss: 0.11849166, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00359042, Test Loss: 0.12666425, Test Accuracy: 0.97900000\n",
            "[tensor(0.2816, grad_fn=<MeanBackward0>), tensor(0.3121, grad_fn=<MeanBackward0>), tensor(0.3332, grad_fn=<MeanBackward0>), tensor(0.3422, grad_fn=<MeanBackward0>), tensor(0.3539, grad_fn=<MeanBackward0>), tensor(0.3557, grad_fn=<MeanBackward0>), tensor(0.3622, grad_fn=<MeanBackward0>), tensor(0.3674, grad_fn=<MeanBackward0>), tensor(0.3701, grad_fn=<MeanBackward0>), tensor(0.3745, grad_fn=<MeanBackward0>), tensor(0.3761, grad_fn=<MeanBackward0>), tensor(0.3816, grad_fn=<MeanBackward0>), tensor(0.3795, grad_fn=<MeanBackward0>), tensor(0.3833, grad_fn=<MeanBackward0>), tensor(0.3888, grad_fn=<MeanBackward0>), tensor(0.3862, grad_fn=<MeanBackward0>), tensor(0.3913, grad_fn=<MeanBackward0>), tensor(0.3915, grad_fn=<MeanBackward0>), tensor(0.3904, grad_fn=<MeanBackward0>), tensor(0.3953, grad_fn=<MeanBackward0>), tensor(0.4004, grad_fn=<MeanBackward0>), tensor(0.3951, grad_fn=<MeanBackward0>), tensor(0.3985, grad_fn=<MeanBackward0>), tensor(0.4027, grad_fn=<MeanBackward0>), tensor(0.4070, grad_fn=<MeanBackward0>), tensor(0.4059, grad_fn=<MeanBackward0>), tensor(0.4060, grad_fn=<MeanBackward0>), tensor(0.4073, grad_fn=<MeanBackward0>), tensor(0.4077, grad_fn=<MeanBackward0>), tensor(0.4108, grad_fn=<MeanBackward0>), tensor(0.4092, grad_fn=<MeanBackward0>), tensor(0.4126, grad_fn=<MeanBackward0>), tensor(0.4108, grad_fn=<MeanBackward0>), tensor(0.4121, grad_fn=<MeanBackward0>), tensor(0.4139, grad_fn=<MeanBackward0>), tensor(0.4162, grad_fn=<MeanBackward0>), tensor(0.4151, grad_fn=<MeanBackward0>), tensor(0.4206, grad_fn=<MeanBackward0>), tensor(0.4185, grad_fn=<MeanBackward0>), tensor(0.4203, grad_fn=<MeanBackward0>), tensor(0.4170, grad_fn=<MeanBackward0>), tensor(0.4130, grad_fn=<MeanBackward0>), tensor(0.4162, grad_fn=<MeanBackward0>), tensor(0.4194, grad_fn=<MeanBackward0>), tensor(0.4186, grad_fn=<MeanBackward0>), tensor(0.4239, grad_fn=<MeanBackward0>), tensor(0.4234, grad_fn=<MeanBackward0>), tensor(0.4183, grad_fn=<MeanBackward0>), tensor(0.4227, grad_fn=<MeanBackward0>), tensor(0.4219, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2432, grad_fn=<MeanBackward0>), tensor(0.2450, grad_fn=<MeanBackward0>), tensor(0.2545, grad_fn=<MeanBackward0>), tensor(0.2650, grad_fn=<MeanBackward0>), tensor(0.2695, grad_fn=<MeanBackward0>), tensor(0.2862, grad_fn=<MeanBackward0>), tensor(0.2956, grad_fn=<MeanBackward0>), tensor(0.3045, grad_fn=<MeanBackward0>), tensor(0.3101, grad_fn=<MeanBackward0>), tensor(0.3218, grad_fn=<MeanBackward0>), tensor(0.3397, grad_fn=<MeanBackward0>), tensor(0.3426, grad_fn=<MeanBackward0>), tensor(0.3443, grad_fn=<MeanBackward0>), tensor(0.3551, grad_fn=<MeanBackward0>), tensor(0.3683, grad_fn=<MeanBackward0>), tensor(0.3697, grad_fn=<MeanBackward0>), tensor(0.3793, grad_fn=<MeanBackward0>), tensor(0.3848, grad_fn=<MeanBackward0>), tensor(0.3948, grad_fn=<MeanBackward0>), tensor(0.3933, grad_fn=<MeanBackward0>), tensor(0.3989, grad_fn=<MeanBackward0>), tensor(0.4101, grad_fn=<MeanBackward0>), tensor(0.4085, grad_fn=<MeanBackward0>), tensor(0.4200, grad_fn=<MeanBackward0>), tensor(0.4245, grad_fn=<MeanBackward0>), tensor(0.4332, grad_fn=<MeanBackward0>), tensor(0.4278, grad_fn=<MeanBackward0>), tensor(0.4392, grad_fn=<MeanBackward0>), tensor(0.4367, grad_fn=<MeanBackward0>), tensor(0.4386, grad_fn=<MeanBackward0>), tensor(0.4432, grad_fn=<MeanBackward0>), tensor(0.4498, grad_fn=<MeanBackward0>), tensor(0.4484, grad_fn=<MeanBackward0>), tensor(0.4518, grad_fn=<MeanBackward0>), tensor(0.4573, grad_fn=<MeanBackward0>), tensor(0.4582, grad_fn=<MeanBackward0>), tensor(0.4580, grad_fn=<MeanBackward0>), tensor(0.4606, grad_fn=<MeanBackward0>), tensor(0.4629, grad_fn=<MeanBackward0>), tensor(0.4643, grad_fn=<MeanBackward0>), tensor(0.4710, grad_fn=<MeanBackward0>), tensor(0.4708, grad_fn=<MeanBackward0>), tensor(0.4641, grad_fn=<MeanBackward0>), tensor(0.4689, grad_fn=<MeanBackward0>), tensor(0.4642, grad_fn=<MeanBackward0>), tensor(0.4702, grad_fn=<MeanBackward0>), tensor(0.4659, grad_fn=<MeanBackward0>), tensor(0.4701, grad_fn=<MeanBackward0>), tensor(0.4719, grad_fn=<MeanBackward0>), tensor(0.4772, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3006, grad_fn=<MeanBackward0>), tensor(0.3095, grad_fn=<MeanBackward0>), tensor(0.3186, grad_fn=<MeanBackward0>), tensor(0.3252, grad_fn=<MeanBackward0>), tensor(0.3395, grad_fn=<MeanBackward0>), tensor(0.3406, grad_fn=<MeanBackward0>), tensor(0.3495, grad_fn=<MeanBackward0>), tensor(0.3494, grad_fn=<MeanBackward0>), tensor(0.3619, grad_fn=<MeanBackward0>), tensor(0.3681, grad_fn=<MeanBackward0>), tensor(0.3617, grad_fn=<MeanBackward0>), tensor(0.3767, grad_fn=<MeanBackward0>), tensor(0.3821, grad_fn=<MeanBackward0>), tensor(0.3810, grad_fn=<MeanBackward0>), tensor(0.3792, grad_fn=<MeanBackward0>), tensor(0.3891, grad_fn=<MeanBackward0>), tensor(0.3866, grad_fn=<MeanBackward0>), tensor(0.3929, grad_fn=<MeanBackward0>), tensor(0.3970, grad_fn=<MeanBackward0>), tensor(0.3925, grad_fn=<MeanBackward0>), tensor(0.3996, grad_fn=<MeanBackward0>), tensor(0.4017, grad_fn=<MeanBackward0>), tensor(0.4004, grad_fn=<MeanBackward0>), tensor(0.4035, grad_fn=<MeanBackward0>), tensor(0.4005, grad_fn=<MeanBackward0>), tensor(0.4062, grad_fn=<MeanBackward0>), tensor(0.4121, grad_fn=<MeanBackward0>), tensor(0.4117, grad_fn=<MeanBackward0>), tensor(0.4163, grad_fn=<MeanBackward0>), tensor(0.4168, grad_fn=<MeanBackward0>), tensor(0.4155, grad_fn=<MeanBackward0>), tensor(0.4180, grad_fn=<MeanBackward0>), tensor(0.4129, grad_fn=<MeanBackward0>), tensor(0.4201, grad_fn=<MeanBackward0>), tensor(0.4212, grad_fn=<MeanBackward0>), tensor(0.4220, grad_fn=<MeanBackward0>), tensor(0.4237, grad_fn=<MeanBackward0>), tensor(0.4257, grad_fn=<MeanBackward0>), tensor(0.4284, grad_fn=<MeanBackward0>), tensor(0.4259, grad_fn=<MeanBackward0>), tensor(0.4239, grad_fn=<MeanBackward0>), tensor(0.4263, grad_fn=<MeanBackward0>), tensor(0.4316, grad_fn=<MeanBackward0>), tensor(0.4305, grad_fn=<MeanBackward0>), tensor(0.4321, grad_fn=<MeanBackward0>), tensor(0.4303, grad_fn=<MeanBackward0>), tensor(0.4349, grad_fn=<MeanBackward0>), tensor(0.4333, grad_fn=<MeanBackward0>), tensor(0.4331, grad_fn=<MeanBackward0>), tensor(0.4345, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2451, grad_fn=<MeanBackward0>), tensor(0.2649, grad_fn=<MeanBackward0>), tensor(0.2825, grad_fn=<MeanBackward0>), tensor(0.2960, grad_fn=<MeanBackward0>), tensor(0.3098, grad_fn=<MeanBackward0>), tensor(0.3232, grad_fn=<MeanBackward0>), tensor(0.3245, grad_fn=<MeanBackward0>), tensor(0.3389, grad_fn=<MeanBackward0>), tensor(0.3425, grad_fn=<MeanBackward0>), tensor(0.3488, grad_fn=<MeanBackward0>), tensor(0.3552, grad_fn=<MeanBackward0>), tensor(0.3521, grad_fn=<MeanBackward0>), tensor(0.3567, grad_fn=<MeanBackward0>), tensor(0.3686, grad_fn=<MeanBackward0>), tensor(0.3736, grad_fn=<MeanBackward0>), tensor(0.3690, grad_fn=<MeanBackward0>), tensor(0.3746, grad_fn=<MeanBackward0>), tensor(0.3786, grad_fn=<MeanBackward0>), tensor(0.3760, grad_fn=<MeanBackward0>), tensor(0.3842, grad_fn=<MeanBackward0>), tensor(0.3845, grad_fn=<MeanBackward0>), tensor(0.3869, grad_fn=<MeanBackward0>), tensor(0.3958, grad_fn=<MeanBackward0>), tensor(0.3896, grad_fn=<MeanBackward0>), tensor(0.4047, grad_fn=<MeanBackward0>), tensor(0.4051, grad_fn=<MeanBackward0>), tensor(0.4116, grad_fn=<MeanBackward0>), tensor(0.4108, grad_fn=<MeanBackward0>), tensor(0.4100, grad_fn=<MeanBackward0>), tensor(0.4148, grad_fn=<MeanBackward0>), tensor(0.4294, grad_fn=<MeanBackward0>), tensor(0.4188, grad_fn=<MeanBackward0>), tensor(0.4316, grad_fn=<MeanBackward0>), tensor(0.4272, grad_fn=<MeanBackward0>), tensor(0.4237, grad_fn=<MeanBackward0>), tensor(0.4347, grad_fn=<MeanBackward0>), tensor(0.4258, grad_fn=<MeanBackward0>), tensor(0.4371, grad_fn=<MeanBackward0>), tensor(0.4438, grad_fn=<MeanBackward0>), tensor(0.4369, grad_fn=<MeanBackward0>), tensor(0.4441, grad_fn=<MeanBackward0>), tensor(0.4359, grad_fn=<MeanBackward0>), tensor(0.4463, grad_fn=<MeanBackward0>), tensor(0.4451, grad_fn=<MeanBackward0>), tensor(0.4496, grad_fn=<MeanBackward0>), tensor(0.4423, grad_fn=<MeanBackward0>), tensor(0.4524, grad_fn=<MeanBackward0>), tensor(0.4553, grad_fn=<MeanBackward0>), tensor(0.4584, grad_fn=<MeanBackward0>), tensor(0.4560, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.267634104937315, 0.2828459218144417, 0.2971903085708618, 0.30710478127002716, 0.3181621879339218, 0.32641564309597015, 0.332942858338356, 0.3400544598698616, 0.3461431935429573, 0.353293314576149, 0.3581799939274788, 0.36326414346694946, 0.36566026508808136, 0.37202855199575424, 0.37744715064764023, 0.37850913405418396, 0.3829544633626938, 0.386959545314312, 0.38956472277641296, 0.3913288861513138, 0.39583589881658554, 0.3984384387731552, 0.40079162269830704, 0.4039528965950012, 0.40917084366083145, 0.4125952422618866, 0.41435714811086655, 0.4172382652759552, 0.41769295185804367, 0.42026493698358536, 0.4243251383304596, 0.42479208111763, 0.4259161278605461, 0.4277781769633293, 0.42904096841812134, 0.43278172612190247, 0.4306294769048691, 0.4360020086169243, 0.43837809562683105, 0.43683530390262604, 0.43899495899677277, 0.43646831065416336, 0.4395561143755913, 0.44095009565353394, 0.4411163926124573, 0.44168999046087265, 0.44417470693588257, 0.4442633390426636, 0.44651738554239273, 0.4473862797021866]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.32472255, Test Loss: 2.30778245, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30940653, Test Loss: 2.30538360, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.30504895, Test Loss: 2.30385843, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.30414631, Test Loss: 2.30156305, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.30311251, Test Loss: 2.30271003, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.30280634, Test Loss: 2.30421965, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.30241329, Test Loss: 2.30187196, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.30236180, Test Loss: 2.30162264, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.30226276, Test Loss: 2.30258062, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.30224197, Test Loss: 2.30129197, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.30211327, Test Loss: 2.30147549, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.30208901, Test Loss: 2.30194131, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.30188317, Test Loss: 2.30093815, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.30185788, Test Loss: 2.30169513, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.30183446, Test Loss: 2.30174162, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.30153028, Test Loss: 2.30066975, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.30149255, Test Loss: 2.30069772, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.30110694, Test Loss: 2.30100843, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 2.30059907, Test Loss: 2.29985386, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 2.29995046, Test Loss: 2.29829600, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 2.29775782, Test Loss: 2.29489251, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 2.29045097, Test Loss: 2.27903566, Test Accuracy: 0.16450000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 2.20490847, Test Loss: 2.01635596, Test Accuracy: 0.26470000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 1.90212216, Test Loss: 1.76812775, Test Accuracy: 0.31810000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 1.70639155, Test Loss: 1.61692866, Test Accuracy: 0.37060000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 1.58316823, Test Loss: 1.50658412, Test Accuracy: 0.41430000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 1.49127364, Test Loss: 1.42470402, Test Accuracy: 0.46290000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 1.39367435, Test Loss: 1.31480310, Test Accuracy: 0.50790000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 1.27472008, Test Loss: 1.17779119, Test Accuracy: 0.58130000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 1.12610747, Test Loss: 0.96869375, Test Accuracy: 0.69030000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.84822849, Test Loss: 0.71851771, Test Accuracy: 0.76760000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.68882707, Test Loss: 0.60984892, Test Accuracy: 0.82420000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.58462217, Test Loss: 0.52073304, Test Accuracy: 0.85840000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.50844961, Test Loss: 0.45544165, Test Accuracy: 0.88080000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.45320698, Test Loss: 0.40884850, Test Accuracy: 0.89190000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.41182485, Test Loss: 0.37991261, Test Accuracy: 0.90230000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.37641085, Test Loss: 0.36050134, Test Accuracy: 0.90810000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.33777103, Test Loss: 0.32110027, Test Accuracy: 0.91650000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.29697440, Test Loss: 0.28950200, Test Accuracy: 0.92530000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.26155129, Test Loss: 0.27324196, Test Accuracy: 0.92870000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.23715473, Test Loss: 0.29452112, Test Accuracy: 0.92330000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.21778315, Test Loss: 0.22168362, Test Accuracy: 0.94050000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.20062624, Test Loss: 0.22618359, Test Accuracy: 0.94060000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.18813137, Test Loss: 0.20124995, Test Accuracy: 0.94610000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.17555071, Test Loss: 0.20228650, Test Accuracy: 0.94460000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.16583115, Test Loss: 0.19722298, Test Accuracy: 0.94750000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.15701749, Test Loss: 0.17799301, Test Accuracy: 0.95240000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.14850828, Test Loss: 0.21331062, Test Accuracy: 0.94190000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.13957012, Test Loss: 0.16412113, Test Accuracy: 0.95400000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.13276905, Test Loss: 0.16496014, Test Accuracy: 0.95400000\n",
            "[tensor(0.0049, grad_fn=<MeanBackward0>), tensor(0.0049, grad_fn=<MeanBackward0>), tensor(0.0050, grad_fn=<MeanBackward0>), tensor(0.0050, grad_fn=<MeanBackward0>), tensor(0.0050, grad_fn=<MeanBackward0>), tensor(0.0051, grad_fn=<MeanBackward0>), tensor(0.0051, grad_fn=<MeanBackward0>), tensor(0.0052, grad_fn=<MeanBackward0>), tensor(0.0052, grad_fn=<MeanBackward0>), tensor(0.0053, grad_fn=<MeanBackward0>), tensor(0.0054, grad_fn=<MeanBackward0>), tensor(0.0055, grad_fn=<MeanBackward0>), tensor(0.0056, grad_fn=<MeanBackward0>), tensor(0.0058, grad_fn=<MeanBackward0>), tensor(0.0060, grad_fn=<MeanBackward0>), tensor(0.0063, grad_fn=<MeanBackward0>), tensor(0.0067, grad_fn=<MeanBackward0>), tensor(0.0072, grad_fn=<MeanBackward0>), tensor(0.0081, grad_fn=<MeanBackward0>), tensor(0.0094, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0165, grad_fn=<MeanBackward0>), tensor(0.0394, grad_fn=<MeanBackward0>), tensor(0.0326, grad_fn=<MeanBackward0>), tensor(0.0398, grad_fn=<MeanBackward0>), tensor(0.0507, grad_fn=<MeanBackward0>), tensor(0.0586, grad_fn=<MeanBackward0>), tensor(0.0671, grad_fn=<MeanBackward0>), tensor(0.0772, grad_fn=<MeanBackward0>), tensor(0.0933, grad_fn=<MeanBackward0>), tensor(0.1087, grad_fn=<MeanBackward0>), tensor(0.1234, grad_fn=<MeanBackward0>), tensor(0.1473, grad_fn=<MeanBackward0>), tensor(0.1665, grad_fn=<MeanBackward0>), tensor(0.1805, grad_fn=<MeanBackward0>), tensor(0.1914, grad_fn=<MeanBackward0>), tensor(0.1993, grad_fn=<MeanBackward0>), tensor(0.2039, grad_fn=<MeanBackward0>), tensor(0.2066, grad_fn=<MeanBackward0>), tensor(0.2108, grad_fn=<MeanBackward0>), tensor(0.2137, grad_fn=<MeanBackward0>), tensor(0.2168, grad_fn=<MeanBackward0>), tensor(0.2184, grad_fn=<MeanBackward0>), tensor(0.2211, grad_fn=<MeanBackward0>), tensor(0.2234, grad_fn=<MeanBackward0>), tensor(0.2262, grad_fn=<MeanBackward0>), tensor(0.2292, grad_fn=<MeanBackward0>), tensor(0.2313, grad_fn=<MeanBackward0>), tensor(0.2345, grad_fn=<MeanBackward0>), tensor(0.2370, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0123, grad_fn=<MeanBackward0>), tensor(0.0124, grad_fn=<MeanBackward0>), tensor(0.0125, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0127, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0122, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0096, grad_fn=<MeanBackward0>), tensor(0.0232, grad_fn=<MeanBackward0>), tensor(0.0535, grad_fn=<MeanBackward0>), tensor(0.0601, grad_fn=<MeanBackward0>), tensor(0.0617, grad_fn=<MeanBackward0>), tensor(0.0634, grad_fn=<MeanBackward0>), tensor(0.0644, grad_fn=<MeanBackward0>), tensor(0.0611, grad_fn=<MeanBackward0>), tensor(0.0631, grad_fn=<MeanBackward0>), tensor(0.0785, grad_fn=<MeanBackward0>), tensor(0.0758, grad_fn=<MeanBackward0>), tensor(0.0761, grad_fn=<MeanBackward0>), tensor(0.0761, grad_fn=<MeanBackward0>), tensor(0.0765, grad_fn=<MeanBackward0>), tensor(0.0761, grad_fn=<MeanBackward0>), tensor(0.0796, grad_fn=<MeanBackward0>), tensor(0.0762, grad_fn=<MeanBackward0>), tensor(0.0765, grad_fn=<MeanBackward0>), tensor(0.0789, grad_fn=<MeanBackward0>), tensor(0.0799, grad_fn=<MeanBackward0>), tensor(0.0780, grad_fn=<MeanBackward0>), tensor(0.0802, grad_fn=<MeanBackward0>), tensor(0.0795, grad_fn=<MeanBackward0>), tensor(0.0789, grad_fn=<MeanBackward0>), tensor(0.0814, grad_fn=<MeanBackward0>), tensor(0.0789, grad_fn=<MeanBackward0>), tensor(0.0804, grad_fn=<MeanBackward0>), tensor(0.0791, grad_fn=<MeanBackward0>), tensor(0.0785, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0124, grad_fn=<MeanBackward0>), tensor(0.0133, grad_fn=<MeanBackward0>), tensor(0.0134, grad_fn=<MeanBackward0>), tensor(0.0134, grad_fn=<MeanBackward0>), tensor(0.0132, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0127, grad_fn=<MeanBackward0>), tensor(0.0125, grad_fn=<MeanBackward0>), tensor(0.0123, grad_fn=<MeanBackward0>), tensor(0.0122, grad_fn=<MeanBackward0>), tensor(0.0120, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0110, grad_fn=<MeanBackward0>), tensor(0.0107, grad_fn=<MeanBackward0>), tensor(0.0103, grad_fn=<MeanBackward0>), tensor(0.0095, grad_fn=<MeanBackward0>), tensor(0.0083, grad_fn=<MeanBackward0>), tensor(0.0060, grad_fn=<MeanBackward0>), tensor(0.0050, grad_fn=<MeanBackward0>), tensor(0.0246, grad_fn=<MeanBackward0>), tensor(0.0551, grad_fn=<MeanBackward0>), tensor(0.0882, grad_fn=<MeanBackward0>), tensor(0.1119, grad_fn=<MeanBackward0>), tensor(0.1372, grad_fn=<MeanBackward0>), tensor(0.1387, grad_fn=<MeanBackward0>), tensor(0.1438, grad_fn=<MeanBackward0>), tensor(0.1476, grad_fn=<MeanBackward0>), tensor(0.1507, grad_fn=<MeanBackward0>), tensor(0.1567, grad_fn=<MeanBackward0>), tensor(0.1628, grad_fn=<MeanBackward0>), tensor(0.1616, grad_fn=<MeanBackward0>), tensor(0.1581, grad_fn=<MeanBackward0>), tensor(0.1635, grad_fn=<MeanBackward0>), tensor(0.1496, grad_fn=<MeanBackward0>), tensor(0.1444, grad_fn=<MeanBackward0>), tensor(0.1483, grad_fn=<MeanBackward0>), tensor(0.1521, grad_fn=<MeanBackward0>), tensor(0.1381, grad_fn=<MeanBackward0>), tensor(0.1433, grad_fn=<MeanBackward0>), tensor(0.1359, grad_fn=<MeanBackward0>), tensor(0.1290, grad_fn=<MeanBackward0>), tensor(0.1414, grad_fn=<MeanBackward0>), tensor(0.1343, grad_fn=<MeanBackward0>), tensor(0.1443, grad_fn=<MeanBackward0>), tensor(0.1312, grad_fn=<MeanBackward0>), tensor(0.1324, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0066, grad_fn=<MeanBackward0>), tensor(0.0035, grad_fn=<MeanBackward0>), tensor(0.0022, grad_fn=<MeanBackward0>), tensor(0.0014, grad_fn=<MeanBackward0>), tensor(0.0011, grad_fn=<MeanBackward0>), tensor(0.0012, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0006, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0006, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0009, grad_fn=<MeanBackward0>), tensor(0.0012, grad_fn=<MeanBackward0>), tensor(0.0021, grad_fn=<MeanBackward0>), tensor(0.0033, grad_fn=<MeanBackward0>), tensor(0.0061, grad_fn=<MeanBackward0>), tensor(0.0105, grad_fn=<MeanBackward0>), tensor(0.0078, grad_fn=<MeanBackward0>), tensor(0.0281, grad_fn=<MeanBackward0>), tensor(0.1274, grad_fn=<MeanBackward0>), tensor(0.2471, grad_fn=<MeanBackward0>), tensor(0.3315, grad_fn=<MeanBackward0>), tensor(0.3732, grad_fn=<MeanBackward0>), tensor(0.4338, grad_fn=<MeanBackward0>), tensor(0.3760, grad_fn=<MeanBackward0>), tensor(0.3254, grad_fn=<MeanBackward0>), tensor(0.3448, grad_fn=<MeanBackward0>), tensor(0.3405, grad_fn=<MeanBackward0>), tensor(0.3296, grad_fn=<MeanBackward0>), tensor(0.3334, grad_fn=<MeanBackward0>), tensor(0.3360, grad_fn=<MeanBackward0>), tensor(0.3140, grad_fn=<MeanBackward0>), tensor(0.3322, grad_fn=<MeanBackward0>), tensor(0.3386, grad_fn=<MeanBackward0>), tensor(0.3212, grad_fn=<MeanBackward0>), tensor(0.3070, grad_fn=<MeanBackward0>), tensor(0.3445, grad_fn=<MeanBackward0>), tensor(0.3253, grad_fn=<MeanBackward0>), tensor(0.3445, grad_fn=<MeanBackward0>), tensor(0.3477, grad_fn=<MeanBackward0>), tensor(0.3308, grad_fn=<MeanBackward0>), tensor(0.3432, grad_fn=<MeanBackward0>), tensor(0.3424, grad_fn=<MeanBackward0>), tensor(0.3579, grad_fn=<MeanBackward0>), tensor(0.3558, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.009069899329915643, 0.008529701561201364, 0.008264877018518746, 0.008089612500043586, 0.007983949239132926, 0.007989918231032789, 0.007861950711230747, 0.007802263993653469, 0.007780054525937885, 0.0077620814117835835, 0.007733740254479926, 0.007744447342702188, 0.0077466231159633026, 0.0077579848148161545, 0.007747451716568321, 0.007824410175089724, 0.007895759597886354, 0.008008062228327617, 0.008266671327874064, 0.008619060565251857, 0.009351185406558216, 0.010671804542653263, 0.018850363441742957, 0.03470508800819516, 0.07058975379914045, 0.11192135512828827, 0.14134302455931902, 0.16045393981039524, 0.17770483251661062, 0.16902933456003666, 0.16507581435143948, 0.1736761387437582, 0.18012123554944992, 0.18374722823500633, 0.18799838423728943, 0.1903698593378067, 0.18911243602633476, 0.19049577228724957, 0.19153404794633389, 0.1898093093186617, 0.18817619234323502, 0.1943462211638689, 0.19180361926555634, 0.19527164474129677, 0.19473923929035664, 0.19494229555130005, 0.19641395658254623, 0.19963864423334599, 0.20069399662315845, 0.2009530309587717]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}