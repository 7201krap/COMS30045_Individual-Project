{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "differentiation_in_autograd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNI6CRQzbfMJrgrtJP+QuFE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/Introduction_to_Pytorch/blob/main/differentiation_in_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdbunkVcKgUM"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcFNOq4JKpMB"
      },
      "source": [
        "# Let's take a look at how autograd collects gradients. \n",
        "# We create two tensors a and b with requires_grad = True. \n",
        "# This signals to autograd that every operation on them should be tracked"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du3uSvmALdJc"
      },
      "source": [
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2n_tvduL5i5"
      },
      "source": [
        "# We create another tensor Q from a and b"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezxSbMCUL6uY"
      },
      "source": [
        "Q = (3 * a ** 3) - (b ** 2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXB0uKCrMbDP"
      },
      "source": [
        "# Let's assume a and b to be parameters of an NN, and Q to be the error.\n",
        "# In NN training, we want gradients of the error w.r.t. parameters, \n",
        "\n",
        "# delta_Q / delta_a = 9*a**2\n",
        "# delta_Q / delta_b = -2*b"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdsNJuCuM0ka"
      },
      "source": [
        "# When we call .backward() on Q, autograd calculates these gradients and stores them \n",
        "# in the respective tensor's .grad attribute"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF3uW6p-NBJL"
      },
      "source": [
        "# 이 부분은 잘 이해 안간다. \n",
        "# We need to explicitly pass a gradent argument in Q.backward because it is a vector.\n",
        "# gradient is a tensor of the same shape as Q, and it represents the gradient of Q w.r.t.\n",
        "# itself. delta_Q / delta_Q = 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imTy4SSuQQkZ"
      },
      "source": [
        "# 이 방법으로 이해 하면 된다. \n",
        "# Equivalently, we can also aggregate Q into a scalar and call backward implicitly like\n",
        "# Q.sum().backward()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-adAo2GQiAG"
      },
      "source": [
        "external_grad = torch.tensor([1., 1.])\n",
        "Q.backward(gradient=external_grad)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtiPkqV1QvTS"
      },
      "source": [
        "# Gradient are now deposited in a.grad and b.grad\n",
        "# check if collected gradients are correct"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4aOJl68Q4mZ",
        "outputId": "74d60e03-4be7-4286-dd17-086ac900aedd"
      },
      "source": [
        "print(9*a**2 == a.grad)\n",
        "print(-2*b   == b.grad)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFjJpffOSCi-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}