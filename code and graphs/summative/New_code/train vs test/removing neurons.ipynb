{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/all_seeds_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7STrWa0P3z_",
    "outputId": "3009eefc-bfb4-43d5-be6c-d122570fff6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0EpXz3ULfb1",
    "outputId": "38dd32de-9a72-4f63-a6c4-2ec82b020a2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-08 13:25:41--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
      "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
      "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
      "--2021-04-08 13:25:41--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
      "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-gzip]\n",
      "Saving to: ‘MNIST.tar.gz’\n",
      "\n",
      "MNIST.tar.gz            [     <=>            ]  33.20M  34.7MB/s    in 1.0s    \n",
      "\n",
      "2021-04-08 13:25:42 (34.7 MB/s) - ‘MNIST.tar.gz’ saved [34813078]\n",
      "\n",
      "MNIST/\n",
      "MNIST/raw/\n",
      "MNIST/raw/train-labels-idx1-ubyte\n",
      "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "MNIST/raw/t10k-labels-idx1-ubyte\n",
      "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "MNIST/raw/train-images-idx3-ubyte\n",
      "MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "MNIST/raw/t10k-images-idx3-ubyte\n",
      "MNIST/raw/train-images-idx3-ubyte.gz\n",
      "MNIST/processed/\n",
      "MNIST/processed/training.pt\n",
      "MNIST/processed/test.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./\n",
       "    Split: Train"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "!tar -zxvf MNIST.tar.gz\n",
    "\n",
    "root_dir = './'\n",
    "torchvision.datasets.MNIST(root=root_dir,download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z4j9WoP-UnAm"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rTW5TOUnP5XY"
   },
   "outputs": [],
   "source": [
    "mnist_trainset = torchvision.datasets.MNIST(root=root_dir, train=True, \n",
    "                                download=True, \n",
    "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "mnist_testset  = torchvision.datasets.MNIST(root=root_dir, \n",
    "                                train=False, \n",
    "                                download=True, \n",
    "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(mnist_trainset, \n",
    "                                               batch_size=50, \n",
    "                                               shuffle=True)\n",
    "\n",
    "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
    "                                               batch_size=50, \n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IXTkEUJ5P6kU"
   },
   "outputs": [],
   "source": [
    "# ************* modify this section for later use *************\n",
    "# Define the model \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # modify this section for later use \n",
    "        self.linear_1 = torch.nn.Linear(784, 256)\n",
    "        self.linear_2 = torch.nn.Linear(256, 10)\n",
    "        self.sigmoid12  = torch.nn.Sigmoid()\n",
    "\n",
    "        self.layer_activations = dict()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # modify this section for later use \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.sigmoid12(x)\n",
    "        pred = self.linear_2(x)\n",
    "        return pred\n",
    "# ************* modify this section for later use *************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BfgvKH6eP9Ou"
   },
   "outputs": [],
   "source": [
    "def get_activation(model, layer_name):    \n",
    "    def hook(module, input, output):\n",
    "        model.layer_activations[layer_name] = output\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OCEw3Ov3Lk5X"
   },
   "outputs": [],
   "source": [
    "def sparsity_calculator(final_spareness):\n",
    "    sparseness_list_avg = list()\n",
    "    sparseness_list_std = list()\n",
    "    for single_epoch_spareness in final_spareness:\n",
    "\n",
    "        hidden_layer_activation_list = single_epoch_spareness\n",
    "        hidden_layer_activation_list = torch.stack(hidden_layer_activation_list)\n",
    "        layer_activations_list = torch.reshape(hidden_layer_activation_list, (10000, 256))\n",
    "\n",
    "        layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
    "        num_neurons = layer_activations_list.shape[1]\n",
    "        population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
    "\n",
    "        mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
    "        std_sparseness_per_epoch  = torch.std(population_sparseness)\n",
    "\n",
    "        sparseness_list_avg.append(mean_sparseness_per_epoch)\n",
    "        sparseness_list_std.append(std_sparseness_per_epoch)\n",
    "\n",
    "    return sparseness_list_avg, sparseness_list_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PvHGO5RSvi6I"
   },
   "outputs": [],
   "source": [
    "def selectivity(hidden_layer_each_neuron):\n",
    "    __selectivity__ = list()\n",
    "    # I will now try to find the average of each class for each neuron.\n",
    "    # check out the next cell \n",
    "    avg_activations = [dict() for x in range(256)]\n",
    "    for i, neuron in enumerate(hidden_layer_each_neuron):\n",
    "        for k, v in neuron.items():\n",
    "            # v is the list of activations for hidden layer's neuron k \n",
    "            avg_activations[i][k] = sum(v) / float(len(v))\n",
    "\n",
    "    # generate 256 lists to get only values in avg_activations\n",
    "    only_activation_vals = [list() for x in range(256)]\n",
    "\n",
    "    # get only values from avg_activations\n",
    "    for i, avg_activation in enumerate(avg_activations):\n",
    "        for value in avg_activation.values():\n",
    "            only_activation_vals[i].append(value)\n",
    "\n",
    "\n",
    "    for activation_val in only_activation_vals:\n",
    "        # find u_max \n",
    "        u_max = np.max(activation_val)\n",
    "\n",
    "        # find u_minus_max \n",
    "        u_minus_max = (np.sum(activation_val) - u_max) / 9\n",
    "\n",
    "        # find selectivity \n",
    "        selectivity = (u_max - u_minus_max) / (u_max + u_minus_max)\n",
    "\n",
    "        # append selectivity value to selectivity\n",
    "        __selectivity__.append(selectivity)\n",
    "\n",
    "    avg_selectivity = np.average(__selectivity__)\n",
    "    std_selectivity = np.std(__selectivity__)\n",
    "                                 \n",
    "    return avg_selectivity, std_selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nUHSwHZqLm3Y"
   },
   "outputs": [],
   "source": [
    "# ************* modify this section for later use *************\n",
    "def avg_std_calculator(_hidden_layer_each_neuron_12):\n",
    "\n",
    "    avg_selectivity12, std_selectivity12 = selectivity(_hidden_layer_each_neuron_12)\n",
    "\n",
    "    final_selectivity_avg = (avg_selectivity12) / 1\n",
    "    final_selecvitity_std = (std_selectivity12) / 1\n",
    "\n",
    "    return final_selectivity_avg, final_selecvitity_std\n",
    "# ************* modify this section for later use *************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "j5PUiBNqUImf"
   },
   "outputs": [],
   "source": [
    "def model_factory(optimizer_name, seed_num):\n",
    "    '''\n",
    "    optimizer_name : choose one of Adagrad, Adadelta, SGD, and Adam \n",
    "\n",
    "    '''\n",
    "    my_model = Model()\n",
    "    print(\"my_model:\", my_model)\n",
    "    my_model.to(device)\n",
    "\n",
    "    # ************* modify this section for later use *************\n",
    "    my_model.sigmoid12.register_forward_hook(get_activation(my_model, 's12'))\n",
    "    # ************* modify this section for later use *************\n",
    "\n",
    "    if optimizer_name == 'Adadelta':\n",
    "        my_optimizer = torch.optim.Adadelta(my_model.parameters(), lr=1.0)\n",
    "\n",
    "    elif optimizer_name == 'Adagrad':\n",
    "        my_optimizer = torch.optim.Adagrad(my_model.parameters(), lr=0.1)\n",
    "\n",
    "    elif optimizer_name == 'SGD':\n",
    "        my_optimizer = torch.optim.SGD(my_model.parameters(), lr=0.1)\n",
    "\n",
    "    elif optimizer_name == 'Adam':\n",
    "        my_optimizer = torch.optim.Adam(my_model.parameters(), lr=0.001)\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR\")\n",
    "    \n",
    "    print(\"my_optimizer:\", my_optimizer)\n",
    "    test_acc, sparsity_avg, sparsity_std, selectivity_list_avg, selectivity_list_std = sparsity_selectivity_trainer(optimizer=my_optimizer, model=my_model)\n",
    "\n",
    "    # ************* modify this section for later use *************\n",
    "    file_saver = open(f\"seed_{seed_num}_sparsity_selectivity_4_optim_testing_{optimizer_name}.txt\", \"w\")\n",
    "    # ************* modify this section for later use *************\n",
    "    file_saver.write(str(test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(sparsity_std)+'\\n'+str(selectivity_list_avg)+'\\n'+str(selectivity_list_std)+'\\n\\n')\n",
    "    file_saver.close()\n",
    "\n",
    "    if seed_num == 1:\n",
    "    # ************* modify this section for later use *************\n",
    "        if optimizer_name == 'Adadelta':\n",
    "            !cp seed_1_sparsity_selectivity_4_optim_testing_Adadelta.txt /content/drive/MyDrive\n",
    "        \n",
    "        elif optimizer_name == 'Adagrad':\n",
    "            !cp seed_1_sparsity_selectivity_4_optim_testing_Adagrad.txt /content/drive/MyDrive\n",
    "\n",
    "        elif optimizer_name == 'SGD':\n",
    "            !cp seed_1_sparsity_selectivity_4_optim_testing_SGD.txt /content/drive/MyDrive\n",
    "\n",
    "        elif optimizer_name == 'Adam':\n",
    "            !cp seed_1_sparsity_selectivity_4_optim_testing_Adam.txt /content/drive/MyDrive\n",
    "        # ************* modify this section for later use *************\n",
    "    \n",
    "    elif seed_num == 100:\n",
    "        # ************* modify this section for later use *************\n",
    "        if optimizer_name == 'Adadelta':\n",
    "            !cp seed_100_sparsity_selectivity_4_optim_testing_Adadelta.txt /content/drive/MyDrive\n",
    "        \n",
    "        elif optimizer_name == 'Adagrad':\n",
    "            !cp seed_100_sparsity_selectivity_4_optim_testing_Adagrad.txt /content/drive/MyDrive\n",
    "\n",
    "        elif optimizer_name == 'SGD':\n",
    "            !cp seed_100_sparsity_selectivity_4_optim_testing_SGD.txt /content/drive/MyDrive\n",
    "\n",
    "        elif optimizer_name == 'Adam':\n",
    "            !cp seed_100_sparsity_selectivity_4_optim_testing_Adam.txt /content/drive/MyDrive\n",
    "        # ************* modify this section for later use *************\n",
    "\n",
    "    elif seed_num == 1234:\n",
    "        # ************* modify this section for later use *************\n",
    "        if optimizer_name == 'Adadelta':\n",
    "            !cp seed_1234_sparsity_selectivity_4_optim_testing_Adadelta.txt /content/drive/MyDrive\n",
    "        \n",
    "        elif optimizer_name == 'Adagrad':\n",
    "            !cp seed_1234_sparsity_selectivity_4_optim_testing_Adagrad.txt /content/drive/MyDrive\n",
    "\n",
    "        elif optimizer_name == 'SGD':\n",
    "            !cp seed_1234_sparsity_selectivity_4_optim_testing_SGD.txt /content/drive/MyDrive\n",
    "\n",
    "        elif optimizer_name == 'Adam':\n",
    "            !cp seed_1234_sparsity_selectivity_4_optim_testing_Adam.txt /content/drive/MyDrive\n",
    "        # ************* modify this section for later use *************\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BXOpwTXEQFKY"
   },
   "outputs": [],
   "source": [
    "no_epochs = 30\n",
    "def sparsity_selectivity_trainer(optimizer, model):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    train_loss = list()\n",
    "    test_loss  = list()\n",
    "    test_acc   = list()\n",
    "\n",
    "    best_test_loss = 1\n",
    "\n",
    "    selectivity_avg_list = list()\n",
    "    selectivity_std_list = list()\n",
    "\n",
    "    # ************* modify this section for later use *************\n",
    "    final_spareness_12 = list()\n",
    "    # ************* modify this section for later use *************\n",
    "\n",
    "    for epoch in range(no_epochs):\n",
    "\n",
    "        # ************* modify this section for later use *************\n",
    "        hidden_layer_each_neuron_12 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
    "        hidden_layer_each_neuron_12 = np.array(hidden_layer_each_neuron_12)\n",
    "        # ************* modify this section for later use *************\n",
    "\n",
    "        total_train_loss = 0\n",
    "        total_test_loss = 0\n",
    "\n",
    "        # ************* modify this section for later use *************\n",
    "        hidden_layer_activation_list_12 = list()\n",
    "        # ************* modify this section for later use *************\n",
    "\n",
    "        # training\n",
    "        # set up training mode \n",
    "        model.train()\n",
    "\n",
    "        for itr, (images, labels) in enumerate(train_dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(images)\n",
    "\n",
    "            loss = criterion(pred, labels)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_train_loss = total_train_loss / (itr + 1)\n",
    "        train_loss.append(total_train_loss)\n",
    "\n",
    "        # testing \n",
    "        # change to evaluation mode \n",
    "        model.eval()\n",
    "        total = 0\n",
    "        for itr, (images, labels) in enumerate(test_dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            pred = model(images)\n",
    "\n",
    "            loss = criterion(pred, labels)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            # we now need softmax because we are testing.\n",
    "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "            for i, p in enumerate(pred):\n",
    "                if labels[i] == torch.max(p.data, 0)[1]:\n",
    "                    total = total + 1\n",
    "\n",
    "            # ***************** sparsity calculation ***************** #\n",
    "            hidden_layer_activation_list_12.append(model.layer_activations['s12'])\n",
    "\n",
    "            # ************* modify this section for later use *************\n",
    "            for activation, label in zip(model.layer_activations['s12'], labels):\n",
    "                label = label.item()\n",
    "                with torch.no_grad():\n",
    "                    activation = activation.numpy()\n",
    "                for i in range(256):    \n",
    "                    hidden_layer_each_neuron_12[i][label].append(activation[i])\n",
    "    \n",
    "        selectivity_avg, selecvitity_std = avg_std_calculator(hidden_layer_each_neuron_12)\n",
    "        # ************* modify this section for later use *************\n",
    "        \n",
    "        selectivity_avg_list.append(selectivity_avg)\n",
    "        selectivity_std_list.append(selecvitity_std)\n",
    "\n",
    "        # this conains activations for all epochs \n",
    "        final_spareness_12.append(hidden_layer_activation_list_12)\n",
    "        # ***************** sparsity calculation ***************** #\n",
    "\n",
    "        # caculate accuracy \n",
    "        accuracy = total / len(mnist_testset)\n",
    "\n",
    "        # append accuracy here\n",
    "        test_acc.append(accuracy)\n",
    "\n",
    "        # append test loss here \n",
    "        total_test_loss = total_test_loss / (itr + 1)\n",
    "        test_loss.append(total_test_loss)\n",
    "\n",
    "        print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Test Loss: {:.8f}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, total_train_loss, total_test_loss, accuracy))\n",
    "\n",
    "    # ***************** sparsity calculation ***************** #\n",
    "    sparsity_list12_avg, sparsity_list12_std = sparsity_calculator(final_spareness_12)\n",
    "\n",
    "    average_sparsity = list()\n",
    "    std_sparsity = list()\n",
    "\n",
    "    for i in range(no_epochs):\n",
    "        average_sparsity.append( (sparsity_list12_avg[i].item()) / 1 )\n",
    "        std_sparsity.append( (sparsity_list12_std[i].item()) / 1 )\n",
    "    # ***************** sparsity calculation ***************** #\n",
    "\n",
    "    return test_acc, average_sparsity, std_sparsity, selectivity_avg_list, selectivity_std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CG_P-QHIhJ1v"
   },
   "outputs": [],
   "source": [
    "# Seed 1\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "model_factory('Adadelta',1)\n",
    "model_factory('Adagrad',1)\n",
    "model_factory('SGD',1)\n",
    "model_factory('Adam',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxxxOoJno86K"
   },
   "outputs": [],
   "source": [
    "# Seed 100\n",
    "\n",
    "torch.manual_seed(100)\n",
    "np.random.seed(100)\n",
    "\n",
    "model_factory('Adadelta',100)\n",
    "model_factory('Adagrad',100)\n",
    "model_factory('SGD',100)\n",
    "model_factory('Adam',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnE6Osc4o-yt",
    "outputId": "f1722386-ada9-49a3-d2a2-1cb1eca66f6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model: Model(\n",
      "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (sigmoid12): Sigmoid()\n",
      ")\n",
      "my_optimizer: Adadelta (\n",
      "Parameter Group 0\n",
      "    eps: 1e-06\n",
      "    lr: 1.0\n",
      "    rho: 0.9\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "Epoch: 1/30, Train Loss: 0.43482734, Test Loss: 0.25056328, Test Accuracy: 0.92680000\n",
      "\n",
      "Epoch: 2/30, Train Loss: 0.21552304, Test Loss: 0.17478676, Test Accuracy: 0.94670000\n",
      "\n",
      "Epoch: 3/30, Train Loss: 0.15641612, Test Loss: 0.13820602, Test Accuracy: 0.95740000\n",
      "\n",
      "Epoch: 4/30, Train Loss: 0.12157690, Test Loss: 0.11924306, Test Accuracy: 0.96300000\n",
      "\n",
      "Epoch: 5/30, Train Loss: 0.09919056, Test Loss: 0.09831684, Test Accuracy: 0.96860000\n",
      "\n",
      "Epoch: 6/30, Train Loss: 0.08336800, Test Loss: 0.09016022, Test Accuracy: 0.97190000\n",
      "\n",
      "Epoch: 7/30, Train Loss: 0.07088853, Test Loss: 0.08719795, Test Accuracy: 0.97320000\n",
      "\n",
      "Epoch: 8/30, Train Loss: 0.06153279, Test Loss: 0.07796417, Test Accuracy: 0.97520000\n",
      "\n",
      "Epoch: 9/30, Train Loss: 0.05413196, Test Loss: 0.07215949, Test Accuracy: 0.97700000\n",
      "\n",
      "Epoch: 10/30, Train Loss: 0.04688475, Test Loss: 0.07165624, Test Accuracy: 0.97650000\n",
      "\n",
      "Epoch: 11/30, Train Loss: 0.04203057, Test Loss: 0.07021961, Test Accuracy: 0.97680000\n",
      "\n",
      "Epoch: 12/30, Train Loss: 0.03695013, Test Loss: 0.07518681, Test Accuracy: 0.97650000\n",
      "\n",
      "Epoch: 13/30, Train Loss: 0.03292638, Test Loss: 0.06627988, Test Accuracy: 0.97940000\n",
      "\n",
      "Epoch: 14/30, Train Loss: 0.02905201, Test Loss: 0.06293267, Test Accuracy: 0.98030000\n",
      "\n",
      "Epoch: 15/30, Train Loss: 0.02590495, Test Loss: 0.06158078, Test Accuracy: 0.98020000\n",
      "\n",
      "Epoch: 16/30, Train Loss: 0.02330130, Test Loss: 0.06297808, Test Accuracy: 0.97920000\n",
      "\n",
      "Epoch: 17/30, Train Loss: 0.02085337, Test Loss: 0.05992444, Test Accuracy: 0.98110000\n",
      "\n",
      "Epoch: 18/30, Train Loss: 0.01860578, Test Loss: 0.06213609, Test Accuracy: 0.98040000\n",
      "\n",
      "Epoch: 19/30, Train Loss: 0.01655125, Test Loss: 0.06208906, Test Accuracy: 0.98020000\n",
      "\n",
      "Epoch: 20/30, Train Loss: 0.01466604, Test Loss: 0.05816114, Test Accuracy: 0.98150000\n",
      "\n",
      "Epoch: 21/30, Train Loss: 0.01332531, Test Loss: 0.06010362, Test Accuracy: 0.98110000\n",
      "\n",
      "Epoch: 22/30, Train Loss: 0.01189256, Test Loss: 0.05997108, Test Accuracy: 0.98140000\n",
      "\n",
      "Epoch: 23/30, Train Loss: 0.01074991, Test Loss: 0.06014585, Test Accuracy: 0.98140000\n",
      "\n",
      "Epoch: 24/30, Train Loss: 0.00972292, Test Loss: 0.06088047, Test Accuracy: 0.98090000\n",
      "\n",
      "Epoch: 25/30, Train Loss: 0.00874879, Test Loss: 0.06209752, Test Accuracy: 0.98090000\n",
      "\n",
      "Epoch: 26/30, Train Loss: 0.00798673, Test Loss: 0.06282990, Test Accuracy: 0.98130000\n",
      "\n",
      "Epoch: 27/30, Train Loss: 0.00720308, Test Loss: 0.05958402, Test Accuracy: 0.98200000\n",
      "\n",
      "Epoch: 28/30, Train Loss: 0.00660240, Test Loss: 0.06011285, Test Accuracy: 0.98150000\n",
      "\n",
      "Epoch: 29/30, Train Loss: 0.00585278, Test Loss: 0.05981551, Test Accuracy: 0.98100000\n",
      "\n",
      "Epoch: 30/30, Train Loss: 0.00549971, Test Loss: 0.06033825, Test Accuracy: 0.98150000\n",
      "my_model: Model(\n",
      "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (sigmoid12): Sigmoid()\n",
      ")\n",
      "my_optimizer: Adagrad (\n",
      "Parameter Group 0\n",
      "    eps: 1e-10\n",
      "    initial_accumulator_value: 0\n",
      "    lr: 0.1\n",
      "    lr_decay: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "Epoch: 1/30, Train Loss: 0.23718886, Test Loss: 0.12977516, Test Accuracy: 0.95950000\n",
      "\n",
      "Epoch: 2/30, Train Loss: 0.09642065, Test Loss: 0.10092658, Test Accuracy: 0.96840000\n",
      "\n",
      "Epoch: 3/30, Train Loss: 0.06664428, Test Loss: 0.08550377, Test Accuracy: 0.97230000\n",
      "\n",
      "Epoch: 4/30, Train Loss: 0.04915988, Test Loss: 0.07767118, Test Accuracy: 0.97540000\n",
      "\n",
      "Epoch: 5/30, Train Loss: 0.03803864, Test Loss: 0.07441393, Test Accuracy: 0.97520000\n",
      "\n",
      "Epoch: 6/30, Train Loss: 0.03025761, Test Loss: 0.07197807, Test Accuracy: 0.97610000\n",
      "\n",
      "Epoch: 7/30, Train Loss: 0.02474231, Test Loss: 0.06945075, Test Accuracy: 0.97720000\n",
      "\n",
      "Epoch: 8/30, Train Loss: 0.02030326, Test Loss: 0.06954807, Test Accuracy: 0.97740000\n",
      "\n",
      "Epoch: 9/30, Train Loss: 0.01700789, Test Loss: 0.06827952, Test Accuracy: 0.97770000\n",
      "\n",
      "Epoch: 10/30, Train Loss: 0.01447830, Test Loss: 0.06662981, Test Accuracy: 0.97840000\n",
      "\n",
      "Epoch: 11/30, Train Loss: 0.01241379, Test Loss: 0.06758368, Test Accuracy: 0.97830000\n",
      "\n",
      "Epoch: 12/30, Train Loss: 0.01059499, Test Loss: 0.06751415, Test Accuracy: 0.97830000\n",
      "\n",
      "Epoch: 13/30, Train Loss: 0.00921490, Test Loss: 0.06693609, Test Accuracy: 0.97910000\n",
      "\n",
      "Epoch: 14/30, Train Loss: 0.00811866, Test Loss: 0.06725046, Test Accuracy: 0.97900000\n",
      "\n",
      "Epoch: 15/30, Train Loss: 0.00709274, Test Loss: 0.06733680, Test Accuracy: 0.97830000\n",
      "\n",
      "Epoch: 16/30, Train Loss: 0.00636019, Test Loss: 0.06739756, Test Accuracy: 0.97910000\n",
      "\n",
      "Epoch: 17/30, Train Loss: 0.00572026, Test Loss: 0.06714072, Test Accuracy: 0.97890000\n",
      "\n",
      "Epoch: 18/30, Train Loss: 0.00518920, Test Loss: 0.06819753, Test Accuracy: 0.97870000\n",
      "\n",
      "Epoch: 19/30, Train Loss: 0.00469056, Test Loss: 0.06811867, Test Accuracy: 0.97850000\n",
      "\n",
      "Epoch: 20/30, Train Loss: 0.00426798, Test Loss: 0.06838907, Test Accuracy: 0.97900000\n",
      "\n",
      "Epoch: 21/30, Train Loss: 0.00393872, Test Loss: 0.06816522, Test Accuracy: 0.97920000\n",
      "\n",
      "Epoch: 22/30, Train Loss: 0.00362471, Test Loss: 0.06862192, Test Accuracy: 0.97940000\n",
      "\n",
      "Epoch: 23/30, Train Loss: 0.00334990, Test Loss: 0.06923180, Test Accuracy: 0.97910000\n",
      "\n",
      "Epoch: 24/30, Train Loss: 0.00311754, Test Loss: 0.06913381, Test Accuracy: 0.97880000\n",
      "\n",
      "Epoch: 25/30, Train Loss: 0.00290044, Test Loss: 0.06951448, Test Accuracy: 0.97890000\n",
      "\n",
      "Epoch: 26/30, Train Loss: 0.00272266, Test Loss: 0.06990538, Test Accuracy: 0.97910000\n",
      "\n",
      "Epoch: 27/30, Train Loss: 0.00254067, Test Loss: 0.07048355, Test Accuracy: 0.97900000\n",
      "\n",
      "Epoch: 28/30, Train Loss: 0.00239491, Test Loss: 0.06990519, Test Accuracy: 0.97950000\n",
      "\n",
      "Epoch: 29/30, Train Loss: 0.00226480, Test Loss: 0.07003555, Test Accuracy: 0.97920000\n",
      "\n",
      "Epoch: 30/30, Train Loss: 0.00213280, Test Loss: 0.07051157, Test Accuracy: 0.97940000\n",
      "my_model: Model(\n",
      "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (sigmoid12): Sigmoid()\n",
      ")\n",
      "my_optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.1\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "Epoch: 1/30, Train Loss: 0.77229016, Test Loss: 0.36556621, Test Accuracy: 0.89740000\n",
      "\n",
      "Epoch: 2/30, Train Loss: 0.34719833, Test Loss: 0.30601284, Test Accuracy: 0.91130000\n",
      "\n",
      "Epoch: 3/30, Train Loss: 0.30693888, Test Loss: 0.28060609, Test Accuracy: 0.92130000\n",
      "\n",
      "Epoch: 4/30, Train Loss: 0.28483134, Test Loss: 0.27108764, Test Accuracy: 0.92220000\n",
      "\n",
      "Epoch: 5/30, Train Loss: 0.26674319, Test Loss: 0.24900326, Test Accuracy: 0.93030000\n",
      "\n",
      "Epoch: 6/30, Train Loss: 0.24984935, Test Loss: 0.23398064, Test Accuracy: 0.93260000\n",
      "\n",
      "Epoch: 7/30, Train Loss: 0.23336247, Test Loss: 0.22511649, Test Accuracy: 0.93520000\n",
      "\n",
      "Epoch: 8/30, Train Loss: 0.21804447, Test Loss: 0.20816296, Test Accuracy: 0.94010000\n",
      "\n",
      "Epoch: 9/30, Train Loss: 0.20462472, Test Loss: 0.19458867, Test Accuracy: 0.94520000\n",
      "\n",
      "Epoch: 10/30, Train Loss: 0.19214305, Test Loss: 0.18808595, Test Accuracy: 0.94570000\n",
      "\n",
      "Epoch: 11/30, Train Loss: 0.18089217, Test Loss: 0.17587552, Test Accuracy: 0.94900000\n",
      "\n",
      "Epoch: 12/30, Train Loss: 0.17059864, Test Loss: 0.16796683, Test Accuracy: 0.94990000\n",
      "\n",
      "Epoch: 13/30, Train Loss: 0.16099815, Test Loss: 0.15870854, Test Accuracy: 0.95330000\n",
      "\n",
      "Epoch: 14/30, Train Loss: 0.15282047, Test Loss: 0.15348154, Test Accuracy: 0.95580000\n",
      "\n",
      "Epoch: 15/30, Train Loss: 0.14522061, Test Loss: 0.14678260, Test Accuracy: 0.95770000\n",
      "\n",
      "Epoch: 16/30, Train Loss: 0.13832390, Test Loss: 0.13840950, Test Accuracy: 0.96100000\n",
      "\n",
      "Epoch: 17/30, Train Loss: 0.13196634, Test Loss: 0.13653209, Test Accuracy: 0.95970000\n",
      "\n",
      "Epoch: 18/30, Train Loss: 0.12606080, Test Loss: 0.12942022, Test Accuracy: 0.96280000\n",
      "\n",
      "Epoch: 19/30, Train Loss: 0.12032454, Test Loss: 0.12580063, Test Accuracy: 0.96360000\n",
      "\n",
      "Epoch: 20/30, Train Loss: 0.11546896, Test Loss: 0.12108934, Test Accuracy: 0.96530000\n",
      "\n",
      "Epoch: 21/30, Train Loss: 0.11083578, Test Loss: 0.11918581, Test Accuracy: 0.96410000\n",
      "\n",
      "Epoch: 22/30, Train Loss: 0.10652338, Test Loss: 0.11451459, Test Accuracy: 0.96600000\n",
      "\n",
      "Epoch: 23/30, Train Loss: 0.10224386, Test Loss: 0.11202619, Test Accuracy: 0.96680000\n",
      "\n",
      "Epoch: 24/30, Train Loss: 0.09870039, Test Loss: 0.10751366, Test Accuracy: 0.96800000\n",
      "\n",
      "Epoch: 25/30, Train Loss: 0.09506041, Test Loss: 0.10633760, Test Accuracy: 0.96880000\n",
      "\n",
      "Epoch: 26/30, Train Loss: 0.09156718, Test Loss: 0.10264896, Test Accuracy: 0.96910000\n",
      "\n",
      "Epoch: 27/30, Train Loss: 0.08833942, Test Loss: 0.10154048, Test Accuracy: 0.96900000\n",
      "\n",
      "Epoch: 28/30, Train Loss: 0.08558544, Test Loss: 0.09941065, Test Accuracy: 0.97010000\n",
      "\n",
      "Epoch: 29/30, Train Loss: 0.08267791, Test Loss: 0.09813298, Test Accuracy: 0.97020000\n",
      "\n",
      "Epoch: 30/30, Train Loss: 0.07989562, Test Loss: 0.09521272, Test Accuracy: 0.97110000\n",
      "my_model: Model(\n",
      "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (sigmoid12): Sigmoid()\n",
      ")\n",
      "my_optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "Epoch: 1/30, Train Loss: 0.43013016, Test Loss: 0.23038319, Test Accuracy: 0.93350000\n",
      "\n",
      "Epoch: 2/30, Train Loss: 0.19845420, Test Loss: 0.16318714, Test Accuracy: 0.95180000\n",
      "\n",
      "Epoch: 3/30, Train Loss: 0.14409275, Test Loss: 0.13112965, Test Accuracy: 0.96150000\n",
      "\n",
      "Epoch: 4/30, Train Loss: 0.11007065, Test Loss: 0.10521115, Test Accuracy: 0.96910000\n",
      "\n",
      "Epoch: 5/30, Train Loss: 0.08675428, Test Loss: 0.09378187, Test Accuracy: 0.97180000\n",
      "\n",
      "Epoch: 6/30, Train Loss: 0.07006374, Test Loss: 0.08096124, Test Accuracy: 0.97590000\n",
      "\n",
      "Epoch: 7/30, Train Loss: 0.05699459, Test Loss: 0.07811322, Test Accuracy: 0.97590000\n",
      "\n",
      "Epoch: 8/30, Train Loss: 0.04634903, Test Loss: 0.07456356, Test Accuracy: 0.97660000\n",
      "\n",
      "Epoch: 9/30, Train Loss: 0.03831605, Test Loss: 0.07117033, Test Accuracy: 0.97780000\n",
      "\n",
      "Epoch: 10/30, Train Loss: 0.03116941, Test Loss: 0.06739555, Test Accuracy: 0.98000000\n",
      "\n",
      "Epoch: 11/30, Train Loss: 0.02555423, Test Loss: 0.06667478, Test Accuracy: 0.97990000\n",
      "\n",
      "Epoch: 12/30, Train Loss: 0.02080868, Test Loss: 0.06638725, Test Accuracy: 0.97950000\n",
      "\n",
      "Epoch: 13/30, Train Loss: 0.01690935, Test Loss: 0.06281901, Test Accuracy: 0.97990000\n",
      "\n",
      "Epoch: 14/30, Train Loss: 0.01330700, Test Loss: 0.06899784, Test Accuracy: 0.97890000\n",
      "\n",
      "Epoch: 15/30, Train Loss: 0.01106236, Test Loss: 0.06525876, Test Accuracy: 0.97980000\n",
      "\n",
      "Epoch: 16/30, Train Loss: 0.00848057, Test Loss: 0.06817348, Test Accuracy: 0.97920000\n",
      "\n",
      "Epoch: 17/30, Train Loss: 0.00698429, Test Loss: 0.06676263, Test Accuracy: 0.97960000\n",
      "\n",
      "Epoch: 18/30, Train Loss: 0.00547083, Test Loss: 0.06740024, Test Accuracy: 0.98010000\n",
      "\n",
      "Epoch: 19/30, Train Loss: 0.00480950, Test Loss: 0.07052146, Test Accuracy: 0.97950000\n",
      "\n",
      "Epoch: 20/30, Train Loss: 0.00362121, Test Loss: 0.07191904, Test Accuracy: 0.98000000\n",
      "\n",
      "Epoch: 21/30, Train Loss: 0.00258812, Test Loss: 0.07331077, Test Accuracy: 0.98060000\n",
      "\n",
      "Epoch: 22/30, Train Loss: 0.00234628, Test Loss: 0.07124675, Test Accuracy: 0.98020000\n",
      "\n",
      "Epoch: 23/30, Train Loss: 0.00173858, Test Loss: 0.07269171, Test Accuracy: 0.98110000\n",
      "\n",
      "Epoch: 24/30, Train Loss: 0.00202080, Test Loss: 0.07442364, Test Accuracy: 0.98050000\n",
      "\n",
      "Epoch: 25/30, Train Loss: 0.00109320, Test Loss: 0.07514408, Test Accuracy: 0.98110000\n",
      "\n",
      "Epoch: 26/30, Train Loss: 0.00076435, Test Loss: 0.07953331, Test Accuracy: 0.97950000\n",
      "\n",
      "Epoch: 27/30, Train Loss: 0.00227620, Test Loss: 0.08126477, Test Accuracy: 0.98090000\n",
      "\n",
      "Epoch: 28/30, Train Loss: 0.00053937, Test Loss: 0.08105319, Test Accuracy: 0.98060000\n",
      "\n",
      "Epoch: 29/30, Train Loss: 0.00035571, Test Loss: 0.08218981, Test Accuracy: 0.98100000\n",
      "\n",
      "Epoch: 30/30, Train Loss: 0.00033691, Test Loss: 0.08346478, Test Accuracy: 0.98130000\n"
     ]
    }
   ],
   "source": [
    "# Seed 1234\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "model_factory('Adadelta',1234)\n",
    "model_factory('Adagrad',1234)\n",
    "model_factory('SGD',1234)\n",
    "model_factory('Adam',1234)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "all_seeds_testing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
