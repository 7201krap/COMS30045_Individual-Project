{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "all_seeds_10subsequent_batchsize1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/all_seeds_10subsequent_batchsize1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7STrWa0P3z_",
        "outputId": "9035828c-6141-4116-e202-d07c2c01f4df"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.utils import shuffle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIyKF1HE7uQW",
        "outputId": "06016f88-dcc9-4f59-c86f-aed82e2366c1"
      },
      "source": [
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "root_dir = './'\n",
        "torchvision.datasets.MNIST(root=root_dir,download=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-10 02:30:01--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
            "--2021-04-10 02:30:02--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘MNIST.tar.gz’\n",
            "\n",
            "MNIST.tar.gz            [     <=>            ]  33.20M  33.9MB/s    in 1.0s    \n",
            "\n",
            "2021-04-10 02:30:03 (33.9 MB/s) - ‘MNIST.tar.gz’ saved [34813078]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte\n",
            "MNIST/raw/train-images-idx3-ubyte.gz\n",
            "MNIST/processed/\n",
            "MNIST/processed/training.pt\n",
            "MNIST/processed/test.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./\n",
              "    Split: Train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4j9WoP-UnAm"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTW5TOUnP5XY"
      },
      "source": [
        "mnist_trainset = torchvision.datasets.MNIST(root=root_dir, train=True, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "mnist_testset  = torchvision.datasets.MNIST(root=root_dir, \n",
        "                                train=False, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow9Sy2SyUYgn"
      },
      "source": [
        "new_mnist_trainset =  [ [[],[]] for i in range(10)]\n",
        "# new_mnist_testset  =  [ [[],[]] for i in range(10)]\n",
        "\n",
        "for i in range(60000):\n",
        "    for j in range(10):\n",
        "        # 만약에 label 이 j 이면, \n",
        "        if mnist_trainset[i][1] == j:\n",
        "            # image \n",
        "            new_mnist_trainset[j][0].append(mnist_trainset[i][0])  \n",
        "            # new_mnist_trainset[j][0] 는 j label 에 해당하는 image 가 들어있다. \n",
        "\n",
        "            # label\n",
        "            new_mnist_trainset[j][1].append(mnist_trainset[i][1])\n",
        "            # new_mnist_trainset[j][1] 는 j label 에 해당하는 label 가 들어있다. \n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyIkUPYTNdLf",
        "outputId": "ab137a29-a629-4ba1-f535-f3fed9dd5820"
      },
      "source": [
        "print(\"0\", len(new_mnist_trainset[0][0]))    # - pop 3 times\n",
        "print(\"1\", len(new_mnist_trainset[1][0]))    # - pop 2 times\n",
        "print(\"2\", len(new_mnist_trainset[2][0]))    # - pop 8 times\n",
        "print(\"3\", len(new_mnist_trainset[3][0]))    # - pop 1 times\n",
        "print(\"4\", len(new_mnist_trainset[4][0]))    # - pop 2 time\n",
        "print(\"5\", len(new_mnist_trainset[5][0]))    # - pop 1 times\n",
        "print(\"6\", len(new_mnist_trainset[6][0]))    # - pop 8 times\n",
        "print(\"7\", len(new_mnist_trainset[7][0]))    # - pop 5 times\n",
        "print(\"8\", len(new_mnist_trainset[8][0]))    # - pop 1 times\n",
        "print(\"9\", len(new_mnist_trainset[9][0]))    # - pop 9 times"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 5923\n",
            "1 6742\n",
            "2 5958\n",
            "3 6131\n",
            "4 5842\n",
            "5 5421\n",
            "6 5918\n",
            "7 6265\n",
            "8 5851\n",
            "9 5949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieTyNQeWP1Sb"
      },
      "source": [
        "# 0 - pop 3 times\n",
        "for i in range(3):\n",
        "    new_mnist_trainset[0][0].pop()\n",
        "    new_mnist_trainset[0][1].pop()\n",
        "\n",
        "# 1 - pop 2 times\n",
        "for i in range(2):\n",
        "    new_mnist_trainset[1][0].pop()\n",
        "    new_mnist_trainset[1][1].pop()\n",
        "\n",
        "# 2 - pop 8 times\n",
        "for i in range(8):\n",
        "    new_mnist_trainset[2][0].pop()\n",
        "    new_mnist_trainset[2][1].pop()\n",
        "\n",
        "# 3 - pop 1 times\n",
        "for i in range(1):\n",
        "    new_mnist_trainset[3][0].pop()\n",
        "    new_mnist_trainset[3][1].pop()\n",
        "\n",
        "# 4 - pop 2 time\n",
        "for i in range(2):\n",
        "    new_mnist_trainset[4][0].pop()\n",
        "    new_mnist_trainset[4][1].pop()\n",
        "\n",
        "# 5 - pop 1 times\n",
        "for i in range(1):\n",
        "    new_mnist_trainset[5][0].pop()\n",
        "    new_mnist_trainset[5][1].pop()\n",
        "\n",
        "# 6 - pop 8 times\n",
        "for i in range(8):\n",
        "    new_mnist_trainset[6][0].pop()\n",
        "    new_mnist_trainset[6][1].pop()\n",
        "\n",
        "# 7 - pop 0 times\n",
        "for i in range(5):\n",
        "    new_mnist_trainset[7][0].pop()\n",
        "    new_mnist_trainset[7][1].pop()\n",
        "\n",
        "# 8 - pop 1 times\n",
        "for i in range(1):\n",
        "    new_mnist_trainset[8][0].pop()\n",
        "    new_mnist_trainset[8][1].pop()\n",
        "\n",
        "# 9 - pop 9 times\n",
        "for i in range(9):\n",
        "    new_mnist_trainset[9][0].pop()\n",
        "    new_mnist_trainset[9][1].pop()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSb6f78RQYGp",
        "outputId": "0b98467e-da45-457b-b896-22fb3f23ede4"
      },
      "source": [
        "print(\"0\", len(new_mnist_trainset[0][0]))    # pop 3 times\n",
        "print(\"0\", len(new_mnist_trainset[0][1]))    # pop 3 times\n",
        "\n",
        "print(\"1\", len(new_mnist_trainset[1][0]))    # pop 2 times\n",
        "print(\"1\", len(new_mnist_trainset[1][1]))    # pop 2 times\n",
        "\n",
        "print(\"2\", len(new_mnist_trainset[2][0]))    # pop 3 times\n",
        "print(\"2\", len(new_mnist_trainset[2][1]))    # pop 3 times\n",
        "\n",
        "print(\"3\", len(new_mnist_trainset[3][0]))    # pop 1 times\n",
        "print(\"3\", len(new_mnist_trainset[3][1]))    # pop 1 times\n",
        "\n",
        "print(\"4\", len(new_mnist_trainset[4][0]))    # pop 2 time\n",
        "print(\"4\", len(new_mnist_trainset[4][1]))    # pop 2 time\n",
        "\n",
        "print(\"5\", len(new_mnist_trainset[5][0]))    # pop 1 times\n",
        "print(\"5\", len(new_mnist_trainset[5][1]))    # pop 1 times\n",
        "\n",
        "print(\"6\", len(new_mnist_trainset[6][0]))    # pop 3 times\n",
        "print(\"6\", len(new_mnist_trainset[6][1]))    # pop 3 times\n",
        "\n",
        "print(\"7\", len(new_mnist_trainset[7][0]))    # pop 0 times\n",
        "print(\"7\", len(new_mnist_trainset[7][1]))    # pop 0 times\n",
        "\n",
        "print(\"8\", len(new_mnist_trainset[8][0]))    # pop 1 times\n",
        "print(\"8\", len(new_mnist_trainset[8][1]))    # pop 1 times\n",
        "\n",
        "print(\"9\", len(new_mnist_trainset[9][0]))    # pop 4 times\n",
        "print(\"9\", len(new_mnist_trainset[9][1]))    # pop 4 times"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 5920\n",
            "0 5920\n",
            "1 6740\n",
            "1 6740\n",
            "2 5950\n",
            "2 5950\n",
            "3 6130\n",
            "3 6130\n",
            "4 5840\n",
            "4 5840\n",
            "5 5420\n",
            "5 5420\n",
            "6 5910\n",
            "6 5910\n",
            "7 6260\n",
            "7 6260\n",
            "8 5850\n",
            "8 5850\n",
            "9 5940\n",
            "9 5940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMFpH3QLNAYQ"
      },
      "source": [
        "image_trainset = list()\n",
        "label_trainset = list()\n",
        "\n",
        "for i in range(10):\n",
        "    image_trainset.append(new_mnist_trainset[i][0])\n",
        "    label_trainset.append(new_mnist_trainset[i][1])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV9vZ4G_NCTB"
      },
      "source": [
        "flattened_image_train = list()\n",
        "flattened_label_train = list()\n",
        "\n",
        "# flattening image \n",
        "for sublist in image_trainset:\n",
        "    for val in sublist:\n",
        "        flattened_image_train.append(val)\n",
        "\n",
        "# flattening label\n",
        "for sublist in label_trainset:\n",
        "    for val in sublist:\n",
        "        flattened_label_train.append(val)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q_z0dBzSR5o",
        "outputId": "1aad7394-46f2-496a-9f30-ffe17f7ef8fe"
      },
      "source": [
        "len(flattened_image_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59960"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onYs9h2CTj41",
        "outputId": "79d2c866-b206-45eb-9714-e108c53c6889"
      },
      "source": [
        "len(flattened_label_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59960"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbZTrSWrYkkG"
      },
      "source": [
        "def split(a, n):\n",
        "    k, m = divmod(len(a), n)\n",
        "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFctZ-9EYlgx"
      },
      "source": [
        "list1 = list(split(flattened_image_train, 5996))\n",
        "list2 = list(split(flattened_label_train, 5996))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlqM-hF2ZuV2"
      },
      "source": [
        "X, y = shuffle(list1, list2)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPTF5pJPZypH"
      },
      "source": [
        "X_final, y_final = shuffle(X, y)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LM44S14PndX",
        "outputId": "7270937b-c101-42de-d070-108c1387ec85"
      },
      "source": [
        "y_final[6]\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OimwHtbgaEtX"
      },
      "source": [
        "flattened_X_final = [val for sublist in X_final for val in sublist] \n",
        "flattened_y_final = [val for sublist in y_final for val in sublist] "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "Xxbuxud6djJF",
        "outputId": "2701b7a6-2ca1-4e71-a1f2-44daa76bc630"
      },
      "source": [
        "print(flattened_y_final[0])\n",
        "plt.imshow(flattened_X_final[0].reshape(28,28))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f77fae3d850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2XVNBlcPl3I"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SkccxZdNEDY"
      },
      "source": [
        "flattened_image_train = torch.stack(flattened_X_final)\n",
        "flattened_label_train = torch.Tensor(flattened_y_final)\n",
        "flattened_label_train = flattened_label_train.type(torch.LongTensor)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trsJx6P7QQsI"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cANEC2zaNFPq"
      },
      "source": [
        "train_dataset = TensorDataset(flattened_image_train, flattened_label_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXTkEUJ5P6kU"
      },
      "source": [
        "# Define the model \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.linear_1 = torch.nn.Linear(784, 256)\n",
        "        self.linear_2 = torch.nn.Linear(256, 10)\n",
        "        self.sigmoid  = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        pred = self.linear_2(x)\n",
        "\n",
        "        return pred"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfgvKH6eP9Ou"
      },
      "source": [
        "def get_activation(model):    \n",
        "    def hook(module, input, output):\n",
        "        model.layer_activations = output\n",
        "    return hook"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uAD1thJ5JvD"
      },
      "source": [
        "def selectivity(hidden_layer_each_neuron):\n",
        "    __selectivity__ = list()\n",
        "    # I will now try to find the average of each class for each neuron.\n",
        "    # check out the next cell \n",
        "    avg_activations = [dict() for x in range(256)]\n",
        "    for i, neuron in enumerate(hidden_layer_each_neuron):\n",
        "        for k, v in neuron.items():\n",
        "            # v is the list of activations for hidden layer's neuron k \n",
        "            avg_activations[i][k] = sum(v) / float(len(v))\n",
        "\n",
        "    # generate 256 lists to get only values in avg_activations\n",
        "    only_activation_vals = [list() for x in range(256)]\n",
        "\n",
        "    # get only values from avg_activations\n",
        "    for i, avg_activation in enumerate(avg_activations):\n",
        "        for value in avg_activation.values():\n",
        "            only_activation_vals[i].append(value)\n",
        "\n",
        "    for activation_val in only_activation_vals:\n",
        "        # find u_max \n",
        "        u_max = np.max(activation_val)\n",
        "\n",
        "        # find u_minus_max \n",
        "        u_minus_max = (np.sum(activation_val) - u_max) / 9\n",
        "\n",
        "        # find selectivity \n",
        "        selectivity = (u_max - u_minus_max) / (u_max + u_minus_max + 1e-7)\n",
        "\n",
        "        # append selectivity value to selectivity\n",
        "        __selectivity__.append(selectivity)\n",
        "\n",
        "    avg_selectivity = np.average(__selectivity__)\n",
        "    std_selectivity = np.std(__selectivity__)\n",
        "                                 \n",
        "    return avg_selectivity, std_selectivity"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9kATNPUz1cA"
      },
      "source": [
        "def sparsity_calculator(final_spareness):\n",
        "    sparseness_list_avg = list()\n",
        "    sparseness_list_std = list()\n",
        "    for single_epoch_spareness in final_spareness:\n",
        "\n",
        "        hidden_layer_activation_list = single_epoch_spareness\n",
        "        hidden_layer_activation_list = torch.stack(hidden_layer_activation_list)\n",
        "        layer_activations_list = torch.reshape(hidden_layer_activation_list, (10000, 256))\n",
        "\n",
        "        layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
        "        num_neurons = layer_activations_list.shape[1]\n",
        "        population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
        "\n",
        "        mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
        "        \n",
        "\n",
        "        sparseness_list_avg.append(mean_sparseness_per_epoch)\n",
        "        \n",
        "\n",
        "    return sparseness_list_avg"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXOpwTXEQFKY"
      },
      "source": [
        "no_epochs = 30\n",
        "def sparsity_selectivity_trainer(optimizer, model):\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    train_loss = list()\n",
        "    test_loss  = list()\n",
        "    test_acc   = list()\n",
        "\n",
        "    final_spareness = list()\n",
        "    \n",
        "    final_selectivity_avg_list = list()\n",
        "    final_selectivity_std_list = list()\n",
        "\n",
        "    best_test_loss = 1\n",
        "\n",
        "    for epoch in range(no_epochs):\n",
        "\n",
        "        _hidden_layer_each_neuron_ = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        _hidden_layer_each_neuron_ = np.array(_hidden_layer_each_neuron_)\n",
        "\n",
        "        hidden_layer_activation_list = list()\n",
        "\n",
        "        total_train_loss = 0\n",
        "        total_test_loss = 0\n",
        "\n",
        "        # training\n",
        "        # set up training mode \n",
        "        model.train()\n",
        "\n",
        "        for itr, (images, labels) in enumerate(train_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print/Append activation of the hidden layer \n",
        "            # print(model.layer_activations.shape)\n",
        "            # model.layer_activations\n",
        "\n",
        "        total_train_loss = total_train_loss / (itr + 1)\n",
        "        train_loss.append(total_train_loss)\n",
        "\n",
        "        # testing \n",
        "        # change to evaluation mode \n",
        "        model.eval()\n",
        "        total = 0\n",
        "        for itr, (images, labels) in enumerate(test_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # we now need softmax because we are testing.\n",
        "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "            for i, p in enumerate(pred):\n",
        "                if labels[i] == torch.max(p.data, 0)[1]:\n",
        "                    total = total + 1\n",
        "\n",
        "            hidden_layer_activation_list.append(model.layer_activations)\n",
        "\n",
        "            \n",
        "            for activation, label in zip(model.layer_activations, labels):\n",
        "                # shape of activation and label: 256 and 1 \n",
        "                \n",
        "                # get the actual value of item. This is because label is now Tensor \n",
        "                label = label.item()\n",
        "\n",
        "                # this is not part of gradient calculcation \n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "\n",
        "                # for each image/label, append activation value of neuron \n",
        "                for i in range(256):    # number of neurons in hidden layer \n",
        "                    _hidden_layer_each_neuron_[i][label].append(activation[i])\n",
        "\n",
        "        avg_selectivity, std_selectivity = selectivity(_hidden_layer_each_neuron_)\n",
        "        \n",
        "        final_selectivity_avg_list.append(avg_selectivity)\n",
        "\n",
        "        final_spareness.append(hidden_layer_activation_list)\n",
        "\n",
        "        # caculate accuracy \n",
        "        accuracy = total / len(mnist_testset)\n",
        "\n",
        "        # append accuracy here\n",
        "        test_acc.append(accuracy)\n",
        "\n",
        "        # append test loss here \n",
        "        total_test_loss = total_test_loss / (itr + 1)\n",
        "        test_loss.append(total_test_loss)\n",
        "\n",
        "        # print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Test Loss: {:.8f}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, total_train_loss, total_test_loss, accuracy))\n",
        "\n",
        "    sparsity_list_avg = sparsity_calculator(final_spareness)\n",
        "\n",
        "    average_sparsity = list()\n",
        "    for i in range(no_epochs):\n",
        "        average_sparsity.append( (sparsity_list_avg[i].item()) / 1 )\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "\n",
        "    return test_acc, average_sparsity, final_selectivity_avg_list, final_selectivity_std_list"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4WytqcJRZxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7842499a-1f34-4331-ead4-7c0550d4a09a"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "# Adadelta\n",
        "model_Adadelta = Model()\n",
        "print(\"model_Adadelta:\", model_Adadelta)\n",
        "model_Adadelta.to(device)\n",
        "model_Adadelta.sigmoid.register_forward_hook(get_activation(model_Adadelta))\n",
        "optimizer_Adadelta = torch.optim.Adadelta(model_Adadelta.parameters(), lr=1.0)\n",
        "Adadelta_test_acc, sparsity_avg, Adadelta_avg_selectivity_list, Adadelta_std_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adadelta, model=model_Adadelta)\n",
        "\n",
        "f = open(\"seed1_10subsebatch1_sparsity_selectivity_Adadelta.txt\", \"w\")\n",
        "f.write(str(Adadelta_test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(Adadelta_avg_selectivity_list)+'\\n'+str(Adadelta_std_selectivity_list)+'\\n\\n')\n",
        "f.close()\n",
        "\n",
        "!cp seed1_10subsebatch1_sparsity_selectivity_Adadelta.txt /content/drive/MyDrive\n",
        "\n",
        "\n",
        "# Adagrad \n",
        "model_Adagrad = Model()\n",
        "print(\"model_Adagrad:\", model_Adagrad)\n",
        "model_Adagrad.to(device)\n",
        "model_Adagrad.sigmoid.register_forward_hook(get_activation(model_Adagrad))\n",
        "optimizer_Adagrad = torch.optim.Adagrad(model_Adagrad.parameters(), lr=0.1)\n",
        "Adagrad_test_acc, sparsity_avg, Adagrad_avg_selectivity_list, Adagrad_std_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adagrad, model=model_Adagrad)\n",
        "\n",
        "f = open(\"seed1_10subsebatch1_sparsity_selectivity_Adagrad.txt\", \"w\")\n",
        "f.write(str(Adagrad_test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(Adagrad_avg_selectivity_list)+'\\n'+str(Adagrad_std_selectivity_list)+'\\n\\n')\n",
        "f.close()\n",
        "\n",
        "!cp seed1_10subsebatch1_sparsity_selectivity_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "\n",
        "# SGD \n",
        "model_SGD = Model()\n",
        "print(\"model_SGD:\", model_SGD)\n",
        "model_SGD.to(device)\n",
        "model_SGD.sigmoid.register_forward_hook(get_activation(model_SGD))\n",
        "optimizer_SGD = torch.optim.SGD(model_SGD.parameters(), lr=0.1)\n",
        "SGD_test_acc, sparsity_avg, SGD_avg_selectivity_list, SGD_std_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_SGD, model=model_SGD)\n",
        "\n",
        "f = open(\"seed1_10subsebatch1_sparsity_selectivity_SGD.txt\", \"w\")\n",
        "f.write(str(SGD_test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(SGD_avg_selectivity_list)+'\\n'+str(SGD_std_selectivity_list)+'\\n\\n')\n",
        "f.close()\n",
        "\n",
        "!cp seed1_10subsebatch1_sparsity_selectivity_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "# Adam \n",
        "model_Adam = Model()\n",
        "print(\"model_Adam:\", model_Adam)\n",
        "model_Adam.to(device)\n",
        "model_Adam.sigmoid.register_forward_hook(get_activation(model_Adam))\n",
        "optimizer_Adam = torch.optim.Adam(model_Adam.parameters(), lr=0.001)\n",
        "Adam_test_acc, sparsity_avg, Adam_avg_selectivity_list, Adam_std_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adam, model=model_Adam)\n",
        "\n",
        "f = open(\"seed1_10subsebatch1_sparsity_selectivity_Adam.txt\", \"w\")\n",
        "f.write(str(Adam_test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(Adam_avg_selectivity_list)+'\\n'+str(Adam_std_selectivity_list)+'\\n\\n')\n",
        "f.close()\n",
        "\n",
        "!cp seed1_10subsebatch1_sparsity_selectivity_Adam.txt /content/drive/MyDrive"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_Adadelta: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "model_Adagrad: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "model_SGD: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "model_Adam: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX-EC-kfda0v",
        "outputId": "dd6fe012-2990-4ef6-adf6-24cd4eb01e6a"
      },
      "source": [
        "torch.manual_seed(100)\n",
        "np.random.seed(100)\n",
        "\n",
        "# Adadelta\n",
        "model_Adadelta = Model()\n",
        "print(\"model_Adadelta:\", model_Adadelta)\n",
        "model_Adadelta.to(device)\n",
        "model_Adadelta.sigmoid.register_forward_hook(get_activation(model_Adadelta))\n",
        "optimizer_Adadelta = torch.optim.Adadelta(model_Adadelta.parameters(), lr=1.0)\n",
        "Adadelta_test_acc, sparsity_avg, Adadelta_avg_selectivity_list, Adadelta_std_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adadelta, model=model_Adadelta)\n",
        "\n",
        "f = open(\"seed100_10subsebatch1_sparsity_selectivity_Adadelta.txt\", \"w\")\n",
        "f.write(str(Adadelta_test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(Adadelta_avg_selectivity_list)+'\\n'+str(Adadelta_std_selectivity_list)+'\\n\\n')\n",
        "f.close()\n",
        "\n",
        "!cp seed100_10subsebatch1_sparsity_selectivity_Adadelta.txt /content/drive/MyDrive\n",
        "\n",
        "\n",
        "# Adagrad \n",
        "model_Adagrad = Model()\n",
        "print(\"model_Adagrad:\", model_Adagrad)\n",
        "model_Adagrad.to(device)\n",
        "model_Adagrad.sigmoid.register_forward_hook(get_activation(model_Adagrad))\n",
        "optimizer_Adagrad = torch.optim.Adagrad(model_Adagrad.parameters(), lr=0.1)\n",
        "Adagrad_test_acc, sparsity_avg, Adagrad_avg_selectivity_list, Adagrad_std_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adagrad, model=model_Adagrad)\n",
        "\n",
        "f = open(\"seed100_10subsebatch1_sparsity_selectivity_Adagrad.txt\", \"w\")\n",
        "f.write(str(Adagrad_test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(Adagrad_avg_selectivity_list)+'\\n'+str(Adagrad_std_selectivity_list)+'\\n\\n')\n",
        "f.close()\n",
        "\n",
        "!cp seed100_10subsebatch1_sparsity_selectivity_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "\n",
        "# SGD \n",
        "model_SGD = Model()\n",
        "print(\"model_SGD:\", model_SGD)\n",
        "model_SGD.to(device)\n",
        "model_SGD.sigmoid.register_forward_hook(get_activation(model_SGD))\n",
        "optimizer_SGD = torch.optim.SGD(model_SGD.parameters(), lr=0.1)\n",
        "SGD_test_acc, sparsity_avg, SGD_avg_selectivity_list, SGD_std_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_SGD, model=model_SGD)\n",
        "\n",
        "f = open(\"seed100_10subsebatch1_sparsity_selectivity_SGD.txt\", \"w\")\n",
        "f.write(str(SGD_test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(SGD_avg_selectivity_list)+'\\n'+str(SGD_std_selectivity_list)+'\\n\\n')\n",
        "f.close()\n",
        "\n",
        "!cp seed100_10subsebatch1_sparsity_selectivity_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "# Adam \n",
        "model_Adam = Model()\n",
        "print(\"model_Adam:\", model_Adam)\n",
        "model_Adam.to(device)\n",
        "model_Adam.sigmoid.register_forward_hook(get_activation(model_Adam))\n",
        "optimizer_Adam = torch.optim.Adam(model_Adam.parameters(), lr=0.001)\n",
        "Adam_test_acc, sparsity_avg, Adam_avg_selectivity_list, Adam_std_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adam, model=model_Adam)\n",
        "\n",
        "f = open(\"seed100_10subsebatch1_sparsity_selectivity_Adam.txt\", \"w\")\n",
        "f.write(str(Adam_test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(Adam_avg_selectivity_list)+'\\n'+str(Adam_std_selectivity_list)+'\\n\\n')\n",
        "f.close()\n",
        "\n",
        "!cp seed100_10subsebatch1_sparsity_selectivity_Adam.txt /content/drive/MyDrive"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_Adadelta: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "model_Adagrad: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "model_SGD: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "model_Adam: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3gMfdJTmn51",
        "outputId": "7c485089-8312-4a96-ab7a-dffe62573b7f"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "# Adadelta\n",
        "model_Adadelta = Model()\n",
        "print(\"model_Adadelta:\", model_Adadelta)\n",
        "model_Adadelta.to(device)\n",
        "model_Adadelta.sigmoid.register_forward_hook(get_activation(model_Adadelta))\n",
        "optimizer_Adadelta = torch.optim.Adadelta(model_Adadelta.parameters(), lr=1.0)\n",
        "Adadelta_test_acc, sparsity_avg, Adadelta_avg_selectivity_list, Adadelta_std_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adadelta, model=model_Adadelta)\n",
        "\n",
        "f = open(\"seed1234_10subsebatch1_sparsity_selectivity_Adadelta.txt\", \"w\")\n",
        "f.write(str(Adadelta_test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(Adadelta_avg_selectivity_list)+'\\n'+str(Adadelta_std_selectivity_list)+'\\n\\n')\n",
        "f.close()\n",
        "\n",
        "!cp seed1234_10subsebatch1_sparsity_selectivity_Adadelta.txt /content/drive/MyDrive\n",
        "\n",
        "\n",
        "# Adagrad \n",
        "model_Adagrad = Model()\n",
        "print(\"model_Adagrad:\", model_Adagrad)\n",
        "model_Adagrad.to(device)\n",
        "model_Adagrad.sigmoid.register_forward_hook(get_activation(model_Adagrad))\n",
        "optimizer_Adagrad = torch.optim.Adagrad(model_Adagrad.parameters(), lr=0.1)\n",
        "Adagrad_test_acc, sparsity_avg, Adagrad_avg_selectivity_list, Adagrad_std_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adagrad, model=model_Adagrad)\n",
        "\n",
        "f = open(\"seed1234_10subsebatch1_sparsity_selectivity_Adagrad.txt\", \"w\")\n",
        "f.write(str(Adagrad_test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(Adagrad_avg_selectivity_list)+'\\n'+str(Adagrad_std_selectivity_list)+'\\n\\n')\n",
        "f.close()\n",
        "\n",
        "!cp seed1234_10subsebatch1_sparsity_selectivity_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "\n",
        "# SGD \n",
        "model_SGD = Model()\n",
        "print(\"model_SGD:\", model_SGD)\n",
        "model_SGD.to(device)\n",
        "model_SGD.sigmoid.register_forward_hook(get_activation(model_SGD))\n",
        "optimizer_SGD = torch.optim.SGD(model_SGD.parameters(), lr=0.1)\n",
        "SGD_test_acc, sparsity_avg, SGD_avg_selectivity_list, SGD_std_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_SGD, model=model_SGD)\n",
        "\n",
        "f = open(\"seed1234_10subsebatch1_sparsity_selectivity_SGD.txt\", \"w\")\n",
        "f.write(str(SGD_test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(SGD_avg_selectivity_list)+'\\n'+str(SGD_std_selectivity_list)+'\\n\\n')\n",
        "f.close()\n",
        "\n",
        "!cp seed1234_10subsebatch1_sparsity_selectivity_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "# Adam \n",
        "model_Adam = Model()\n",
        "print(\"model_Adam:\", model_Adam)\n",
        "model_Adam.to(device)\n",
        "model_Adam.sigmoid.register_forward_hook(get_activation(model_Adam))\n",
        "optimizer_Adam = torch.optim.Adam(model_Adam.parameters(), lr=0.001)\n",
        "Adam_test_acc, sparsity_avg, Adam_avg_selectivity_list, Adam_std_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adam, model=model_Adam)\n",
        "\n",
        "f = open(\"seed1234_10subsebatch1_sparsity_selectivity_Adam.txt\", \"w\")\n",
        "f.write(str(Adam_test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(Adam_avg_selectivity_list)+'\\n'+str(Adam_std_selectivity_list)+'\\n\\n')\n",
        "f.close()\n",
        "\n",
        "!cp seed1234_10subsebatch1_sparsity_selectivity_Adam.txt /content/drive/MyDrive"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_Adadelta: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "model_Adagrad: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "model_SGD: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "model_Adam: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}