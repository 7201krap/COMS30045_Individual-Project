{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "all_seeds_unsorted.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/all_seeds_unsorted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7STrWa0P3z_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53fd22cb-712c-4910-fc21-0ae45bcce4ab"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0EpXz3ULfb1",
        "outputId": "7e1bf6b9-fd1b-40bc-ad03-afb1b2f58a36"
      },
      "source": [
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "root_dir = './'\n",
        "torchvision.datasets.MNIST(root=root_dir,download=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-12 17:16:58--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
            "--2021-04-12 17:16:58--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘MNIST.tar.gz.1’\n",
            "\n",
            "MNIST.tar.gz.1          [             <=>    ]  33.20M  10.0MB/s    in 4.1s    \n",
            "\n",
            "2021-04-12 17:17:02 (8.07 MB/s) - ‘MNIST.tar.gz.1’ saved [34813078]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte\n",
            "MNIST/raw/train-images-idx3-ubyte.gz\n",
            "MNIST/processed/\n",
            "MNIST/processed/training.pt\n",
            "MNIST/processed/test.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./\n",
              "    Split: Train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4j9WoP-UnAm"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApOU7hvb95W4"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTW5TOUnP5XY"
      },
      "source": [
        "mnist_trainset = torchvision.datasets.MNIST(root=root_dir, train=True, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "mnist_testset  = torchvision.datasets.MNIST(root=root_dir, \n",
        "                                train=False, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(mnist_trainset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=True)\n",
        "\n",
        "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXTkEUJ5P6kU"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# Define the model \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        # modify this section for later use \n",
        "        self.linear_1 = torch.nn.Linear(784, 256)\n",
        "        self.linear_2 = torch.nn.Linear(256, 10)\n",
        "        self.sigmoid12  = torch.nn.Sigmoid()\n",
        "\n",
        "        self.layer_activations = dict()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # modify this section for later use \n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.sigmoid12(x)\n",
        "        pred = self.linear_2(x)\n",
        "        return pred\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfgvKH6eP9Ou"
      },
      "source": [
        "def get_activation(model, layer_name):    \n",
        "    def hook(module, input, output):\n",
        "        model.layer_activations[layer_name] = output\n",
        "    return hook"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCEw3Ov3Lk5X"
      },
      "source": [
        "def sparsity_calculator(final_spareness):\n",
        "    sparseness_list = list()\n",
        "    for single_epoch_spareness in final_spareness:\n",
        "\n",
        "        hidden_layer_activation_list = single_epoch_spareness\n",
        "        hidden_layer_activation_list = torch.stack(hidden_layer_activation_list)\n",
        "        layer_activations_list = torch.reshape(hidden_layer_activation_list, (10000, 256))\n",
        "\n",
        "        layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
        "        num_neurons = layer_activations_list.shape[1]\n",
        "        population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
        "        mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
        "\n",
        "        sparseness_list.append(mean_sparseness_per_epoch)\n",
        "\n",
        "    return sparseness_list"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvHGO5RSvi6I"
      },
      "source": [
        "def selectivity(hidden_layer_each_neuron):\n",
        "    __selectivity__ = list()\n",
        "    # I will now try to find the average of each class for each neuron.\n",
        "    # check out the next cell \n",
        "    avg_activations = [dict() for x in range(256)]\n",
        "    for i, neuron in enumerate(hidden_layer_each_neuron):\n",
        "        for k, v in neuron.items():\n",
        "            # v is the list of activations for hidden layer's neuron k \n",
        "            avg_activations[i][k] = sum(v) / float(len(v))\n",
        "\n",
        "    # generate 256 lists to get only values in avg_activations\n",
        "    only_activation_vals = [list() for x in range(256)]\n",
        "\n",
        "    # get only values from avg_activations\n",
        "    for i, avg_activation in enumerate(avg_activations):\n",
        "        for value in avg_activation.values():\n",
        "            only_activation_vals[i].append(value)\n",
        "\n",
        "\n",
        "    for activation_val in only_activation_vals:\n",
        "        # find u_max \n",
        "        u_max = np.max(activation_val)\n",
        "\n",
        "        # find u_minus_max \n",
        "        u_minus_max = (np.sum(activation_val) - u_max) / 9\n",
        "\n",
        "        # find selectivity \n",
        "        selectivity = (u_max - u_minus_max) / (u_max + u_minus_max)\n",
        "\n",
        "        # append selectivity value to selectivity\n",
        "        __selectivity__.append(selectivity)\n",
        "\n",
        "    avg_selectivity = np.average(__selectivity__)\n",
        "    std_selectivity = np.std(__selectivity__)\n",
        "                                 \n",
        "    return avg_selectivity, std_selectivity"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUHSwHZqLm3Y"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "def avg_std_calculator(_hidden_layer_each_neuron_12):\n",
        "\n",
        "    avg_selectivity12, std_selectivity12 = selectivity(_hidden_layer_each_neuron_12)\n",
        "\n",
        "    final_selectivity_avg = (avg_selectivity12) / 1\n",
        "    final_selecvitity_std = (std_selectivity12) / 1\n",
        "\n",
        "    return final_selectivity_avg, final_selecvitity_std\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5PUiBNqUImf"
      },
      "source": [
        "def model_factory(optimizer_name, seed_num):\n",
        "    '''\n",
        "    optimizer_name : choose one of Adagrad, Adadelta, SGD, and Adam \n",
        "\n",
        "    '''\n",
        "    my_model = Model()\n",
        "    print(\"my_model:\", my_model)\n",
        "    my_model.to(device)\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    my_model.sigmoid12.register_forward_hook(get_activation(my_model, 's12'))\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        my_optimizer = torch.optim.Adadelta(my_model.parameters(), lr=1.0)\n",
        "\n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        my_optimizer = torch.optim.Adagrad(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        my_optimizer = torch.optim.SGD(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        my_optimizer = torch.optim.Adam(my_model.parameters(), lr=0.001)\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")\n",
        "    \n",
        "    print(\"my_optimizer:\", my_optimizer)\n",
        "    test_acc, sparsity, selectivity_list_avg, selectivity_list_std = selectivity_trainer(optimizer=my_optimizer, model=my_model)\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver = open(f\"seed{seed_num}_unsorted_batch_50_{optimizer_name}.txt\", \"w\")\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver.write(str(test_acc)+'\\n'+str(sparsity)+'\\n'+str(selectivity_list_avg)+'\\n'+str(selectivity_list_std)+'\\n\\n')\n",
        "    file_saver.close()\n",
        "\n",
        "    if seed_num == 1:\n",
        "        # ************* modify this section for later use *************\n",
        "        if optimizer_name == 'Adadelta':\n",
        "            !cp seed1_unsorted_batch_50_Adadelta.txt /content/drive/MyDrive\n",
        "        \n",
        "        elif optimizer_name == 'Adagrad':\n",
        "            !cp seed1_unsorted_batch_50_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "        elif optimizer_name == 'SGD':\n",
        "            !cp seed1_unsorted_batch_50_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "        elif optimizer_name == 'Adam':\n",
        "            !cp seed1_unsorted_batch_50_Adam.txt /content/drive/MyDrive\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        else:\n",
        "            print(\"ERROR\")\n",
        "\n",
        "    elif seed_num == 100:\n",
        "        # ************* modify this section for later use *************\n",
        "        if optimizer_name == 'Adadelta':\n",
        "            !cp seed100_unsorted_batch_50_Adadelta.txt /content/drive/MyDrive\n",
        "        \n",
        "        elif optimizer_name == 'Adagrad':\n",
        "            !cp seed100_unsorted_batch_50_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "        elif optimizer_name == 'SGD':\n",
        "            !cp seed100_unsorted_batch_50_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "        elif optimizer_name == 'Adam':\n",
        "            !cp seed100_unsorted_batch_50_Adam.txt /content/drive/MyDrive\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        else:\n",
        "            print(\"ERROR\")\n",
        "    \n",
        "    elif seed_num == 1234:\n",
        "        # ************* modify this section for later use *************\n",
        "        if optimizer_name == 'Adadelta':\n",
        "            !cp seed1234_unsorted_batch_50_Adadelta.txt /content/drive/MyDrive\n",
        "        \n",
        "        elif optimizer_name == 'Adagrad':\n",
        "            !cp seed1234_unsorted_batch_50_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "        elif optimizer_name == 'SGD':\n",
        "            !cp seed1234_unsorted_batch_50_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "        elif optimizer_name == 'Adam':\n",
        "            !cp seed1234_unsorted_batch_50_Adam.txt /content/drive/MyDrive\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        else:\n",
        "            print(\"ERROR\")\n",
        "    \n",
        "    else:\n",
        "        print(\"error\")\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXOpwTXEQFKY"
      },
      "source": [
        "no_epochs = 100\n",
        "def selectivity_trainer(optimizer, model):\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    train_loss = list()\n",
        "    test_loss  = list()\n",
        "    test_acc   = list()\n",
        "\n",
        "    best_test_loss = 1\n",
        "\n",
        "    selectivity_avg_list = list()\n",
        "    selectivity_std_list = list()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    final_spareness_12 = list()\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    for epoch in range(no_epochs):\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_each_neuron_12 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_12 = np.array(hidden_layer_each_neuron_12)\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "\n",
        "        total_train_loss = 0\n",
        "        total_test_loss = 0\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_activation_list_12 = list()\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        # training\n",
        "        # set up training mode \n",
        "        model.train()\n",
        "\n",
        "        for itr, (images, labels) in enumerate(train_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_train_loss = total_train_loss / (itr + 1)\n",
        "        train_loss.append(total_train_loss)\n",
        "\n",
        "        # testing \n",
        "        # change to evaluation mode \n",
        "        model.eval()\n",
        "        total = 0\n",
        "        for itr, (images, labels) in enumerate(test_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # we now need softmax because we are testing.\n",
        "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "            for i, p in enumerate(pred):\n",
        "                if labels[i] == torch.max(p.data, 0)[1]:\n",
        "                    total = total + 1\n",
        "\n",
        "            # ***************** sparsity calculation ***************** #\n",
        "            hidden_layer_activation_list_12.append(model.layer_activations['s12'])\n",
        "\n",
        "            # ************* modify this section for later use *************\n",
        "            for activation, label in zip(model.layer_activations['s12'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_12[i][label].append(activation[i])\n",
        "    \n",
        "        selectivity_avg, selecvitity_std = avg_std_calculator(hidden_layer_each_neuron_12)\n",
        "        # ************* modify this section for later use *************\n",
        "        \n",
        "        selectivity_avg_list.append(selectivity_avg)\n",
        "        selectivity_std_list.append(selecvitity_std)\n",
        "\n",
        "        # this conains activations for all epochs \n",
        "        final_spareness_12.append(hidden_layer_activation_list_12)\n",
        "        # ***************** sparsity calculation ***************** #\n",
        "\n",
        "        # caculate accuracy \n",
        "        accuracy = total / len(mnist_testset)\n",
        "\n",
        "        # append accuracy here\n",
        "        test_acc.append(accuracy)\n",
        "\n",
        "        # append test loss here \n",
        "        total_test_loss = total_test_loss / (itr + 1)\n",
        "        test_loss.append(total_test_loss)\n",
        "\n",
        "        print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Test Loss: {:.8f}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, total_train_loss, total_test_loss, accuracy))\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "    sparsity_list12 = sparsity_calculator(final_spareness_12)\n",
        "\n",
        "    print(sparsity_list12)\n",
        "\n",
        "    average_sparsity = list()\n",
        "    for i in range(no_epochs):\n",
        "        average_sparsity.append( (sparsity_list12[i].item()) / 1 )\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "\n",
        "    print(\"average_sparsity:\", average_sparsity)\n",
        "\n",
        "    return test_acc, average_sparsity, selectivity_avg_list, selectivity_std_list"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILIJTJb2UdfI"
      },
      "source": [
        "# seed 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UH0qDnFUfaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07aac1d5-11b5-4348-80b5-b2eb815e0fae"
      },
      "source": [
        "# seed 1\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "model_factory('Adadelta', 1)\n",
        "model_factory('Adagrad', 1)\n",
        "model_factory('SGD', 1)\n",
        "model_factory('Adam', 1)\n",
        "\n",
        "torch.manual_seed(100)\n",
        "np.random.seed(100)\n",
        "\n",
        "# seed 100 \n",
        "model_factory('Adadelta', 100)\n",
        "model_factory('Adagrad', 100)\n",
        "model_factory('SGD', 100)\n",
        "model_factory('Adam', 100)\n",
        "\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "# seed 1234 \n",
        "model_factory('Adadelta', 1234)\n",
        "model_factory('Adagrad', 1234)\n",
        "model_factory('SGD', 1234)\n",
        "model_factory('Adam', 1234)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-06\n",
            "    lr: 1.0\n",
            "    rho: 0.9\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/100, Train Loss: 0.43367423, Test Loss: 0.24769610, Test Accuracy: 0.92820000\n",
            "\n",
            "Epoch: 2/100, Train Loss: 0.21293067, Test Loss: 0.16937759, Test Accuracy: 0.95030000\n",
            "\n",
            "Epoch: 3/100, Train Loss: 0.15384629, Test Loss: 0.13693974, Test Accuracy: 0.95810000\n",
            "\n",
            "Epoch: 4/100, Train Loss: 0.11938237, Test Loss: 0.11120874, Test Accuracy: 0.96690000\n",
            "\n",
            "Epoch: 5/100, Train Loss: 0.09754113, Test Loss: 0.10118736, Test Accuracy: 0.96920000\n",
            "\n",
            "Epoch: 6/100, Train Loss: 0.08210463, Test Loss: 0.08805496, Test Accuracy: 0.97290000\n",
            "\n",
            "Epoch: 7/100, Train Loss: 0.07022873, Test Loss: 0.08261694, Test Accuracy: 0.97520000\n",
            "\n",
            "Epoch: 8/100, Train Loss: 0.06054088, Test Loss: 0.07835781, Test Accuracy: 0.97550000\n",
            "\n",
            "Epoch: 9/100, Train Loss: 0.05297692, Test Loss: 0.07122227, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 10/100, Train Loss: 0.04653793, Test Loss: 0.06854505, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 11/100, Train Loss: 0.04127996, Test Loss: 0.06628083, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 12/100, Train Loss: 0.03663752, Test Loss: 0.06434867, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 13/100, Train Loss: 0.03223948, Test Loss: 0.06359609, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 14/100, Train Loss: 0.02886765, Test Loss: 0.06305145, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 15/100, Train Loss: 0.02567128, Test Loss: 0.06103475, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 16/100, Train Loss: 0.02309877, Test Loss: 0.06108506, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 17/100, Train Loss: 0.02045728, Test Loss: 0.05936160, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 18/100, Train Loss: 0.01811930, Test Loss: 0.06005852, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 19/100, Train Loss: 0.01627151, Test Loss: 0.05778773, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 20/100, Train Loss: 0.01456378, Test Loss: 0.05661087, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 21/100, Train Loss: 0.01313530, Test Loss: 0.05669619, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 22/100, Train Loss: 0.01173591, Test Loss: 0.05950331, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 23/100, Train Loss: 0.01040517, Test Loss: 0.05658945, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 24/100, Train Loss: 0.00965810, Test Loss: 0.05953826, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 25/100, Train Loss: 0.00864837, Test Loss: 0.05904300, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 26/100, Train Loss: 0.00774501, Test Loss: 0.05858385, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 27/100, Train Loss: 0.00723446, Test Loss: 0.05746656, Test Accuracy: 0.98240000\n",
            "\n",
            "Epoch: 28/100, Train Loss: 0.00647983, Test Loss: 0.05823194, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 29/100, Train Loss: 0.00593030, Test Loss: 0.05768706, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 30/100, Train Loss: 0.00546829, Test Loss: 0.05866862, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 31/100, Train Loss: 0.00486898, Test Loss: 0.05890863, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 32/100, Train Loss: 0.00450938, Test Loss: 0.05873954, Test Accuracy: 0.98250000\n",
            "\n",
            "Epoch: 33/100, Train Loss: 0.00419674, Test Loss: 0.05854549, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 34/100, Train Loss: 0.00383803, Test Loss: 0.05864786, Test Accuracy: 0.98290000\n",
            "\n",
            "Epoch: 35/100, Train Loss: 0.00351895, Test Loss: 0.05887988, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 36/100, Train Loss: 0.00330900, Test Loss: 0.06043576, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 37/100, Train Loss: 0.00301114, Test Loss: 0.05986314, Test Accuracy: 0.98240000\n",
            "\n",
            "Epoch: 38/100, Train Loss: 0.00282867, Test Loss: 0.05948904, Test Accuracy: 0.98320000\n",
            "\n",
            "Epoch: 39/100, Train Loss: 0.00268407, Test Loss: 0.05973213, Test Accuracy: 0.98270000\n",
            "\n",
            "Epoch: 40/100, Train Loss: 0.00246238, Test Loss: 0.05994175, Test Accuracy: 0.98280000\n",
            "\n",
            "Epoch: 41/100, Train Loss: 0.00231686, Test Loss: 0.06168083, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 42/100, Train Loss: 0.00219333, Test Loss: 0.06152779, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 43/100, Train Loss: 0.00207424, Test Loss: 0.06178357, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 44/100, Train Loss: 0.00195870, Test Loss: 0.06109471, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 45/100, Train Loss: 0.00183827, Test Loss: 0.06259796, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 46/100, Train Loss: 0.00174682, Test Loss: 0.06146000, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 47/100, Train Loss: 0.00166840, Test Loss: 0.06173165, Test Accuracy: 0.98250000\n",
            "\n",
            "Epoch: 48/100, Train Loss: 0.00157727, Test Loss: 0.06193300, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 49/100, Train Loss: 0.00153157, Test Loss: 0.06197492, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 50/100, Train Loss: 0.00145532, Test Loss: 0.06181478, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 51/100, Train Loss: 0.00140393, Test Loss: 0.06233445, Test Accuracy: 0.98250000\n",
            "\n",
            "Epoch: 52/100, Train Loss: 0.00132505, Test Loss: 0.06182081, Test Accuracy: 0.98270000\n",
            "\n",
            "Epoch: 53/100, Train Loss: 0.00127710, Test Loss: 0.06248116, Test Accuracy: 0.98290000\n",
            "\n",
            "Epoch: 54/100, Train Loss: 0.00122769, Test Loss: 0.06292106, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 55/100, Train Loss: 0.00119194, Test Loss: 0.06258128, Test Accuracy: 0.98260000\n",
            "\n",
            "Epoch: 56/100, Train Loss: 0.00115517, Test Loss: 0.06313077, Test Accuracy: 0.98290000\n",
            "\n",
            "Epoch: 57/100, Train Loss: 0.00110638, Test Loss: 0.06339395, Test Accuracy: 0.98240000\n",
            "\n",
            "Epoch: 58/100, Train Loss: 0.00106795, Test Loss: 0.06390401, Test Accuracy: 0.98250000\n",
            "\n",
            "Epoch: 59/100, Train Loss: 0.00103450, Test Loss: 0.06406347, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 60/100, Train Loss: 0.00099816, Test Loss: 0.06346252, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 61/100, Train Loss: 0.00096739, Test Loss: 0.06335277, Test Accuracy: 0.98240000\n",
            "\n",
            "Epoch: 62/100, Train Loss: 0.00094453, Test Loss: 0.06402869, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 63/100, Train Loss: 0.00091343, Test Loss: 0.06370444, Test Accuracy: 0.98270000\n",
            "\n",
            "Epoch: 64/100, Train Loss: 0.00088817, Test Loss: 0.06402484, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 65/100, Train Loss: 0.00085991, Test Loss: 0.06441832, Test Accuracy: 0.98270000\n",
            "\n",
            "Epoch: 66/100, Train Loss: 0.00084374, Test Loss: 0.06463266, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 67/100, Train Loss: 0.00081594, Test Loss: 0.06454248, Test Accuracy: 0.98250000\n",
            "\n",
            "Epoch: 68/100, Train Loss: 0.00079675, Test Loss: 0.06421346, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 69/100, Train Loss: 0.00077656, Test Loss: 0.06489795, Test Accuracy: 0.98260000\n",
            "\n",
            "Epoch: 70/100, Train Loss: 0.00075345, Test Loss: 0.06499462, Test Accuracy: 0.98260000\n",
            "\n",
            "Epoch: 71/100, Train Loss: 0.00073660, Test Loss: 0.06531729, Test Accuracy: 0.98280000\n",
            "\n",
            "Epoch: 72/100, Train Loss: 0.00072261, Test Loss: 0.06490285, Test Accuracy: 0.98270000\n",
            "\n",
            "Epoch: 73/100, Train Loss: 0.00070166, Test Loss: 0.06503470, Test Accuracy: 0.98250000\n",
            "\n",
            "Epoch: 74/100, Train Loss: 0.00068971, Test Loss: 0.06545583, Test Accuracy: 0.98260000\n",
            "\n",
            "Epoch: 75/100, Train Loss: 0.00067272, Test Loss: 0.06521722, Test Accuracy: 0.98290000\n",
            "\n",
            "Epoch: 76/100, Train Loss: 0.00065678, Test Loss: 0.06526128, Test Accuracy: 0.98290000\n",
            "\n",
            "Epoch: 77/100, Train Loss: 0.00064613, Test Loss: 0.06541667, Test Accuracy: 0.98290000\n",
            "\n",
            "Epoch: 78/100, Train Loss: 0.00062893, Test Loss: 0.06561868, Test Accuracy: 0.98290000\n",
            "\n",
            "Epoch: 79/100, Train Loss: 0.00061733, Test Loss: 0.06568788, Test Accuracy: 0.98270000\n",
            "\n",
            "Epoch: 80/100, Train Loss: 0.00060599, Test Loss: 0.06568479, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 81/100, Train Loss: 0.00059280, Test Loss: 0.06561964, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 82/100, Train Loss: 0.00058264, Test Loss: 0.06575837, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 83/100, Train Loss: 0.00056997, Test Loss: 0.06596406, Test Accuracy: 0.98280000\n",
            "\n",
            "Epoch: 84/100, Train Loss: 0.00056115, Test Loss: 0.06605138, Test Accuracy: 0.98260000\n",
            "\n",
            "Epoch: 85/100, Train Loss: 0.00055057, Test Loss: 0.06593391, Test Accuracy: 0.98290000\n",
            "\n",
            "Epoch: 86/100, Train Loss: 0.00053980, Test Loss: 0.06638385, Test Accuracy: 0.98280000\n",
            "\n",
            "Epoch: 87/100, Train Loss: 0.00052880, Test Loss: 0.06633977, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 88/100, Train Loss: 0.00052125, Test Loss: 0.06627764, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 89/100, Train Loss: 0.00051200, Test Loss: 0.06647945, Test Accuracy: 0.98300000\n",
            "\n",
            "Epoch: 90/100, Train Loss: 0.00050222, Test Loss: 0.06655237, Test Accuracy: 0.98260000\n",
            "\n",
            "Epoch: 91/100, Train Loss: 0.00049308, Test Loss: 0.06650411, Test Accuracy: 0.98290000\n",
            "\n",
            "Epoch: 92/100, Train Loss: 0.00048525, Test Loss: 0.06656415, Test Accuracy: 0.98290000\n",
            "\n",
            "Epoch: 93/100, Train Loss: 0.00047886, Test Loss: 0.06692592, Test Accuracy: 0.98280000\n",
            "\n",
            "Epoch: 94/100, Train Loss: 0.00046921, Test Loss: 0.06697536, Test Accuracy: 0.98260000\n",
            "\n",
            "Epoch: 95/100, Train Loss: 0.00046302, Test Loss: 0.06712207, Test Accuracy: 0.98320000\n",
            "\n",
            "Epoch: 96/100, Train Loss: 0.00045523, Test Loss: 0.06701042, Test Accuracy: 0.98320000\n",
            "\n",
            "Epoch: 97/100, Train Loss: 0.00044749, Test Loss: 0.06727766, Test Accuracy: 0.98300000\n",
            "\n",
            "Epoch: 98/100, Train Loss: 0.00044140, Test Loss: 0.06733001, Test Accuracy: 0.98260000\n",
            "\n",
            "Epoch: 99/100, Train Loss: 0.00043600, Test Loss: 0.06727837, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 100/100, Train Loss: 0.00042755, Test Loss: 0.06750008, Test Accuracy: 0.98270000\n",
            "[tensor(0.2043, grad_fn=<MeanBackward0>), tensor(0.2537, grad_fn=<MeanBackward0>), tensor(0.2854, grad_fn=<MeanBackward0>), tensor(0.3001, grad_fn=<MeanBackward0>), tensor(0.3092, grad_fn=<MeanBackward0>), tensor(0.3128, grad_fn=<MeanBackward0>), tensor(0.3200, grad_fn=<MeanBackward0>), tensor(0.3202, grad_fn=<MeanBackward0>), tensor(0.3232, grad_fn=<MeanBackward0>), tensor(0.3279, grad_fn=<MeanBackward0>), tensor(0.3299, grad_fn=<MeanBackward0>), tensor(0.3288, grad_fn=<MeanBackward0>), tensor(0.3317, grad_fn=<MeanBackward0>), tensor(0.3307, grad_fn=<MeanBackward0>), tensor(0.3327, grad_fn=<MeanBackward0>), tensor(0.3319, grad_fn=<MeanBackward0>), tensor(0.3326, grad_fn=<MeanBackward0>), tensor(0.3326, grad_fn=<MeanBackward0>), tensor(0.3315, grad_fn=<MeanBackward0>), tensor(0.3333, grad_fn=<MeanBackward0>), tensor(0.3335, grad_fn=<MeanBackward0>), tensor(0.3321, grad_fn=<MeanBackward0>), tensor(0.3325, grad_fn=<MeanBackward0>), tensor(0.3326, grad_fn=<MeanBackward0>), tensor(0.3314, grad_fn=<MeanBackward0>), tensor(0.3335, grad_fn=<MeanBackward0>), tensor(0.3327, grad_fn=<MeanBackward0>), tensor(0.3324, grad_fn=<MeanBackward0>), tensor(0.3326, grad_fn=<MeanBackward0>), tensor(0.3324, grad_fn=<MeanBackward0>), tensor(0.3333, grad_fn=<MeanBackward0>), tensor(0.3324, grad_fn=<MeanBackward0>), tensor(0.3334, grad_fn=<MeanBackward0>), tensor(0.3331, grad_fn=<MeanBackward0>), tensor(0.3331, grad_fn=<MeanBackward0>), tensor(0.3339, grad_fn=<MeanBackward0>), tensor(0.3334, grad_fn=<MeanBackward0>), tensor(0.3333, grad_fn=<MeanBackward0>), tensor(0.3338, grad_fn=<MeanBackward0>), tensor(0.3333, grad_fn=<MeanBackward0>), tensor(0.3337, grad_fn=<MeanBackward0>), tensor(0.3332, grad_fn=<MeanBackward0>), tensor(0.3331, grad_fn=<MeanBackward0>), tensor(0.3331, grad_fn=<MeanBackward0>), tensor(0.3332, grad_fn=<MeanBackward0>), tensor(0.3332, grad_fn=<MeanBackward0>), tensor(0.3335, grad_fn=<MeanBackward0>), tensor(0.3335, grad_fn=<MeanBackward0>), tensor(0.3337, grad_fn=<MeanBackward0>), tensor(0.3343, grad_fn=<MeanBackward0>), tensor(0.3340, grad_fn=<MeanBackward0>), tensor(0.3342, grad_fn=<MeanBackward0>), tensor(0.3339, grad_fn=<MeanBackward0>), tensor(0.3341, grad_fn=<MeanBackward0>), tensor(0.3340, grad_fn=<MeanBackward0>), tensor(0.3340, grad_fn=<MeanBackward0>), tensor(0.3342, grad_fn=<MeanBackward0>), tensor(0.3343, grad_fn=<MeanBackward0>), tensor(0.3342, grad_fn=<MeanBackward0>), tensor(0.3344, grad_fn=<MeanBackward0>), tensor(0.3345, grad_fn=<MeanBackward0>), tensor(0.3346, grad_fn=<MeanBackward0>), tensor(0.3345, grad_fn=<MeanBackward0>), tensor(0.3348, grad_fn=<MeanBackward0>), tensor(0.3347, grad_fn=<MeanBackward0>), tensor(0.3346, grad_fn=<MeanBackward0>), tensor(0.3349, grad_fn=<MeanBackward0>), tensor(0.3349, grad_fn=<MeanBackward0>), tensor(0.3349, grad_fn=<MeanBackward0>), tensor(0.3351, grad_fn=<MeanBackward0>), tensor(0.3349, grad_fn=<MeanBackward0>), tensor(0.3351, grad_fn=<MeanBackward0>), tensor(0.3351, grad_fn=<MeanBackward0>), tensor(0.3352, grad_fn=<MeanBackward0>), tensor(0.3352, grad_fn=<MeanBackward0>), tensor(0.3354, grad_fn=<MeanBackward0>), tensor(0.3354, grad_fn=<MeanBackward0>), tensor(0.3353, grad_fn=<MeanBackward0>), tensor(0.3354, grad_fn=<MeanBackward0>), tensor(0.3356, grad_fn=<MeanBackward0>), tensor(0.3355, grad_fn=<MeanBackward0>), tensor(0.3355, grad_fn=<MeanBackward0>), tensor(0.3356, grad_fn=<MeanBackward0>), tensor(0.3355, grad_fn=<MeanBackward0>), tensor(0.3355, grad_fn=<MeanBackward0>), tensor(0.3356, grad_fn=<MeanBackward0>), tensor(0.3357, grad_fn=<MeanBackward0>), tensor(0.3357, grad_fn=<MeanBackward0>), tensor(0.3357, grad_fn=<MeanBackward0>), tensor(0.3358, grad_fn=<MeanBackward0>), tensor(0.3358, grad_fn=<MeanBackward0>), tensor(0.3359, grad_fn=<MeanBackward0>), tensor(0.3359, grad_fn=<MeanBackward0>), tensor(0.3360, grad_fn=<MeanBackward0>), tensor(0.3359, grad_fn=<MeanBackward0>), tensor(0.3360, grad_fn=<MeanBackward0>), tensor(0.3361, grad_fn=<MeanBackward0>), tensor(0.3361, grad_fn=<MeanBackward0>), tensor(0.3362, grad_fn=<MeanBackward0>), tensor(0.3362, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.20429444313049316, 0.25370654463768005, 0.2854355573654175, 0.3000636100769043, 0.30918341875076294, 0.3127942383289337, 0.32004567980766296, 0.32018035650253296, 0.32318153977394104, 0.3279113173484802, 0.3298693895339966, 0.32881712913513184, 0.3317136764526367, 0.33065977692604065, 0.3327459394931793, 0.3319459557533264, 0.3325510621070862, 0.33255040645599365, 0.33153384923934937, 0.3332768678665161, 0.3335469365119934, 0.33209431171417236, 0.3325299918651581, 0.33261382579803467, 0.3313661515712738, 0.3335404694080353, 0.33265912532806396, 0.33240726590156555, 0.33258354663848877, 0.3323870003223419, 0.3332729637622833, 0.3324348032474518, 0.33343803882598877, 0.3331477642059326, 0.333108514547348, 0.33391261100769043, 0.3334139287471771, 0.3332606256008148, 0.3337903618812561, 0.33333858847618103, 0.333688348531723, 0.33321812748908997, 0.3330555260181427, 0.33311885595321655, 0.33318278193473816, 0.3331507742404938, 0.33345845341682434, 0.3335398733615875, 0.33374491333961487, 0.3342529833316803, 0.33401888608932495, 0.33419373631477356, 0.33385494351387024, 0.33412012457847595, 0.33396077156066895, 0.33402013778686523, 0.3341735303401947, 0.3343339264392853, 0.33420926332473755, 0.33444514870643616, 0.334530234336853, 0.3346295952796936, 0.3345436751842499, 0.33479544520378113, 0.33471235632896423, 0.3346395492553711, 0.33488014340400696, 0.3348905146121979, 0.334911972284317, 0.3351040184497833, 0.334911972284317, 0.33508408069610596, 0.3350751996040344, 0.3352445065975189, 0.3352189362049103, 0.3354220986366272, 0.335392028093338, 0.33530953526496887, 0.33542948961257935, 0.3355577290058136, 0.3355342149734497, 0.335509717464447, 0.3355867564678192, 0.3355156183242798, 0.33550596237182617, 0.33560264110565186, 0.3356945812702179, 0.3356708884239197, 0.33571165800094604, 0.3357611298561096, 0.3357589542865753, 0.33589932322502136, 0.33593329787254333, 0.33595556020736694, 0.3359212279319763, 0.33601537346839905, 0.3360852599143982, 0.3360859751701355, 0.33617427945137024, 0.3361918032169342]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adagrad (\n",
            "Parameter Group 0\n",
            "    eps: 1e-10\n",
            "    initial_accumulator_value: 0\n",
            "    lr: 0.1\n",
            "    lr_decay: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/100, Train Loss: 0.23799736, Test Loss: 0.12712018, Test Accuracy: 0.96150000\n",
            "\n",
            "Epoch: 2/100, Train Loss: 0.09902049, Test Loss: 0.09876513, Test Accuracy: 0.97130000\n",
            "\n",
            "Epoch: 3/100, Train Loss: 0.06931650, Test Loss: 0.08544120, Test Accuracy: 0.97310000\n",
            "\n",
            "Epoch: 4/100, Train Loss: 0.05298245, Test Loss: 0.07862551, Test Accuracy: 0.97540000\n",
            "\n",
            "Epoch: 5/100, Train Loss: 0.04204195, Test Loss: 0.07328748, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 6/100, Train Loss: 0.03408809, Test Loss: 0.07263948, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 7/100, Train Loss: 0.02813515, Test Loss: 0.06878624, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 8/100, Train Loss: 0.02339400, Test Loss: 0.06761227, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 9/100, Train Loss: 0.02003184, Test Loss: 0.06725907, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 10/100, Train Loss: 0.01708671, Test Loss: 0.06588416, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 11/100, Train Loss: 0.01483999, Test Loss: 0.06692972, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 12/100, Train Loss: 0.01289438, Test Loss: 0.06538644, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 13/100, Train Loss: 0.01131234, Test Loss: 0.06446315, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 14/100, Train Loss: 0.00997306, Test Loss: 0.06476224, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 15/100, Train Loss: 0.00889497, Test Loss: 0.06535631, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 16/100, Train Loss: 0.00794691, Test Loss: 0.06461605, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 17/100, Train Loss: 0.00714790, Test Loss: 0.06675198, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 18/100, Train Loss: 0.00649331, Test Loss: 0.06395268, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 19/100, Train Loss: 0.00584125, Test Loss: 0.06602902, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 20/100, Train Loss: 0.00537479, Test Loss: 0.06493909, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 21/100, Train Loss: 0.00495928, Test Loss: 0.06505999, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 22/100, Train Loss: 0.00454422, Test Loss: 0.06478813, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 23/100, Train Loss: 0.00422180, Test Loss: 0.06471856, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 24/100, Train Loss: 0.00389512, Test Loss: 0.06572512, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 25/100, Train Loss: 0.00364045, Test Loss: 0.06541344, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 26/100, Train Loss: 0.00340641, Test Loss: 0.06561879, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 27/100, Train Loss: 0.00319139, Test Loss: 0.06585623, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 28/100, Train Loss: 0.00300042, Test Loss: 0.06590693, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 29/100, Train Loss: 0.00282585, Test Loss: 0.06537316, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 30/100, Train Loss: 0.00266103, Test Loss: 0.06601857, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 31/100, Train Loss: 0.00252480, Test Loss: 0.06635249, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 32/100, Train Loss: 0.00239070, Test Loss: 0.06631926, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 33/100, Train Loss: 0.00227228, Test Loss: 0.06617001, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 34/100, Train Loss: 0.00216922, Test Loss: 0.06609212, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 35/100, Train Loss: 0.00206463, Test Loss: 0.06623247, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 36/100, Train Loss: 0.00197274, Test Loss: 0.06638147, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 37/100, Train Loss: 0.00188850, Test Loss: 0.06667950, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 38/100, Train Loss: 0.00180379, Test Loss: 0.06671492, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 39/100, Train Loss: 0.00173394, Test Loss: 0.06663216, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 40/100, Train Loss: 0.00166238, Test Loss: 0.06732911, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 41/100, Train Loss: 0.00159484, Test Loss: 0.06751853, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 42/100, Train Loss: 0.00153386, Test Loss: 0.06743446, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 43/100, Train Loss: 0.00148011, Test Loss: 0.06747840, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 44/100, Train Loss: 0.00142901, Test Loss: 0.06726807, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 45/100, Train Loss: 0.00137715, Test Loss: 0.06785696, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 46/100, Train Loss: 0.00133166, Test Loss: 0.06760427, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 47/100, Train Loss: 0.00128742, Test Loss: 0.06750510, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 48/100, Train Loss: 0.00124448, Test Loss: 0.06758624, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 49/100, Train Loss: 0.00120424, Test Loss: 0.06767651, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 50/100, Train Loss: 0.00116732, Test Loss: 0.06817510, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 51/100, Train Loss: 0.00113261, Test Loss: 0.06793214, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 52/100, Train Loss: 0.00109821, Test Loss: 0.06809995, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 53/100, Train Loss: 0.00106483, Test Loss: 0.06792700, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 54/100, Train Loss: 0.00103636, Test Loss: 0.06837192, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 55/100, Train Loss: 0.00100709, Test Loss: 0.06866548, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 56/100, Train Loss: 0.00098020, Test Loss: 0.06878756, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 57/100, Train Loss: 0.00095415, Test Loss: 0.06865761, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 58/100, Train Loss: 0.00092898, Test Loss: 0.06863381, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 59/100, Train Loss: 0.00090387, Test Loss: 0.06903149, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 60/100, Train Loss: 0.00088244, Test Loss: 0.06899110, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 61/100, Train Loss: 0.00086153, Test Loss: 0.06905191, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 62/100, Train Loss: 0.00083938, Test Loss: 0.06894023, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 63/100, Train Loss: 0.00082008, Test Loss: 0.06903190, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 64/100, Train Loss: 0.00080105, Test Loss: 0.06895147, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 65/100, Train Loss: 0.00078169, Test Loss: 0.06927704, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 66/100, Train Loss: 0.00076490, Test Loss: 0.06969082, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 67/100, Train Loss: 0.00074757, Test Loss: 0.06910170, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 68/100, Train Loss: 0.00073194, Test Loss: 0.06962748, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 69/100, Train Loss: 0.00071606, Test Loss: 0.06993770, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 70/100, Train Loss: 0.00070048, Test Loss: 0.06945677, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 71/100, Train Loss: 0.00068565, Test Loss: 0.06981518, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 72/100, Train Loss: 0.00067220, Test Loss: 0.06980023, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 73/100, Train Loss: 0.00065891, Test Loss: 0.06997167, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 74/100, Train Loss: 0.00064612, Test Loss: 0.06985686, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 75/100, Train Loss: 0.00063290, Test Loss: 0.06989051, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 76/100, Train Loss: 0.00062107, Test Loss: 0.06981554, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 77/100, Train Loss: 0.00060897, Test Loss: 0.06992063, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 78/100, Train Loss: 0.00059721, Test Loss: 0.06987405, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 79/100, Train Loss: 0.00058696, Test Loss: 0.07023499, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 80/100, Train Loss: 0.00057568, Test Loss: 0.07030028, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 81/100, Train Loss: 0.00056573, Test Loss: 0.07011729, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 82/100, Train Loss: 0.00055597, Test Loss: 0.07060805, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 83/100, Train Loss: 0.00054605, Test Loss: 0.07049453, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 84/100, Train Loss: 0.00053624, Test Loss: 0.07045122, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 85/100, Train Loss: 0.00052742, Test Loss: 0.07053538, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 86/100, Train Loss: 0.00051914, Test Loss: 0.07049926, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 87/100, Train Loss: 0.00051015, Test Loss: 0.07075113, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 88/100, Train Loss: 0.00050251, Test Loss: 0.07068485, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 89/100, Train Loss: 0.00049364, Test Loss: 0.07080628, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 90/100, Train Loss: 0.00048620, Test Loss: 0.07090525, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 91/100, Train Loss: 0.00047898, Test Loss: 0.07100048, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 92/100, Train Loss: 0.00047096, Test Loss: 0.07078253, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 93/100, Train Loss: 0.00046367, Test Loss: 0.07120099, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 94/100, Train Loss: 0.00045665, Test Loss: 0.07120094, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 95/100, Train Loss: 0.00045012, Test Loss: 0.07143319, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 96/100, Train Loss: 0.00044335, Test Loss: 0.07144003, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 97/100, Train Loss: 0.00043642, Test Loss: 0.07133997, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 98/100, Train Loss: 0.00043049, Test Loss: 0.07157277, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 99/100, Train Loss: 0.00042409, Test Loss: 0.07153659, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 100/100, Train Loss: 0.00041826, Test Loss: 0.07175816, Test Accuracy: 0.98170000\n",
            "[tensor(0.6442, grad_fn=<MeanBackward0>), tensor(0.6399, grad_fn=<MeanBackward0>), tensor(0.6330, grad_fn=<MeanBackward0>), tensor(0.6332, grad_fn=<MeanBackward0>), tensor(0.6282, grad_fn=<MeanBackward0>), tensor(0.6279, grad_fn=<MeanBackward0>), tensor(0.6243, grad_fn=<MeanBackward0>), tensor(0.6184, grad_fn=<MeanBackward0>), tensor(0.6194, grad_fn=<MeanBackward0>), tensor(0.6170, grad_fn=<MeanBackward0>), tensor(0.6157, grad_fn=<MeanBackward0>), tensor(0.6148, grad_fn=<MeanBackward0>), tensor(0.6129, grad_fn=<MeanBackward0>), tensor(0.6121, grad_fn=<MeanBackward0>), tensor(0.6093, grad_fn=<MeanBackward0>), tensor(0.6093, grad_fn=<MeanBackward0>), tensor(0.6089, grad_fn=<MeanBackward0>), tensor(0.6083, grad_fn=<MeanBackward0>), tensor(0.6077, grad_fn=<MeanBackward0>), tensor(0.6069, grad_fn=<MeanBackward0>), tensor(0.6055, grad_fn=<MeanBackward0>), tensor(0.6043, grad_fn=<MeanBackward0>), tensor(0.6042, grad_fn=<MeanBackward0>), tensor(0.6034, grad_fn=<MeanBackward0>), tensor(0.6029, grad_fn=<MeanBackward0>), tensor(0.6020, grad_fn=<MeanBackward0>), tensor(0.6018, grad_fn=<MeanBackward0>), tensor(0.6011, grad_fn=<MeanBackward0>), tensor(0.6002, grad_fn=<MeanBackward0>), tensor(0.6001, grad_fn=<MeanBackward0>), tensor(0.5995, grad_fn=<MeanBackward0>), tensor(0.5992, grad_fn=<MeanBackward0>), tensor(0.5988, grad_fn=<MeanBackward0>), tensor(0.5984, grad_fn=<MeanBackward0>), tensor(0.5978, grad_fn=<MeanBackward0>), tensor(0.5980, grad_fn=<MeanBackward0>), tensor(0.5971, grad_fn=<MeanBackward0>), tensor(0.5971, grad_fn=<MeanBackward0>), tensor(0.5966, grad_fn=<MeanBackward0>), tensor(0.5963, grad_fn=<MeanBackward0>), tensor(0.5957, grad_fn=<MeanBackward0>), tensor(0.5954, grad_fn=<MeanBackward0>), tensor(0.5950, grad_fn=<MeanBackward0>), tensor(0.5953, grad_fn=<MeanBackward0>), tensor(0.5947, grad_fn=<MeanBackward0>), tensor(0.5949, grad_fn=<MeanBackward0>), tensor(0.5940, grad_fn=<MeanBackward0>), tensor(0.5941, grad_fn=<MeanBackward0>), tensor(0.5933, grad_fn=<MeanBackward0>), tensor(0.5936, grad_fn=<MeanBackward0>), tensor(0.5930, grad_fn=<MeanBackward0>), tensor(0.5930, grad_fn=<MeanBackward0>), tensor(0.5928, grad_fn=<MeanBackward0>), tensor(0.5920, grad_fn=<MeanBackward0>), tensor(0.5920, grad_fn=<MeanBackward0>), tensor(0.5915, grad_fn=<MeanBackward0>), tensor(0.5916, grad_fn=<MeanBackward0>), tensor(0.5918, grad_fn=<MeanBackward0>), tensor(0.5915, grad_fn=<MeanBackward0>), tensor(0.5913, grad_fn=<MeanBackward0>), tensor(0.5910, grad_fn=<MeanBackward0>), tensor(0.5908, grad_fn=<MeanBackward0>), tensor(0.5908, grad_fn=<MeanBackward0>), tensor(0.5908, grad_fn=<MeanBackward0>), tensor(0.5905, grad_fn=<MeanBackward0>), tensor(0.5898, grad_fn=<MeanBackward0>), tensor(0.5903, grad_fn=<MeanBackward0>), tensor(0.5897, grad_fn=<MeanBackward0>), tensor(0.5898, grad_fn=<MeanBackward0>), tensor(0.5896, grad_fn=<MeanBackward0>), tensor(0.5896, grad_fn=<MeanBackward0>), tensor(0.5890, grad_fn=<MeanBackward0>), tensor(0.5890, grad_fn=<MeanBackward0>), tensor(0.5887, grad_fn=<MeanBackward0>), tensor(0.5887, grad_fn=<MeanBackward0>), tensor(0.5885, grad_fn=<MeanBackward0>), tensor(0.5883, grad_fn=<MeanBackward0>), tensor(0.5882, grad_fn=<MeanBackward0>), tensor(0.5882, grad_fn=<MeanBackward0>), tensor(0.5879, grad_fn=<MeanBackward0>), tensor(0.5879, grad_fn=<MeanBackward0>), tensor(0.5878, grad_fn=<MeanBackward0>), tensor(0.5876, grad_fn=<MeanBackward0>), tensor(0.5873, grad_fn=<MeanBackward0>), tensor(0.5872, grad_fn=<MeanBackward0>), tensor(0.5872, grad_fn=<MeanBackward0>), tensor(0.5870, grad_fn=<MeanBackward0>), tensor(0.5868, grad_fn=<MeanBackward0>), tensor(0.5869, grad_fn=<MeanBackward0>), tensor(0.5867, grad_fn=<MeanBackward0>), tensor(0.5866, grad_fn=<MeanBackward0>), tensor(0.5865, grad_fn=<MeanBackward0>), tensor(0.5862, grad_fn=<MeanBackward0>), tensor(0.5862, grad_fn=<MeanBackward0>), tensor(0.5861, grad_fn=<MeanBackward0>), tensor(0.5861, grad_fn=<MeanBackward0>), tensor(0.5860, grad_fn=<MeanBackward0>), tensor(0.5859, grad_fn=<MeanBackward0>), tensor(0.5859, grad_fn=<MeanBackward0>), tensor(0.5857, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.6441723704338074, 0.639880359172821, 0.6329922676086426, 0.6331930756568909, 0.628234326839447, 0.6279375553131104, 0.6243018507957458, 0.6184349656105042, 0.6193803548812866, 0.6170200705528259, 0.6156507730484009, 0.6147743463516235, 0.6129075288772583, 0.6120781302452087, 0.609305739402771, 0.6092929840087891, 0.6088777184486389, 0.6082967519760132, 0.6076813340187073, 0.6069186329841614, 0.6054530143737793, 0.6042590141296387, 0.6041961312294006, 0.6034425497055054, 0.6029272675514221, 0.6020409464836121, 0.6018491983413696, 0.6011179089546204, 0.600212812423706, 0.6000988483428955, 0.5995231866836548, 0.5991579294204712, 0.5988467931747437, 0.5984362959861755, 0.5977811813354492, 0.5979925394058228, 0.5971283316612244, 0.5970621705055237, 0.5965986251831055, 0.5963151454925537, 0.5956975817680359, 0.5953654050827026, 0.5949962139129639, 0.5953003168106079, 0.5947483777999878, 0.5949453711509705, 0.594017744064331, 0.5941211581230164, 0.5932636857032776, 0.5935529470443726, 0.5929712057113647, 0.5929809808731079, 0.5928134918212891, 0.5920361876487732, 0.5920097231864929, 0.5915483236312866, 0.5915532112121582, 0.5917989015579224, 0.5915297269821167, 0.5913171172142029, 0.5909630060195923, 0.590795636177063, 0.5907723903656006, 0.590752899646759, 0.5904816389083862, 0.5898116827011108, 0.5902984142303467, 0.5897046327590942, 0.5897561311721802, 0.589622974395752, 0.5895809531211853, 0.5889846682548523, 0.5890279412269592, 0.5886693596839905, 0.5887089371681213, 0.5885417461395264, 0.5883296728134155, 0.5882418155670166, 0.588187575340271, 0.5878889560699463, 0.5878812074661255, 0.5877624750137329, 0.5876429677009583, 0.5872830748558044, 0.5871548652648926, 0.5871613025665283, 0.5870468020439148, 0.5868450403213501, 0.5869408845901489, 0.5867487788200378, 0.5865885615348816, 0.5865069031715393, 0.5861828327178955, 0.5861728191375732, 0.5860627889633179, 0.5860698223114014, 0.585974931716919, 0.585907518863678, 0.5859187841415405, 0.5857242345809937]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/100, Train Loss: 0.77421946, Test Loss: 0.37305286, Test Accuracy: 0.89480000\n",
            "\n",
            "Epoch: 2/100, Train Loss: 0.34722827, Test Loss: 0.30677518, Test Accuracy: 0.91240000\n",
            "\n",
            "Epoch: 3/100, Train Loss: 0.30695693, Test Loss: 0.28378901, Test Accuracy: 0.91840000\n",
            "\n",
            "Epoch: 4/100, Train Loss: 0.28431358, Test Loss: 0.26458013, Test Accuracy: 0.92540000\n",
            "\n",
            "Epoch: 5/100, Train Loss: 0.26633437, Test Loss: 0.24878048, Test Accuracy: 0.92770000\n",
            "\n",
            "Epoch: 6/100, Train Loss: 0.24847586, Test Loss: 0.23618893, Test Accuracy: 0.93310000\n",
            "\n",
            "Epoch: 7/100, Train Loss: 0.23249883, Test Loss: 0.22488630, Test Accuracy: 0.93460000\n",
            "\n",
            "Epoch: 8/100, Train Loss: 0.21718840, Test Loss: 0.20738012, Test Accuracy: 0.93910000\n",
            "\n",
            "Epoch: 9/100, Train Loss: 0.20377999, Test Loss: 0.19652968, Test Accuracy: 0.94250000\n",
            "\n",
            "Epoch: 10/100, Train Loss: 0.19126395, Test Loss: 0.18260289, Test Accuracy: 0.94650000\n",
            "\n",
            "Epoch: 11/100, Train Loss: 0.18022341, Test Loss: 0.17426965, Test Accuracy: 0.94820000\n",
            "\n",
            "Epoch: 12/100, Train Loss: 0.16998188, Test Loss: 0.16677840, Test Accuracy: 0.95120000\n",
            "\n",
            "Epoch: 13/100, Train Loss: 0.16101732, Test Loss: 0.16048739, Test Accuracy: 0.95330000\n",
            "\n",
            "Epoch: 14/100, Train Loss: 0.15233810, Test Loss: 0.15199983, Test Accuracy: 0.95470000\n",
            "\n",
            "Epoch: 15/100, Train Loss: 0.14489929, Test Loss: 0.15203544, Test Accuracy: 0.95510000\n",
            "\n",
            "Epoch: 16/100, Train Loss: 0.13793692, Test Loss: 0.14177988, Test Accuracy: 0.95890000\n",
            "\n",
            "Epoch: 17/100, Train Loss: 0.13129659, Test Loss: 0.13462991, Test Accuracy: 0.96050000\n",
            "\n",
            "Epoch: 18/100, Train Loss: 0.12578471, Test Loss: 0.13289512, Test Accuracy: 0.96050000\n",
            "\n",
            "Epoch: 19/100, Train Loss: 0.12025985, Test Loss: 0.12516766, Test Accuracy: 0.96100000\n",
            "\n",
            "Epoch: 20/100, Train Loss: 0.11523354, Test Loss: 0.12431941, Test Accuracy: 0.96380000\n",
            "\n",
            "Epoch: 21/100, Train Loss: 0.11067360, Test Loss: 0.11928807, Test Accuracy: 0.96450000\n",
            "\n",
            "Epoch: 22/100, Train Loss: 0.10622514, Test Loss: 0.11659014, Test Accuracy: 0.96500000\n",
            "\n",
            "Epoch: 23/100, Train Loss: 0.10217846, Test Loss: 0.11144079, Test Accuracy: 0.96630000\n",
            "\n",
            "Epoch: 24/100, Train Loss: 0.09837889, Test Loss: 0.10974600, Test Accuracy: 0.96600000\n",
            "\n",
            "Epoch: 25/100, Train Loss: 0.09492232, Test Loss: 0.10831933, Test Accuracy: 0.96750000\n",
            "\n",
            "Epoch: 26/100, Train Loss: 0.09146381, Test Loss: 0.10473694, Test Accuracy: 0.96860000\n",
            "\n",
            "Epoch: 27/100, Train Loss: 0.08801551, Test Loss: 0.10226229, Test Accuracy: 0.96970000\n",
            "\n",
            "Epoch: 28/100, Train Loss: 0.08514621, Test Loss: 0.10114688, Test Accuracy: 0.96910000\n",
            "\n",
            "Epoch: 29/100, Train Loss: 0.08248764, Test Loss: 0.09809833, Test Accuracy: 0.97030000\n",
            "\n",
            "Epoch: 30/100, Train Loss: 0.07982263, Test Loss: 0.09727612, Test Accuracy: 0.96990000\n",
            "\n",
            "Epoch: 31/100, Train Loss: 0.07736750, Test Loss: 0.09341661, Test Accuracy: 0.97210000\n",
            "\n",
            "Epoch: 32/100, Train Loss: 0.07491439, Test Loss: 0.09233156, Test Accuracy: 0.97170000\n",
            "\n",
            "Epoch: 33/100, Train Loss: 0.07265705, Test Loss: 0.09287582, Test Accuracy: 0.97100000\n",
            "\n",
            "Epoch: 34/100, Train Loss: 0.07045150, Test Loss: 0.08778731, Test Accuracy: 0.97280000\n",
            "\n",
            "Epoch: 35/100, Train Loss: 0.06833531, Test Loss: 0.08830125, Test Accuracy: 0.97380000\n",
            "\n",
            "Epoch: 36/100, Train Loss: 0.06634223, Test Loss: 0.08645028, Test Accuracy: 0.97330000\n",
            "\n",
            "Epoch: 37/100, Train Loss: 0.06446091, Test Loss: 0.08578052, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 38/100, Train Loss: 0.06251007, Test Loss: 0.08340318, Test Accuracy: 0.97440000\n",
            "\n",
            "Epoch: 39/100, Train Loss: 0.06094238, Test Loss: 0.08326107, Test Accuracy: 0.97530000\n",
            "\n",
            "Epoch: 40/100, Train Loss: 0.05909992, Test Loss: 0.08154843, Test Accuracy: 0.97480000\n",
            "\n",
            "Epoch: 41/100, Train Loss: 0.05766680, Test Loss: 0.07974128, Test Accuracy: 0.97530000\n",
            "\n",
            "Epoch: 42/100, Train Loss: 0.05605382, Test Loss: 0.07934087, Test Accuracy: 0.97550000\n",
            "\n",
            "Epoch: 43/100, Train Loss: 0.05449839, Test Loss: 0.07850883, Test Accuracy: 0.97620000\n",
            "\n",
            "Epoch: 44/100, Train Loss: 0.05317523, Test Loss: 0.07994805, Test Accuracy: 0.97500000\n",
            "\n",
            "Epoch: 45/100, Train Loss: 0.05173022, Test Loss: 0.07751520, Test Accuracy: 0.97620000\n",
            "\n",
            "Epoch: 46/100, Train Loss: 0.05047777, Test Loss: 0.07652632, Test Accuracy: 0.97580000\n",
            "\n",
            "Epoch: 47/100, Train Loss: 0.04929532, Test Loss: 0.07652371, Test Accuracy: 0.97660000\n",
            "\n",
            "Epoch: 48/100, Train Loss: 0.04805817, Test Loss: 0.07472950, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 49/100, Train Loss: 0.04691070, Test Loss: 0.07364124, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 50/100, Train Loss: 0.04564459, Test Loss: 0.07391462, Test Accuracy: 0.97670000\n",
            "\n",
            "Epoch: 51/100, Train Loss: 0.04459542, Test Loss: 0.07301473, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 52/100, Train Loss: 0.04358817, Test Loss: 0.07315604, Test Accuracy: 0.97630000\n",
            "\n",
            "Epoch: 53/100, Train Loss: 0.04238176, Test Loss: 0.07314479, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 54/100, Train Loss: 0.04142441, Test Loss: 0.07169621, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 55/100, Train Loss: 0.04048739, Test Loss: 0.07098694, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 56/100, Train Loss: 0.03946696, Test Loss: 0.07067592, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 57/100, Train Loss: 0.03850354, Test Loss: 0.07016357, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 58/100, Train Loss: 0.03784355, Test Loss: 0.06909807, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 59/100, Train Loss: 0.03680196, Test Loss: 0.06898443, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 60/100, Train Loss: 0.03616707, Test Loss: 0.06872519, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 61/100, Train Loss: 0.03527931, Test Loss: 0.06778634, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 62/100, Train Loss: 0.03446742, Test Loss: 0.06897709, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 63/100, Train Loss: 0.03381503, Test Loss: 0.06755413, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 64/100, Train Loss: 0.03297213, Test Loss: 0.06689328, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 65/100, Train Loss: 0.03229407, Test Loss: 0.06772120, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 66/100, Train Loss: 0.03152494, Test Loss: 0.06636624, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 67/100, Train Loss: 0.03089216, Test Loss: 0.06798346, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 68/100, Train Loss: 0.03033729, Test Loss: 0.06649131, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 69/100, Train Loss: 0.02967544, Test Loss: 0.06584108, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 70/100, Train Loss: 0.02883883, Test Loss: 0.06539845, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 71/100, Train Loss: 0.02828104, Test Loss: 0.06588376, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 72/100, Train Loss: 0.02774002, Test Loss: 0.06546181, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 73/100, Train Loss: 0.02715646, Test Loss: 0.06463620, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 74/100, Train Loss: 0.02666664, Test Loss: 0.06405841, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 75/100, Train Loss: 0.02602324, Test Loss: 0.06476031, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 76/100, Train Loss: 0.02559735, Test Loss: 0.06423933, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 77/100, Train Loss: 0.02503091, Test Loss: 0.06447425, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 78/100, Train Loss: 0.02444963, Test Loss: 0.06538242, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 79/100, Train Loss: 0.02399013, Test Loss: 0.06483737, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 80/100, Train Loss: 0.02352054, Test Loss: 0.06374137, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 81/100, Train Loss: 0.02309711, Test Loss: 0.06354616, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 82/100, Train Loss: 0.02266657, Test Loss: 0.06342026, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 83/100, Train Loss: 0.02213982, Test Loss: 0.06337099, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 84/100, Train Loss: 0.02168196, Test Loss: 0.06317838, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 85/100, Train Loss: 0.02127990, Test Loss: 0.06410585, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 86/100, Train Loss: 0.02090491, Test Loss: 0.06308416, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 87/100, Train Loss: 0.02058410, Test Loss: 0.06243504, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 88/100, Train Loss: 0.02014672, Test Loss: 0.06356228, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 89/100, Train Loss: 0.01966918, Test Loss: 0.06285291, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 90/100, Train Loss: 0.01936911, Test Loss: 0.06264203, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 91/100, Train Loss: 0.01904780, Test Loss: 0.06232237, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 92/100, Train Loss: 0.01867653, Test Loss: 0.06254039, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 93/100, Train Loss: 0.01835915, Test Loss: 0.06258828, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 94/100, Train Loss: 0.01798779, Test Loss: 0.06330119, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 95/100, Train Loss: 0.01763533, Test Loss: 0.06332325, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 96/100, Train Loss: 0.01728000, Test Loss: 0.06238724, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 97/100, Train Loss: 0.01700672, Test Loss: 0.06291097, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 98/100, Train Loss: 0.01670673, Test Loss: 0.06166638, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 99/100, Train Loss: 0.01648639, Test Loss: 0.06273852, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 100/100, Train Loss: 0.01613770, Test Loss: 0.06247815, Test Accuracy: 0.97980000\n",
            "[tensor(0.0821, grad_fn=<MeanBackward0>), tensor(0.0949, grad_fn=<MeanBackward0>), tensor(0.1046, grad_fn=<MeanBackward0>), tensor(0.1152, grad_fn=<MeanBackward0>), tensor(0.1257, grad_fn=<MeanBackward0>), tensor(0.1367, grad_fn=<MeanBackward0>), tensor(0.1465, grad_fn=<MeanBackward0>), tensor(0.1557, grad_fn=<MeanBackward0>), tensor(0.1631, grad_fn=<MeanBackward0>), tensor(0.1713, grad_fn=<MeanBackward0>), tensor(0.1780, grad_fn=<MeanBackward0>), tensor(0.1839, grad_fn=<MeanBackward0>), tensor(0.1893, grad_fn=<MeanBackward0>), tensor(0.1938, grad_fn=<MeanBackward0>), tensor(0.1981, grad_fn=<MeanBackward0>), tensor(0.2020, grad_fn=<MeanBackward0>), tensor(0.2055, grad_fn=<MeanBackward0>), tensor(0.2087, grad_fn=<MeanBackward0>), tensor(0.2120, grad_fn=<MeanBackward0>), tensor(0.2147, grad_fn=<MeanBackward0>), tensor(0.2168, grad_fn=<MeanBackward0>), tensor(0.2190, grad_fn=<MeanBackward0>), tensor(0.2214, grad_fn=<MeanBackward0>), tensor(0.2233, grad_fn=<MeanBackward0>), tensor(0.2248, grad_fn=<MeanBackward0>), tensor(0.2264, grad_fn=<MeanBackward0>), tensor(0.2281, grad_fn=<MeanBackward0>), tensor(0.2292, grad_fn=<MeanBackward0>), tensor(0.2304, grad_fn=<MeanBackward0>), tensor(0.2314, grad_fn=<MeanBackward0>), tensor(0.2328, grad_fn=<MeanBackward0>), tensor(0.2342, grad_fn=<MeanBackward0>), tensor(0.2348, grad_fn=<MeanBackward0>), tensor(0.2356, grad_fn=<MeanBackward0>), tensor(0.2368, grad_fn=<MeanBackward0>), tensor(0.2379, grad_fn=<MeanBackward0>), tensor(0.2386, grad_fn=<MeanBackward0>), tensor(0.2388, grad_fn=<MeanBackward0>), tensor(0.2401, grad_fn=<MeanBackward0>), tensor(0.2403, grad_fn=<MeanBackward0>), tensor(0.2412, grad_fn=<MeanBackward0>), tensor(0.2417, grad_fn=<MeanBackward0>), tensor(0.2420, grad_fn=<MeanBackward0>), tensor(0.2432, grad_fn=<MeanBackward0>), tensor(0.2435, grad_fn=<MeanBackward0>), tensor(0.2445, grad_fn=<MeanBackward0>), tensor(0.2447, grad_fn=<MeanBackward0>), tensor(0.2451, grad_fn=<MeanBackward0>), tensor(0.2458, grad_fn=<MeanBackward0>), tensor(0.2460, grad_fn=<MeanBackward0>), tensor(0.2462, grad_fn=<MeanBackward0>), tensor(0.2466, grad_fn=<MeanBackward0>), tensor(0.2474, grad_fn=<MeanBackward0>), tensor(0.2482, grad_fn=<MeanBackward0>), tensor(0.2482, grad_fn=<MeanBackward0>), tensor(0.2485, grad_fn=<MeanBackward0>), tensor(0.2488, grad_fn=<MeanBackward0>), tensor(0.2489, grad_fn=<MeanBackward0>), tensor(0.2496, grad_fn=<MeanBackward0>), tensor(0.2501, grad_fn=<MeanBackward0>), tensor(0.2503, grad_fn=<MeanBackward0>), tensor(0.2508, grad_fn=<MeanBackward0>), tensor(0.2511, grad_fn=<MeanBackward0>), tensor(0.2511, grad_fn=<MeanBackward0>), tensor(0.2518, grad_fn=<MeanBackward0>), tensor(0.2517, grad_fn=<MeanBackward0>), tensor(0.2523, grad_fn=<MeanBackward0>), tensor(0.2526, grad_fn=<MeanBackward0>), tensor(0.2527, grad_fn=<MeanBackward0>), tensor(0.2531, grad_fn=<MeanBackward0>), tensor(0.2532, grad_fn=<MeanBackward0>), tensor(0.2538, grad_fn=<MeanBackward0>), tensor(0.2538, grad_fn=<MeanBackward0>), tensor(0.2543, grad_fn=<MeanBackward0>), tensor(0.2547, grad_fn=<MeanBackward0>), tensor(0.2549, grad_fn=<MeanBackward0>), tensor(0.2550, grad_fn=<MeanBackward0>), tensor(0.2553, grad_fn=<MeanBackward0>), tensor(0.2554, grad_fn=<MeanBackward0>), tensor(0.2556, grad_fn=<MeanBackward0>), tensor(0.2557, grad_fn=<MeanBackward0>), tensor(0.2562, grad_fn=<MeanBackward0>), tensor(0.2564, grad_fn=<MeanBackward0>), tensor(0.2566, grad_fn=<MeanBackward0>), tensor(0.2568, grad_fn=<MeanBackward0>), tensor(0.2571, grad_fn=<MeanBackward0>), tensor(0.2573, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2574, grad_fn=<MeanBackward0>), tensor(0.2578, grad_fn=<MeanBackward0>), tensor(0.2581, grad_fn=<MeanBackward0>), tensor(0.2582, grad_fn=<MeanBackward0>), tensor(0.2586, grad_fn=<MeanBackward0>), tensor(0.2585, grad_fn=<MeanBackward0>), tensor(0.2585, grad_fn=<MeanBackward0>), tensor(0.2587, grad_fn=<MeanBackward0>), tensor(0.2589, grad_fn=<MeanBackward0>), tensor(0.2591, grad_fn=<MeanBackward0>), tensor(0.2593, grad_fn=<MeanBackward0>), tensor(0.2595, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.08211790025234222, 0.09485159814357758, 0.1045578271150589, 0.11520053446292877, 0.12565474212169647, 0.13669657707214355, 0.14651291072368622, 0.15568478405475616, 0.16306579113006592, 0.17128412425518036, 0.1780158132314682, 0.18388833105564117, 0.1893126368522644, 0.19383323192596436, 0.1981160193681717, 0.20195959508419037, 0.2054770290851593, 0.20868095755577087, 0.2119721621274948, 0.214699849486351, 0.21675720810890198, 0.21898066997528076, 0.22139255702495575, 0.22334668040275574, 0.2248491644859314, 0.22640913724899292, 0.2280767858028412, 0.22922900319099426, 0.23039931058883667, 0.2314370572566986, 0.2327665239572525, 0.23416133224964142, 0.2347687929868698, 0.23557670414447784, 0.23682554066181183, 0.23787270486354828, 0.2385980188846588, 0.2388424575328827, 0.24012523889541626, 0.24034255743026733, 0.24120065569877625, 0.24167859554290771, 0.2419670671224594, 0.24316531419754028, 0.2435305416584015, 0.24446699023246765, 0.2446936070919037, 0.24512484669685364, 0.24581575393676758, 0.246009960770607, 0.24620510637760162, 0.246644526720047, 0.24741536378860474, 0.24817070364952087, 0.24815082550048828, 0.24851743876934052, 0.2488175332546234, 0.24888432025909424, 0.24961547553539276, 0.250120609998703, 0.2503296732902527, 0.250833123922348, 0.251140832901001, 0.25112834572792053, 0.25184065103530884, 0.25170838832855225, 0.2522587180137634, 0.2525913119316101, 0.25270143151283264, 0.25305697321891785, 0.2532384395599365, 0.2537592947483063, 0.253788024187088, 0.2543204724788666, 0.2547331154346466, 0.2548518180847168, 0.25501537322998047, 0.25527581572532654, 0.2554488182067871, 0.25563859939575195, 0.25573161244392395, 0.2561964690685272, 0.25635960698127747, 0.25655117630958557, 0.2567684054374695, 0.2571316063404083, 0.25726285576820374, 0.25746700167655945, 0.2574286460876465, 0.25779989361763, 0.25810176134109497, 0.25822243094444275, 0.2585579454898834, 0.25846925377845764, 0.2584819197654724, 0.258731871843338, 0.25892359018325806, 0.2590988278388977, 0.2593485414981842, 0.25946244597435]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/100, Train Loss: 0.43587386, Test Loss: 0.22569584, Test Accuracy: 0.93460000\n",
            "\n",
            "Epoch: 2/100, Train Loss: 0.19870455, Test Loss: 0.16843512, Test Accuracy: 0.95150000\n",
            "\n",
            "Epoch: 3/100, Train Loss: 0.14503833, Test Loss: 0.12926292, Test Accuracy: 0.96040000\n",
            "\n",
            "Epoch: 4/100, Train Loss: 0.11114018, Test Loss: 0.10628664, Test Accuracy: 0.96700000\n",
            "\n",
            "Epoch: 5/100, Train Loss: 0.08751093, Test Loss: 0.09976286, Test Accuracy: 0.96890000\n",
            "\n",
            "Epoch: 6/100, Train Loss: 0.06998728, Test Loss: 0.08619848, Test Accuracy: 0.97300000\n",
            "\n",
            "Epoch: 7/100, Train Loss: 0.05701579, Test Loss: 0.07408106, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 8/100, Train Loss: 0.04635719, Test Loss: 0.06964089, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 9/100, Train Loss: 0.03781805, Test Loss: 0.07314579, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 10/100, Train Loss: 0.03085419, Test Loss: 0.06557341, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 11/100, Train Loss: 0.02531071, Test Loss: 0.06249125, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 12/100, Train Loss: 0.02045832, Test Loss: 0.06068292, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 13/100, Train Loss: 0.01684215, Test Loss: 0.06176831, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 14/100, Train Loss: 0.01347844, Test Loss: 0.06284948, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 15/100, Train Loss: 0.01054477, Test Loss: 0.06250906, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 16/100, Train Loss: 0.00860561, Test Loss: 0.06163674, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 17/100, Train Loss: 0.00675934, Test Loss: 0.06527537, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 18/100, Train Loss: 0.00557551, Test Loss: 0.06247863, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 19/100, Train Loss: 0.00445220, Test Loss: 0.06673220, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 20/100, Train Loss: 0.00352000, Test Loss: 0.06569550, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 21/100, Train Loss: 0.00265650, Test Loss: 0.06699072, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 22/100, Train Loss: 0.00229214, Test Loss: 0.07026209, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 23/100, Train Loss: 0.00179173, Test Loss: 0.07105568, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 24/100, Train Loss: 0.00201435, Test Loss: 0.07008397, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 25/100, Train Loss: 0.00101414, Test Loss: 0.07160853, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 26/100, Train Loss: 0.00154006, Test Loss: 0.08035249, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 27/100, Train Loss: 0.00075947, Test Loss: 0.07328650, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 28/100, Train Loss: 0.00134312, Test Loss: 0.08316237, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 29/100, Train Loss: 0.00061225, Test Loss: 0.07401249, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 30/100, Train Loss: 0.00037533, Test Loss: 0.07530403, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 31/100, Train Loss: 0.00028212, Test Loss: 0.07665795, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 32/100, Train Loss: 0.00181784, Test Loss: 0.08062021, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 33/100, Train Loss: 0.00029297, Test Loss: 0.07828596, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 34/100, Train Loss: 0.00018468, Test Loss: 0.07929190, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 35/100, Train Loss: 0.00015571, Test Loss: 0.08175020, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 36/100, Train Loss: 0.00111310, Test Loss: 0.08453836, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 37/100, Train Loss: 0.00016918, Test Loss: 0.08273899, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 38/100, Train Loss: 0.00010838, Test Loss: 0.08255240, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 39/100, Train Loss: 0.00010147, Test Loss: 0.08408602, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 40/100, Train Loss: 0.00134182, Test Loss: 0.09243893, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 41/100, Train Loss: 0.00056687, Test Loss: 0.08276672, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 42/100, Train Loss: 0.00009321, Test Loss: 0.08452461, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 43/100, Train Loss: 0.00007026, Test Loss: 0.08544995, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 44/100, Train Loss: 0.00005753, Test Loss: 0.08708240, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 45/100, Train Loss: 0.00004912, Test Loss: 0.08958076, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 46/100, Train Loss: 0.00005781, Test Loss: 0.09197779, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 47/100, Train Loss: 0.00132461, Test Loss: 0.08779907, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 48/100, Train Loss: 0.00007291, Test Loss: 0.08865765, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 49/100, Train Loss: 0.00004433, Test Loss: 0.08911078, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 50/100, Train Loss: 0.00003487, Test Loss: 0.08982807, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 51/100, Train Loss: 0.00002864, Test Loss: 0.09117349, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 52/100, Train Loss: 0.00002471, Test Loss: 0.09176544, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 53/100, Train Loss: 0.00002580, Test Loss: 0.09326539, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 54/100, Train Loss: 0.00146352, Test Loss: 0.09587273, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 55/100, Train Loss: 0.00010601, Test Loss: 0.09332769, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 56/100, Train Loss: 0.00003496, Test Loss: 0.09424800, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 57/100, Train Loss: 0.00002547, Test Loss: 0.09456440, Test Accuracy: 0.98260000\n",
            "\n",
            "Epoch: 58/100, Train Loss: 0.00001976, Test Loss: 0.09526663, Test Accuracy: 0.98240000\n",
            "\n",
            "Epoch: 59/100, Train Loss: 0.00001580, Test Loss: 0.09561228, Test Accuracy: 0.98250000\n",
            "\n",
            "Epoch: 60/100, Train Loss: 0.00074152, Test Loss: 0.10876760, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 61/100, Train Loss: 0.00085400, Test Loss: 0.09997327, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 62/100, Train Loss: 0.00004453, Test Loss: 0.09970325, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 63/100, Train Loss: 0.00002240, Test Loss: 0.09984675, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 64/100, Train Loss: 0.00001716, Test Loss: 0.09961701, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 65/100, Train Loss: 0.00001375, Test Loss: 0.10025766, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 66/100, Train Loss: 0.00001139, Test Loss: 0.10033757, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 67/100, Train Loss: 0.00000933, Test Loss: 0.10230142, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 68/100, Train Loss: 0.00099438, Test Loss: 0.11668766, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 69/100, Train Loss: 0.00013359, Test Loss: 0.10412428, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 70/100, Train Loss: 0.00002290, Test Loss: 0.10357938, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 71/100, Train Loss: 0.00001476, Test Loss: 0.10383528, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 72/100, Train Loss: 0.00001121, Test Loss: 0.10370999, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 73/100, Train Loss: 0.00000880, Test Loss: 0.10382758, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 74/100, Train Loss: 0.00000705, Test Loss: 0.10485744, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 75/100, Train Loss: 0.00000579, Test Loss: 0.10598515, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 76/100, Train Loss: 0.00000472, Test Loss: 0.10803105, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 77/100, Train Loss: 0.00072357, Test Loss: 0.14652831, Test Accuracy: 0.97610000\n",
            "\n",
            "Epoch: 78/100, Train Loss: 0.00046872, Test Loss: 0.10868313, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 79/100, Train Loss: 0.00001843, Test Loss: 0.10778044, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 80/100, Train Loss: 0.00001086, Test Loss: 0.10706870, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 81/100, Train Loss: 0.00000813, Test Loss: 0.10714984, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 82/100, Train Loss: 0.00000621, Test Loss: 0.10750502, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 83/100, Train Loss: 0.00000482, Test Loss: 0.10750172, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 84/100, Train Loss: 0.00000383, Test Loss: 0.10773482, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 85/100, Train Loss: 0.00000312, Test Loss: 0.10823423, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 86/100, Train Loss: 0.00000253, Test Loss: 0.11008703, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 87/100, Train Loss: 0.00110078, Test Loss: 0.11623838, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 88/100, Train Loss: 0.00017461, Test Loss: 0.11570143, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 89/100, Train Loss: 0.00001284, Test Loss: 0.11408245, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 90/100, Train Loss: 0.00000832, Test Loss: 0.11343260, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 91/100, Train Loss: 0.00000611, Test Loss: 0.11301736, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 92/100, Train Loss: 0.00000459, Test Loss: 0.11323880, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 93/100, Train Loss: 0.00000349, Test Loss: 0.11291725, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 94/100, Train Loss: 0.00000268, Test Loss: 0.11256502, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 95/100, Train Loss: 0.00000211, Test Loss: 0.11323559, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 96/100, Train Loss: 0.00000168, Test Loss: 0.11403720, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 97/100, Train Loss: 0.00000139, Test Loss: 0.11264582, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 98/100, Train Loss: 0.00000114, Test Loss: 0.11540608, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 99/100, Train Loss: 0.00000094, Test Loss: 0.11579345, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 100/100, Train Loss: 0.00154201, Test Loss: 0.13009376, Test Accuracy: 0.98000000\n",
            "[tensor(0.2007, grad_fn=<MeanBackward0>), tensor(0.2298, grad_fn=<MeanBackward0>), tensor(0.2439, grad_fn=<MeanBackward0>), tensor(0.2510, grad_fn=<MeanBackward0>), tensor(0.2599, grad_fn=<MeanBackward0>), tensor(0.2676, grad_fn=<MeanBackward0>), tensor(0.2705, grad_fn=<MeanBackward0>), tensor(0.2723, grad_fn=<MeanBackward0>), tensor(0.2741, grad_fn=<MeanBackward0>), tensor(0.2764, grad_fn=<MeanBackward0>), tensor(0.2788, grad_fn=<MeanBackward0>), tensor(0.2792, grad_fn=<MeanBackward0>), tensor(0.2814, grad_fn=<MeanBackward0>), tensor(0.2825, grad_fn=<MeanBackward0>), tensor(0.2823, grad_fn=<MeanBackward0>), tensor(0.2814, grad_fn=<MeanBackward0>), tensor(0.2839, grad_fn=<MeanBackward0>), tensor(0.2836, grad_fn=<MeanBackward0>), tensor(0.2834, grad_fn=<MeanBackward0>), tensor(0.2853, grad_fn=<MeanBackward0>), tensor(0.2855, grad_fn=<MeanBackward0>), tensor(0.2892, grad_fn=<MeanBackward0>), tensor(0.2887, grad_fn=<MeanBackward0>), tensor(0.2926, grad_fn=<MeanBackward0>), tensor(0.2893, grad_fn=<MeanBackward0>), tensor(0.2923, grad_fn=<MeanBackward0>), tensor(0.2911, grad_fn=<MeanBackward0>), tensor(0.2921, grad_fn=<MeanBackward0>), tensor(0.2914, grad_fn=<MeanBackward0>), tensor(0.2917, grad_fn=<MeanBackward0>), tensor(0.2901, grad_fn=<MeanBackward0>), tensor(0.2972, grad_fn=<MeanBackward0>), tensor(0.2948, grad_fn=<MeanBackward0>), tensor(0.2935, grad_fn=<MeanBackward0>), tensor(0.2938, grad_fn=<MeanBackward0>), tensor(0.2955, grad_fn=<MeanBackward0>), tensor(0.2938, grad_fn=<MeanBackward0>), tensor(0.2934, grad_fn=<MeanBackward0>), tensor(0.2940, grad_fn=<MeanBackward0>), tensor(0.3008, grad_fn=<MeanBackward0>), tensor(0.2974, grad_fn=<MeanBackward0>), tensor(0.2964, grad_fn=<MeanBackward0>), tensor(0.2952, grad_fn=<MeanBackward0>), tensor(0.2947, grad_fn=<MeanBackward0>), tensor(0.2952, grad_fn=<MeanBackward0>), tensor(0.2959, grad_fn=<MeanBackward0>), tensor(0.3004, grad_fn=<MeanBackward0>), tensor(0.2994, grad_fn=<MeanBackward0>), tensor(0.2985, grad_fn=<MeanBackward0>), tensor(0.2975, grad_fn=<MeanBackward0>), tensor(0.2968, grad_fn=<MeanBackward0>), tensor(0.2960, grad_fn=<MeanBackward0>), tensor(0.2970, grad_fn=<MeanBackward0>), tensor(0.3012, grad_fn=<MeanBackward0>), tensor(0.2996, grad_fn=<MeanBackward0>), tensor(0.2991, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2983, grad_fn=<MeanBackward0>), tensor(0.2978, grad_fn=<MeanBackward0>), tensor(0.3073, grad_fn=<MeanBackward0>), tensor(0.3076, grad_fn=<MeanBackward0>), tensor(0.3053, grad_fn=<MeanBackward0>), tensor(0.3038, grad_fn=<MeanBackward0>), tensor(0.3024, grad_fn=<MeanBackward0>), tensor(0.3011, grad_fn=<MeanBackward0>), tensor(0.3003, grad_fn=<MeanBackward0>), tensor(0.2992, grad_fn=<MeanBackward0>), tensor(0.3061, grad_fn=<MeanBackward0>), tensor(0.3040, grad_fn=<MeanBackward0>), tensor(0.3033, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3020, grad_fn=<MeanBackward0>), tensor(0.3012, grad_fn=<MeanBackward0>), tensor(0.3004, grad_fn=<MeanBackward0>), tensor(0.3000, grad_fn=<MeanBackward0>), tensor(0.2993, grad_fn=<MeanBackward0>), tensor(0.3053, grad_fn=<MeanBackward0>), tensor(0.3061, grad_fn=<MeanBackward0>), tensor(0.3055, grad_fn=<MeanBackward0>), tensor(0.3051, grad_fn=<MeanBackward0>), tensor(0.3047, grad_fn=<MeanBackward0>), tensor(0.3040, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3031, grad_fn=<MeanBackward0>), tensor(0.3030, grad_fn=<MeanBackward0>), tensor(0.3025, grad_fn=<MeanBackward0>), tensor(0.3080, grad_fn=<MeanBackward0>), tensor(0.3071, grad_fn=<MeanBackward0>), tensor(0.3067, grad_fn=<MeanBackward0>), tensor(0.3065, grad_fn=<MeanBackward0>), tensor(0.3061, grad_fn=<MeanBackward0>), tensor(0.3057, grad_fn=<MeanBackward0>), tensor(0.3054, grad_fn=<MeanBackward0>), tensor(0.3048, grad_fn=<MeanBackward0>), tensor(0.3045, grad_fn=<MeanBackward0>), tensor(0.3044, grad_fn=<MeanBackward0>), tensor(0.3041, grad_fn=<MeanBackward0>), tensor(0.3037, grad_fn=<MeanBackward0>), tensor(0.3030, grad_fn=<MeanBackward0>), tensor(0.3115, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.20068952441215515, 0.22981514036655426, 0.24390387535095215, 0.2510327696800232, 0.25993016362190247, 0.267591267824173, 0.2704581916332245, 0.2723095118999481, 0.2740606367588043, 0.27641424536705017, 0.2788291573524475, 0.27919262647628784, 0.28142601251602173, 0.2825215458869934, 0.28233394026756287, 0.2814362347126007, 0.28393128514289856, 0.28364843130111694, 0.28343120217323303, 0.28533461689949036, 0.2855154871940613, 0.28922218084335327, 0.2887269854545593, 0.2926013767719269, 0.2893335819244385, 0.2923462688922882, 0.2911265194416046, 0.2921389937400818, 0.29135042428970337, 0.29171666502952576, 0.290081650018692, 0.2972012162208557, 0.294810950756073, 0.29347312450408936, 0.2937717139720917, 0.29548248648643494, 0.293840229511261, 0.2934413254261017, 0.29402241110801697, 0.30084487795829773, 0.2974317967891693, 0.29643714427948, 0.29517269134521484, 0.29468029737472534, 0.2952164113521576, 0.2959228456020355, 0.30041247606277466, 0.2993805706501007, 0.2984555661678314, 0.2975272834300995, 0.2967742383480072, 0.2959812879562378, 0.29700732231140137, 0.30117443203926086, 0.29955098032951355, 0.2991328239440918, 0.29860350489616394, 0.2983482778072357, 0.2978423237800598, 0.30725768208503723, 0.30757540464401245, 0.3053019046783447, 0.3038264513015747, 0.3024221658706665, 0.3011012077331543, 0.3003014028072357, 0.2991819679737091, 0.30613386631011963, 0.3039834201335907, 0.3033079504966736, 0.3027110993862152, 0.3020052909851074, 0.3012385368347168, 0.3004395663738251, 0.29999473690986633, 0.2993120849132538, 0.30525410175323486, 0.3060705065727234, 0.30554839968681335, 0.30508604645729065, 0.3046509325504303, 0.30397582054138184, 0.30364522337913513, 0.3030925989151001, 0.30303555727005005, 0.3024521768093109, 0.30798158049583435, 0.30714672803878784, 0.3066616654396057, 0.3064517676830292, 0.306114137172699, 0.30569347739219666, 0.3053596317768097, 0.3048495650291443, 0.3044630289077759, 0.30435729026794434, 0.30412474274635315, 0.3037034571170807, 0.30303940176963806, 0.31153374910354614]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-06\n",
            "    lr: 1.0\n",
            "    rho: 0.9\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/100, Train Loss: 0.43683185, Test Loss: 0.24944479, Test Accuracy: 0.92370000\n",
            "\n",
            "Epoch: 2/100, Train Loss: 0.21723592, Test Loss: 0.17235525, Test Accuracy: 0.94810000\n",
            "\n",
            "Epoch: 3/100, Train Loss: 0.15765823, Test Loss: 0.13301787, Test Accuracy: 0.96000000\n",
            "\n",
            "Epoch: 4/100, Train Loss: 0.12271754, Test Loss: 0.11626818, Test Accuracy: 0.96310000\n",
            "\n",
            "Epoch: 5/100, Train Loss: 0.10048587, Test Loss: 0.10125696, Test Accuracy: 0.96830000\n",
            "\n",
            "Epoch: 6/100, Train Loss: 0.08466160, Test Loss: 0.08926426, Test Accuracy: 0.97140000\n",
            "\n",
            "Epoch: 7/100, Train Loss: 0.07181358, Test Loss: 0.08376566, Test Accuracy: 0.97400000\n",
            "\n",
            "Epoch: 8/100, Train Loss: 0.06274416, Test Loss: 0.07770842, Test Accuracy: 0.97540000\n",
            "\n",
            "Epoch: 9/100, Train Loss: 0.05443553, Test Loss: 0.07416744, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 10/100, Train Loss: 0.04802722, Test Loss: 0.07129007, Test Accuracy: 0.97650000\n",
            "\n",
            "Epoch: 11/100, Train Loss: 0.04254853, Test Loss: 0.07110556, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 12/100, Train Loss: 0.03764671, Test Loss: 0.06873165, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 13/100, Train Loss: 0.03371667, Test Loss: 0.06629884, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 14/100, Train Loss: 0.03009185, Test Loss: 0.06527633, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 15/100, Train Loss: 0.02646879, Test Loss: 0.06124664, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 16/100, Train Loss: 0.02411883, Test Loss: 0.06227959, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 17/100, Train Loss: 0.02148900, Test Loss: 0.06568754, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 18/100, Train Loss: 0.01910287, Test Loss: 0.06089246, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 19/100, Train Loss: 0.01719468, Test Loss: 0.06059535, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 20/100, Train Loss: 0.01537351, Test Loss: 0.05938970, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 21/100, Train Loss: 0.01381504, Test Loss: 0.06164776, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 22/100, Train Loss: 0.01236067, Test Loss: 0.05908692, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 23/100, Train Loss: 0.01123361, Test Loss: 0.05841572, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 24/100, Train Loss: 0.01002167, Test Loss: 0.05875591, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 25/100, Train Loss: 0.00901164, Test Loss: 0.06104921, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 26/100, Train Loss: 0.00829708, Test Loss: 0.05934340, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 27/100, Train Loss: 0.00744241, Test Loss: 0.05925830, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 28/100, Train Loss: 0.00659223, Test Loss: 0.06103759, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 29/100, Train Loss: 0.00616560, Test Loss: 0.05887143, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 30/100, Train Loss: 0.00561073, Test Loss: 0.06050443, Test Accuracy: 0.98290000\n",
            "\n",
            "Epoch: 31/100, Train Loss: 0.00515571, Test Loss: 0.06016808, Test Accuracy: 0.98260000\n",
            "\n",
            "Epoch: 32/100, Train Loss: 0.00467986, Test Loss: 0.05989098, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 33/100, Train Loss: 0.00429836, Test Loss: 0.05960913, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 34/100, Train Loss: 0.00391386, Test Loss: 0.05984783, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 35/100, Train Loss: 0.00365438, Test Loss: 0.06052895, Test Accuracy: 0.98270000\n",
            "\n",
            "Epoch: 36/100, Train Loss: 0.00331204, Test Loss: 0.06113481, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 37/100, Train Loss: 0.00309196, Test Loss: 0.06150372, Test Accuracy: 0.98300000\n",
            "\n",
            "Epoch: 38/100, Train Loss: 0.00289320, Test Loss: 0.06215888, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 39/100, Train Loss: 0.00270760, Test Loss: 0.06037115, Test Accuracy: 0.98270000\n",
            "\n",
            "Epoch: 40/100, Train Loss: 0.00249622, Test Loss: 0.06067007, Test Accuracy: 0.98320000\n",
            "\n",
            "Epoch: 41/100, Train Loss: 0.00232249, Test Loss: 0.06097001, Test Accuracy: 0.98320000\n",
            "\n",
            "Epoch: 42/100, Train Loss: 0.00220851, Test Loss: 0.06189613, Test Accuracy: 0.98340000\n",
            "\n",
            "Epoch: 43/100, Train Loss: 0.00204466, Test Loss: 0.06226400, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 44/100, Train Loss: 0.00197031, Test Loss: 0.06146820, Test Accuracy: 0.98300000\n",
            "\n",
            "Epoch: 45/100, Train Loss: 0.00182483, Test Loss: 0.06197475, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 46/100, Train Loss: 0.00175059, Test Loss: 0.06187630, Test Accuracy: 0.98290000\n",
            "\n",
            "Epoch: 47/100, Train Loss: 0.00165553, Test Loss: 0.06290734, Test Accuracy: 0.98320000\n",
            "\n",
            "Epoch: 48/100, Train Loss: 0.00160038, Test Loss: 0.06262833, Test Accuracy: 0.98290000\n",
            "\n",
            "Epoch: 49/100, Train Loss: 0.00151580, Test Loss: 0.06275204, Test Accuracy: 0.98300000\n",
            "\n",
            "Epoch: 50/100, Train Loss: 0.00145462, Test Loss: 0.06303898, Test Accuracy: 0.98280000\n",
            "\n",
            "Epoch: 51/100, Train Loss: 0.00138089, Test Loss: 0.06324572, Test Accuracy: 0.98360000\n",
            "\n",
            "Epoch: 52/100, Train Loss: 0.00132177, Test Loss: 0.06357721, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 53/100, Train Loss: 0.00127368, Test Loss: 0.06316163, Test Accuracy: 0.98350000\n",
            "\n",
            "Epoch: 54/100, Train Loss: 0.00123493, Test Loss: 0.06364810, Test Accuracy: 0.98270000\n",
            "\n",
            "Epoch: 55/100, Train Loss: 0.00118073, Test Loss: 0.06322148, Test Accuracy: 0.98330000\n",
            "\n",
            "Epoch: 56/100, Train Loss: 0.00113521, Test Loss: 0.06410802, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 57/100, Train Loss: 0.00109267, Test Loss: 0.06369184, Test Accuracy: 0.98330000\n",
            "\n",
            "Epoch: 58/100, Train Loss: 0.00105486, Test Loss: 0.06398029, Test Accuracy: 0.98330000\n",
            "\n",
            "Epoch: 59/100, Train Loss: 0.00102228, Test Loss: 0.06399042, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 60/100, Train Loss: 0.00099243, Test Loss: 0.06424930, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 61/100, Train Loss: 0.00095929, Test Loss: 0.06397599, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 62/100, Train Loss: 0.00093168, Test Loss: 0.06422201, Test Accuracy: 0.98350000\n",
            "\n",
            "Epoch: 63/100, Train Loss: 0.00090118, Test Loss: 0.06451762, Test Accuracy: 0.98360000\n",
            "\n",
            "Epoch: 64/100, Train Loss: 0.00087743, Test Loss: 0.06466285, Test Accuracy: 0.98320000\n",
            "\n",
            "Epoch: 65/100, Train Loss: 0.00085079, Test Loss: 0.06502392, Test Accuracy: 0.98370000\n",
            "\n",
            "Epoch: 66/100, Train Loss: 0.00083294, Test Loss: 0.06510956, Test Accuracy: 0.98360000\n",
            "\n",
            "Epoch: 67/100, Train Loss: 0.00080876, Test Loss: 0.06475879, Test Accuracy: 0.98320000\n",
            "\n",
            "Epoch: 68/100, Train Loss: 0.00078838, Test Loss: 0.06482774, Test Accuracy: 0.98380000\n",
            "\n",
            "Epoch: 69/100, Train Loss: 0.00077011, Test Loss: 0.06536090, Test Accuracy: 0.98330000\n",
            "\n",
            "Epoch: 70/100, Train Loss: 0.00074741, Test Loss: 0.06527088, Test Accuracy: 0.98350000\n",
            "\n",
            "Epoch: 71/100, Train Loss: 0.00072714, Test Loss: 0.06540496, Test Accuracy: 0.98350000\n",
            "\n",
            "Epoch: 72/100, Train Loss: 0.00071576, Test Loss: 0.06524165, Test Accuracy: 0.98370000\n",
            "\n",
            "Epoch: 73/100, Train Loss: 0.00069542, Test Loss: 0.06579786, Test Accuracy: 0.98350000\n",
            "\n",
            "Epoch: 74/100, Train Loss: 0.00068126, Test Loss: 0.06535653, Test Accuracy: 0.98360000\n",
            "\n",
            "Epoch: 75/100, Train Loss: 0.00066524, Test Loss: 0.06570670, Test Accuracy: 0.98350000\n",
            "\n",
            "Epoch: 76/100, Train Loss: 0.00065177, Test Loss: 0.06571199, Test Accuracy: 0.98360000\n",
            "\n",
            "Epoch: 77/100, Train Loss: 0.00063568, Test Loss: 0.06540029, Test Accuracy: 0.98380000\n",
            "\n",
            "Epoch: 78/100, Train Loss: 0.00061990, Test Loss: 0.06613830, Test Accuracy: 0.98310000\n",
            "\n",
            "Epoch: 79/100, Train Loss: 0.00061198, Test Loss: 0.06604079, Test Accuracy: 0.98360000\n",
            "\n",
            "Epoch: 80/100, Train Loss: 0.00059685, Test Loss: 0.06608121, Test Accuracy: 0.98350000\n",
            "\n",
            "Epoch: 81/100, Train Loss: 0.00058717, Test Loss: 0.06626060, Test Accuracy: 0.98330000\n",
            "\n",
            "Epoch: 82/100, Train Loss: 0.00057352, Test Loss: 0.06646566, Test Accuracy: 0.98380000\n",
            "\n",
            "Epoch: 83/100, Train Loss: 0.00056201, Test Loss: 0.06666088, Test Accuracy: 0.98360000\n",
            "\n",
            "Epoch: 84/100, Train Loss: 0.00055605, Test Loss: 0.06645803, Test Accuracy: 0.98370000\n",
            "\n",
            "Epoch: 85/100, Train Loss: 0.00054067, Test Loss: 0.06663521, Test Accuracy: 0.98350000\n",
            "\n",
            "Epoch: 86/100, Train Loss: 0.00053162, Test Loss: 0.06646242, Test Accuracy: 0.98350000\n",
            "\n",
            "Epoch: 87/100, Train Loss: 0.00052243, Test Loss: 0.06664217, Test Accuracy: 0.98390000\n",
            "\n",
            "Epoch: 88/100, Train Loss: 0.00051510, Test Loss: 0.06674164, Test Accuracy: 0.98360000\n",
            "\n",
            "Epoch: 89/100, Train Loss: 0.00050560, Test Loss: 0.06701102, Test Accuracy: 0.98390000\n",
            "\n",
            "Epoch: 90/100, Train Loss: 0.00049571, Test Loss: 0.06731508, Test Accuracy: 0.98380000\n",
            "\n",
            "Epoch: 91/100, Train Loss: 0.00048765, Test Loss: 0.06710234, Test Accuracy: 0.98380000\n",
            "\n",
            "Epoch: 92/100, Train Loss: 0.00047999, Test Loss: 0.06694495, Test Accuracy: 0.98380000\n",
            "\n",
            "Epoch: 93/100, Train Loss: 0.00047232, Test Loss: 0.06730597, Test Accuracy: 0.98370000\n",
            "\n",
            "Epoch: 94/100, Train Loss: 0.00046454, Test Loss: 0.06724179, Test Accuracy: 0.98370000\n",
            "\n",
            "Epoch: 95/100, Train Loss: 0.00045678, Test Loss: 0.06742484, Test Accuracy: 0.98370000\n",
            "\n",
            "Epoch: 96/100, Train Loss: 0.00044895, Test Loss: 0.06768983, Test Accuracy: 0.98350000\n",
            "\n",
            "Epoch: 97/100, Train Loss: 0.00044349, Test Loss: 0.06739544, Test Accuracy: 0.98360000\n",
            "\n",
            "Epoch: 98/100, Train Loss: 0.00043715, Test Loss: 0.06759359, Test Accuracy: 0.98390000\n",
            "\n",
            "Epoch: 99/100, Train Loss: 0.00043016, Test Loss: 0.06768129, Test Accuracy: 0.98370000\n",
            "\n",
            "Epoch: 100/100, Train Loss: 0.00042226, Test Loss: 0.06783176, Test Accuracy: 0.98370000\n",
            "[tensor(0.1996, grad_fn=<MeanBackward0>), tensor(0.2525, grad_fn=<MeanBackward0>), tensor(0.2755, grad_fn=<MeanBackward0>), tensor(0.2900, grad_fn=<MeanBackward0>), tensor(0.3001, grad_fn=<MeanBackward0>), tensor(0.3080, grad_fn=<MeanBackward0>), tensor(0.3121, grad_fn=<MeanBackward0>), tensor(0.3159, grad_fn=<MeanBackward0>), tensor(0.3186, grad_fn=<MeanBackward0>), tensor(0.3238, grad_fn=<MeanBackward0>), tensor(0.3240, grad_fn=<MeanBackward0>), tensor(0.3241, grad_fn=<MeanBackward0>), tensor(0.3231, grad_fn=<MeanBackward0>), tensor(0.3243, grad_fn=<MeanBackward0>), tensor(0.3259, grad_fn=<MeanBackward0>), tensor(0.3281, grad_fn=<MeanBackward0>), tensor(0.3277, grad_fn=<MeanBackward0>), tensor(0.3290, grad_fn=<MeanBackward0>), tensor(0.3290, grad_fn=<MeanBackward0>), tensor(0.3295, grad_fn=<MeanBackward0>), tensor(0.3315, grad_fn=<MeanBackward0>), tensor(0.3301, grad_fn=<MeanBackward0>), tensor(0.3307, grad_fn=<MeanBackward0>), tensor(0.3309, grad_fn=<MeanBackward0>), tensor(0.3301, grad_fn=<MeanBackward0>), tensor(0.3318, grad_fn=<MeanBackward0>), tensor(0.3307, grad_fn=<MeanBackward0>), tensor(0.3307, grad_fn=<MeanBackward0>), tensor(0.3308, grad_fn=<MeanBackward0>), tensor(0.3308, grad_fn=<MeanBackward0>), tensor(0.3306, grad_fn=<MeanBackward0>), tensor(0.3307, grad_fn=<MeanBackward0>), tensor(0.3308, grad_fn=<MeanBackward0>), tensor(0.3312, grad_fn=<MeanBackward0>), tensor(0.3307, grad_fn=<MeanBackward0>), tensor(0.3311, grad_fn=<MeanBackward0>), tensor(0.3309, grad_fn=<MeanBackward0>), tensor(0.3308, grad_fn=<MeanBackward0>), tensor(0.3308, grad_fn=<MeanBackward0>), tensor(0.3311, grad_fn=<MeanBackward0>), tensor(0.3312, grad_fn=<MeanBackward0>), tensor(0.3314, grad_fn=<MeanBackward0>), tensor(0.3317, grad_fn=<MeanBackward0>), tensor(0.3316, grad_fn=<MeanBackward0>), tensor(0.3317, grad_fn=<MeanBackward0>), tensor(0.3314, grad_fn=<MeanBackward0>), tensor(0.3317, grad_fn=<MeanBackward0>), tensor(0.3320, grad_fn=<MeanBackward0>), tensor(0.3319, grad_fn=<MeanBackward0>), tensor(0.3322, grad_fn=<MeanBackward0>), tensor(0.3321, grad_fn=<MeanBackward0>), tensor(0.3322, grad_fn=<MeanBackward0>), tensor(0.3325, grad_fn=<MeanBackward0>), tensor(0.3324, grad_fn=<MeanBackward0>), tensor(0.3325, grad_fn=<MeanBackward0>), tensor(0.3325, grad_fn=<MeanBackward0>), tensor(0.3325, grad_fn=<MeanBackward0>), tensor(0.3329, grad_fn=<MeanBackward0>), tensor(0.3331, grad_fn=<MeanBackward0>), tensor(0.3331, grad_fn=<MeanBackward0>), tensor(0.3330, grad_fn=<MeanBackward0>), tensor(0.3332, grad_fn=<MeanBackward0>), tensor(0.3332, grad_fn=<MeanBackward0>), tensor(0.3333, grad_fn=<MeanBackward0>), tensor(0.3333, grad_fn=<MeanBackward0>), tensor(0.3334, grad_fn=<MeanBackward0>), tensor(0.3335, grad_fn=<MeanBackward0>), tensor(0.3334, grad_fn=<MeanBackward0>), tensor(0.3335, grad_fn=<MeanBackward0>), tensor(0.3336, grad_fn=<MeanBackward0>), tensor(0.3338, grad_fn=<MeanBackward0>), tensor(0.3337, grad_fn=<MeanBackward0>), tensor(0.3338, grad_fn=<MeanBackward0>), tensor(0.3339, grad_fn=<MeanBackward0>), tensor(0.3339, grad_fn=<MeanBackward0>), tensor(0.3340, grad_fn=<MeanBackward0>), tensor(0.3340, grad_fn=<MeanBackward0>), tensor(0.3340, grad_fn=<MeanBackward0>), tensor(0.3340, grad_fn=<MeanBackward0>), tensor(0.3341, grad_fn=<MeanBackward0>), tensor(0.3342, grad_fn=<MeanBackward0>), tensor(0.3343, grad_fn=<MeanBackward0>), tensor(0.3343, grad_fn=<MeanBackward0>), tensor(0.3344, grad_fn=<MeanBackward0>), tensor(0.3344, grad_fn=<MeanBackward0>), tensor(0.3344, grad_fn=<MeanBackward0>), tensor(0.3344, grad_fn=<MeanBackward0>), tensor(0.3345, grad_fn=<MeanBackward0>), tensor(0.3346, grad_fn=<MeanBackward0>), tensor(0.3347, grad_fn=<MeanBackward0>), tensor(0.3346, grad_fn=<MeanBackward0>), tensor(0.3346, grad_fn=<MeanBackward0>), tensor(0.3347, grad_fn=<MeanBackward0>), tensor(0.3348, grad_fn=<MeanBackward0>), tensor(0.3348, grad_fn=<MeanBackward0>), tensor(0.3348, grad_fn=<MeanBackward0>), tensor(0.3348, grad_fn=<MeanBackward0>), tensor(0.3349, grad_fn=<MeanBackward0>), tensor(0.3350, grad_fn=<MeanBackward0>), tensor(0.3350, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.19960281252861023, 0.2525157630443573, 0.2754727005958557, 0.2899818420410156, 0.3000873625278473, 0.3080095052719116, 0.3120957612991333, 0.3158601224422455, 0.3186027407646179, 0.32379254698753357, 0.3240338861942291, 0.3241097629070282, 0.3231092095375061, 0.32434186339378357, 0.3258989155292511, 0.3280726969242096, 0.32768866419792175, 0.3290053606033325, 0.32895734906196594, 0.329484224319458, 0.3315449655056, 0.33006343245506287, 0.3306863605976105, 0.3309146463871002, 0.33010655641555786, 0.331804484128952, 0.33069926500320435, 0.3306589722633362, 0.33077991008758545, 0.33075419068336487, 0.330554336309433, 0.3306637704372406, 0.33076390624046326, 0.3311757445335388, 0.33073610067367554, 0.33109837770462036, 0.330915629863739, 0.3308291435241699, 0.33084017038345337, 0.33114689588546753, 0.3312349021434784, 0.3314373791217804, 0.3317420780658722, 0.3315989375114441, 0.331701397895813, 0.33140498399734497, 0.33170998096466064, 0.33198803663253784, 0.3318972885608673, 0.33221882581710815, 0.3320673108100891, 0.33222103118896484, 0.33252257108688354, 0.33243080973625183, 0.3324609398841858, 0.3324773609638214, 0.33247578144073486, 0.33286675810813904, 0.333082377910614, 0.3330850899219513, 0.33300477266311646, 0.3331545293331146, 0.33323949575424194, 0.3332979381084442, 0.3332994282245636, 0.33339911699295044, 0.33348339796066284, 0.33337560296058655, 0.3334716856479645, 0.3336165249347687, 0.3337634205818176, 0.3337474465370178, 0.3338320851325989, 0.33389055728912354, 0.33392947912216187, 0.3340108096599579, 0.3339892029762268, 0.3339982032775879, 0.3340245485305786, 0.33412015438079834, 0.3341858386993408, 0.33428794145584106, 0.33428043127059937, 0.3343829810619354, 0.33442750573158264, 0.3343645930290222, 0.33443355560302734, 0.33450958132743835, 0.3345790505409241, 0.3346654176712036, 0.33464622497558594, 0.3346494734287262, 0.33472713828086853, 0.33477315306663513, 0.3347835838794708, 0.3347552716732025, 0.3347976803779602, 0.3349044620990753, 0.3349825441837311, 0.3349668085575104]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adagrad (\n",
            "Parameter Group 0\n",
            "    eps: 1e-10\n",
            "    initial_accumulator_value: 0\n",
            "    lr: 0.1\n",
            "    lr_decay: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/100, Train Loss: 0.24041610, Test Loss: 0.12457711, Test Accuracy: 0.96240000\n",
            "\n",
            "Epoch: 2/100, Train Loss: 0.09753305, Test Loss: 0.09569839, Test Accuracy: 0.97070000\n",
            "\n",
            "Epoch: 3/100, Train Loss: 0.06806091, Test Loss: 0.08376909, Test Accuracy: 0.97400000\n",
            "\n",
            "Epoch: 4/100, Train Loss: 0.05110194, Test Loss: 0.07772879, Test Accuracy: 0.97560000\n",
            "\n",
            "Epoch: 5/100, Train Loss: 0.04051159, Test Loss: 0.07295245, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 6/100, Train Loss: 0.03275624, Test Loss: 0.07199098, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 7/100, Train Loss: 0.02703137, Test Loss: 0.07162829, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 8/100, Train Loss: 0.02279799, Test Loss: 0.06757111, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 9/100, Train Loss: 0.01943259, Test Loss: 0.06890113, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 10/100, Train Loss: 0.01664366, Test Loss: 0.06726461, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 11/100, Train Loss: 0.01450113, Test Loss: 0.06742746, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 12/100, Train Loss: 0.01264670, Test Loss: 0.06576392, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 13/100, Train Loss: 0.01121097, Test Loss: 0.06563189, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 14/100, Train Loss: 0.00993615, Test Loss: 0.06533383, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 15/100, Train Loss: 0.00884156, Test Loss: 0.06586601, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 16/100, Train Loss: 0.00796848, Test Loss: 0.06591270, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 17/100, Train Loss: 0.00713166, Test Loss: 0.06616360, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 18/100, Train Loss: 0.00643879, Test Loss: 0.06661978, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 19/100, Train Loss: 0.00584257, Test Loss: 0.06658785, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 20/100, Train Loss: 0.00535366, Test Loss: 0.06676073, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 21/100, Train Loss: 0.00490834, Test Loss: 0.06708763, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 22/100, Train Loss: 0.00453575, Test Loss: 0.06695519, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 23/100, Train Loss: 0.00417808, Test Loss: 0.06673647, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 24/100, Train Loss: 0.00387506, Test Loss: 0.06732976, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 25/100, Train Loss: 0.00362432, Test Loss: 0.06759356, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 26/100, Train Loss: 0.00337809, Test Loss: 0.06790616, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 27/100, Train Loss: 0.00316163, Test Loss: 0.06737276, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 28/100, Train Loss: 0.00295725, Test Loss: 0.06812894, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 29/100, Train Loss: 0.00278244, Test Loss: 0.06716122, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 30/100, Train Loss: 0.00261600, Test Loss: 0.06763857, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 31/100, Train Loss: 0.00247747, Test Loss: 0.06742636, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 32/100, Train Loss: 0.00233359, Test Loss: 0.06782041, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 33/100, Train Loss: 0.00220504, Test Loss: 0.06815783, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 34/100, Train Loss: 0.00210083, Test Loss: 0.06895143, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 35/100, Train Loss: 0.00198848, Test Loss: 0.06873442, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 36/100, Train Loss: 0.00189475, Test Loss: 0.06907023, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 37/100, Train Loss: 0.00180766, Test Loss: 0.06812968, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 38/100, Train Loss: 0.00172909, Test Loss: 0.06936809, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 39/100, Train Loss: 0.00165098, Test Loss: 0.06844828, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 40/100, Train Loss: 0.00158288, Test Loss: 0.06908753, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 41/100, Train Loss: 0.00151547, Test Loss: 0.06921237, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 42/100, Train Loss: 0.00145586, Test Loss: 0.06899526, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 43/100, Train Loss: 0.00139845, Test Loss: 0.06957950, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 44/100, Train Loss: 0.00134348, Test Loss: 0.06938318, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 45/100, Train Loss: 0.00129658, Test Loss: 0.06939055, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 46/100, Train Loss: 0.00125178, Test Loss: 0.06915909, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 47/100, Train Loss: 0.00120841, Test Loss: 0.06985898, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 48/100, Train Loss: 0.00116899, Test Loss: 0.07005649, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 49/100, Train Loss: 0.00113345, Test Loss: 0.07020765, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 50/100, Train Loss: 0.00109400, Test Loss: 0.07026882, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 51/100, Train Loss: 0.00106460, Test Loss: 0.07051627, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 52/100, Train Loss: 0.00103353, Test Loss: 0.07036852, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 53/100, Train Loss: 0.00100222, Test Loss: 0.07019520, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 54/100, Train Loss: 0.00097470, Test Loss: 0.07071655, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 55/100, Train Loss: 0.00094778, Test Loss: 0.07041211, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 56/100, Train Loss: 0.00092150, Test Loss: 0.07067962, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 57/100, Train Loss: 0.00089726, Test Loss: 0.07109325, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 58/100, Train Loss: 0.00087293, Test Loss: 0.07093171, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 59/100, Train Loss: 0.00085143, Test Loss: 0.07131264, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 60/100, Train Loss: 0.00083062, Test Loss: 0.07117935, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 61/100, Train Loss: 0.00080811, Test Loss: 0.07111980, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 62/100, Train Loss: 0.00079049, Test Loss: 0.07141773, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 63/100, Train Loss: 0.00077228, Test Loss: 0.07166859, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 64/100, Train Loss: 0.00075401, Test Loss: 0.07151229, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 65/100, Train Loss: 0.00073702, Test Loss: 0.07186312, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 66/100, Train Loss: 0.00072023, Test Loss: 0.07178080, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 67/100, Train Loss: 0.00070520, Test Loss: 0.07211545, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 68/100, Train Loss: 0.00068972, Test Loss: 0.07234266, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 69/100, Train Loss: 0.00067600, Test Loss: 0.07238670, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 70/100, Train Loss: 0.00066163, Test Loss: 0.07248312, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 71/100, Train Loss: 0.00064749, Test Loss: 0.07247728, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 72/100, Train Loss: 0.00063478, Test Loss: 0.07242344, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 73/100, Train Loss: 0.00062099, Test Loss: 0.07254768, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 74/100, Train Loss: 0.00061000, Test Loss: 0.07283909, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 75/100, Train Loss: 0.00059763, Test Loss: 0.07283220, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 76/100, Train Loss: 0.00058726, Test Loss: 0.07334532, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 77/100, Train Loss: 0.00057613, Test Loss: 0.07267985, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 78/100, Train Loss: 0.00056522, Test Loss: 0.07311312, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 79/100, Train Loss: 0.00055568, Test Loss: 0.07293530, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 80/100, Train Loss: 0.00054585, Test Loss: 0.07332563, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 81/100, Train Loss: 0.00053611, Test Loss: 0.07355018, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 82/100, Train Loss: 0.00052671, Test Loss: 0.07314891, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 83/100, Train Loss: 0.00051796, Test Loss: 0.07349375, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 84/100, Train Loss: 0.00050955, Test Loss: 0.07365096, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 85/100, Train Loss: 0.00050091, Test Loss: 0.07360136, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 86/100, Train Loss: 0.00049285, Test Loss: 0.07354581, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 87/100, Train Loss: 0.00048527, Test Loss: 0.07364723, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 88/100, Train Loss: 0.00047764, Test Loss: 0.07384397, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 89/100, Train Loss: 0.00046960, Test Loss: 0.07396737, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 90/100, Train Loss: 0.00046297, Test Loss: 0.07383289, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 91/100, Train Loss: 0.00045547, Test Loss: 0.07398176, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 92/100, Train Loss: 0.00044893, Test Loss: 0.07436775, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 93/100, Train Loss: 0.00044158, Test Loss: 0.07459855, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 94/100, Train Loss: 0.00043573, Test Loss: 0.07433401, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 95/100, Train Loss: 0.00042936, Test Loss: 0.07458432, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 96/100, Train Loss: 0.00042297, Test Loss: 0.07464841, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 97/100, Train Loss: 0.00041690, Test Loss: 0.07471312, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 98/100, Train Loss: 0.00041119, Test Loss: 0.07491308, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 99/100, Train Loss: 0.00040564, Test Loss: 0.07483873, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 100/100, Train Loss: 0.00040031, Test Loss: 0.07485439, Test Accuracy: 0.98090000\n",
            "[tensor(0.5922, grad_fn=<MeanBackward0>), tensor(0.5900, grad_fn=<MeanBackward0>), tensor(0.5821, grad_fn=<MeanBackward0>), tensor(0.5824, grad_fn=<MeanBackward0>), tensor(0.5784, grad_fn=<MeanBackward0>), tensor(0.5741, grad_fn=<MeanBackward0>), tensor(0.5754, grad_fn=<MeanBackward0>), tensor(0.5732, grad_fn=<MeanBackward0>), tensor(0.5696, grad_fn=<MeanBackward0>), tensor(0.5691, grad_fn=<MeanBackward0>), tensor(0.5677, grad_fn=<MeanBackward0>), tensor(0.5658, grad_fn=<MeanBackward0>), tensor(0.5670, grad_fn=<MeanBackward0>), tensor(0.5658, grad_fn=<MeanBackward0>), tensor(0.5641, grad_fn=<MeanBackward0>), tensor(0.5630, grad_fn=<MeanBackward0>), tensor(0.5623, grad_fn=<MeanBackward0>), tensor(0.5611, grad_fn=<MeanBackward0>), tensor(0.5593, grad_fn=<MeanBackward0>), tensor(0.5595, grad_fn=<MeanBackward0>), tensor(0.5578, grad_fn=<MeanBackward0>), tensor(0.5577, grad_fn=<MeanBackward0>), tensor(0.5576, grad_fn=<MeanBackward0>), tensor(0.5572, grad_fn=<MeanBackward0>), tensor(0.5559, grad_fn=<MeanBackward0>), tensor(0.5558, grad_fn=<MeanBackward0>), tensor(0.5558, grad_fn=<MeanBackward0>), tensor(0.5549, grad_fn=<MeanBackward0>), tensor(0.5549, grad_fn=<MeanBackward0>), tensor(0.5538, grad_fn=<MeanBackward0>), tensor(0.5537, grad_fn=<MeanBackward0>), tensor(0.5530, grad_fn=<MeanBackward0>), tensor(0.5534, grad_fn=<MeanBackward0>), tensor(0.5530, grad_fn=<MeanBackward0>), tensor(0.5522, grad_fn=<MeanBackward0>), tensor(0.5524, grad_fn=<MeanBackward0>), tensor(0.5523, grad_fn=<MeanBackward0>), tensor(0.5516, grad_fn=<MeanBackward0>), tensor(0.5518, grad_fn=<MeanBackward0>), tensor(0.5515, grad_fn=<MeanBackward0>), tensor(0.5508, grad_fn=<MeanBackward0>), tensor(0.5509, grad_fn=<MeanBackward0>), tensor(0.5506, grad_fn=<MeanBackward0>), tensor(0.5509, grad_fn=<MeanBackward0>), tensor(0.5506, grad_fn=<MeanBackward0>), tensor(0.5505, grad_fn=<MeanBackward0>), tensor(0.5500, grad_fn=<MeanBackward0>), tensor(0.5500, grad_fn=<MeanBackward0>), tensor(0.5498, grad_fn=<MeanBackward0>), tensor(0.5497, grad_fn=<MeanBackward0>), tensor(0.5492, grad_fn=<MeanBackward0>), tensor(0.5494, grad_fn=<MeanBackward0>), tensor(0.5494, grad_fn=<MeanBackward0>), tensor(0.5492, grad_fn=<MeanBackward0>), tensor(0.5489, grad_fn=<MeanBackward0>), tensor(0.5488, grad_fn=<MeanBackward0>), tensor(0.5487, grad_fn=<MeanBackward0>), tensor(0.5488, grad_fn=<MeanBackward0>), tensor(0.5485, grad_fn=<MeanBackward0>), tensor(0.5484, grad_fn=<MeanBackward0>), tensor(0.5481, grad_fn=<MeanBackward0>), tensor(0.5481, grad_fn=<MeanBackward0>), tensor(0.5480, grad_fn=<MeanBackward0>), tensor(0.5478, grad_fn=<MeanBackward0>), tensor(0.5479, grad_fn=<MeanBackward0>), tensor(0.5477, grad_fn=<MeanBackward0>), tensor(0.5476, grad_fn=<MeanBackward0>), tensor(0.5473, grad_fn=<MeanBackward0>), tensor(0.5474, grad_fn=<MeanBackward0>), tensor(0.5474, grad_fn=<MeanBackward0>), tensor(0.5472, grad_fn=<MeanBackward0>), tensor(0.5472, grad_fn=<MeanBackward0>), tensor(0.5472, grad_fn=<MeanBackward0>), tensor(0.5469, grad_fn=<MeanBackward0>), tensor(0.5469, grad_fn=<MeanBackward0>), tensor(0.5468, grad_fn=<MeanBackward0>), tensor(0.5466, grad_fn=<MeanBackward0>), tensor(0.5465, grad_fn=<MeanBackward0>), tensor(0.5466, grad_fn=<MeanBackward0>), tensor(0.5465, grad_fn=<MeanBackward0>), tensor(0.5464, grad_fn=<MeanBackward0>), tensor(0.5465, grad_fn=<MeanBackward0>), tensor(0.5462, grad_fn=<MeanBackward0>), tensor(0.5462, grad_fn=<MeanBackward0>), tensor(0.5461, grad_fn=<MeanBackward0>), tensor(0.5463, grad_fn=<MeanBackward0>), tensor(0.5461, grad_fn=<MeanBackward0>), tensor(0.5460, grad_fn=<MeanBackward0>), tensor(0.5460, grad_fn=<MeanBackward0>), tensor(0.5460, grad_fn=<MeanBackward0>), tensor(0.5459, grad_fn=<MeanBackward0>), tensor(0.5458, grad_fn=<MeanBackward0>), tensor(0.5456, grad_fn=<MeanBackward0>), tensor(0.5456, grad_fn=<MeanBackward0>), tensor(0.5455, grad_fn=<MeanBackward0>), tensor(0.5456, grad_fn=<MeanBackward0>), tensor(0.5454, grad_fn=<MeanBackward0>), tensor(0.5453, grad_fn=<MeanBackward0>), tensor(0.5453, grad_fn=<MeanBackward0>), tensor(0.5452, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.5921794772148132, 0.5900229215621948, 0.5820962190628052, 0.5824187397956848, 0.5784282088279724, 0.5740782618522644, 0.5753570199012756, 0.5731782913208008, 0.5696231722831726, 0.5691453814506531, 0.5676524043083191, 0.5657893419265747, 0.5669960379600525, 0.5657826662063599, 0.5641123056411743, 0.5630064606666565, 0.5623316168785095, 0.5610942244529724, 0.5593127608299255, 0.5595426559448242, 0.557775616645813, 0.557735025882721, 0.5575805902481079, 0.5571603775024414, 0.5558975338935852, 0.555768609046936, 0.5557897686958313, 0.5549450516700745, 0.5548604726791382, 0.5537886619567871, 0.5537328124046326, 0.5529690980911255, 0.5534396171569824, 0.5530017018318176, 0.552177369594574, 0.5524258017539978, 0.5523156523704529, 0.5515547394752502, 0.5518370866775513, 0.5515241026878357, 0.5508352518081665, 0.5508831739425659, 0.5506277084350586, 0.5508896708488464, 0.5506312251091003, 0.5505454540252686, 0.5499682426452637, 0.550011396408081, 0.5497687458992004, 0.5497435331344604, 0.5491971969604492, 0.5493940711021423, 0.5493659377098083, 0.5491513609886169, 0.5489227771759033, 0.5487968921661377, 0.5486789345741272, 0.5488267540931702, 0.548488974571228, 0.5484175086021423, 0.5480902791023254, 0.5480963587760925, 0.5479700565338135, 0.5477830767631531, 0.5479093790054321, 0.5477400422096252, 0.5475816130638123, 0.5473161935806274, 0.5473830103874207, 0.5474117398262024, 0.5471891164779663, 0.5471727252006531, 0.54718416929245, 0.5468669533729553, 0.5469005703926086, 0.5467947721481323, 0.5466083884239197, 0.54654860496521, 0.5465860366821289, 0.5465424060821533, 0.5464380979537964, 0.5465028882026672, 0.5461878776550293, 0.546205997467041, 0.5461228489875793, 0.5462675094604492, 0.5461137890815735, 0.5459829568862915, 0.5459686517715454, 0.5459628701210022, 0.5458606481552124, 0.5458056330680847, 0.545609712600708, 0.5455574989318848, 0.5454651713371277, 0.5455617904663086, 0.5453565716743469, 0.5453301072120667, 0.54531329870224, 0.545197069644928]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/100, Train Loss: 0.76643844, Test Loss: 0.37410800, Test Accuracy: 0.89680000\n",
            "\n",
            "Epoch: 2/100, Train Loss: 0.34814471, Test Loss: 0.30799823, Test Accuracy: 0.90980000\n",
            "\n",
            "Epoch: 3/100, Train Loss: 0.30784205, Test Loss: 0.28595026, Test Accuracy: 0.91950000\n",
            "\n",
            "Epoch: 4/100, Train Loss: 0.28512044, Test Loss: 0.26304810, Test Accuracy: 0.92440000\n",
            "\n",
            "Epoch: 5/100, Train Loss: 0.26616527, Test Loss: 0.25099742, Test Accuracy: 0.92900000\n",
            "\n",
            "Epoch: 6/100, Train Loss: 0.24935076, Test Loss: 0.23724255, Test Accuracy: 0.93030000\n",
            "\n",
            "Epoch: 7/100, Train Loss: 0.23317807, Test Loss: 0.21971573, Test Accuracy: 0.93670000\n",
            "\n",
            "Epoch: 8/100, Train Loss: 0.21802042, Test Loss: 0.20748050, Test Accuracy: 0.93950000\n",
            "\n",
            "Epoch: 9/100, Train Loss: 0.20446057, Test Loss: 0.19648775, Test Accuracy: 0.94400000\n",
            "\n",
            "Epoch: 10/100, Train Loss: 0.19211516, Test Loss: 0.18752948, Test Accuracy: 0.94550000\n",
            "\n",
            "Epoch: 11/100, Train Loss: 0.18077453, Test Loss: 0.17528562, Test Accuracy: 0.94810000\n",
            "\n",
            "Epoch: 12/100, Train Loss: 0.17092370, Test Loss: 0.16790927, Test Accuracy: 0.95040000\n",
            "\n",
            "Epoch: 13/100, Train Loss: 0.16191689, Test Loss: 0.16379513, Test Accuracy: 0.95200000\n",
            "\n",
            "Epoch: 14/100, Train Loss: 0.15365610, Test Loss: 0.15285568, Test Accuracy: 0.95560000\n",
            "\n",
            "Epoch: 15/100, Train Loss: 0.14590767, Test Loss: 0.14615983, Test Accuracy: 0.95740000\n",
            "\n",
            "Epoch: 16/100, Train Loss: 0.13913568, Test Loss: 0.14244840, Test Accuracy: 0.95850000\n",
            "\n",
            "Epoch: 17/100, Train Loss: 0.13266358, Test Loss: 0.14170185, Test Accuracy: 0.96030000\n",
            "\n",
            "Epoch: 18/100, Train Loss: 0.12720352, Test Loss: 0.13361660, Test Accuracy: 0.96020000\n",
            "\n",
            "Epoch: 19/100, Train Loss: 0.12135044, Test Loss: 0.12752225, Test Accuracy: 0.96220000\n",
            "\n",
            "Epoch: 20/100, Train Loss: 0.11646536, Test Loss: 0.12607057, Test Accuracy: 0.96240000\n",
            "\n",
            "Epoch: 21/100, Train Loss: 0.11183031, Test Loss: 0.12001220, Test Accuracy: 0.96480000\n",
            "\n",
            "Epoch: 22/100, Train Loss: 0.10735744, Test Loss: 0.11832255, Test Accuracy: 0.96570000\n",
            "\n",
            "Epoch: 23/100, Train Loss: 0.10328804, Test Loss: 0.11341393, Test Accuracy: 0.96600000\n",
            "\n",
            "Epoch: 24/100, Train Loss: 0.09958032, Test Loss: 0.10998703, Test Accuracy: 0.96640000\n",
            "\n",
            "Epoch: 25/100, Train Loss: 0.09603748, Test Loss: 0.10956861, Test Accuracy: 0.96680000\n",
            "\n",
            "Epoch: 26/100, Train Loss: 0.09253288, Test Loss: 0.10481347, Test Accuracy: 0.96630000\n",
            "\n",
            "Epoch: 27/100, Train Loss: 0.08933332, Test Loss: 0.10415195, Test Accuracy: 0.96840000\n",
            "\n",
            "Epoch: 28/100, Train Loss: 0.08623812, Test Loss: 0.10016011, Test Accuracy: 0.96950000\n",
            "\n",
            "Epoch: 29/100, Train Loss: 0.08339271, Test Loss: 0.09990070, Test Accuracy: 0.96900000\n",
            "\n",
            "Epoch: 30/100, Train Loss: 0.08061778, Test Loss: 0.09605066, Test Accuracy: 0.97120000\n",
            "\n",
            "Epoch: 31/100, Train Loss: 0.07803653, Test Loss: 0.09506469, Test Accuracy: 0.97030000\n",
            "\n",
            "Epoch: 32/100, Train Loss: 0.07560446, Test Loss: 0.09460604, Test Accuracy: 0.97080000\n",
            "\n",
            "Epoch: 33/100, Train Loss: 0.07337942, Test Loss: 0.09233915, Test Accuracy: 0.97180000\n",
            "\n",
            "Epoch: 34/100, Train Loss: 0.07083553, Test Loss: 0.09092508, Test Accuracy: 0.97150000\n",
            "\n",
            "Epoch: 35/100, Train Loss: 0.06894293, Test Loss: 0.08861677, Test Accuracy: 0.97270000\n",
            "\n",
            "Epoch: 36/100, Train Loss: 0.06695762, Test Loss: 0.08727464, Test Accuracy: 0.97400000\n",
            "\n",
            "Epoch: 37/100, Train Loss: 0.06499035, Test Loss: 0.08754324, Test Accuracy: 0.97420000\n",
            "\n",
            "Epoch: 38/100, Train Loss: 0.06302706, Test Loss: 0.08521004, Test Accuracy: 0.97400000\n",
            "\n",
            "Epoch: 39/100, Train Loss: 0.06154270, Test Loss: 0.08358016, Test Accuracy: 0.97480000\n",
            "\n",
            "Epoch: 40/100, Train Loss: 0.05956461, Test Loss: 0.08378402, Test Accuracy: 0.97420000\n",
            "\n",
            "Epoch: 41/100, Train Loss: 0.05798657, Test Loss: 0.08113087, Test Accuracy: 0.97500000\n",
            "\n",
            "Epoch: 42/100, Train Loss: 0.05662291, Test Loss: 0.08257957, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 43/100, Train Loss: 0.05483605, Test Loss: 0.08007925, Test Accuracy: 0.97580000\n",
            "\n",
            "Epoch: 44/100, Train Loss: 0.05346793, Test Loss: 0.07985624, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 45/100, Train Loss: 0.05205318, Test Loss: 0.07826346, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 46/100, Train Loss: 0.05058824, Test Loss: 0.07710173, Test Accuracy: 0.97580000\n",
            "\n",
            "Epoch: 47/100, Train Loss: 0.04935784, Test Loss: 0.07650973, Test Accuracy: 0.97650000\n",
            "\n",
            "Epoch: 48/100, Train Loss: 0.04810082, Test Loss: 0.07491782, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 49/100, Train Loss: 0.04682165, Test Loss: 0.07464869, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 50/100, Train Loss: 0.04572831, Test Loss: 0.07456347, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 51/100, Train Loss: 0.04459282, Test Loss: 0.07465163, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 52/100, Train Loss: 0.04356053, Test Loss: 0.07371474, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 53/100, Train Loss: 0.04236630, Test Loss: 0.07365170, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 54/100, Train Loss: 0.04134247, Test Loss: 0.07261654, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 55/100, Train Loss: 0.04038602, Test Loss: 0.07147780, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 56/100, Train Loss: 0.03940429, Test Loss: 0.07429695, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 57/100, Train Loss: 0.03848977, Test Loss: 0.07020154, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 58/100, Train Loss: 0.03759882, Test Loss: 0.07002695, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 59/100, Train Loss: 0.03674124, Test Loss: 0.06965740, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 60/100, Train Loss: 0.03574173, Test Loss: 0.07017102, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 61/100, Train Loss: 0.03505645, Test Loss: 0.06993454, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 62/100, Train Loss: 0.03428161, Test Loss: 0.06893472, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 63/100, Train Loss: 0.03353502, Test Loss: 0.06851647, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 64/100, Train Loss: 0.03255615, Test Loss: 0.06807362, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 65/100, Train Loss: 0.03195318, Test Loss: 0.06772194, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 66/100, Train Loss: 0.03117958, Test Loss: 0.06647006, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 67/100, Train Loss: 0.03051026, Test Loss: 0.06822373, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 68/100, Train Loss: 0.02976387, Test Loss: 0.06648887, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 69/100, Train Loss: 0.02913999, Test Loss: 0.06621383, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 70/100, Train Loss: 0.02846187, Test Loss: 0.06602608, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 71/100, Train Loss: 0.02782687, Test Loss: 0.06571408, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 72/100, Train Loss: 0.02736463, Test Loss: 0.06554699, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 73/100, Train Loss: 0.02680101, Test Loss: 0.06517119, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 74/100, Train Loss: 0.02622134, Test Loss: 0.06565638, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 75/100, Train Loss: 0.02554643, Test Loss: 0.06498554, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 76/100, Train Loss: 0.02506778, Test Loss: 0.06554633, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 77/100, Train Loss: 0.02449388, Test Loss: 0.06477959, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 78/100, Train Loss: 0.02397361, Test Loss: 0.06437958, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 79/100, Train Loss: 0.02353538, Test Loss: 0.06498783, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 80/100, Train Loss: 0.02298833, Test Loss: 0.06529095, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 81/100, Train Loss: 0.02255618, Test Loss: 0.06422379, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 82/100, Train Loss: 0.02207516, Test Loss: 0.06418678, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 83/100, Train Loss: 0.02167836, Test Loss: 0.06398824, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 84/100, Train Loss: 0.02112078, Test Loss: 0.06412238, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 85/100, Train Loss: 0.02080295, Test Loss: 0.06268109, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 86/100, Train Loss: 0.02036936, Test Loss: 0.06336516, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 87/100, Train Loss: 0.02003449, Test Loss: 0.06389769, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 88/100, Train Loss: 0.01966901, Test Loss: 0.06339677, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 89/100, Train Loss: 0.01918168, Test Loss: 0.06239919, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 90/100, Train Loss: 0.01885023, Test Loss: 0.06309534, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 91/100, Train Loss: 0.01846545, Test Loss: 0.06278401, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 92/100, Train Loss: 0.01816425, Test Loss: 0.06321843, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 93/100, Train Loss: 0.01776826, Test Loss: 0.06284865, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 94/100, Train Loss: 0.01750400, Test Loss: 0.06217417, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 95/100, Train Loss: 0.01713925, Test Loss: 0.06281351, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 96/100, Train Loss: 0.01680714, Test Loss: 0.06249272, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 97/100, Train Loss: 0.01649461, Test Loss: 0.06211562, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 98/100, Train Loss: 0.01621167, Test Loss: 0.06210573, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 99/100, Train Loss: 0.01586203, Test Loss: 0.06240110, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 100/100, Train Loss: 0.01555206, Test Loss: 0.06383581, Test Accuracy: 0.98020000\n",
            "[tensor(0.0796, grad_fn=<MeanBackward0>), tensor(0.0909, grad_fn=<MeanBackward0>), tensor(0.1014, grad_fn=<MeanBackward0>), tensor(0.1114, grad_fn=<MeanBackward0>), tensor(0.1225, grad_fn=<MeanBackward0>), tensor(0.1323, grad_fn=<MeanBackward0>), tensor(0.1424, grad_fn=<MeanBackward0>), tensor(0.1510, grad_fn=<MeanBackward0>), tensor(0.1598, grad_fn=<MeanBackward0>), tensor(0.1669, grad_fn=<MeanBackward0>), tensor(0.1733, grad_fn=<MeanBackward0>), tensor(0.1790, grad_fn=<MeanBackward0>), tensor(0.1838, grad_fn=<MeanBackward0>), tensor(0.1887, grad_fn=<MeanBackward0>), tensor(0.1933, grad_fn=<MeanBackward0>), tensor(0.1967, grad_fn=<MeanBackward0>), tensor(0.2001, grad_fn=<MeanBackward0>), tensor(0.2041, grad_fn=<MeanBackward0>), tensor(0.2070, grad_fn=<MeanBackward0>), tensor(0.2096, grad_fn=<MeanBackward0>), tensor(0.2121, grad_fn=<MeanBackward0>), tensor(0.2142, grad_fn=<MeanBackward0>), tensor(0.2167, grad_fn=<MeanBackward0>), tensor(0.2181, grad_fn=<MeanBackward0>), tensor(0.2202, grad_fn=<MeanBackward0>), tensor(0.2222, grad_fn=<MeanBackward0>), tensor(0.2237, grad_fn=<MeanBackward0>), tensor(0.2252, grad_fn=<MeanBackward0>), tensor(0.2267, grad_fn=<MeanBackward0>), tensor(0.2281, grad_fn=<MeanBackward0>), tensor(0.2293, grad_fn=<MeanBackward0>), tensor(0.2302, grad_fn=<MeanBackward0>), tensor(0.2315, grad_fn=<MeanBackward0>), tensor(0.2324, grad_fn=<MeanBackward0>), tensor(0.2334, grad_fn=<MeanBackward0>), tensor(0.2344, grad_fn=<MeanBackward0>), tensor(0.2353, grad_fn=<MeanBackward0>), tensor(0.2359, grad_fn=<MeanBackward0>), tensor(0.2370, grad_fn=<MeanBackward0>), tensor(0.2376, grad_fn=<MeanBackward0>), tensor(0.2381, grad_fn=<MeanBackward0>), tensor(0.2389, grad_fn=<MeanBackward0>), tensor(0.2398, grad_fn=<MeanBackward0>), tensor(0.2403, grad_fn=<MeanBackward0>), tensor(0.2411, grad_fn=<MeanBackward0>), tensor(0.2414, grad_fn=<MeanBackward0>), tensor(0.2421, grad_fn=<MeanBackward0>), tensor(0.2429, grad_fn=<MeanBackward0>), tensor(0.2431, grad_fn=<MeanBackward0>), tensor(0.2436, grad_fn=<MeanBackward0>), tensor(0.2442, grad_fn=<MeanBackward0>), tensor(0.2448, grad_fn=<MeanBackward0>), tensor(0.2452, grad_fn=<MeanBackward0>), tensor(0.2457, grad_fn=<MeanBackward0>), tensor(0.2463, grad_fn=<MeanBackward0>), tensor(0.2466, grad_fn=<MeanBackward0>), tensor(0.2469, grad_fn=<MeanBackward0>), tensor(0.2473, grad_fn=<MeanBackward0>), tensor(0.2475, grad_fn=<MeanBackward0>), tensor(0.2482, grad_fn=<MeanBackward0>), tensor(0.2488, grad_fn=<MeanBackward0>), tensor(0.2489, grad_fn=<MeanBackward0>), tensor(0.2493, grad_fn=<MeanBackward0>), tensor(0.2495, grad_fn=<MeanBackward0>), tensor(0.2498, grad_fn=<MeanBackward0>), tensor(0.2500, grad_fn=<MeanBackward0>), tensor(0.2505, grad_fn=<MeanBackward0>), tensor(0.2510, grad_fn=<MeanBackward0>), tensor(0.2511, grad_fn=<MeanBackward0>), tensor(0.2514, grad_fn=<MeanBackward0>), tensor(0.2518, grad_fn=<MeanBackward0>), tensor(0.2521, grad_fn=<MeanBackward0>), tensor(0.2523, grad_fn=<MeanBackward0>), tensor(0.2526, grad_fn=<MeanBackward0>), tensor(0.2530, grad_fn=<MeanBackward0>), tensor(0.2530, grad_fn=<MeanBackward0>), tensor(0.2533, grad_fn=<MeanBackward0>), tensor(0.2535, grad_fn=<MeanBackward0>), tensor(0.2536, grad_fn=<MeanBackward0>), tensor(0.2540, grad_fn=<MeanBackward0>), tensor(0.2542, grad_fn=<MeanBackward0>), tensor(0.2545, grad_fn=<MeanBackward0>), tensor(0.2547, grad_fn=<MeanBackward0>), tensor(0.2548, grad_fn=<MeanBackward0>), tensor(0.2550, grad_fn=<MeanBackward0>), tensor(0.2552, grad_fn=<MeanBackward0>), tensor(0.2555, grad_fn=<MeanBackward0>), tensor(0.2559, grad_fn=<MeanBackward0>), tensor(0.2561, grad_fn=<MeanBackward0>), tensor(0.2563, grad_fn=<MeanBackward0>), tensor(0.2565, grad_fn=<MeanBackward0>), tensor(0.2566, grad_fn=<MeanBackward0>), tensor(0.2569, grad_fn=<MeanBackward0>), tensor(0.2569, grad_fn=<MeanBackward0>), tensor(0.2569, grad_fn=<MeanBackward0>), tensor(0.2571, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2577, grad_fn=<MeanBackward0>), tensor(0.2578, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.0795522928237915, 0.09087848663330078, 0.10135768353939056, 0.11141665279865265, 0.12254133075475693, 0.13231950998306274, 0.14240272343158722, 0.15104065835475922, 0.15978318452835083, 0.16688665747642517, 0.17325986921787262, 0.1790071278810501, 0.18383441865444183, 0.18872973322868347, 0.1932518035173416, 0.1966683715581894, 0.20006532967090607, 0.20412951707839966, 0.2069847583770752, 0.2096051275730133, 0.2120870053768158, 0.21422238647937775, 0.21674735844135284, 0.21813330054283142, 0.22019580006599426, 0.2221708744764328, 0.2236543744802475, 0.22519780695438385, 0.22669249773025513, 0.2281220704317093, 0.22926808893680573, 0.23024821281433105, 0.23150160908699036, 0.2323564887046814, 0.2333805114030838, 0.23441721498966217, 0.23528319597244263, 0.23589521646499634, 0.23695851862430573, 0.23761123418807983, 0.23810862004756927, 0.23894807696342468, 0.23976287245750427, 0.24033667147159576, 0.24107614159584045, 0.24135781824588776, 0.2420894056558609, 0.24290522933006287, 0.24310795962810516, 0.24364213645458221, 0.24424509704113007, 0.2447679191827774, 0.24523183703422546, 0.245666965842247, 0.24626359343528748, 0.24657051265239716, 0.24685917794704437, 0.24731525778770447, 0.2475443184375763, 0.24821893870830536, 0.2488355040550232, 0.2488846629858017, 0.24926118552684784, 0.24951840937137604, 0.2497548758983612, 0.25004419684410095, 0.25054365396499634, 0.25095394253730774, 0.251113623380661, 0.2514471113681793, 0.2518472671508789, 0.2520809471607208, 0.25230735540390015, 0.2525533139705658, 0.2529665231704712, 0.25302115082740784, 0.25325527787208557, 0.2534559667110443, 0.25358521938323975, 0.25396549701690674, 0.25420403480529785, 0.25448766350746155, 0.2546670436859131, 0.25484585762023926, 0.2549927830696106, 0.2552280128002167, 0.2555388808250427, 0.2559049427509308, 0.25609126687049866, 0.25629109144210815, 0.2565476596355438, 0.2566184401512146, 0.2568742334842682, 0.2569165825843811, 0.25687703490257263, 0.25710204243659973, 0.25745514035224915, 0.2575218677520752, 0.2576519548892975, 0.2578125596046448]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/100, Train Loss: 0.43016725, Test Loss: 0.22552646, Test Accuracy: 0.93350000\n",
            "\n",
            "Epoch: 2/100, Train Loss: 0.19592043, Test Loss: 0.16696855, Test Accuracy: 0.95170000\n",
            "\n",
            "Epoch: 3/100, Train Loss: 0.14392484, Test Loss: 0.13296503, Test Accuracy: 0.95990000\n",
            "\n",
            "Epoch: 4/100, Train Loss: 0.11038528, Test Loss: 0.10825833, Test Accuracy: 0.96710000\n",
            "\n",
            "Epoch: 5/100, Train Loss: 0.08766113, Test Loss: 0.09210007, Test Accuracy: 0.97040000\n",
            "\n",
            "Epoch: 6/100, Train Loss: 0.06998499, Test Loss: 0.08457528, Test Accuracy: 0.97440000\n",
            "\n",
            "Epoch: 7/100, Train Loss: 0.05741045, Test Loss: 0.07907838, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 8/100, Train Loss: 0.04676618, Test Loss: 0.07496855, Test Accuracy: 0.97680000\n",
            "\n",
            "Epoch: 9/100, Train Loss: 0.03774127, Test Loss: 0.07073184, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 10/100, Train Loss: 0.03073235, Test Loss: 0.06709821, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 11/100, Train Loss: 0.02513482, Test Loss: 0.06555179, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 12/100, Train Loss: 0.02061786, Test Loss: 0.07018048, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 13/100, Train Loss: 0.01670443, Test Loss: 0.06155436, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 14/100, Train Loss: 0.01293446, Test Loss: 0.07170856, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 15/100, Train Loss: 0.01030129, Test Loss: 0.06436206, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 16/100, Train Loss: 0.00836587, Test Loss: 0.06434070, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 17/100, Train Loss: 0.00662668, Test Loss: 0.06322916, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 18/100, Train Loss: 0.00531257, Test Loss: 0.06969858, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 19/100, Train Loss: 0.00422188, Test Loss: 0.06677818, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 20/100, Train Loss: 0.00365408, Test Loss: 0.06737795, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 21/100, Train Loss: 0.00253597, Test Loss: 0.06921397, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 22/100, Train Loss: 0.00222501, Test Loss: 0.07656676, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 23/100, Train Loss: 0.00201128, Test Loss: 0.07241505, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 24/100, Train Loss: 0.00161877, Test Loss: 0.07427554, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 25/100, Train Loss: 0.00114405, Test Loss: 0.07446493, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 26/100, Train Loss: 0.00066819, Test Loss: 0.07845502, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 27/100, Train Loss: 0.00169116, Test Loss: 0.07958694, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 28/100, Train Loss: 0.00057503, Test Loss: 0.07914426, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 29/100, Train Loss: 0.00037451, Test Loss: 0.08268214, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 30/100, Train Loss: 0.00139398, Test Loss: 0.08195109, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 31/100, Train Loss: 0.00028307, Test Loss: 0.08118538, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 32/100, Train Loss: 0.00023543, Test Loss: 0.08141463, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 33/100, Train Loss: 0.00152973, Test Loss: 0.09138743, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 34/100, Train Loss: 0.00050676, Test Loss: 0.08607968, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 35/100, Train Loss: 0.00015970, Test Loss: 0.08674745, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 36/100, Train Loss: 0.00012415, Test Loss: 0.08655945, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 37/100, Train Loss: 0.00010507, Test Loss: 0.08723036, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 38/100, Train Loss: 0.00021694, Test Loss: 0.10253410, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 39/100, Train Loss: 0.00154113, Test Loss: 0.09178668, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 40/100, Train Loss: 0.00010611, Test Loss: 0.09070329, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 41/100, Train Loss: 0.00007725, Test Loss: 0.09004830, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 42/100, Train Loss: 0.00006533, Test Loss: 0.08972635, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 43/100, Train Loss: 0.00151726, Test Loss: 0.10197719, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 44/100, Train Loss: 0.00057764, Test Loss: 0.09350388, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 45/100, Train Loss: 0.00012507, Test Loss: 0.09312347, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 46/100, Train Loss: 0.00005803, Test Loss: 0.09264811, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 47/100, Train Loss: 0.00004808, Test Loss: 0.09440707, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 48/100, Train Loss: 0.00138063, Test Loss: 0.09797874, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 49/100, Train Loss: 0.00022275, Test Loss: 0.09348464, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 50/100, Train Loss: 0.00006687, Test Loss: 0.09231300, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 51/100, Train Loss: 0.00004601, Test Loss: 0.09281939, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 52/100, Train Loss: 0.00003663, Test Loss: 0.09272780, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 53/100, Train Loss: 0.00003041, Test Loss: 0.09383198, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 54/100, Train Loss: 0.00002641, Test Loss: 0.09511300, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 55/100, Train Loss: 0.00119308, Test Loss: 0.10221008, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 56/100, Train Loss: 0.00012468, Test Loss: 0.09840238, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 57/100, Train Loss: 0.00003931, Test Loss: 0.09870418, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 58/100, Train Loss: 0.00002832, Test Loss: 0.09895368, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 59/100, Train Loss: 0.00002212, Test Loss: 0.09958865, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 60/100, Train Loss: 0.00001836, Test Loss: 0.10013855, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 61/100, Train Loss: 0.00001559, Test Loss: 0.10103651, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 62/100, Train Loss: 0.00001296, Test Loss: 0.10078314, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 63/100, Train Loss: 0.00168299, Test Loss: 0.11383732, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 64/100, Train Loss: 0.00026652, Test Loss: 0.11665488, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 65/100, Train Loss: 0.00007902, Test Loss: 0.10341403, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 66/100, Train Loss: 0.00002231, Test Loss: 0.10304577, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 67/100, Train Loss: 0.00001649, Test Loss: 0.10313120, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 68/100, Train Loss: 0.00001282, Test Loss: 0.10321133, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 69/100, Train Loss: 0.00001016, Test Loss: 0.10353598, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 70/100, Train Loss: 0.00000827, Test Loss: 0.10481500, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 71/100, Train Loss: 0.00000681, Test Loss: 0.10402627, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 72/100, Train Loss: 0.00000634, Test Loss: 0.10455889, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 73/100, Train Loss: 0.00124539, Test Loss: 0.10785265, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 74/100, Train Loss: 0.00011008, Test Loss: 0.10519064, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 75/100, Train Loss: 0.00002087, Test Loss: 0.10591280, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 76/100, Train Loss: 0.00001318, Test Loss: 0.10576076, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 77/100, Train Loss: 0.00000985, Test Loss: 0.10574266, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 78/100, Train Loss: 0.00000757, Test Loss: 0.10628624, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 79/100, Train Loss: 0.00000593, Test Loss: 0.10684659, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 80/100, Train Loss: 0.00000467, Test Loss: 0.10786090, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 81/100, Train Loss: 0.00000381, Test Loss: 0.10942571, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 82/100, Train Loss: 0.00000349, Test Loss: 0.11230085, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 83/100, Train Loss: 0.00107196, Test Loss: 0.11779482, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 84/100, Train Loss: 0.00003216, Test Loss: 0.11557264, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 85/100, Train Loss: 0.00001087, Test Loss: 0.11503476, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 86/100, Train Loss: 0.00000779, Test Loss: 0.11502099, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 87/100, Train Loss: 0.00000583, Test Loss: 0.11535058, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 88/100, Train Loss: 0.00000447, Test Loss: 0.11528252, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 89/100, Train Loss: 0.00000346, Test Loss: 0.11548574, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 90/100, Train Loss: 0.00000274, Test Loss: 0.11608356, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 91/100, Train Loss: 0.00000220, Test Loss: 0.11753611, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 92/100, Train Loss: 0.00000175, Test Loss: 0.11666814, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 93/100, Train Loss: 0.00000145, Test Loss: 0.11763378, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 94/100, Train Loss: 0.00124012, Test Loss: 0.13205600, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 95/100, Train Loss: 0.00011089, Test Loss: 0.12599033, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 96/100, Train Loss: 0.00001233, Test Loss: 0.12331403, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 97/100, Train Loss: 0.00000675, Test Loss: 0.12246870, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 98/100, Train Loss: 0.00000493, Test Loss: 0.12173075, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 99/100, Train Loss: 0.00000363, Test Loss: 0.12161655, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 100/100, Train Loss: 0.00000273, Test Loss: 0.12156520, Test Accuracy: 0.98120000\n",
            "[tensor(0.2044, grad_fn=<MeanBackward0>), tensor(0.2322, grad_fn=<MeanBackward0>), tensor(0.2472, grad_fn=<MeanBackward0>), tensor(0.2599, grad_fn=<MeanBackward0>), tensor(0.2638, grad_fn=<MeanBackward0>), tensor(0.2669, grad_fn=<MeanBackward0>), tensor(0.2743, grad_fn=<MeanBackward0>), tensor(0.2772, grad_fn=<MeanBackward0>), tensor(0.2796, grad_fn=<MeanBackward0>), tensor(0.2797, grad_fn=<MeanBackward0>), tensor(0.2824, grad_fn=<MeanBackward0>), tensor(0.2864, grad_fn=<MeanBackward0>), tensor(0.2849, grad_fn=<MeanBackward0>), tensor(0.2858, grad_fn=<MeanBackward0>), tensor(0.2864, grad_fn=<MeanBackward0>), tensor(0.2885, grad_fn=<MeanBackward0>), tensor(0.2904, grad_fn=<MeanBackward0>), tensor(0.2896, grad_fn=<MeanBackward0>), tensor(0.2900, grad_fn=<MeanBackward0>), tensor(0.2938, grad_fn=<MeanBackward0>), tensor(0.2915, grad_fn=<MeanBackward0>), tensor(0.2936, grad_fn=<MeanBackward0>), tensor(0.2963, grad_fn=<MeanBackward0>), tensor(0.2981, grad_fn=<MeanBackward0>), tensor(0.2951, grad_fn=<MeanBackward0>), tensor(0.2941, grad_fn=<MeanBackward0>), tensor(0.2993, grad_fn=<MeanBackward0>), tensor(0.2961, grad_fn=<MeanBackward0>), tensor(0.2964, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2980, grad_fn=<MeanBackward0>), tensor(0.2969, grad_fn=<MeanBackward0>), tensor(0.3080, grad_fn=<MeanBackward0>), tensor(0.3013, grad_fn=<MeanBackward0>), tensor(0.2995, grad_fn=<MeanBackward0>), tensor(0.2984, grad_fn=<MeanBackward0>), tensor(0.2981, grad_fn=<MeanBackward0>), tensor(0.3069, grad_fn=<MeanBackward0>), tensor(0.3020, grad_fn=<MeanBackward0>), tensor(0.3010, grad_fn=<MeanBackward0>), tensor(0.3004, grad_fn=<MeanBackward0>), tensor(0.2998, grad_fn=<MeanBackward0>), tensor(0.3084, grad_fn=<MeanBackward0>), tensor(0.3050, grad_fn=<MeanBackward0>), tensor(0.3039, grad_fn=<MeanBackward0>), tensor(0.3030, grad_fn=<MeanBackward0>), tensor(0.3018, grad_fn=<MeanBackward0>), tensor(0.3078, grad_fn=<MeanBackward0>), tensor(0.3047, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3029, grad_fn=<MeanBackward0>), tensor(0.3029, grad_fn=<MeanBackward0>), tensor(0.3022, grad_fn=<MeanBackward0>), tensor(0.3023, grad_fn=<MeanBackward0>), tensor(0.3162, grad_fn=<MeanBackward0>), tensor(0.3110, grad_fn=<MeanBackward0>), tensor(0.3091, grad_fn=<MeanBackward0>), tensor(0.3074, grad_fn=<MeanBackward0>), tensor(0.3057, grad_fn=<MeanBackward0>), tensor(0.3047, grad_fn=<MeanBackward0>), tensor(0.3037, grad_fn=<MeanBackward0>), tensor(0.3036, grad_fn=<MeanBackward0>), tensor(0.3099, grad_fn=<MeanBackward0>), tensor(0.3098, grad_fn=<MeanBackward0>), tensor(0.3089, grad_fn=<MeanBackward0>), tensor(0.3084, grad_fn=<MeanBackward0>), tensor(0.3076, grad_fn=<MeanBackward0>), tensor(0.3067, grad_fn=<MeanBackward0>), tensor(0.3057, grad_fn=<MeanBackward0>), tensor(0.3047, grad_fn=<MeanBackward0>), tensor(0.3037, grad_fn=<MeanBackward0>), tensor(0.3038, grad_fn=<MeanBackward0>), tensor(0.3111, grad_fn=<MeanBackward0>), tensor(0.3099, grad_fn=<MeanBackward0>), tensor(0.3094, grad_fn=<MeanBackward0>), tensor(0.3087, grad_fn=<MeanBackward0>), tensor(0.3079, grad_fn=<MeanBackward0>), tensor(0.3071, grad_fn=<MeanBackward0>), tensor(0.3064, grad_fn=<MeanBackward0>), tensor(0.3059, grad_fn=<MeanBackward0>), tensor(0.3052, grad_fn=<MeanBackward0>), tensor(0.3052, grad_fn=<MeanBackward0>), tensor(0.3118, grad_fn=<MeanBackward0>), tensor(0.3104, grad_fn=<MeanBackward0>), tensor(0.3100, grad_fn=<MeanBackward0>), tensor(0.3094, grad_fn=<MeanBackward0>), tensor(0.3087, grad_fn=<MeanBackward0>), tensor(0.3081, grad_fn=<MeanBackward0>), tensor(0.3075, grad_fn=<MeanBackward0>), tensor(0.3070, grad_fn=<MeanBackward0>), tensor(0.3066, grad_fn=<MeanBackward0>), tensor(0.3060, grad_fn=<MeanBackward0>), tensor(0.3063, grad_fn=<MeanBackward0>), tensor(0.3107, grad_fn=<MeanBackward0>), tensor(0.3115, grad_fn=<MeanBackward0>), tensor(0.3113, grad_fn=<MeanBackward0>), tensor(0.3109, grad_fn=<MeanBackward0>), tensor(0.3105, grad_fn=<MeanBackward0>), tensor(0.3100, grad_fn=<MeanBackward0>), tensor(0.3095, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.20438428223133087, 0.23218537867069244, 0.2472435086965561, 0.2599426805973053, 0.263775110244751, 0.26693782210350037, 0.27425339818000793, 0.2772102653980255, 0.2795628011226654, 0.27966538071632385, 0.2824040949344635, 0.28642866015434265, 0.2849336862564087, 0.28580036759376526, 0.28635647892951965, 0.28851351141929626, 0.29036206007003784, 0.2896218001842499, 0.2899804711341858, 0.29383599758148193, 0.29145658016204834, 0.2935582399368286, 0.29630452394485474, 0.29814547300338745, 0.2951444685459137, 0.2940784990787506, 0.2992947995662689, 0.29609256982803345, 0.29642176628112793, 0.2986391484737396, 0.29803919792175293, 0.29689472913742065, 0.3079741597175598, 0.3012596070766449, 0.2995442748069763, 0.29842016100883484, 0.2981041371822357, 0.3069142699241638, 0.3020375967025757, 0.30099228024482727, 0.3004476726055145, 0.2998198866844177, 0.30840757489204407, 0.30502456426620483, 0.3038838803768158, 0.3030150234699249, 0.30183783173561096, 0.3078153729438782, 0.30465713143348694, 0.3035998046398163, 0.30293551087379456, 0.3028884530067444, 0.3022446930408478, 0.3022926151752472, 0.3161628544330597, 0.31100472807884216, 0.3091067969799042, 0.30738526582717896, 0.30565643310546875, 0.30469459295272827, 0.3036718964576721, 0.3035503327846527, 0.3099493682384491, 0.30979830026626587, 0.30889734625816345, 0.30840829014778137, 0.3075855076313019, 0.3067030906677246, 0.30571380257606506, 0.30468592047691345, 0.30366578698158264, 0.30375251173973083, 0.3111361265182495, 0.3099234998226166, 0.30939844250679016, 0.3086913526058197, 0.30785760283470154, 0.30711784958839417, 0.3063806891441345, 0.30585575103759766, 0.3051871955394745, 0.30523014068603516, 0.31182149052619934, 0.31044384837150574, 0.3100137710571289, 0.3094225823879242, 0.30869415402412415, 0.3081395626068115, 0.3074553608894348, 0.30699652433395386, 0.3066261112689972, 0.3059980273246765, 0.30625638365745544, 0.31073376536369324, 0.31146395206451416, 0.31128719449043274, 0.31091734766960144, 0.31049662828445435, 0.3099634051322937, 0.3095025420188904]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-06\n",
            "    lr: 1.0\n",
            "    rho: 0.9\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/100, Train Loss: 0.43482734, Test Loss: 0.25056328, Test Accuracy: 0.92680000\n",
            "\n",
            "Epoch: 2/100, Train Loss: 0.21552304, Test Loss: 0.17478676, Test Accuracy: 0.94670000\n",
            "\n",
            "Epoch: 3/100, Train Loss: 0.15641612, Test Loss: 0.13820602, Test Accuracy: 0.95740000\n",
            "\n",
            "Epoch: 4/100, Train Loss: 0.12157690, Test Loss: 0.11924306, Test Accuracy: 0.96300000\n",
            "\n",
            "Epoch: 5/100, Train Loss: 0.09919056, Test Loss: 0.09831684, Test Accuracy: 0.96860000\n",
            "\n",
            "Epoch: 6/100, Train Loss: 0.08336800, Test Loss: 0.09016022, Test Accuracy: 0.97190000\n",
            "\n",
            "Epoch: 7/100, Train Loss: 0.07088853, Test Loss: 0.08719795, Test Accuracy: 0.97320000\n",
            "\n",
            "Epoch: 8/100, Train Loss: 0.06153279, Test Loss: 0.07796417, Test Accuracy: 0.97520000\n",
            "\n",
            "Epoch: 9/100, Train Loss: 0.05413196, Test Loss: 0.07215949, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 10/100, Train Loss: 0.04688475, Test Loss: 0.07165624, Test Accuracy: 0.97650000\n",
            "\n",
            "Epoch: 11/100, Train Loss: 0.04203057, Test Loss: 0.07021961, Test Accuracy: 0.97680000\n",
            "\n",
            "Epoch: 12/100, Train Loss: 0.03695013, Test Loss: 0.07518681, Test Accuracy: 0.97650000\n",
            "\n",
            "Epoch: 13/100, Train Loss: 0.03292638, Test Loss: 0.06627988, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 14/100, Train Loss: 0.02905201, Test Loss: 0.06293267, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 15/100, Train Loss: 0.02590495, Test Loss: 0.06158078, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 16/100, Train Loss: 0.02330130, Test Loss: 0.06297808, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 17/100, Train Loss: 0.02085337, Test Loss: 0.05992444, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 18/100, Train Loss: 0.01860578, Test Loss: 0.06213609, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 19/100, Train Loss: 0.01655125, Test Loss: 0.06208906, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 20/100, Train Loss: 0.01466604, Test Loss: 0.05816114, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 21/100, Train Loss: 0.01332531, Test Loss: 0.06010362, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 22/100, Train Loss: 0.01189256, Test Loss: 0.05997108, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 23/100, Train Loss: 0.01074991, Test Loss: 0.06014585, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 24/100, Train Loss: 0.00972292, Test Loss: 0.06088047, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 25/100, Train Loss: 0.00874879, Test Loss: 0.06209752, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 26/100, Train Loss: 0.00798673, Test Loss: 0.06282990, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 27/100, Train Loss: 0.00720308, Test Loss: 0.05958402, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 28/100, Train Loss: 0.00660240, Test Loss: 0.06011285, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 29/100, Train Loss: 0.00585278, Test Loss: 0.05981551, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 30/100, Train Loss: 0.00549971, Test Loss: 0.06033825, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 31/100, Train Loss: 0.00497890, Test Loss: 0.06167402, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 32/100, Train Loss: 0.00459431, Test Loss: 0.06092216, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 33/100, Train Loss: 0.00422030, Test Loss: 0.06224211, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 34/100, Train Loss: 0.00391374, Test Loss: 0.06023347, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 35/100, Train Loss: 0.00355816, Test Loss: 0.06167564, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 36/100, Train Loss: 0.00332572, Test Loss: 0.06170639, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 37/100, Train Loss: 0.00307764, Test Loss: 0.06269425, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 38/100, Train Loss: 0.00282516, Test Loss: 0.06239437, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 39/100, Train Loss: 0.00265662, Test Loss: 0.06236873, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 40/100, Train Loss: 0.00248192, Test Loss: 0.06255401, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 41/100, Train Loss: 0.00232812, Test Loss: 0.06271843, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 42/100, Train Loss: 0.00217415, Test Loss: 0.06312888, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 43/100, Train Loss: 0.00204353, Test Loss: 0.06352514, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 44/100, Train Loss: 0.00195104, Test Loss: 0.06366121, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 45/100, Train Loss: 0.00185330, Test Loss: 0.06410796, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 46/100, Train Loss: 0.00175034, Test Loss: 0.06405796, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 47/100, Train Loss: 0.00168147, Test Loss: 0.06397212, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 48/100, Train Loss: 0.00158922, Test Loss: 0.06415348, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 49/100, Train Loss: 0.00150504, Test Loss: 0.06462129, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 50/100, Train Loss: 0.00144744, Test Loss: 0.06444899, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 51/100, Train Loss: 0.00137095, Test Loss: 0.06473418, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 52/100, Train Loss: 0.00132612, Test Loss: 0.06485539, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 53/100, Train Loss: 0.00127853, Test Loss: 0.06486067, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 54/100, Train Loss: 0.00122862, Test Loss: 0.06482370, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 55/100, Train Loss: 0.00117929, Test Loss: 0.06492565, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 56/100, Train Loss: 0.00113925, Test Loss: 0.06514036, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 57/100, Train Loss: 0.00110386, Test Loss: 0.06574180, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 58/100, Train Loss: 0.00106020, Test Loss: 0.06624755, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 59/100, Train Loss: 0.00102670, Test Loss: 0.06575526, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 60/100, Train Loss: 0.00099993, Test Loss: 0.06535941, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 61/100, Train Loss: 0.00097288, Test Loss: 0.06594782, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 62/100, Train Loss: 0.00093574, Test Loss: 0.06629929, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 63/100, Train Loss: 0.00091178, Test Loss: 0.06642192, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 64/100, Train Loss: 0.00088413, Test Loss: 0.06631079, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 65/100, Train Loss: 0.00085602, Test Loss: 0.06661142, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 66/100, Train Loss: 0.00083457, Test Loss: 0.06634535, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 67/100, Train Loss: 0.00081456, Test Loss: 0.06706424, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 68/100, Train Loss: 0.00079161, Test Loss: 0.06695624, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 69/100, Train Loss: 0.00076887, Test Loss: 0.06708340, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 70/100, Train Loss: 0.00075089, Test Loss: 0.06720122, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 71/100, Train Loss: 0.00073691, Test Loss: 0.06737289, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 72/100, Train Loss: 0.00071630, Test Loss: 0.06761748, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 73/100, Train Loss: 0.00070376, Test Loss: 0.06761112, Test Accuracy: 0.98260000\n",
            "\n",
            "Epoch: 74/100, Train Loss: 0.00068733, Test Loss: 0.06740224, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 75/100, Train Loss: 0.00067360, Test Loss: 0.06788946, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 76/100, Train Loss: 0.00065809, Test Loss: 0.06771993, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 77/100, Train Loss: 0.00064293, Test Loss: 0.06770645, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 78/100, Train Loss: 0.00062981, Test Loss: 0.06786575, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 79/100, Train Loss: 0.00061527, Test Loss: 0.06816718, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 80/100, Train Loss: 0.00060227, Test Loss: 0.06861351, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 81/100, Train Loss: 0.00059230, Test Loss: 0.06794028, Test Accuracy: 0.98250000\n",
            "\n",
            "Epoch: 82/100, Train Loss: 0.00058108, Test Loss: 0.06818315, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 83/100, Train Loss: 0.00057032, Test Loss: 0.06828025, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 84/100, Train Loss: 0.00056030, Test Loss: 0.06867325, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 85/100, Train Loss: 0.00054871, Test Loss: 0.06919018, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 86/100, Train Loss: 0.00053701, Test Loss: 0.06869586, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 87/100, Train Loss: 0.00052691, Test Loss: 0.06884092, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 88/100, Train Loss: 0.00052073, Test Loss: 0.06879113, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 89/100, Train Loss: 0.00051154, Test Loss: 0.06923882, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 90/100, Train Loss: 0.00050180, Test Loss: 0.06899088, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 91/100, Train Loss: 0.00049362, Test Loss: 0.06900522, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 92/100, Train Loss: 0.00048474, Test Loss: 0.06919745, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 93/100, Train Loss: 0.00047678, Test Loss: 0.06946677, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 94/100, Train Loss: 0.00046831, Test Loss: 0.06933362, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 95/100, Train Loss: 0.00046080, Test Loss: 0.06960107, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 96/100, Train Loss: 0.00045528, Test Loss: 0.06966458, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 97/100, Train Loss: 0.00044851, Test Loss: 0.06991868, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 98/100, Train Loss: 0.00044148, Test Loss: 0.06980308, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 99/100, Train Loss: 0.00043410, Test Loss: 0.06983349, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 100/100, Train Loss: 0.00042754, Test Loss: 0.06997724, Test Accuracy: 0.98230000\n",
            "[tensor(0.2028, grad_fn=<MeanBackward0>), tensor(0.2540, grad_fn=<MeanBackward0>), tensor(0.2806, grad_fn=<MeanBackward0>), tensor(0.2913, grad_fn=<MeanBackward0>), tensor(0.2996, grad_fn=<MeanBackward0>), tensor(0.3083, grad_fn=<MeanBackward0>), tensor(0.3130, grad_fn=<MeanBackward0>), tensor(0.3171, grad_fn=<MeanBackward0>), tensor(0.3220, grad_fn=<MeanBackward0>), tensor(0.3228, grad_fn=<MeanBackward0>), tensor(0.3270, grad_fn=<MeanBackward0>), tensor(0.3274, grad_fn=<MeanBackward0>), tensor(0.3277, grad_fn=<MeanBackward0>), tensor(0.3261, grad_fn=<MeanBackward0>), tensor(0.3258, grad_fn=<MeanBackward0>), tensor(0.3286, grad_fn=<MeanBackward0>), tensor(0.3286, grad_fn=<MeanBackward0>), tensor(0.3290, grad_fn=<MeanBackward0>), tensor(0.3291, grad_fn=<MeanBackward0>), tensor(0.3284, grad_fn=<MeanBackward0>), tensor(0.3275, grad_fn=<MeanBackward0>), tensor(0.3276, grad_fn=<MeanBackward0>), tensor(0.3274, grad_fn=<MeanBackward0>), tensor(0.3284, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3266, grad_fn=<MeanBackward0>), tensor(0.3275, grad_fn=<MeanBackward0>), tensor(0.3268, grad_fn=<MeanBackward0>), tensor(0.3271, grad_fn=<MeanBackward0>), tensor(0.3268, grad_fn=<MeanBackward0>), tensor(0.3269, grad_fn=<MeanBackward0>), tensor(0.3280, grad_fn=<MeanBackward0>), tensor(0.3265, grad_fn=<MeanBackward0>), tensor(0.3277, grad_fn=<MeanBackward0>), tensor(0.3267, grad_fn=<MeanBackward0>), tensor(0.3269, grad_fn=<MeanBackward0>), tensor(0.3268, grad_fn=<MeanBackward0>), tensor(0.3272, grad_fn=<MeanBackward0>), tensor(0.3274, grad_fn=<MeanBackward0>), tensor(0.3268, grad_fn=<MeanBackward0>), tensor(0.3278, grad_fn=<MeanBackward0>), tensor(0.3268, grad_fn=<MeanBackward0>), tensor(0.3272, grad_fn=<MeanBackward0>), tensor(0.3269, grad_fn=<MeanBackward0>), tensor(0.3274, grad_fn=<MeanBackward0>), tensor(0.3275, grad_fn=<MeanBackward0>), tensor(0.3277, grad_fn=<MeanBackward0>), tensor(0.3275, grad_fn=<MeanBackward0>), tensor(0.3277, grad_fn=<MeanBackward0>), tensor(0.3274, grad_fn=<MeanBackward0>), tensor(0.3276, grad_fn=<MeanBackward0>), tensor(0.3278, grad_fn=<MeanBackward0>), tensor(0.3278, grad_fn=<MeanBackward0>), tensor(0.3280, grad_fn=<MeanBackward0>), tensor(0.3280, grad_fn=<MeanBackward0>), tensor(0.3282, grad_fn=<MeanBackward0>), tensor(0.3280, grad_fn=<MeanBackward0>), tensor(0.3281, grad_fn=<MeanBackward0>), tensor(0.3284, grad_fn=<MeanBackward0>), tensor(0.3286, grad_fn=<MeanBackward0>), tensor(0.3285, grad_fn=<MeanBackward0>), tensor(0.3285, grad_fn=<MeanBackward0>), tensor(0.3287, grad_fn=<MeanBackward0>), tensor(0.3288, grad_fn=<MeanBackward0>), tensor(0.3288, grad_fn=<MeanBackward0>), tensor(0.3289, grad_fn=<MeanBackward0>), tensor(0.3289, grad_fn=<MeanBackward0>), tensor(0.3289, grad_fn=<MeanBackward0>), tensor(0.3287, grad_fn=<MeanBackward0>), tensor(0.3289, grad_fn=<MeanBackward0>), tensor(0.3292, grad_fn=<MeanBackward0>), tensor(0.3292, grad_fn=<MeanBackward0>), tensor(0.3291, grad_fn=<MeanBackward0>), tensor(0.3292, grad_fn=<MeanBackward0>), tensor(0.3293, grad_fn=<MeanBackward0>), tensor(0.3294, grad_fn=<MeanBackward0>), tensor(0.3294, grad_fn=<MeanBackward0>), tensor(0.3294, grad_fn=<MeanBackward0>), tensor(0.3295, grad_fn=<MeanBackward0>), tensor(0.3298, grad_fn=<MeanBackward0>), tensor(0.3296, grad_fn=<MeanBackward0>), tensor(0.3298, grad_fn=<MeanBackward0>), tensor(0.3297, grad_fn=<MeanBackward0>), tensor(0.3298, grad_fn=<MeanBackward0>), tensor(0.3299, grad_fn=<MeanBackward0>), tensor(0.3299, grad_fn=<MeanBackward0>), tensor(0.3299, grad_fn=<MeanBackward0>), tensor(0.3299, grad_fn=<MeanBackward0>), tensor(0.3300, grad_fn=<MeanBackward0>), tensor(0.3300, grad_fn=<MeanBackward0>), tensor(0.3301, grad_fn=<MeanBackward0>), tensor(0.3301, grad_fn=<MeanBackward0>), tensor(0.3302, grad_fn=<MeanBackward0>), tensor(0.3301, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3302, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3304, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.2028178870677948, 0.25404423475265503, 0.2805868685245514, 0.2913151681423187, 0.2995907664299011, 0.30828145146369934, 0.31296297907829285, 0.31714025139808655, 0.3219888210296631, 0.322830468416214, 0.32704800367355347, 0.3274392783641815, 0.3277331590652466, 0.3260919153690338, 0.3257737159729004, 0.3285520374774933, 0.3285638093948364, 0.328978031873703, 0.3290500044822693, 0.32837212085723877, 0.32749810814857483, 0.327630877494812, 0.32743287086486816, 0.32841256260871887, 0.32726937532424927, 0.326596736907959, 0.327540785074234, 0.32678136229515076, 0.3271014988422394, 0.32682862877845764, 0.3268653452396393, 0.32802456617355347, 0.32645806670188904, 0.3277241587638855, 0.3267463743686676, 0.32694771885871887, 0.32681918144226074, 0.32717347145080566, 0.3274465799331665, 0.3268373906612396, 0.32780516147613525, 0.32677730917930603, 0.3271656632423401, 0.3269147276878357, 0.3274418115615845, 0.32752078771591187, 0.32767364382743835, 0.3275425434112549, 0.3277435898780823, 0.3273734748363495, 0.3276451528072357, 0.3278162479400635, 0.3278065621852875, 0.3279575705528259, 0.3280107378959656, 0.3282259702682495, 0.3280436396598816, 0.3280843496322632, 0.32844823598861694, 0.3286215662956238, 0.3285253047943115, 0.32852739095687866, 0.3287215530872345, 0.32877135276794434, 0.3288410007953644, 0.3288811147212982, 0.3288935422897339, 0.32892969250679016, 0.32873380184173584, 0.3288894295692444, 0.32919397950172424, 0.3291902244091034, 0.32913804054260254, 0.32920321822166443, 0.32929661870002747, 0.3294094502925873, 0.3294040560722351, 0.32936152815818787, 0.3295465409755707, 0.32975032925605774, 0.32958441972732544, 0.32975339889526367, 0.329746812582016, 0.3297732472419739, 0.3298587501049042, 0.3299129009246826, 0.32986539602279663, 0.3299373984336853, 0.33003324270248413, 0.3300465941429138, 0.33005762100219727, 0.33012208342552185, 0.33016741275787354, 0.33010804653167725, 0.3302678167819977, 0.3302226960659027, 0.33026182651519775, 0.3302757441997528, 0.33029329776763916, 0.3303713798522949]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adagrad (\n",
            "Parameter Group 0\n",
            "    eps: 1e-10\n",
            "    initial_accumulator_value: 0\n",
            "    lr: 0.1\n",
            "    lr_decay: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/100, Train Loss: 0.24606548, Test Loss: 0.13536450, Test Accuracy: 0.95980000\n",
            "\n",
            "Epoch: 2/100, Train Loss: 0.10241979, Test Loss: 0.10852858, Test Accuracy: 0.96680000\n",
            "\n",
            "Epoch: 3/100, Train Loss: 0.07168549, Test Loss: 0.09410936, Test Accuracy: 0.97090000\n",
            "\n",
            "Epoch: 4/100, Train Loss: 0.05519162, Test Loss: 0.08580516, Test Accuracy: 0.97380000\n",
            "\n",
            "Epoch: 5/100, Train Loss: 0.04349245, Test Loss: 0.08040658, Test Accuracy: 0.97400000\n",
            "\n",
            "Epoch: 6/100, Train Loss: 0.03543697, Test Loss: 0.07855718, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 7/100, Train Loss: 0.02920640, Test Loss: 0.07723335, Test Accuracy: 0.97660000\n",
            "\n",
            "Epoch: 8/100, Train Loss: 0.02438300, Test Loss: 0.07551392, Test Accuracy: 0.97670000\n",
            "\n",
            "Epoch: 9/100, Train Loss: 0.02062593, Test Loss: 0.07322012, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 10/100, Train Loss: 0.01767099, Test Loss: 0.07364485, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 11/100, Train Loss: 0.01514810, Test Loss: 0.07102652, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 12/100, Train Loss: 0.01310133, Test Loss: 0.07203271, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 13/100, Train Loss: 0.01153694, Test Loss: 0.07102464, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 14/100, Train Loss: 0.01012674, Test Loss: 0.07074779, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 15/100, Train Loss: 0.00895998, Test Loss: 0.07042581, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 16/100, Train Loss: 0.00794515, Test Loss: 0.07055352, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 17/100, Train Loss: 0.00711313, Test Loss: 0.07062465, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 18/100, Train Loss: 0.00644606, Test Loss: 0.07066553, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 19/100, Train Loss: 0.00582985, Test Loss: 0.06977665, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 20/100, Train Loss: 0.00531207, Test Loss: 0.07005393, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 21/100, Train Loss: 0.00484159, Test Loss: 0.07024651, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 22/100, Train Loss: 0.00446682, Test Loss: 0.07031596, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 23/100, Train Loss: 0.00413866, Test Loss: 0.07068985, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 24/100, Train Loss: 0.00381685, Test Loss: 0.07009457, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 25/100, Train Loss: 0.00357203, Test Loss: 0.07045228, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 26/100, Train Loss: 0.00334557, Test Loss: 0.07058064, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 27/100, Train Loss: 0.00312927, Test Loss: 0.07012955, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 28/100, Train Loss: 0.00293906, Test Loss: 0.07070125, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 29/100, Train Loss: 0.00277206, Test Loss: 0.07054616, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 30/100, Train Loss: 0.00261793, Test Loss: 0.07136901, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 31/100, Train Loss: 0.00247283, Test Loss: 0.07088573, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 32/100, Train Loss: 0.00234777, Test Loss: 0.07127601, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 33/100, Train Loss: 0.00223829, Test Loss: 0.07129722, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 34/100, Train Loss: 0.00212533, Test Loss: 0.07142555, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 35/100, Train Loss: 0.00202993, Test Loss: 0.07137874, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 36/100, Train Loss: 0.00194003, Test Loss: 0.07157761, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 37/100, Train Loss: 0.00185792, Test Loss: 0.07187842, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 38/100, Train Loss: 0.00177498, Test Loss: 0.07177959, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 39/100, Train Loss: 0.00170281, Test Loss: 0.07185778, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 40/100, Train Loss: 0.00163926, Test Loss: 0.07181892, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 41/100, Train Loss: 0.00157458, Test Loss: 0.07254231, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 42/100, Train Loss: 0.00151617, Test Loss: 0.07235276, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 43/100, Train Loss: 0.00146094, Test Loss: 0.07230989, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 44/100, Train Loss: 0.00140682, Test Loss: 0.07233281, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 45/100, Train Loss: 0.00135613, Test Loss: 0.07290620, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 46/100, Train Loss: 0.00131242, Test Loss: 0.07297239, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 47/100, Train Loss: 0.00127295, Test Loss: 0.07281148, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 48/100, Train Loss: 0.00122901, Test Loss: 0.07309437, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 49/100, Train Loss: 0.00119007, Test Loss: 0.07345917, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 50/100, Train Loss: 0.00115249, Test Loss: 0.07329906, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 51/100, Train Loss: 0.00111422, Test Loss: 0.07340487, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 52/100, Train Loss: 0.00108455, Test Loss: 0.07358334, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 53/100, Train Loss: 0.00105167, Test Loss: 0.07358023, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 54/100, Train Loss: 0.00102319, Test Loss: 0.07378625, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 55/100, Train Loss: 0.00099645, Test Loss: 0.07391908, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 56/100, Train Loss: 0.00096786, Test Loss: 0.07405001, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 57/100, Train Loss: 0.00094292, Test Loss: 0.07379667, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 58/100, Train Loss: 0.00091890, Test Loss: 0.07408591, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 59/100, Train Loss: 0.00089314, Test Loss: 0.07437647, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 60/100, Train Loss: 0.00087246, Test Loss: 0.07452156, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 61/100, Train Loss: 0.00085162, Test Loss: 0.07442770, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 62/100, Train Loss: 0.00083110, Test Loss: 0.07469455, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 63/100, Train Loss: 0.00080956, Test Loss: 0.07475424, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 64/100, Train Loss: 0.00079302, Test Loss: 0.07479124, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 65/100, Train Loss: 0.00077348, Test Loss: 0.07542390, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 66/100, Train Loss: 0.00075713, Test Loss: 0.07558397, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 67/100, Train Loss: 0.00074201, Test Loss: 0.07531839, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 68/100, Train Loss: 0.00072387, Test Loss: 0.07563930, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 69/100, Train Loss: 0.00070864, Test Loss: 0.07551326, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 70/100, Train Loss: 0.00069392, Test Loss: 0.07578650, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 71/100, Train Loss: 0.00067985, Test Loss: 0.07581900, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 72/100, Train Loss: 0.00066548, Test Loss: 0.07579379, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 73/100, Train Loss: 0.00065253, Test Loss: 0.07572645, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 74/100, Train Loss: 0.00063963, Test Loss: 0.07603424, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 75/100, Train Loss: 0.00062627, Test Loss: 0.07569373, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 76/100, Train Loss: 0.00061487, Test Loss: 0.07641722, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 77/100, Train Loss: 0.00060318, Test Loss: 0.07611066, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 78/100, Train Loss: 0.00059081, Test Loss: 0.07642391, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 79/100, Train Loss: 0.00058077, Test Loss: 0.07617585, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 80/100, Train Loss: 0.00057052, Test Loss: 0.07633477, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 81/100, Train Loss: 0.00055984, Test Loss: 0.07652587, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 82/100, Train Loss: 0.00055041, Test Loss: 0.07653810, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 83/100, Train Loss: 0.00054019, Test Loss: 0.07670812, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 84/100, Train Loss: 0.00053086, Test Loss: 0.07692174, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 85/100, Train Loss: 0.00052217, Test Loss: 0.07697049, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 86/100, Train Loss: 0.00051281, Test Loss: 0.07673791, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 87/100, Train Loss: 0.00050497, Test Loss: 0.07704916, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 88/100, Train Loss: 0.00049650, Test Loss: 0.07712143, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 89/100, Train Loss: 0.00048832, Test Loss: 0.07718253, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 90/100, Train Loss: 0.00048034, Test Loss: 0.07743182, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 91/100, Train Loss: 0.00047257, Test Loss: 0.07759142, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 92/100, Train Loss: 0.00046529, Test Loss: 0.07747033, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 93/100, Train Loss: 0.00045820, Test Loss: 0.07762924, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 94/100, Train Loss: 0.00045107, Test Loss: 0.07750848, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 95/100, Train Loss: 0.00044487, Test Loss: 0.07771354, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 96/100, Train Loss: 0.00043791, Test Loss: 0.07765569, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 97/100, Train Loss: 0.00043141, Test Loss: 0.07808281, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 98/100, Train Loss: 0.00042500, Test Loss: 0.07779339, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 99/100, Train Loss: 0.00041900, Test Loss: 0.07796646, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 100/100, Train Loss: 0.00041317, Test Loss: 0.07816617, Test Accuracy: 0.98070000\n",
            "[tensor(0.6822, grad_fn=<MeanBackward0>), tensor(0.6734, grad_fn=<MeanBackward0>), tensor(0.6662, grad_fn=<MeanBackward0>), tensor(0.6638, grad_fn=<MeanBackward0>), tensor(0.6563, grad_fn=<MeanBackward0>), tensor(0.6567, grad_fn=<MeanBackward0>), tensor(0.6473, grad_fn=<MeanBackward0>), tensor(0.6470, grad_fn=<MeanBackward0>), tensor(0.6440, grad_fn=<MeanBackward0>), tensor(0.6411, grad_fn=<MeanBackward0>), tensor(0.6413, grad_fn=<MeanBackward0>), tensor(0.6391, grad_fn=<MeanBackward0>), tensor(0.6368, grad_fn=<MeanBackward0>), tensor(0.6370, grad_fn=<MeanBackward0>), tensor(0.6354, grad_fn=<MeanBackward0>), tensor(0.6332, grad_fn=<MeanBackward0>), tensor(0.6324, grad_fn=<MeanBackward0>), tensor(0.6314, grad_fn=<MeanBackward0>), tensor(0.6308, grad_fn=<MeanBackward0>), tensor(0.6294, grad_fn=<MeanBackward0>), tensor(0.6278, grad_fn=<MeanBackward0>), tensor(0.6281, grad_fn=<MeanBackward0>), tensor(0.6274, grad_fn=<MeanBackward0>), tensor(0.6269, grad_fn=<MeanBackward0>), tensor(0.6259, grad_fn=<MeanBackward0>), tensor(0.6250, grad_fn=<MeanBackward0>), tensor(0.6249, grad_fn=<MeanBackward0>), tensor(0.6243, grad_fn=<MeanBackward0>), tensor(0.6235, grad_fn=<MeanBackward0>), tensor(0.6234, grad_fn=<MeanBackward0>), tensor(0.6227, grad_fn=<MeanBackward0>), tensor(0.6226, grad_fn=<MeanBackward0>), tensor(0.6220, grad_fn=<MeanBackward0>), tensor(0.6214, grad_fn=<MeanBackward0>), tensor(0.6216, grad_fn=<MeanBackward0>), tensor(0.6207, grad_fn=<MeanBackward0>), tensor(0.6209, grad_fn=<MeanBackward0>), tensor(0.6200, grad_fn=<MeanBackward0>), tensor(0.6201, grad_fn=<MeanBackward0>), tensor(0.6195, grad_fn=<MeanBackward0>), tensor(0.6198, grad_fn=<MeanBackward0>), tensor(0.6191, grad_fn=<MeanBackward0>), tensor(0.6187, grad_fn=<MeanBackward0>), tensor(0.6187, grad_fn=<MeanBackward0>), tensor(0.6188, grad_fn=<MeanBackward0>), tensor(0.6187, grad_fn=<MeanBackward0>), tensor(0.6181, grad_fn=<MeanBackward0>), tensor(0.6180, grad_fn=<MeanBackward0>), tensor(0.6179, grad_fn=<MeanBackward0>), tensor(0.6174, grad_fn=<MeanBackward0>), tensor(0.6175, grad_fn=<MeanBackward0>), tensor(0.6173, grad_fn=<MeanBackward0>), tensor(0.6170, grad_fn=<MeanBackward0>), tensor(0.6165, grad_fn=<MeanBackward0>), tensor(0.6166, grad_fn=<MeanBackward0>), tensor(0.6163, grad_fn=<MeanBackward0>), tensor(0.6159, grad_fn=<MeanBackward0>), tensor(0.6161, grad_fn=<MeanBackward0>), tensor(0.6155, grad_fn=<MeanBackward0>), tensor(0.6154, grad_fn=<MeanBackward0>), tensor(0.6153, grad_fn=<MeanBackward0>), tensor(0.6151, grad_fn=<MeanBackward0>), tensor(0.6152, grad_fn=<MeanBackward0>), tensor(0.6150, grad_fn=<MeanBackward0>), tensor(0.6149, grad_fn=<MeanBackward0>), tensor(0.6147, grad_fn=<MeanBackward0>), tensor(0.6146, grad_fn=<MeanBackward0>), tensor(0.6143, grad_fn=<MeanBackward0>), tensor(0.6140, grad_fn=<MeanBackward0>), tensor(0.6143, grad_fn=<MeanBackward0>), tensor(0.6141, grad_fn=<MeanBackward0>), tensor(0.6138, grad_fn=<MeanBackward0>), tensor(0.6137, grad_fn=<MeanBackward0>), tensor(0.6137, grad_fn=<MeanBackward0>), tensor(0.6134, grad_fn=<MeanBackward0>), tensor(0.6135, grad_fn=<MeanBackward0>), tensor(0.6135, grad_fn=<MeanBackward0>), tensor(0.6133, grad_fn=<MeanBackward0>), tensor(0.6129, grad_fn=<MeanBackward0>), tensor(0.6130, grad_fn=<MeanBackward0>), tensor(0.6128, grad_fn=<MeanBackward0>), tensor(0.6128, grad_fn=<MeanBackward0>), tensor(0.6127, grad_fn=<MeanBackward0>), tensor(0.6125, grad_fn=<MeanBackward0>), tensor(0.6123, grad_fn=<MeanBackward0>), tensor(0.6124, grad_fn=<MeanBackward0>), tensor(0.6123, grad_fn=<MeanBackward0>), tensor(0.6121, grad_fn=<MeanBackward0>), tensor(0.6120, grad_fn=<MeanBackward0>), tensor(0.6119, grad_fn=<MeanBackward0>), tensor(0.6119, grad_fn=<MeanBackward0>), tensor(0.6117, grad_fn=<MeanBackward0>), tensor(0.6118, grad_fn=<MeanBackward0>), tensor(0.6117, grad_fn=<MeanBackward0>), tensor(0.6116, grad_fn=<MeanBackward0>), tensor(0.6115, grad_fn=<MeanBackward0>), tensor(0.6115, grad_fn=<MeanBackward0>), tensor(0.6115, grad_fn=<MeanBackward0>), tensor(0.6112, grad_fn=<MeanBackward0>), tensor(0.6114, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.6822378039360046, 0.6734055876731873, 0.666247546672821, 0.6638466715812683, 0.656337320804596, 0.6566547751426697, 0.647308886051178, 0.6470393538475037, 0.6440447568893433, 0.6410574316978455, 0.6412938237190247, 0.6390615701675415, 0.6368066668510437, 0.6369675993919373, 0.6353985071182251, 0.6331662535667419, 0.6324071288108826, 0.6313661336898804, 0.6307770609855652, 0.6294049024581909, 0.6278010606765747, 0.6281253099441528, 0.627437174320221, 0.6268825531005859, 0.6258841753005981, 0.6250402927398682, 0.6248721480369568, 0.624284029006958, 0.6235304474830627, 0.6234461665153503, 0.622658371925354, 0.6226286292076111, 0.621999204158783, 0.6214426755905151, 0.6215775609016418, 0.6207312345504761, 0.6209354400634766, 0.6199754476547241, 0.6200790405273438, 0.6195287108421326, 0.6197706460952759, 0.6191418170928955, 0.6187160611152649, 0.6187375783920288, 0.6187522411346436, 0.618730366230011, 0.618132472038269, 0.6179758310317993, 0.617895781993866, 0.6174048781394958, 0.6174620389938354, 0.6172520518302917, 0.6170427203178406, 0.6165431141853333, 0.616600513458252, 0.6163315176963806, 0.6158537864685059, 0.6160922646522522, 0.6154982447624207, 0.6153997778892517, 0.6153433322906494, 0.6150757074356079, 0.6151959896087646, 0.6149984002113342, 0.6149479150772095, 0.6146720051765442, 0.6145924925804138, 0.6142674088478088, 0.614048421382904, 0.6143087148666382, 0.6140708327293396, 0.6138299107551575, 0.6137032508850098, 0.6137042045593262, 0.613394558429718, 0.6135364174842834, 0.6135063767433167, 0.6133080124855042, 0.6129065155982971, 0.6130298376083374, 0.6128050088882446, 0.6127970218658447, 0.6127331256866455, 0.6124603152275085, 0.6123425960540771, 0.6124120950698853, 0.612270712852478, 0.6120576858520508, 0.6120079755783081, 0.6119384765625, 0.611907958984375, 0.6117228865623474, 0.6117746233940125, 0.611701250076294, 0.6116437911987305, 0.6114686727523804, 0.6115208268165588, 0.6114763021469116, 0.611221969127655, 0.6113997101783752]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/100, Train Loss: 0.76565957, Test Loss: 0.36426025, Test Accuracy: 0.89780000\n",
            "\n",
            "Epoch: 2/100, Train Loss: 0.34656719, Test Loss: 0.30185559, Test Accuracy: 0.91490000\n",
            "\n",
            "Epoch: 3/100, Train Loss: 0.30716575, Test Loss: 0.28228335, Test Accuracy: 0.91980000\n",
            "\n",
            "Epoch: 4/100, Train Loss: 0.28465809, Test Loss: 0.26459620, Test Accuracy: 0.92320000\n",
            "\n",
            "Epoch: 5/100, Train Loss: 0.26676440, Test Loss: 0.25525985, Test Accuracy: 0.92630000\n",
            "\n",
            "Epoch: 6/100, Train Loss: 0.25019379, Test Loss: 0.23763010, Test Accuracy: 0.93390000\n",
            "\n",
            "Epoch: 7/100, Train Loss: 0.23405676, Test Loss: 0.22358629, Test Accuracy: 0.93590000\n",
            "\n",
            "Epoch: 8/100, Train Loss: 0.21841103, Test Loss: 0.20822534, Test Accuracy: 0.93800000\n",
            "\n",
            "Epoch: 9/100, Train Loss: 0.20472462, Test Loss: 0.19776261, Test Accuracy: 0.94300000\n",
            "\n",
            "Epoch: 10/100, Train Loss: 0.19215491, Test Loss: 0.18883254, Test Accuracy: 0.94380000\n",
            "\n",
            "Epoch: 11/100, Train Loss: 0.18024588, Test Loss: 0.17727404, Test Accuracy: 0.94820000\n",
            "\n",
            "Epoch: 12/100, Train Loss: 0.17008533, Test Loss: 0.16967918, Test Accuracy: 0.94910000\n",
            "\n",
            "Epoch: 13/100, Train Loss: 0.16088630, Test Loss: 0.15953736, Test Accuracy: 0.95290000\n",
            "\n",
            "Epoch: 14/100, Train Loss: 0.15263872, Test Loss: 0.15080839, Test Accuracy: 0.95520000\n",
            "\n",
            "Epoch: 15/100, Train Loss: 0.14478223, Test Loss: 0.14545626, Test Accuracy: 0.95740000\n",
            "\n",
            "Epoch: 16/100, Train Loss: 0.13799206, Test Loss: 0.14231008, Test Accuracy: 0.95810000\n",
            "\n",
            "Epoch: 17/100, Train Loss: 0.13151341, Test Loss: 0.13502571, Test Accuracy: 0.96070000\n",
            "\n",
            "Epoch: 18/100, Train Loss: 0.12539914, Test Loss: 0.13099937, Test Accuracy: 0.96090000\n",
            "\n",
            "Epoch: 19/100, Train Loss: 0.12011479, Test Loss: 0.12599710, Test Accuracy: 0.96200000\n",
            "\n",
            "Epoch: 20/100, Train Loss: 0.11495074, Test Loss: 0.12346852, Test Accuracy: 0.96170000\n",
            "\n",
            "Epoch: 21/100, Train Loss: 0.11020207, Test Loss: 0.11886121, Test Accuracy: 0.96360000\n",
            "\n",
            "Epoch: 22/100, Train Loss: 0.10575211, Test Loss: 0.11755457, Test Accuracy: 0.96410000\n",
            "\n",
            "Epoch: 23/100, Train Loss: 0.10187168, Test Loss: 0.11029562, Test Accuracy: 0.96670000\n",
            "\n",
            "Epoch: 24/100, Train Loss: 0.09802387, Test Loss: 0.10964573, Test Accuracy: 0.96640000\n",
            "\n",
            "Epoch: 25/100, Train Loss: 0.09408490, Test Loss: 0.10844097, Test Accuracy: 0.96650000\n",
            "\n",
            "Epoch: 26/100, Train Loss: 0.09090001, Test Loss: 0.10638889, Test Accuracy: 0.96660000\n",
            "\n",
            "Epoch: 27/100, Train Loss: 0.08785441, Test Loss: 0.10298670, Test Accuracy: 0.96880000\n",
            "\n",
            "Epoch: 28/100, Train Loss: 0.08449577, Test Loss: 0.10153408, Test Accuracy: 0.96790000\n",
            "\n",
            "Epoch: 29/100, Train Loss: 0.08172680, Test Loss: 0.09679001, Test Accuracy: 0.97050000\n",
            "\n",
            "Epoch: 30/100, Train Loss: 0.07920074, Test Loss: 0.09583677, Test Accuracy: 0.97070000\n",
            "\n",
            "Epoch: 31/100, Train Loss: 0.07653470, Test Loss: 0.09431497, Test Accuracy: 0.97100000\n",
            "\n",
            "Epoch: 32/100, Train Loss: 0.07421322, Test Loss: 0.09280256, Test Accuracy: 0.97210000\n",
            "\n",
            "Epoch: 33/100, Train Loss: 0.07196891, Test Loss: 0.09210993, Test Accuracy: 0.97170000\n",
            "\n",
            "Epoch: 34/100, Train Loss: 0.06964641, Test Loss: 0.08970299, Test Accuracy: 0.97200000\n",
            "\n",
            "Epoch: 35/100, Train Loss: 0.06762905, Test Loss: 0.08849357, Test Accuracy: 0.97320000\n",
            "\n",
            "Epoch: 36/100, Train Loss: 0.06560482, Test Loss: 0.08828343, Test Accuracy: 0.97260000\n",
            "\n",
            "Epoch: 37/100, Train Loss: 0.06357993, Test Loss: 0.08516870, Test Accuracy: 0.97380000\n",
            "\n",
            "Epoch: 38/100, Train Loss: 0.06163272, Test Loss: 0.08481318, Test Accuracy: 0.97340000\n",
            "\n",
            "Epoch: 39/100, Train Loss: 0.06024263, Test Loss: 0.08404950, Test Accuracy: 0.97410000\n",
            "\n",
            "Epoch: 40/100, Train Loss: 0.05861816, Test Loss: 0.08183470, Test Accuracy: 0.97430000\n",
            "\n",
            "Epoch: 41/100, Train Loss: 0.05689642, Test Loss: 0.08279795, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 42/100, Train Loss: 0.05534425, Test Loss: 0.08116692, Test Accuracy: 0.97520000\n",
            "\n",
            "Epoch: 43/100, Train Loss: 0.05385507, Test Loss: 0.07963975, Test Accuracy: 0.97520000\n",
            "\n",
            "Epoch: 44/100, Train Loss: 0.05236694, Test Loss: 0.07839182, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 45/100, Train Loss: 0.05100355, Test Loss: 0.07749440, Test Accuracy: 0.97550000\n",
            "\n",
            "Epoch: 46/100, Train Loss: 0.04977005, Test Loss: 0.07806048, Test Accuracy: 0.97550000\n",
            "\n",
            "Epoch: 47/100, Train Loss: 0.04863277, Test Loss: 0.07621212, Test Accuracy: 0.97530000\n",
            "\n",
            "Epoch: 48/100, Train Loss: 0.04702726, Test Loss: 0.07550057, Test Accuracy: 0.97540000\n",
            "\n",
            "Epoch: 49/100, Train Loss: 0.04607129, Test Loss: 0.07439600, Test Accuracy: 0.97600000\n",
            "\n",
            "Epoch: 50/100, Train Loss: 0.04485388, Test Loss: 0.07419469, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 51/100, Train Loss: 0.04378197, Test Loss: 0.07354512, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 52/100, Train Loss: 0.04276508, Test Loss: 0.07249886, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 53/100, Train Loss: 0.04166702, Test Loss: 0.07197376, Test Accuracy: 0.97600000\n",
            "\n",
            "Epoch: 54/100, Train Loss: 0.04057769, Test Loss: 0.07210148, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 55/100, Train Loss: 0.03972491, Test Loss: 0.07139974, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 56/100, Train Loss: 0.03877532, Test Loss: 0.06983120, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 57/100, Train Loss: 0.03787075, Test Loss: 0.07039461, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 58/100, Train Loss: 0.03684161, Test Loss: 0.06944337, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 59/100, Train Loss: 0.03605726, Test Loss: 0.06904584, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 60/100, Train Loss: 0.03531988, Test Loss: 0.07073060, Test Accuracy: 0.97630000\n",
            "\n",
            "Epoch: 61/100, Train Loss: 0.03433481, Test Loss: 0.06956675, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 62/100, Train Loss: 0.03365556, Test Loss: 0.06883795, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 63/100, Train Loss: 0.03292664, Test Loss: 0.06801445, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 64/100, Train Loss: 0.03222394, Test Loss: 0.06866509, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 65/100, Train Loss: 0.03137665, Test Loss: 0.06773129, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 66/100, Train Loss: 0.03073466, Test Loss: 0.06633268, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 67/100, Train Loss: 0.03003812, Test Loss: 0.06745102, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 68/100, Train Loss: 0.02942018, Test Loss: 0.06694734, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 69/100, Train Loss: 0.02880541, Test Loss: 0.06611821, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 70/100, Train Loss: 0.02818052, Test Loss: 0.06578239, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 71/100, Train Loss: 0.02748612, Test Loss: 0.06688772, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 72/100, Train Loss: 0.02694920, Test Loss: 0.06577607, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 73/100, Train Loss: 0.02641259, Test Loss: 0.06517347, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 74/100, Train Loss: 0.02590936, Test Loss: 0.06567650, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 75/100, Train Loss: 0.02526047, Test Loss: 0.06466837, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 76/100, Train Loss: 0.02474091, Test Loss: 0.06463903, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 77/100, Train Loss: 0.02435023, Test Loss: 0.06489556, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 78/100, Train Loss: 0.02371185, Test Loss: 0.06510066, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 79/100, Train Loss: 0.02326612, Test Loss: 0.06487930, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 80/100, Train Loss: 0.02284243, Test Loss: 0.06423468, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 81/100, Train Loss: 0.02239891, Test Loss: 0.06328026, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 82/100, Train Loss: 0.02190845, Test Loss: 0.06349884, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 83/100, Train Loss: 0.02145669, Test Loss: 0.06346509, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 84/100, Train Loss: 0.02101997, Test Loss: 0.06356288, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 85/100, Train Loss: 0.02062347, Test Loss: 0.06328906, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 86/100, Train Loss: 0.02020482, Test Loss: 0.06324295, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 87/100, Train Loss: 0.01980836, Test Loss: 0.06337567, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 88/100, Train Loss: 0.01951752, Test Loss: 0.06252822, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 89/100, Train Loss: 0.01901390, Test Loss: 0.06394624, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 90/100, Train Loss: 0.01864333, Test Loss: 0.06359705, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 91/100, Train Loss: 0.01837426, Test Loss: 0.06274391, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 92/100, Train Loss: 0.01796656, Test Loss: 0.06290108, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 93/100, Train Loss: 0.01769912, Test Loss: 0.06329827, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 94/100, Train Loss: 0.01736738, Test Loss: 0.06223528, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 95/100, Train Loss: 0.01699941, Test Loss: 0.06303501, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 96/100, Train Loss: 0.01670271, Test Loss: 0.06296223, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 97/100, Train Loss: 0.01642348, Test Loss: 0.06260834, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 98/100, Train Loss: 0.01609636, Test Loss: 0.06155811, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 99/100, Train Loss: 0.01586194, Test Loss: 0.06223969, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 100/100, Train Loss: 0.01555348, Test Loss: 0.06206416, Test Accuracy: 0.97940000\n",
            "[tensor(0.0809, grad_fn=<MeanBackward0>), tensor(0.0928, grad_fn=<MeanBackward0>), tensor(0.1022, grad_fn=<MeanBackward0>), tensor(0.1114, grad_fn=<MeanBackward0>), tensor(0.1221, grad_fn=<MeanBackward0>), tensor(0.1313, grad_fn=<MeanBackward0>), tensor(0.1404, grad_fn=<MeanBackward0>), tensor(0.1487, grad_fn=<MeanBackward0>), tensor(0.1569, grad_fn=<MeanBackward0>), tensor(0.1641, grad_fn=<MeanBackward0>), tensor(0.1711, grad_fn=<MeanBackward0>), tensor(0.1766, grad_fn=<MeanBackward0>), tensor(0.1822, grad_fn=<MeanBackward0>), tensor(0.1873, grad_fn=<MeanBackward0>), tensor(0.1917, grad_fn=<MeanBackward0>), tensor(0.1955, grad_fn=<MeanBackward0>), tensor(0.1992, grad_fn=<MeanBackward0>), tensor(0.2034, grad_fn=<MeanBackward0>), tensor(0.2062, grad_fn=<MeanBackward0>), tensor(0.2093, grad_fn=<MeanBackward0>), tensor(0.2115, grad_fn=<MeanBackward0>), tensor(0.2138, grad_fn=<MeanBackward0>), tensor(0.2160, grad_fn=<MeanBackward0>), tensor(0.2182, grad_fn=<MeanBackward0>), tensor(0.2205, grad_fn=<MeanBackward0>), tensor(0.2222, grad_fn=<MeanBackward0>), tensor(0.2237, grad_fn=<MeanBackward0>), tensor(0.2250, grad_fn=<MeanBackward0>), tensor(0.2266, grad_fn=<MeanBackward0>), tensor(0.2284, grad_fn=<MeanBackward0>), tensor(0.2292, grad_fn=<MeanBackward0>), tensor(0.2306, grad_fn=<MeanBackward0>), tensor(0.2319, grad_fn=<MeanBackward0>), tensor(0.2332, grad_fn=<MeanBackward0>), tensor(0.2344, grad_fn=<MeanBackward0>), tensor(0.2352, grad_fn=<MeanBackward0>), tensor(0.2357, grad_fn=<MeanBackward0>), tensor(0.2367, grad_fn=<MeanBackward0>), tensor(0.2375, grad_fn=<MeanBackward0>), tensor(0.2386, grad_fn=<MeanBackward0>), tensor(0.2391, grad_fn=<MeanBackward0>), tensor(0.2400, grad_fn=<MeanBackward0>), tensor(0.2405, grad_fn=<MeanBackward0>), tensor(0.2412, grad_fn=<MeanBackward0>), tensor(0.2419, grad_fn=<MeanBackward0>), tensor(0.2425, grad_fn=<MeanBackward0>), tensor(0.2431, grad_fn=<MeanBackward0>), tensor(0.2434, grad_fn=<MeanBackward0>), tensor(0.2443, grad_fn=<MeanBackward0>), tensor(0.2446, grad_fn=<MeanBackward0>), tensor(0.2454, grad_fn=<MeanBackward0>), tensor(0.2455, grad_fn=<MeanBackward0>), tensor(0.2461, grad_fn=<MeanBackward0>), tensor(0.2467, grad_fn=<MeanBackward0>), tensor(0.2471, grad_fn=<MeanBackward0>), tensor(0.2475, grad_fn=<MeanBackward0>), tensor(0.2478, grad_fn=<MeanBackward0>), tensor(0.2484, grad_fn=<MeanBackward0>), tensor(0.2486, grad_fn=<MeanBackward0>), tensor(0.2488, grad_fn=<MeanBackward0>), tensor(0.2495, grad_fn=<MeanBackward0>), tensor(0.2497, grad_fn=<MeanBackward0>), tensor(0.2499, grad_fn=<MeanBackward0>), tensor(0.2504, grad_fn=<MeanBackward0>), tensor(0.2508, grad_fn=<MeanBackward0>), tensor(0.2509, grad_fn=<MeanBackward0>), tensor(0.2511, grad_fn=<MeanBackward0>), tensor(0.2514, grad_fn=<MeanBackward0>), tensor(0.2520, grad_fn=<MeanBackward0>), tensor(0.2523, grad_fn=<MeanBackward0>), tensor(0.2525, grad_fn=<MeanBackward0>), tensor(0.2527, grad_fn=<MeanBackward0>), tensor(0.2529, grad_fn=<MeanBackward0>), tensor(0.2534, grad_fn=<MeanBackward0>), tensor(0.2535, grad_fn=<MeanBackward0>), tensor(0.2538, grad_fn=<MeanBackward0>), tensor(0.2539, grad_fn=<MeanBackward0>), tensor(0.2541, grad_fn=<MeanBackward0>), tensor(0.2544, grad_fn=<MeanBackward0>), tensor(0.2545, grad_fn=<MeanBackward0>), tensor(0.2547, grad_fn=<MeanBackward0>), tensor(0.2548, grad_fn=<MeanBackward0>), tensor(0.2551, grad_fn=<MeanBackward0>), tensor(0.2552, grad_fn=<MeanBackward0>), tensor(0.2556, grad_fn=<MeanBackward0>), tensor(0.2558, grad_fn=<MeanBackward0>), tensor(0.2560, grad_fn=<MeanBackward0>), tensor(0.2564, grad_fn=<MeanBackward0>), tensor(0.2565, grad_fn=<MeanBackward0>), tensor(0.2566, grad_fn=<MeanBackward0>), tensor(0.2567, grad_fn=<MeanBackward0>), tensor(0.2567, grad_fn=<MeanBackward0>), tensor(0.2572, grad_fn=<MeanBackward0>), tensor(0.2572, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2573, grad_fn=<MeanBackward0>), tensor(0.2578, grad_fn=<MeanBackward0>), tensor(0.2580, grad_fn=<MeanBackward0>), tensor(0.2581, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.0808916985988617, 0.09275198727846146, 0.10220954567193985, 0.11142546683549881, 0.12212937325239182, 0.1312970668077469, 0.14040687680244446, 0.1487163007259369, 0.15692181885242462, 0.16409394145011902, 0.17114141583442688, 0.17661194503307343, 0.18219754099845886, 0.18725860118865967, 0.19167344272136688, 0.19549472630023956, 0.19918034970760345, 0.20337346196174622, 0.20623710751533508, 0.2093372792005539, 0.2115270048379898, 0.21377788484096527, 0.21603107452392578, 0.21815864741802216, 0.2205333262681961, 0.2221529334783554, 0.22365239262580872, 0.22504529356956482, 0.22663423418998718, 0.22836872935295105, 0.22920601069927216, 0.2306380569934845, 0.23191088438034058, 0.23317836225032806, 0.2343921810388565, 0.23524819314479828, 0.2357109636068344, 0.236733078956604, 0.23753051459789276, 0.23859179019927979, 0.23907341063022614, 0.2400352507829666, 0.24052971601486206, 0.24119983613491058, 0.2418852597475052, 0.24246571958065033, 0.2430911809206009, 0.24337206780910492, 0.24429868161678314, 0.24464084208011627, 0.24544332921504974, 0.24552901089191437, 0.24608071148395538, 0.2466660588979721, 0.24705252051353455, 0.24746084213256836, 0.24783359467983246, 0.2484392374753952, 0.2486090511083603, 0.2488146424293518, 0.24949033558368683, 0.24973906576633453, 0.24987788498401642, 0.25044989585876465, 0.2508279085159302, 0.25086626410484314, 0.2511048913002014, 0.25142520666122437, 0.2520139813423157, 0.2522830665111542, 0.25253328680992126, 0.25273728370666504, 0.25287020206451416, 0.2533622682094574, 0.2534538209438324, 0.2537696361541748, 0.25385817885398865, 0.25405383110046387, 0.2544043958187103, 0.25452783703804016, 0.25472599267959595, 0.25476083159446716, 0.2551320195198059, 0.255221426486969, 0.25559571385383606, 0.25576263666152954, 0.25602447986602783, 0.2564222514629364, 0.2564729154109955, 0.25656425952911377, 0.25665274262428284, 0.25672754645347595, 0.2572191059589386, 0.2571614384651184, 0.25748831033706665, 0.2575218975543976, 0.25732263922691345, 0.25776544213294983, 0.25801509618759155, 0.2580949664115906]\n",
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/100, Train Loss: 0.43066128, Test Loss: 0.22646753, Test Accuracy: 0.93570000\n",
            "\n",
            "Epoch: 2/100, Train Loss: 0.19788576, Test Loss: 0.17390600, Test Accuracy: 0.95010000\n",
            "\n",
            "Epoch: 3/100, Train Loss: 0.14398957, Test Loss: 0.12911878, Test Accuracy: 0.96090000\n",
            "\n",
            "Epoch: 4/100, Train Loss: 0.11027267, Test Loss: 0.10537287, Test Accuracy: 0.96620000\n",
            "\n",
            "Epoch: 5/100, Train Loss: 0.08713313, Test Loss: 0.09371806, Test Accuracy: 0.97010000\n",
            "\n",
            "Epoch: 6/100, Train Loss: 0.07009222, Test Loss: 0.08415451, Test Accuracy: 0.97360000\n",
            "\n",
            "Epoch: 7/100, Train Loss: 0.05682405, Test Loss: 0.08033553, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 8/100, Train Loss: 0.04619637, Test Loss: 0.07170702, Test Accuracy: 0.97660000\n",
            "\n",
            "Epoch: 9/100, Train Loss: 0.03724053, Test Loss: 0.06874974, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 10/100, Train Loss: 0.03068309, Test Loss: 0.06630095, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 11/100, Train Loss: 0.02453953, Test Loss: 0.06723710, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 12/100, Train Loss: 0.02027815, Test Loss: 0.06674808, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 13/100, Train Loss: 0.01619612, Test Loss: 0.06754852, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 14/100, Train Loss: 0.01308141, Test Loss: 0.06189100, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 15/100, Train Loss: 0.01016553, Test Loss: 0.06320561, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 16/100, Train Loss: 0.00850301, Test Loss: 0.06625620, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 17/100, Train Loss: 0.00636781, Test Loss: 0.06601350, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 18/100, Train Loss: 0.00531478, Test Loss: 0.07253457, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 19/100, Train Loss: 0.00424669, Test Loss: 0.06869798, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 20/100, Train Loss: 0.00319636, Test Loss: 0.07106294, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 21/100, Train Loss: 0.00284222, Test Loss: 0.07833460, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 22/100, Train Loss: 0.00193651, Test Loss: 0.07425011, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 23/100, Train Loss: 0.00186982, Test Loss: 0.07275937, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 24/100, Train Loss: 0.00137200, Test Loss: 0.07418089, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 25/100, Train Loss: 0.00160378, Test Loss: 0.07523913, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 26/100, Train Loss: 0.00077672, Test Loss: 0.07544714, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 27/100, Train Loss: 0.00080069, Test Loss: 0.07981737, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 28/100, Train Loss: 0.00085362, Test Loss: 0.07864203, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 29/100, Train Loss: 0.00075828, Test Loss: 0.09254619, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 30/100, Train Loss: 0.00093795, Test Loss: 0.08108590, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 31/100, Train Loss: 0.00023000, Test Loss: 0.08077510, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 32/100, Train Loss: 0.00020354, Test Loss: 0.08336782, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 33/100, Train Loss: 0.00156405, Test Loss: 0.09095169, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 34/100, Train Loss: 0.00025123, Test Loss: 0.08430032, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 35/100, Train Loss: 0.00013919, Test Loss: 0.08557387, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 36/100, Train Loss: 0.00011617, Test Loss: 0.08581379, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 37/100, Train Loss: 0.00059940, Test Loss: 0.09755009, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 38/100, Train Loss: 0.00061045, Test Loss: 0.09061642, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 39/100, Train Loss: 0.00009522, Test Loss: 0.08842030, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 40/100, Train Loss: 0.00007504, Test Loss: 0.08875726, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 41/100, Train Loss: 0.00006391, Test Loss: 0.08871933, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 42/100, Train Loss: 0.00044025, Test Loss: 0.11552347, Test Accuracy: 0.97570000\n",
            "\n",
            "Epoch: 43/100, Train Loss: 0.00081435, Test Loss: 0.09061575, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 44/100, Train Loss: 0.00007348, Test Loss: 0.09288573, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 45/100, Train Loss: 0.00005323, Test Loss: 0.09367861, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 46/100, Train Loss: 0.00004266, Test Loss: 0.09450936, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 47/100, Train Loss: 0.00003575, Test Loss: 0.09527491, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 48/100, Train Loss: 0.00003182, Test Loss: 0.09590622, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 49/100, Train Loss: 0.00134944, Test Loss: 0.10512784, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 50/100, Train Loss: 0.00007903, Test Loss: 0.10345095, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 51/100, Train Loss: 0.00003987, Test Loss: 0.10245090, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 52/100, Train Loss: 0.00003104, Test Loss: 0.10143943, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 53/100, Train Loss: 0.00002487, Test Loss: 0.10231318, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 54/100, Train Loss: 0.00002087, Test Loss: 0.10187488, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 55/100, Train Loss: 0.00001815, Test Loss: 0.10221201, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 56/100, Train Loss: 0.00145346, Test Loss: 0.11141504, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 57/100, Train Loss: 0.00019598, Test Loss: 0.10320613, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 58/100, Train Loss: 0.00003919, Test Loss: 0.10387602, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 59/100, Train Loss: 0.00002552, Test Loss: 0.10314560, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 60/100, Train Loss: 0.00001956, Test Loss: 0.10410410, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 61/100, Train Loss: 0.00001548, Test Loss: 0.10416790, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 62/100, Train Loss: 0.00001251, Test Loss: 0.10451690, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 63/100, Train Loss: 0.00001062, Test Loss: 0.10542452, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 64/100, Train Loss: 0.00001036, Test Loss: 0.10763990, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 65/100, Train Loss: 0.00131376, Test Loss: 0.10548089, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 66/100, Train Loss: 0.00002735, Test Loss: 0.10551810, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 67/100, Train Loss: 0.00001755, Test Loss: 0.10589732, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 68/100, Train Loss: 0.00001335, Test Loss: 0.10636030, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 69/100, Train Loss: 0.00001040, Test Loss: 0.10650446, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 70/100, Train Loss: 0.00000832, Test Loss: 0.10658881, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 71/100, Train Loss: 0.00000676, Test Loss: 0.10798217, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 72/100, Train Loss: 0.00000587, Test Loss: 0.10840995, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 73/100, Train Loss: 0.00080233, Test Loss: 0.12045060, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 74/100, Train Loss: 0.00007734, Test Loss: 0.11405223, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 75/100, Train Loss: 0.00001621, Test Loss: 0.11305126, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 76/100, Train Loss: 0.00001050, Test Loss: 0.11313785, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 77/100, Train Loss: 0.00000784, Test Loss: 0.11347417, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 78/100, Train Loss: 0.00000613, Test Loss: 0.11357502, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 79/100, Train Loss: 0.00000480, Test Loss: 0.11405058, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 80/100, Train Loss: 0.00000388, Test Loss: 0.11450722, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 81/100, Train Loss: 0.00000320, Test Loss: 0.11501582, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 82/100, Train Loss: 0.00000267, Test Loss: 0.11504119, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 83/100, Train Loss: 0.00085776, Test Loss: 0.14347572, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 84/100, Train Loss: 0.00012048, Test Loss: 0.12123199, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 85/100, Train Loss: 0.00001038, Test Loss: 0.12138339, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 86/100, Train Loss: 0.00000700, Test Loss: 0.12103517, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 87/100, Train Loss: 0.00000517, Test Loss: 0.12040674, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 88/100, Train Loss: 0.00000395, Test Loss: 0.12043251, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 89/100, Train Loss: 0.00000308, Test Loss: 0.11986401, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 90/100, Train Loss: 0.00000245, Test Loss: 0.12042289, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 91/100, Train Loss: 0.00000198, Test Loss: 0.12033043, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 92/100, Train Loss: 0.00000160, Test Loss: 0.11956326, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 93/100, Train Loss: 0.00000133, Test Loss: 0.12236862, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 94/100, Train Loss: 0.00000113, Test Loss: 0.12219437, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 95/100, Train Loss: 0.00124768, Test Loss: 0.13390674, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 96/100, Train Loss: 0.00003695, Test Loss: 0.12581130, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 97/100, Train Loss: 0.00000804, Test Loss: 0.12557174, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 98/100, Train Loss: 0.00000547, Test Loss: 0.12555711, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 99/100, Train Loss: 0.00000399, Test Loss: 0.12555233, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 100/100, Train Loss: 0.00000297, Test Loss: 0.12550569, Test Accuracy: 0.98090000\n",
            "[tensor(0.1995, grad_fn=<MeanBackward0>), tensor(0.2227, grad_fn=<MeanBackward0>), tensor(0.2392, grad_fn=<MeanBackward0>), tensor(0.2498, grad_fn=<MeanBackward0>), tensor(0.2574, grad_fn=<MeanBackward0>), tensor(0.2608, grad_fn=<MeanBackward0>), tensor(0.2675, grad_fn=<MeanBackward0>), tensor(0.2704, grad_fn=<MeanBackward0>), tensor(0.2735, grad_fn=<MeanBackward0>), tensor(0.2710, grad_fn=<MeanBackward0>), tensor(0.2747, grad_fn=<MeanBackward0>), tensor(0.2778, grad_fn=<MeanBackward0>), tensor(0.2775, grad_fn=<MeanBackward0>), tensor(0.2794, grad_fn=<MeanBackward0>), tensor(0.2801, grad_fn=<MeanBackward0>), tensor(0.2800, grad_fn=<MeanBackward0>), tensor(0.2804, grad_fn=<MeanBackward0>), tensor(0.2821, grad_fn=<MeanBackward0>), tensor(0.2848, grad_fn=<MeanBackward0>), tensor(0.2833, grad_fn=<MeanBackward0>), tensor(0.2845, grad_fn=<MeanBackward0>), tensor(0.2842, grad_fn=<MeanBackward0>), tensor(0.2890, grad_fn=<MeanBackward0>), tensor(0.2862, grad_fn=<MeanBackward0>), tensor(0.2860, grad_fn=<MeanBackward0>), tensor(0.2869, grad_fn=<MeanBackward0>), tensor(0.2885, grad_fn=<MeanBackward0>), tensor(0.2902, grad_fn=<MeanBackward0>), tensor(0.2930, grad_fn=<MeanBackward0>), tensor(0.2906, grad_fn=<MeanBackward0>), tensor(0.2901, grad_fn=<MeanBackward0>), tensor(0.2897, grad_fn=<MeanBackward0>), tensor(0.2954, grad_fn=<MeanBackward0>), tensor(0.2925, grad_fn=<MeanBackward0>), tensor(0.2915, grad_fn=<MeanBackward0>), tensor(0.2909, grad_fn=<MeanBackward0>), tensor(0.2964, grad_fn=<MeanBackward0>), tensor(0.2946, grad_fn=<MeanBackward0>), tensor(0.2932, grad_fn=<MeanBackward0>), tensor(0.2931, grad_fn=<MeanBackward0>), tensor(0.2927, grad_fn=<MeanBackward0>), tensor(0.2960, grad_fn=<MeanBackward0>), tensor(0.2969, grad_fn=<MeanBackward0>), tensor(0.2956, grad_fn=<MeanBackward0>), tensor(0.2948, grad_fn=<MeanBackward0>), tensor(0.2944, grad_fn=<MeanBackward0>), tensor(0.2943, grad_fn=<MeanBackward0>), tensor(0.2941, grad_fn=<MeanBackward0>), tensor(0.2958, grad_fn=<MeanBackward0>), tensor(0.2959, grad_fn=<MeanBackward0>), tensor(0.2955, grad_fn=<MeanBackward0>), tensor(0.2950, grad_fn=<MeanBackward0>), tensor(0.2946, grad_fn=<MeanBackward0>), tensor(0.2944, grad_fn=<MeanBackward0>), tensor(0.2945, grad_fn=<MeanBackward0>), tensor(0.2975, grad_fn=<MeanBackward0>), tensor(0.2994, grad_fn=<MeanBackward0>), tensor(0.2988, grad_fn=<MeanBackward0>), tensor(0.2981, grad_fn=<MeanBackward0>), tensor(0.2975, grad_fn=<MeanBackward0>), tensor(0.2969, grad_fn=<MeanBackward0>), tensor(0.2964, grad_fn=<MeanBackward0>), tensor(0.2958, grad_fn=<MeanBackward0>), tensor(0.2943, grad_fn=<MeanBackward0>), tensor(0.3011, grad_fn=<MeanBackward0>), tensor(0.3000, grad_fn=<MeanBackward0>), tensor(0.2992, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2981, grad_fn=<MeanBackward0>), tensor(0.2977, grad_fn=<MeanBackward0>), tensor(0.2976, grad_fn=<MeanBackward0>), tensor(0.2970, grad_fn=<MeanBackward0>), tensor(0.3001, grad_fn=<MeanBackward0>), tensor(0.2987, grad_fn=<MeanBackward0>), tensor(0.2985, grad_fn=<MeanBackward0>), tensor(0.2982, grad_fn=<MeanBackward0>), tensor(0.2980, grad_fn=<MeanBackward0>), tensor(0.2979, grad_fn=<MeanBackward0>), tensor(0.2978, grad_fn=<MeanBackward0>), tensor(0.2979, grad_fn=<MeanBackward0>), tensor(0.2975, grad_fn=<MeanBackward0>), tensor(0.2978, grad_fn=<MeanBackward0>), tensor(0.2981, grad_fn=<MeanBackward0>), tensor(0.3008, grad_fn=<MeanBackward0>), tensor(0.3008, grad_fn=<MeanBackward0>), tensor(0.3006, grad_fn=<MeanBackward0>), tensor(0.3003, grad_fn=<MeanBackward0>), tensor(0.3001, grad_fn=<MeanBackward0>), tensor(0.2998, grad_fn=<MeanBackward0>), tensor(0.2994, grad_fn=<MeanBackward0>), tensor(0.2992, grad_fn=<MeanBackward0>), tensor(0.2988, grad_fn=<MeanBackward0>), tensor(0.2989, grad_fn=<MeanBackward0>), tensor(0.2994, grad_fn=<MeanBackward0>), tensor(0.3053, grad_fn=<MeanBackward0>), tensor(0.3049, grad_fn=<MeanBackward0>), tensor(0.3046, grad_fn=<MeanBackward0>), tensor(0.3042, grad_fn=<MeanBackward0>), tensor(0.3037, grad_fn=<MeanBackward0>), tensor(0.3032, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.19952097535133362, 0.2226582020521164, 0.239157572388649, 0.24979428946971893, 0.25739651918411255, 0.2608119249343872, 0.26746615767478943, 0.27040210366249084, 0.27345719933509827, 0.27095821499824524, 0.2747306227684021, 0.277843713760376, 0.27752768993377686, 0.27941760420799255, 0.2801474630832672, 0.28000015020370483, 0.2803889513015747, 0.2820722758769989, 0.28483372926712036, 0.2832659184932709, 0.2845155596733093, 0.28424540162086487, 0.2890392243862152, 0.2862035036087036, 0.286039799451828, 0.2869355082511902, 0.2884855568408966, 0.29022306203842163, 0.29302579164505005, 0.2906018793582916, 0.29013824462890625, 0.2897372543811798, 0.29536473751068115, 0.2925041615962982, 0.29147493839263916, 0.29094231128692627, 0.2963907718658447, 0.2945556044578552, 0.29319700598716736, 0.2930756211280823, 0.29267647862434387, 0.29596221446990967, 0.2968583106994629, 0.2955813407897949, 0.29484379291534424, 0.2943870723247528, 0.2943248152732849, 0.2941252589225769, 0.29576581716537476, 0.2958909571170807, 0.2954829931259155, 0.294968843460083, 0.2946094870567322, 0.294394850730896, 0.2945305109024048, 0.29754793643951416, 0.299353688955307, 0.298816442489624, 0.2981157898902893, 0.29753348231315613, 0.29692012071609497, 0.2964000105857849, 0.2957717180252075, 0.2942666709423065, 0.30107665061950684, 0.3000069856643677, 0.29920655488967896, 0.2986301779747009, 0.2980537414550781, 0.29772061109542847, 0.2976410686969757, 0.2970448136329651, 0.3000887632369995, 0.29870566725730896, 0.2984965145587921, 0.29815831780433655, 0.29797154664993286, 0.2978565990924835, 0.2978309094905853, 0.29789498448371887, 0.29745572805404663, 0.2977718412876129, 0.29810863733291626, 0.3008412718772888, 0.30079102516174316, 0.3006165623664856, 0.30033960938453674, 0.30010244250297546, 0.2998221814632416, 0.2993672788143158, 0.29918235540390015, 0.29878783226013184, 0.2989480495452881, 0.29936593770980835, 0.3052782714366913, 0.3049076199531555, 0.3046049475669861, 0.3042055070400238, 0.30374205112457275, 0.30318519473075867]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}