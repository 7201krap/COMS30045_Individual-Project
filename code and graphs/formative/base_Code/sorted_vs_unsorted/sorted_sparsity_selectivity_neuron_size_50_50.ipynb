{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sorted_sparsity_selectivity_neuron_size_50_50.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/sorted_sparsity_selectivity_neuron_size_50_50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7STrWa0P3z_",
        "outputId": "e43c9d3c-c776-4f9f-f184-0d8a24458f86"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEoCy1vYac8G"
      },
      "source": [
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "root_dir = './'\n",
        "torchvision.datasets.MNIST(root=root_dir,download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4j9WoP-UnAm"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApOU7hvb95W4"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTW5TOUnP5XY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ba562f-accd-49c4-84e5-e609fec63ed4"
      },
      "source": [
        "mnist_trainset = torchvision.datasets.MNIST(root=root_dir, train=True, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "mnist_testset  = torchvision.datasets.MNIST(root=root_dir, \n",
        "                                train=False, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset size:  60000\n",
            "Testing dataset size:  10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz3DIxF_Xb2C"
      },
      "source": [
        "new_mnist_trainset =  [ [[],[]] for i in range(10)]\n",
        "# new_mnist_testset  =  [ [[],[]] for i in range(10)]\n",
        "\n",
        "for i in range(60000):\n",
        "    for j in range(10):\n",
        "        if mnist_trainset[i][1] == j:\n",
        "            # image \n",
        "            new_mnist_trainset[j][0].append(mnist_trainset[i][0])  \n",
        "            # label\n",
        "            new_mnist_trainset[j][1].append(mnist_trainset[i][1])\n",
        "\n",
        "# for i in range(10000):\n",
        "#     for j in range(10):\n",
        "#         if mnist_testset[i][1] == j:\n",
        "#             # image \n",
        "#             new_mnist_testset[j][0].append(mnist_testset[i][0])  \n",
        "#             # label\n",
        "#             new_mnist_testset[j][1].append(mnist_testset[i][1])\n",
        "\n",
        "image_trainset = list()\n",
        "label_trainset = list()\n",
        "\n",
        "# image_testset = list()\n",
        "# label_testset = list()\n",
        "\n",
        "for i in range(10):\n",
        "    image_trainset.append(new_mnist_trainset[i][0])\n",
        "    label_trainset.append(new_mnist_trainset[i][1])\n",
        "\n",
        "# for i in range(10):\n",
        "#     image_testset.append(new_mnist_testset[i][0])\n",
        "#     label_testset.append(new_mnist_testset[i][1])\n",
        "\n",
        "flattened_image_train = list()\n",
        "flattened_label_train = list()\n",
        "\n",
        "# flattened_image_test = list()\n",
        "# flattened_label_test = list()\n",
        "\n",
        "# flattening image \n",
        "for sublist in image_trainset:\n",
        "    for val in sublist:\n",
        "        flattened_image_train.append(val)\n",
        "\n",
        "# flattening label\n",
        "for sublist in label_trainset:\n",
        "    for val in sublist:\n",
        "        flattened_label_train.append(val)\n",
        "\n",
        "# # flattening image \n",
        "# for sublist in image_testset:\n",
        "#     for val in sublist:\n",
        "#         flattened_image_test.append(val)\n",
        "\n",
        "# # flattening label\n",
        "# for sublist in label_testset:\n",
        "#     for val in sublist:\n",
        "#         flattened_label_test.append(val)\n",
        "\n",
        "flattened_image_train = torch.stack(flattened_image_train)\n",
        "flattened_label_train = torch.Tensor(flattened_label_train)\n",
        "flattened_label_train = flattened_label_train.type(torch.LongTensor)\n",
        "\n",
        "# flattened_image_test = torch.stack(flattened_image_test)\n",
        "# flattened_label_test = torch.Tensor(flattened_label_test)\n",
        "# flattened_label_test = flattened_label_test.type(torch.LongTensor)\n",
        "\n",
        "train_dataset = TensorDataset(flattened_image_train, flattened_label_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=50)\n",
        "\n",
        "# test_dataset = TensorDataset(flattened_image_test, flattened_label_test)\n",
        "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXTkEUJ5P6kU"
      },
      "source": [
        "# Define the model \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, hidden_layer_neurons):\n",
        "        super(Model, self).__init__()\n",
        "        self.linear_1 = torch.nn.Linear(784, hidden_layer_neurons)\n",
        "        self.linear_2 = torch.nn.Linear(hidden_layer_neurons, 10)\n",
        "        self.sigmoid  = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        pred = self.linear_2(x)\n",
        "\n",
        "        return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfgvKH6eP9Ou"
      },
      "source": [
        "def get_activation(model):    \n",
        "    def hook(module, input, output):\n",
        "        model.layer_activations = output\n",
        "    return hook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXOpwTXEQFKY"
      },
      "source": [
        "no_epochs = 30\n",
        "def sparsity_selectivity_trainer(optimizer, model, hidden_layer_neurons):\n",
        "\n",
        "    hidden_layer_each_neuron = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(hidden_layer_neurons)]\n",
        "    hidden_layer_each_neuron = np.array(hidden_layer_each_neuron)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    train_loss = list()\n",
        "    test_loss  = list()\n",
        "    test_acc   = list()\n",
        "\n",
        "    best_test_loss = 1\n",
        "\n",
        "    # ***************************** sparsity calc *****************************\n",
        "    final_spareness = list()\n",
        "    # ***************************** sparsity calc *****************************\n",
        "\n",
        "    for epoch in range(no_epochs):\n",
        "        total_train_loss = 0\n",
        "        total_test_loss = 0\n",
        "\n",
        "        # ***************************** sparsity calc *****************************\n",
        "        hidden_layer_activation_list = list()\n",
        "        # ***************************** sparsity calc *****************************\n",
        "\n",
        "        # training\n",
        "        # set up training mode \n",
        "        model.train()\n",
        "\n",
        "        for itr, (images, labels) in enumerate(train_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_train_loss = total_train_loss / (itr + 1)\n",
        "        train_loss.append(total_train_loss)\n",
        "\n",
        "        # testing \n",
        "        # change to evaluation mode \n",
        "        model.eval()\n",
        "        total = 0\n",
        "        for itr, (images, labels) in enumerate(test_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # we now need softmax because we are testing.\n",
        "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "            for i, p in enumerate(pred):\n",
        "                if labels[i] == torch.max(p.data, 0)[1]:\n",
        "                    total = total + 1\n",
        "\n",
        "            # ***************************** sparsity calc *****************************\n",
        "            hidden_layer_activation_list.append(model.layer_activations)\n",
        "            # ***************************** sparsity calc *****************************\n",
        "\n",
        "            # find selectivity at the final epoch \n",
        "            if epoch == no_epochs - 1: # last epoch \n",
        "                for activation, label in zip(model.layer_activations, labels):\n",
        "                    # shape of activation and label: 256 and 1 \n",
        "                    \n",
        "                    # get the actual value of item. This is because label is now Tensor \n",
        "                    label = label.item()\n",
        "\n",
        "                    # this is not part of gradient calculcation \n",
        "                    with torch.no_grad():\n",
        "                        activation = activation.numpy()\n",
        "\n",
        "                    # for each image/label, append activation value of neuron \n",
        "                    for i in range(hidden_layer_neurons):    # number of neurons in hidden layer \n",
        "                        hidden_layer_each_neuron[i][label].append(activation[i])\n",
        "\n",
        "        # ***************************** sparsity calc *****************************\n",
        "        final_spareness.append(hidden_layer_activation_list)\n",
        "        # ***************************** sparsity calc *****************************\n",
        "\n",
        "        # caculate accuracy \n",
        "        accuracy = total / len(mnist_testset)\n",
        "\n",
        "        # append accuracy here\n",
        "        test_acc.append(accuracy)\n",
        "\n",
        "        # append test loss here \n",
        "        total_test_loss = total_test_loss / (itr + 1)\n",
        "        test_loss.append(total_test_loss)\n",
        "\n",
        "        print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Test Loss: {:.8f}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, total_train_loss, total_test_loss, accuracy))\n",
        "\n",
        "    # ***************************** selectivity calc *****************************\n",
        "    # I will now try to find the average of each class for each neuron.\n",
        "    # check out the next cell \n",
        "    avg_activations = [dict() for x in range(hidden_layer_neurons)]\n",
        "    for i, neuron in enumerate(hidden_layer_each_neuron):\n",
        "        for k, v in neuron.items():\n",
        "            # v is the list of activations for hidden layer's neuron k \n",
        "            avg_activations[i][k] = sum(v) / float(len(v))\n",
        "\n",
        "    # generate 256 lists to get only values in avg_activations\n",
        "    only_activation_vals = [list() for x in range(hidden_layer_neurons)]\n",
        "\n",
        "    # selectivity_list contains all of the selectivity of each neuron \n",
        "    selectivity_list = list()\n",
        "\n",
        "    # get only values from avg_activations\n",
        "    for i, avg_activation in enumerate(avg_activations):\n",
        "        for value in avg_activation.values():\n",
        "            only_activation_vals[i].append(value)\n",
        "\n",
        "\n",
        "    for activation_val in only_activation_vals:\n",
        "        # find u_max \n",
        "        u_max = np.max(activation_val)\n",
        "\n",
        "        # find u_minus_max \n",
        "        u_minus_max = (np.sum(activation_val) - u_max) / 9\n",
        "\n",
        "        # find selectivity \n",
        "        selectivity = (u_max - u_minus_max) / (u_max + u_minus_max)\n",
        "\n",
        "        # append selectivity value to selectivity_list\n",
        "        selectivity_list.append(selectivity)\n",
        "    # ***************************** selectivity calc *****************************\n",
        "\n",
        "    # ***************************** sparsity calc *****************************\n",
        "    sparseness_list = list()\n",
        "\n",
        "    for single_epoch_spareness in final_spareness:\n",
        "        single_epoch_spareness = torch.stack(single_epoch_spareness)\n",
        "        layer_activations_list = torch.reshape(single_epoch_spareness, (10000, hidden_layer_neurons))\n",
        "\n",
        "        layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
        "        num_neurons = layer_activations_list.shape[1]\n",
        "        population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
        "        mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
        "\n",
        "        sparseness_list.append(mean_sparseness_per_epoch)\n",
        "     # ***************************** sparsity calc *****************************\n",
        "\n",
        "    print(\"sparseness_list\", sparseness_list)\n",
        "    print(\"selectivity_list\", selectivity_list)\n",
        "\n",
        "    return test_acc, sparseness_list, selectivity_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npZ3QvIhNQas"
      },
      "source": [
        "hidden_layer_neurons1 = [64, 128, 256, 384, 512, 768, 1024]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WKq9qSgMADr"
      },
      "source": [
        "# AdaDelta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4WytqcJRZxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25caa98-bbf8-4cd2-8db6-028dc8a09dac"
      },
      "source": [
        "for i in range(len(hidden_layer_neurons)):\n",
        "    print(f\"Entering {i}-th loop\")\n",
        "    model_Adadelta = Model(hidden_layer_neurons[i])\n",
        "    print(\"model_Adadelta:\", model_Adadelta)\n",
        "    model_Adadelta.to(device)\n",
        "    model_Adadelta.sigmoid.register_forward_hook(get_activation(model_Adadelta))\n",
        "    optimizer_Adadelta = torch.optim.Adadelta(model_Adadelta.parameters(), lr=1.0)\n",
        "    Adadelta_test_acc, Adadelta_sparsity_list, Adadelta_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adadelta, model=model_Adadelta, hidden_layer_neurons=hidden_layer_neurons[i])\n",
        "    \n",
        "    if i == 0:\n",
        "        f = open(\"neurons_sparsity_selectivity_1_50_Adadelta.txt\", \"w\")\n",
        "        f.write(str(i)+'\\n'+str(Adadelta_test_acc)+'\\n'+str(Adadelta_sparsity_list)+'\\n'+str(np.average(Adadelta_selectivity_list))+'\\n'+str(np.std(Adadelta_selectivity_list))+'\\n\\n')\n",
        "    else:\n",
        "        f = open(\"neurons_sparsity_selectivity_1_50_Adadelta.txt\", \"a\")\n",
        "        f.write(str(i)+'\\n'+str(Adadelta_test_acc)+'\\n'+str(Adadelta_sparsity_list)+'\\n'+str(np.average(Adadelta_selectivity_list))+'\\n'+str(np.std(Adadelta_selectivity_list))+'\\n\\n')\n",
        "\n",
        "f.close()\n",
        "\n",
        "!cp neurons_sparsity_selectivity_1_50_Adadelta.txt /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entering 0-th loop\n",
            "model_Adadelta: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (linear_2): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hXfQe4vMDKB"
      },
      "source": [
        "# AdaGrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb-4TPM5MGuE"
      },
      "source": [
        "for i in range(len(hidden_layer_neurons)):\n",
        "    print(f\"Entering {i}-th loop\")\n",
        "    model_Adagrad = Model(hidden_layer_neurons[i])\n",
        "    print(\"model_Adagrad:\", model_Adagrad)\n",
        "    model_Adagrad.to(device)\n",
        "    model_Adagrad.sigmoid.register_forward_hook(get_activation(model_Adagrad))\n",
        "    optimizer_Adagrad = torch.optim.Adagrad(model_Adagrad.parameters(), lr=0.1)\n",
        "    Adagrad_test_acc, Adagrad_sparsity_list, Adagrad_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adagrad, model=model_Adagrad, hidden_layer_neurons=hidden_layer_neurons[i])\n",
        "    \n",
        "    if i == 0:\n",
        "        f = open(\"neurons_sparsity_selectivity_Adagrad_1_50.txt\", \"w\")\n",
        "        f.write(str(i)+'\\n'+str(Adagrad_test_acc)+'\\n'+str(Adagrad_sparsity_list)+'\\n'+str(np.average(Adagrad_selectivity_list))+'\\n'+str(np.std(Adagrad_selectivity_list))+'\\n\\n')\n",
        "    else:\n",
        "        f = open(\"neurons_sparsity_selectivity_Adagrad_1_50.txt\", \"a\")\n",
        "        f.write(str(i)+'\\n'+str(Adagrad_test_acc)+'\\n'+str(Adagrad_sparsity_list)+'\\n'+str(np.average(Adagrad_selectivity_list))+'\\n'+str(np.std(Adagrad_selectivity_list))+'\\n\\n')\n",
        "\n",
        "f.close()\n",
        "\n",
        "!cp neurons_sparsity_selectivity_Adagrad_1_50.txt /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmLJ4Zr2MnoS"
      },
      "source": [
        "# SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ObsEJHuMoPy"
      },
      "source": [
        "for i in range(len(hidden_layer_neurons)):\n",
        "    print(f\"Entering {i}-th loop\")\n",
        "    model_SGD = Model(hidden_layer_neurons[i])\n",
        "    print(\"model_SGD:\", model_SGD)\n",
        "    model_SGD.to(device)\n",
        "    model_SGD.sigmoid.register_forward_hook(get_activation(model_SGD))\n",
        "    optimizer_SGD = torch.optim.SGD(model_SGD.parameters(), lr=0.1)\n",
        "    SGD_test_acc, SGD_sparsity_list, SGD_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_SGD, model=model_SGD, hidden_layer_neurons=hidden_layer_neurons[i])\n",
        "    \n",
        "    if i == 0:\n",
        "        f = open(\"neurons_sparsity_selectivity_SGD_1_50.txt\", \"w\")\n",
        "        f.write(str(i)+'\\n'+str(SGD_test_acc)+'\\n'+str(SGD_sparsity_list)+'\\n'+str(np.average(SGD_selectivity_list))+'\\n'+str(np.std(SGD_selectivity_list))+'\\n\\n')\n",
        "    else:\n",
        "        f = open(\"neurons_sparsity_selectivity_SGD_1_50.txt\", \"a\")\n",
        "        f.write(str(i)+'\\n'+str(SGD_test_acc)+'\\n'+str(SGD_sparsity_list)+'\\n'+str(np.average(SGD_selectivity_list))+'\\n'+str(np.std(SGD_selectivity_list))+'\\n\\n')\n",
        "\n",
        "f.close()\n",
        "\n",
        "!cp neurons_sparsity_selectivity_SGD_1_50.txt /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvQxaN_fRXLq"
      },
      "source": [
        "# Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkqfFoVkRXxP"
      },
      "source": [
        "for i in range(len(hidden_layer_neurons)):\n",
        "    print(f\"Entering {i}-th loop\")\n",
        "    model_Adam = Model(hidden_layer_neurons[i])\n",
        "    print(\"model_Adam:\", model_Adam)\n",
        "    model_Adam.to(device)\n",
        "    model_Adam.sigmoid.register_forward_hook(get_activation(model_Adam))\n",
        "    optimizer_Adam = torch.optim.Adam(model_Adam.parameters(), lr=0.001)\n",
        "    Adam_test_acc, Adam_sparsity_list, Adam_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adam, model=model_Adam, hidden_layer_neurons=hidden_layer_neurons[i])\n",
        "    \n",
        "    if i == 0:\n",
        "        f = open(\"neurons_sparsity_selectivity_Adam_1_50.txt\", \"w\")\n",
        "        f.write(str(i)+'\\n'+str(Adam_test_acc)+'\\n'+str(Adam_sparsity_list)+'\\n'+str(np.average(Adam_selectivity_list))+'\\n'+str(np.std(Adam_selectivity_list))+'\\n\\n')\n",
        "    else:\n",
        "        f = open(\"neurons_sparsity_selectivity_Adam_1_50.txt\", \"a\")\n",
        "        f.write(str(i)+'\\n'+str(Adam_test_acc)+'\\n'+str(Adam_sparsity_list)+'\\n'+str(np.average(Adam_selectivity_list))+'\\n'+str(np.std(Adam_selectivity_list))+'\\n\\n')\n",
        "\n",
        "f.close()\n",
        "\n",
        "!cp neurons_sparsity_selectivity_Adam_1_50.txt /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y63CbCBo_Tk"
      },
      "source": [
        "hidden_layer_neurons1 = [64, 128, 256, 384, 512, 768, 1024]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPw5kqt3akGt",
        "outputId": "97fbf775-7b33-474c-ec36-095410b8630b"
      },
      "source": [
        "model_Adam = Model(64)\n",
        "print(\"model_Adam:\", model_Adam)\n",
        "model_Adam.to(device)\n",
        "model_Adam.sigmoid.register_forward_hook(get_activation(model_Adam))\n",
        "optimizer_Adam = torch.optim.Adam(model_Adam.parameters(), lr=0.001)\n",
        "Adam_test_acc, Adam_sparsity_list, Adam_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adam, model=model_Adam, hidden_layer_neurons=64)\n",
        "print(Adam_test_acc)\n",
        "print(Adam_sparsity_list)\n",
        "print(np.average(Adam_selectivity_list))\n",
        "print(np.std(Adam_selectivity_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_Adam: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (linear_2): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "Epoch: 1/30, Train Loss: 0.25198588, Test Loss: 0.15855411, Test Accuracy: 0.95370000\n",
            "\n",
            "Epoch: 2/30, Train Loss: 0.12578949, Test Loss: 0.11986096, Test Accuracy: 0.96460000\n",
            "\n",
            "Epoch: 3/30, Train Loss: 0.09790932, Test Loss: 0.10899269, Test Accuracy: 0.96650000\n",
            "\n",
            "Epoch: 4/30, Train Loss: 0.08397265, Test Loss: 0.11409606, Test Accuracy: 0.96610000\n",
            "\n",
            "Epoch: 5/30, Train Loss: 0.07209467, Test Loss: 0.10792961, Test Accuracy: 0.96870000\n",
            "\n",
            "Epoch: 6/30, Train Loss: 0.06334574, Test Loss: 0.10678061, Test Accuracy: 0.97040000\n",
            "\n",
            "Epoch: 7/30, Train Loss: 0.05699084, Test Loss: 0.11253214, Test Accuracy: 0.97020000\n",
            "\n",
            "Epoch: 8/30, Train Loss: 0.05108855, Test Loss: 0.11307145, Test Accuracy: 0.97040000\n",
            "\n",
            "Epoch: 9/30, Train Loss: 0.04738297, Test Loss: 0.11672325, Test Accuracy: 0.96930000\n",
            "\n",
            "Epoch: 10/30, Train Loss: 0.04253052, Test Loss: 0.11899182, Test Accuracy: 0.96740000\n",
            "\n",
            "Epoch: 11/30, Train Loss: 0.03756785, Test Loss: 0.12899380, Test Accuracy: 0.96860000\n",
            "\n",
            "Epoch: 12/30, Train Loss: 0.03536385, Test Loss: 0.12656445, Test Accuracy: 0.96940000\n",
            "\n",
            "Epoch: 13/30, Train Loss: 0.03215978, Test Loss: 0.12733732, Test Accuracy: 0.97020000\n",
            "\n",
            "Epoch: 14/30, Train Loss: 0.02968824, Test Loss: 0.13899680, Test Accuracy: 0.96690000\n",
            "\n",
            "Epoch: 15/30, Train Loss: 0.02686568, Test Loss: 0.13453795, Test Accuracy: 0.96870000\n",
            "\n",
            "Epoch: 16/30, Train Loss: 0.02421440, Test Loss: 0.13403659, Test Accuracy: 0.96980000\n",
            "\n",
            "Epoch: 17/30, Train Loss: 0.02221478, Test Loss: 0.15166027, Test Accuracy: 0.96920000\n",
            "\n",
            "Epoch: 18/30, Train Loss: 0.02177951, Test Loss: 0.15682498, Test Accuracy: 0.96570000\n",
            "\n",
            "Epoch: 19/30, Train Loss: 0.02057014, Test Loss: 0.14841648, Test Accuracy: 0.96980000\n",
            "\n",
            "Epoch: 20/30, Train Loss: 0.01774318, Test Loss: 0.15235751, Test Accuracy: 0.97010000\n",
            "\n",
            "Epoch: 21/30, Train Loss: 0.01605315, Test Loss: 0.16462445, Test Accuracy: 0.96670000\n",
            "\n",
            "Epoch: 22/30, Train Loss: 0.01517349, Test Loss: 0.16689862, Test Accuracy: 0.96750000\n",
            "\n",
            "Epoch: 23/30, Train Loss: 0.01492208, Test Loss: 0.17066774, Test Accuracy: 0.96790000\n",
            "\n",
            "Epoch: 24/30, Train Loss: 0.01413810, Test Loss: 0.17818036, Test Accuracy: 0.96610000\n",
            "\n",
            "Epoch: 25/30, Train Loss: 0.01266468, Test Loss: 0.17734496, Test Accuracy: 0.96680000\n",
            "\n",
            "Epoch: 26/30, Train Loss: 0.01131327, Test Loss: 0.17811388, Test Accuracy: 0.96830000\n",
            "\n",
            "Epoch: 27/30, Train Loss: 0.01221015, Test Loss: 0.17583875, Test Accuracy: 0.96820000\n",
            "\n",
            "Epoch: 28/30, Train Loss: 0.01068154, Test Loss: 0.18931571, Test Accuracy: 0.96860000\n",
            "\n",
            "Epoch: 29/30, Train Loss: 0.00995743, Test Loss: 0.19045753, Test Accuracy: 0.96790000\n",
            "\n",
            "Epoch: 30/30, Train Loss: 0.00959276, Test Loss: 0.19263161, Test Accuracy: 0.96690000\n",
            "sparseness_list [tensor(0.3263, grad_fn=<MeanBackward0>), tensor(0.3387, grad_fn=<MeanBackward0>), tensor(0.3548, grad_fn=<MeanBackward0>), tensor(0.3411, grad_fn=<MeanBackward0>), tensor(0.3441, grad_fn=<MeanBackward0>), tensor(0.3492, grad_fn=<MeanBackward0>), tensor(0.3471, grad_fn=<MeanBackward0>), tensor(0.3502, grad_fn=<MeanBackward0>), tensor(0.3519, grad_fn=<MeanBackward0>), tensor(0.3537, grad_fn=<MeanBackward0>), tensor(0.3513, grad_fn=<MeanBackward0>), tensor(0.3545, grad_fn=<MeanBackward0>), tensor(0.3556, grad_fn=<MeanBackward0>), tensor(0.3570, grad_fn=<MeanBackward0>), tensor(0.3607, grad_fn=<MeanBackward0>), tensor(0.3598, grad_fn=<MeanBackward0>), tensor(0.3528, grad_fn=<MeanBackward0>), tensor(0.3531, grad_fn=<MeanBackward0>), tensor(0.3576, grad_fn=<MeanBackward0>), tensor(0.3597, grad_fn=<MeanBackward0>), tensor(0.3598, grad_fn=<MeanBackward0>), tensor(0.3507, grad_fn=<MeanBackward0>), tensor(0.3543, grad_fn=<MeanBackward0>), tensor(0.3522, grad_fn=<MeanBackward0>), tensor(0.3596, grad_fn=<MeanBackward0>), tensor(0.3609, grad_fn=<MeanBackward0>), tensor(0.3641, grad_fn=<MeanBackward0>), tensor(0.3612, grad_fn=<MeanBackward0>), tensor(0.3584, grad_fn=<MeanBackward0>), tensor(0.3611, grad_fn=<MeanBackward0>)]\n",
            "selectivity_list [0.15836512702014116, 0.5116081422874834, 0.6392683845768206, 0.5629762954788554, 0.5420759174245162, 0.22685844375808076, 0.17002851667898838, 0.5386732250237811, 0.5073878426203432, 0.6196684707063687, 0.38117161146504774, 0.2302413785169062, 0.6104356457322929, 0.2483466191038374, 0.5056619262815261, 0.5484583607161082, 0.6020361492034194, 0.32893668829857076, 0.3316400837041926, 0.4157956501158505, 0.2541051407269003, 0.5649107073952185, 0.5513472366020369, 0.5331307256867327, 0.055773740621787325, 0.18925986911059356, 0.12118201445005353, 0.6826208440536664, 0.2612004739890481, 0.6106574804970468, 0.2956504687637625, 0.7426886860606903, 0.42402213266829086, 0.16820005697916876, 0.753574895156045, 0.365222201300701, 0.23966072965139906, 0.6014411696235474, 0.27807128404317016, 0.514274977060865, 0.43015542608098634, 0.26682247789595404, 0.4360958188376517, 0.12024991773436713, 0.06867194860639263, 0.5867989722895229, 0.18850030653019653, 0.285538577558044, 0.46748768130758817, 0.5717177654113332, 0.3504604757735495, 0.09054771891021142, 0.28014740668324484, 0.5330021167310944, 0.3680694430246601, 0.20794793241824977, 0.6126626296147132, 0.3116268713283772, 0.08103160314339279, 0.8159339649801536, 0.21916333401462826, 0.49683973019547084, 0.7938757526266095, 0.5561527454090676]\n",
            "[0.9537, 0.9646, 0.9665, 0.9661, 0.9687, 0.9704, 0.9702, 0.9704, 0.9693, 0.9674, 0.9686, 0.9694, 0.9702, 0.9669, 0.9687, 0.9698, 0.9692, 0.9657, 0.9698, 0.9701, 0.9667, 0.9675, 0.9679, 0.9661, 0.9668, 0.9683, 0.9682, 0.9686, 0.9679, 0.9669]\n",
            "[tensor(0.3263, grad_fn=<MeanBackward0>), tensor(0.3387, grad_fn=<MeanBackward0>), tensor(0.3548, grad_fn=<MeanBackward0>), tensor(0.3411, grad_fn=<MeanBackward0>), tensor(0.3441, grad_fn=<MeanBackward0>), tensor(0.3492, grad_fn=<MeanBackward0>), tensor(0.3471, grad_fn=<MeanBackward0>), tensor(0.3502, grad_fn=<MeanBackward0>), tensor(0.3519, grad_fn=<MeanBackward0>), tensor(0.3537, grad_fn=<MeanBackward0>), tensor(0.3513, grad_fn=<MeanBackward0>), tensor(0.3545, grad_fn=<MeanBackward0>), tensor(0.3556, grad_fn=<MeanBackward0>), tensor(0.3570, grad_fn=<MeanBackward0>), tensor(0.3607, grad_fn=<MeanBackward0>), tensor(0.3598, grad_fn=<MeanBackward0>), tensor(0.3528, grad_fn=<MeanBackward0>), tensor(0.3531, grad_fn=<MeanBackward0>), tensor(0.3576, grad_fn=<MeanBackward0>), tensor(0.3597, grad_fn=<MeanBackward0>), tensor(0.3598, grad_fn=<MeanBackward0>), tensor(0.3507, grad_fn=<MeanBackward0>), tensor(0.3543, grad_fn=<MeanBackward0>), tensor(0.3522, grad_fn=<MeanBackward0>), tensor(0.3596, grad_fn=<MeanBackward0>), tensor(0.3609, grad_fn=<MeanBackward0>), tensor(0.3641, grad_fn=<MeanBackward0>), tensor(0.3612, grad_fn=<MeanBackward0>), tensor(0.3584, grad_fn=<MeanBackward0>), tensor(0.3611, grad_fn=<MeanBackward0>)]\n",
            "0.4066582801603018\n",
            "0.19537547854468862\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7KvaniNpIBd",
        "outputId": "c782afa6-0372-48a7-a140-6cb948c4c43b"
      },
      "source": [
        "model_Adam = Model(128)\n",
        "print(\"model_Adam:\", model_Adam)\n",
        "model_Adam.to(device)\n",
        "model_Adam.sigmoid.register_forward_hook(get_activation(model_Adam))\n",
        "optimizer_Adam = torch.optim.Adam(model_Adam.parameters(), lr=0.001)\n",
        "Adam_test_acc, Adam_sparsity_list, Adam_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adam, model=model_Adam, hidden_layer_neurons=128)\n",
        "print(Adam_test_acc)\n",
        "print(Adam_sparsity_list)\n",
        "print(np.average(Adam_selectivity_list))\n",
        "print(np.std(Adam_selectivity_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_Adam: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (linear_2): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "Epoch: 1/30, Train Loss: 0.22432642, Test Loss: 0.12577930, Test Accuracy: 0.96170000\n",
            "\n",
            "Epoch: 2/30, Train Loss: 0.10123075, Test Loss: 0.10302111, Test Accuracy: 0.97040000\n",
            "\n",
            "Epoch: 3/30, Train Loss: 0.07435209, Test Loss: 0.08469324, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 4/30, Train Loss: 0.05626258, Test Loss: 0.10261932, Test Accuracy: 0.97170000\n",
            "\n",
            "Epoch: 5/30, Train Loss: 0.04623749, Test Loss: 0.10283497, Test Accuracy: 0.97150000\n",
            "\n",
            "Epoch: 6/30, Train Loss: 0.03917257, Test Loss: 0.08818581, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 7/30, Train Loss: 0.03178251, Test Loss: 0.09448713, Test Accuracy: 0.97610000\n",
            "\n",
            "Epoch: 8/30, Train Loss: 0.02650865, Test Loss: 0.11751689, Test Accuracy: 0.97140000\n",
            "\n",
            "Epoch: 9/30, Train Loss: 0.02329846, Test Loss: 0.10171043, Test Accuracy: 0.97560000\n",
            "\n",
            "Epoch: 10/30, Train Loss: 0.02044091, Test Loss: 0.10279876, Test Accuracy: 0.97520000\n",
            "\n",
            "Epoch: 11/30, Train Loss: 0.01855946, Test Loss: 0.11653364, Test Accuracy: 0.97390000\n",
            "\n",
            "Epoch: 12/30, Train Loss: 0.01546357, Test Loss: 0.10724661, Test Accuracy: 0.97620000\n",
            "\n",
            "Epoch: 13/30, Train Loss: 0.01380396, Test Loss: 0.11087756, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 14/30, Train Loss: 0.01195094, Test Loss: 0.11165124, Test Accuracy: 0.97620000\n",
            "\n",
            "Epoch: 15/30, Train Loss: 0.01015270, Test Loss: 0.11928251, Test Accuracy: 0.97660000\n",
            "\n",
            "Epoch: 16/30, Train Loss: 0.00879981, Test Loss: 0.12308275, Test Accuracy: 0.97670000\n",
            "\n",
            "Epoch: 17/30, Train Loss: 0.00728258, Test Loss: 0.13143275, Test Accuracy: 0.97600000\n",
            "\n",
            "Epoch: 18/30, Train Loss: 0.00697771, Test Loss: 0.12478718, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 19/30, Train Loss: 0.00651018, Test Loss: 0.13557122, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 20/30, Train Loss: 0.00617579, Test Loss: 0.14274092, Test Accuracy: 0.97670000\n",
            "\n",
            "Epoch: 21/30, Train Loss: 0.00580347, Test Loss: 0.13081486, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 22/30, Train Loss: 0.00503089, Test Loss: 0.14619987, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 23/30, Train Loss: 0.00457278, Test Loss: 0.14561370, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 24/30, Train Loss: 0.00375564, Test Loss: 0.15436276, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 25/30, Train Loss: 0.00404544, Test Loss: 0.15908680, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 26/30, Train Loss: 0.00426119, Test Loss: 0.16476214, Test Accuracy: 0.97570000\n",
            "\n",
            "Epoch: 27/30, Train Loss: 0.00313831, Test Loss: 0.16884801, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 28/30, Train Loss: 0.00313084, Test Loss: 0.16016449, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 29/30, Train Loss: 0.00304761, Test Loss: 0.17873752, Test Accuracy: 0.97440000\n",
            "\n",
            "Epoch: 30/30, Train Loss: 0.00245983, Test Loss: 0.17537077, Test Accuracy: 0.97520000\n",
            "sparseness_list [tensor(0.3698, grad_fn=<MeanBackward0>), tensor(0.3826, grad_fn=<MeanBackward0>), tensor(0.3855, grad_fn=<MeanBackward0>), tensor(0.3956, grad_fn=<MeanBackward0>), tensor(0.3924, grad_fn=<MeanBackward0>), tensor(0.4050, grad_fn=<MeanBackward0>), tensor(0.3909, grad_fn=<MeanBackward0>), tensor(0.3974, grad_fn=<MeanBackward0>), tensor(0.4047, grad_fn=<MeanBackward0>), tensor(0.3953, grad_fn=<MeanBackward0>), tensor(0.3953, grad_fn=<MeanBackward0>), tensor(0.3984, grad_fn=<MeanBackward0>), tensor(0.3906, grad_fn=<MeanBackward0>), tensor(0.4005, grad_fn=<MeanBackward0>), tensor(0.4001, grad_fn=<MeanBackward0>), tensor(0.4002, grad_fn=<MeanBackward0>), tensor(0.3992, grad_fn=<MeanBackward0>), tensor(0.4038, grad_fn=<MeanBackward0>), tensor(0.3944, grad_fn=<MeanBackward0>), tensor(0.3977, grad_fn=<MeanBackward0>), tensor(0.4018, grad_fn=<MeanBackward0>), tensor(0.3968, grad_fn=<MeanBackward0>), tensor(0.4005, grad_fn=<MeanBackward0>), tensor(0.3920, grad_fn=<MeanBackward0>), tensor(0.3983, grad_fn=<MeanBackward0>), tensor(0.3944, grad_fn=<MeanBackward0>), tensor(0.3987, grad_fn=<MeanBackward0>), tensor(0.4003, grad_fn=<MeanBackward0>), tensor(0.3993, grad_fn=<MeanBackward0>), tensor(0.3959, grad_fn=<MeanBackward0>)]\n",
            "selectivity_list [0.5304827888204722, 0.42830071658148966, 0.5111162344085276, 0.1874233924682725, 0.297978868687607, 0.39183407429688716, 0.597465916208158, 0.4087405044674609, 0.13248113449180893, 0.4349907005084638, 0.6504259038412692, 0.45645869822886087, 0.4357322389990589, 0.3877108323537808, 0.3768961997874145, 0.35738432557024774, 0.2850990058164588, 0.2525313090652042, 0.3487073317105299, 0.3259328049767409, 0.5571313870899068, 0.4987617755783032, 0.5463916256408423, 0.1497160310840819, 0.5520406743836884, 0.5754396748605554, 0.47427851507493707, 0.46836369404678757, 0.22154523413954508, 0.4491393344596099, 0.5313562157336554, 0.35519466334181077, 0.605962023867144, 0.23030950376901194, 0.230753510773438, 0.19680521613986734, 0.580041098294704, 0.38619247779950416, 0.6719307297201341, 0.5596195006533203, 0.34683113088231565, 0.6712425229441651, 0.5627567590023265, 0.48802579059638, 0.2658444232955021, 0.3080211704959308, 0.5170022998938877, 0.4929997006163652, 0.7590379580945313, 0.3832450754789896, 0.6194380150278855, 0.5895832853470626, 0.44287552693780585, 0.40829083273967787, 0.16414892786857352, 0.339709827378912, 0.2604641861490399, 0.5137229755458571, 0.687017866549333, 0.08648150554813286, 0.49102688759197016, 0.14782567254780032, 0.41653075387921396, 0.28839376477809286, 0.32586416581735395, 0.6845789406620694, 0.6964036861273537, 0.5671135211162428, 0.2626403114786205, 0.251470840878773, 0.36894411974994584, 0.27330318710458784, 0.6071089838652586, 0.34772764545638407, 0.4600677889883135, 0.5087621003397591, 0.762925831002813, 0.6900420261145763, 0.3678183366801363, 0.5158611316763737, 0.2013684393458634, 0.26002258883419255, 0.23972451210840554, 0.5362116660237954, 0.5301182344348532, 0.4552123965237775, 0.31974978394786097, 0.19973969585074292, 0.49758281237222246, 0.5066967889411029, 0.22670460680289872, 0.7455304278548375, 0.5607314087358871, 0.40197128378570113, 0.7043280516899351, 0.29399407018516965, 0.4563172117542331, 0.4123070596088194, 0.562526791666843, 0.6114521301396201, 0.6745646023431413, 0.42338407061361316, 0.25047890362314096, 0.48474207441902334, 0.4680254897409695, 0.8163594066180334, 0.633694746744088, 0.619943976789013, 0.23003103895309876, 0.3845780053825567, 0.3749483057380423, 0.472752795267515, 0.34347747035131587, 0.47633692152935436, 0.4716930789718571, 0.4712284072391394, 0.42492030275255693, 0.268219014415115, 0.2542904578279726, 0.4307189069526357, 0.3814941175918605, 0.5256638315598654, 0.5299652135307604, 0.1815861227711344, 0.7730840078471767, 0.3752700510411146, 0.6219337297294508, 0.567581106631576]\n",
            "[0.9617, 0.9704, 0.9751, 0.9717, 0.9715, 0.9779, 0.9761, 0.9714, 0.9756, 0.9752, 0.9739, 0.9762, 0.9769, 0.9762, 0.9766, 0.9767, 0.976, 0.9772, 0.9776, 0.9767, 0.9788, 0.9774, 0.9773, 0.977, 0.977, 0.9757, 0.9771, 0.9778, 0.9744, 0.9752]\n",
            "[tensor(0.3698, grad_fn=<MeanBackward0>), tensor(0.3826, grad_fn=<MeanBackward0>), tensor(0.3855, grad_fn=<MeanBackward0>), tensor(0.3956, grad_fn=<MeanBackward0>), tensor(0.3924, grad_fn=<MeanBackward0>), tensor(0.4050, grad_fn=<MeanBackward0>), tensor(0.3909, grad_fn=<MeanBackward0>), tensor(0.3974, grad_fn=<MeanBackward0>), tensor(0.4047, grad_fn=<MeanBackward0>), tensor(0.3953, grad_fn=<MeanBackward0>), tensor(0.3953, grad_fn=<MeanBackward0>), tensor(0.3984, grad_fn=<MeanBackward0>), tensor(0.3906, grad_fn=<MeanBackward0>), tensor(0.4005, grad_fn=<MeanBackward0>), tensor(0.4001, grad_fn=<MeanBackward0>), tensor(0.4002, grad_fn=<MeanBackward0>), tensor(0.3992, grad_fn=<MeanBackward0>), tensor(0.4038, grad_fn=<MeanBackward0>), tensor(0.3944, grad_fn=<MeanBackward0>), tensor(0.3977, grad_fn=<MeanBackward0>), tensor(0.4018, grad_fn=<MeanBackward0>), tensor(0.3968, grad_fn=<MeanBackward0>), tensor(0.4005, grad_fn=<MeanBackward0>), tensor(0.3920, grad_fn=<MeanBackward0>), tensor(0.3983, grad_fn=<MeanBackward0>), tensor(0.3944, grad_fn=<MeanBackward0>), tensor(0.3987, grad_fn=<MeanBackward0>), tensor(0.4003, grad_fn=<MeanBackward0>), tensor(0.3993, grad_fn=<MeanBackward0>), tensor(0.3959, grad_fn=<MeanBackward0>)]\n",
            "0.43932087079351373\n",
            "0.15880468145726365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFCcmlQ_qAzc",
        "outputId": "e5c1462f-12e5-4c08-afcf-6109943c362e"
      },
      "source": [
        "model_Adam = Model(256)\n",
        "print(\"model_Adam:\", model_Adam)\n",
        "model_Adam.to(device)\n",
        "model_Adam.sigmoid.register_forward_hook(get_activation(model_Adam))\n",
        "optimizer_Adam = torch.optim.Adam(model_Adam.parameters(), lr=0.001)\n",
        "Adam_test_acc, Adam_sparsity_list, Adam_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adam, model=model_Adam, hidden_layer_neurons=256)\n",
        "print(Adam_test_acc)\n",
        "print(Adam_sparsity_list)\n",
        "print(np.average(Adam_selectivity_list))\n",
        "print(np.std(Adam_selectivity_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_Adam: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "Epoch: 1/30, Train Loss: 0.20718922, Test Loss: 0.10255038, Test Accuracy: 0.97040000\n",
            "\n",
            "Epoch: 2/30, Train Loss: 0.08990572, Test Loss: 0.10265782, Test Accuracy: 0.96970000\n",
            "\n",
            "Epoch: 3/30, Train Loss: 0.06350397, Test Loss: 0.08522690, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 4/30, Train Loss: 0.04829945, Test Loss: 0.08215662, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 5/30, Train Loss: 0.03661278, Test Loss: 0.09014102, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 6/30, Train Loss: 0.03028339, Test Loss: 0.09336657, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 7/30, Train Loss: 0.02300705, Test Loss: 0.11021034, Test Accuracy: 0.97670000\n",
            "\n",
            "Epoch: 8/30, Train Loss: 0.01939068, Test Loss: 0.12636753, Test Accuracy: 0.97600000\n",
            "\n",
            "Epoch: 9/30, Train Loss: 0.01669428, Test Loss: 0.09757675, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 10/30, Train Loss: 0.01345836, Test Loss: 0.10867214, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 11/30, Train Loss: 0.01219790, Test Loss: 0.13218978, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 12/30, Train Loss: 0.01054516, Test Loss: 0.11463100, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 13/30, Train Loss: 0.00978257, Test Loss: 0.13450801, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 14/30, Train Loss: 0.00666936, Test Loss: 0.13761329, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 15/30, Train Loss: 0.00661207, Test Loss: 0.12190056, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 16/30, Train Loss: 0.00543951, Test Loss: 0.13638827, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 17/30, Train Loss: 0.00508945, Test Loss: 0.13558586, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 18/30, Train Loss: 0.00530257, Test Loss: 0.15665202, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 19/30, Train Loss: 0.00484270, Test Loss: 0.14189654, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 20/30, Train Loss: 0.00411230, Test Loss: 0.15625987, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 21/30, Train Loss: 0.00380206, Test Loss: 0.16404695, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 22/30, Train Loss: 0.00408099, Test Loss: 0.16577947, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 23/30, Train Loss: 0.00210564, Test Loss: 0.16317501, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 24/30, Train Loss: 0.00323314, Test Loss: 0.17120059, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 25/30, Train Loss: 0.00226530, Test Loss: 0.16126852, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 26/30, Train Loss: 0.00231578, Test Loss: 0.17319358, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 27/30, Train Loss: 0.00324859, Test Loss: 0.16958356, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 28/30, Train Loss: 0.00321068, Test Loss: 0.18622008, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 29/30, Train Loss: 0.00215368, Test Loss: 0.19069607, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 30/30, Train Loss: 0.00193435, Test Loss: 0.18689076, Test Accuracy: 0.97990000\n",
            "sparseness_list [tensor(0.4632, grad_fn=<MeanBackward0>), tensor(0.4862, grad_fn=<MeanBackward0>), tensor(0.5000, grad_fn=<MeanBackward0>), tensor(0.4890, grad_fn=<MeanBackward0>), tensor(0.5005, grad_fn=<MeanBackward0>), tensor(0.5052, grad_fn=<MeanBackward0>), tensor(0.4893, grad_fn=<MeanBackward0>), tensor(0.4941, grad_fn=<MeanBackward0>), tensor(0.4976, grad_fn=<MeanBackward0>), tensor(0.4984, grad_fn=<MeanBackward0>), tensor(0.5009, grad_fn=<MeanBackward0>), tensor(0.4964, grad_fn=<MeanBackward0>), tensor(0.4932, grad_fn=<MeanBackward0>), tensor(0.4938, grad_fn=<MeanBackward0>), tensor(0.4979, grad_fn=<MeanBackward0>), tensor(0.4969, grad_fn=<MeanBackward0>), tensor(0.4967, grad_fn=<MeanBackward0>), tensor(0.4916, grad_fn=<MeanBackward0>), tensor(0.4984, grad_fn=<MeanBackward0>), tensor(0.4937, grad_fn=<MeanBackward0>), tensor(0.4885, grad_fn=<MeanBackward0>), tensor(0.4823, grad_fn=<MeanBackward0>), tensor(0.4889, grad_fn=<MeanBackward0>), tensor(0.4884, grad_fn=<MeanBackward0>), tensor(0.4947, grad_fn=<MeanBackward0>), tensor(0.4917, grad_fn=<MeanBackward0>), tensor(0.4866, grad_fn=<MeanBackward0>), tensor(0.4832, grad_fn=<MeanBackward0>), tensor(0.4894, grad_fn=<MeanBackward0>), tensor(0.4957, grad_fn=<MeanBackward0>)]\n",
            "selectivity_list [0.7229088130827884, 0.3900837231873913, 0.7344789922588568, 0.20703213659644956, 0.44209449765144715, 0.7848854304850185, 0.43371646385120555, 0.7654554440083141, 0.739707377657261, 0.5987194561027649, 0.641556733700784, 0.5453865322624861, 0.6843225796821777, 0.5062143371605017, 0.586881847834447, 0.6227003168050124, 0.7239141915059946, 0.5937848107840459, 0.7050074706315097, 0.5375922761417157, 0.6195126698274734, 0.35845506763693674, 0.5376130888028811, 0.6085871361645848, 0.6398951525182603, 0.6359760186326314, 0.5948900318337601, 0.6388393757943475, 0.5220717682110676, 0.6174151839041407, 0.6347822263916938, 0.5512397596817014, 0.6221772745299606, 0.039830186259213404, 0.29317291225167225, 0.7199277835155554, 0.5885928311808255, 0.008555497698778192, 0.581553127784204, 0.20146888960870726, 0.2630332872908487, 0.22972204898271645, 0.4087352220687429, 0.604344398769083, 0.3407344145762955, 0.6759590058448612, 0.5338154933560193, 0.7581211153279995, 0.5334093845370613, 0.24960649119863995, 0.3400867295145549, 0.5019583837917305, 0.7745237424832366, 0.83585546826387, 0.4429883383941189, 0.32697586896563047, 0.5647245058251332, 0.17103256820229445, 0.4481465746814949, 0.22379215739189254, 0.5104619448026079, 0.348697694337908, 0.49984909231695174, 0.29816920593077323, 0.35277467164969367, 0.48074609220156206, 0.009987803106098625, 0.8882119371190403, 0.5309966913704052, 0.4705836159933484, 0.5794536184972254, 0.6424944587835568, 0.42491771954783497, 0.5583954535327181, 0.7247608307706127, 0.19591817684254914, 0.6170191558012585, 0.6881070111807152, 0.30266038805667883, 0.660742710479215, 0.22965527930986956, 0.5745899767418456, 0.6069579351172125, 0.3402830312962727, 0.6985088071975628, 0.5535159190731815, 0.7684416179117002, 0.4652632735103836, 0.6890992169275122, 0.5541289029934854, 0.7057849923066284, 0.6892683703024789, 0.6142713628583806, 0.5927487835014675, 0.38900989293440674, 0.47476784107433034, 0.6333247668647691, 0.6095754790751462, 0.5354867220131553, 0.6637123122972741, 0.48142690158999785, 0.2514484179546768, 0.7446749419009948, 0.35052953494647937, 0.4728858192975381, 0.49868289077436045, 0.5437432854528679, 0.6818523390750093, 0.025350814349633627, 0.6016824478670971, 0.5374578818120374, 0.5411580051342163, 0.26631580967814533, 0.8731479404431306, 0.2549797930863693, 0.5717751118486399, 0.6877104472168095, 0.620565458960222, 0.7501278026780871, 0.8119005197576167, 0.35624390255075156, 0.4920904208111312, 0.32095954087718764, 0.3731860145513812, 0.7127695659168656, 0.49052793229071495, 0.3315358042505123, 0.7025731120052695, 0.4872763270952485, 0.6096878814074335, 0.1720513314971924, 0.6534095288750249, 0.6640739671463822, 0.7193232023578642, 0.7538679770510306, 0.4553539059737694, 0.6657525179778283, 0.4196612678740196, 0.29146732248554086, 0.7724019358394062, 0.6594621462972629, 0.26718504451545, 0.5093114148955479, 0.2978554974764595, 0.4057760259649483, 0.6640116416211275, 0.7477163784765603, 0.749812457573974, 0.45774845291767596, 0.41949160044401484, 0.6702247399403707, 0.6236169091485494, 0.461675810101699, 0.37020617768226677, 0.488215349418462, 0.6801782629591258, 0.6693745524737252, 0.45582593345426686, 0.5969864905149358, 0.6232066402292821, 0.6484587987273737, 0.666662230375178, 0.627145153869909, 0.4757934404394347, 0.6376480708737795, 0.44707125479422927, 0.7166193182876556, 0.47465351302765585, 0.7016697988253734, 0.333169898711744, 0.6245574458187222, 0.6256480973714201, 0.18504614598840194, 0.6567453045120721, 0.657102342227957, 0.41402974355927585, 0.660801677271349, 0.2160779448520592, 0.6571247537737459, 0.6218280158542699, 0.5134806351232365, 0.3919587370349693, 0.5231307969017822, 0.31809706229537543, 0.7874965494454601, 0.555541860214621, 0.4053796642399369, 0.5237713188879974, 0.6492527358009914, 0.6343748167894364, 0.6276428652850486, 0.6726591222601555, 0.3974419387413341, 0.47040025561137605, 0.4605615654871454, 0.5544477301412548, 0.6466081856352394, 0.6316743329023854, 0.6729074877930101, 0.37640940477848817, 0.7750560978851981, 0.5880452162396442, 0.816081693938189, 0.6200040228324963, 0.5590153843767447, 0.6517743311738349, 0.6516202590236618, 0.590709085075148, 0.41024872856214695, 0.6872376996097369, 0.5322554398083371, 0.6669327973077834, 0.9150534882101357, 0.7307362914481712, 0.7194471961067113, 0.2830979863192462, 0.6576081612297237, 0.5016718308662951, 0.5805233596295294, 0.4239543104460795, 0.6934218907151519, 0.6017546198347359, 0.40093923932108144, 0.5229063260077351, 0.6484133325701483, 0.6039610726869022, 0.5846360563248533, 0.41176099851732534, 0.5598965760795659, 0.7876048587453659, 0.44247296032010935, 0.5542479676253518, 0.5220759706298687, 0.32371065816632555, 0.7505753831895243, 0.4144665176894106, 0.22497562539118357, 0.37924243456563195, 0.6252033578782169, 0.550488098319892, 0.48612795027110556, 0.6514412891055998, 0.8899515410963446, 0.5884515701707475, 0.7085461317039442, 0.7353825322860421, 0.469325113561171, 0.741472022743002, 0.5277095629465338, 0.6042138010653654, 0.2771555135009021, 0.40740425199307256, 0.7182378311173047, 0.5587130859493118, 0.5753056906124988, 0.5314389814412972]\n",
            "[0.9704, 0.9697, 0.9769, 0.9797, 0.9788, 0.9791, 0.9767, 0.976, 0.9801, 0.9802, 0.9775, 0.9804, 0.9792, 0.9779, 0.9813, 0.9789, 0.9805, 0.9788, 0.9801, 0.9789, 0.9808, 0.9813, 0.9805, 0.9802, 0.9805, 0.9807, 0.9806, 0.9802, 0.9804, 0.9799]\n",
            "[tensor(0.4632, grad_fn=<MeanBackward0>), tensor(0.4862, grad_fn=<MeanBackward0>), tensor(0.5000, grad_fn=<MeanBackward0>), tensor(0.4890, grad_fn=<MeanBackward0>), tensor(0.5005, grad_fn=<MeanBackward0>), tensor(0.5052, grad_fn=<MeanBackward0>), tensor(0.4893, grad_fn=<MeanBackward0>), tensor(0.4941, grad_fn=<MeanBackward0>), tensor(0.4976, grad_fn=<MeanBackward0>), tensor(0.4984, grad_fn=<MeanBackward0>), tensor(0.5009, grad_fn=<MeanBackward0>), tensor(0.4964, grad_fn=<MeanBackward0>), tensor(0.4932, grad_fn=<MeanBackward0>), tensor(0.4938, grad_fn=<MeanBackward0>), tensor(0.4979, grad_fn=<MeanBackward0>), tensor(0.4969, grad_fn=<MeanBackward0>), tensor(0.4967, grad_fn=<MeanBackward0>), tensor(0.4916, grad_fn=<MeanBackward0>), tensor(0.4984, grad_fn=<MeanBackward0>), tensor(0.4937, grad_fn=<MeanBackward0>), tensor(0.4885, grad_fn=<MeanBackward0>), tensor(0.4823, grad_fn=<MeanBackward0>), tensor(0.4889, grad_fn=<MeanBackward0>), tensor(0.4884, grad_fn=<MeanBackward0>), tensor(0.4947, grad_fn=<MeanBackward0>), tensor(0.4917, grad_fn=<MeanBackward0>), tensor(0.4866, grad_fn=<MeanBackward0>), tensor(0.4832, grad_fn=<MeanBackward0>), tensor(0.4894, grad_fn=<MeanBackward0>), tensor(0.4957, grad_fn=<MeanBackward0>)]\n",
            "0.5410437997118714\n",
            "0.1694473714804751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhex0lGyqA3B",
        "outputId": "091a4b47-5836-4d3c-9b58-67f993cdca35"
      },
      "source": [
        "model_Adam = Model(384)\n",
        "print(\"model_Adam:\", model_Adam)\n",
        "model_Adam.to(device)\n",
        "model_Adam.sigmoid.register_forward_hook(get_activation(model_Adam))\n",
        "optimizer_Adam = torch.optim.Adam(model_Adam.parameters(), lr=0.001)\n",
        "Adam_test_acc, Adam_sparsity_list, Adam_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adam, model=model_Adam, hidden_layer_neurons=384)\n",
        "print(Adam_test_acc)\n",
        "print(Adam_sparsity_list)\n",
        "print(np.average(Adam_selectivity_list))\n",
        "print(np.std(Adam_selectivity_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_Adam: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=384, bias=True)\n",
            "  (linear_2): Linear(in_features=384, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "Epoch: 1/30, Train Loss: 0.20096606, Test Loss: 0.10209418, Test Accuracy: 0.96890000\n",
            "\n",
            "Epoch: 2/30, Train Loss: 0.08614406, Test Loss: 0.11023695, Test Accuracy: 0.96770000\n",
            "\n",
            "Epoch: 3/30, Train Loss: 0.06218170, Test Loss: 0.07732103, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 4/30, Train Loss: 0.04602217, Test Loss: 0.08442723, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 5/30, Train Loss: 0.03598697, Test Loss: 0.09661062, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 6/30, Train Loss: 0.02912243, Test Loss: 0.10361650, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 7/30, Train Loss: 0.02413755, Test Loss: 0.10349518, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 8/30, Train Loss: 0.02036208, Test Loss: 0.11466192, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 9/30, Train Loss: 0.01604663, Test Loss: 0.11515065, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 10/30, Train Loss: 0.01422882, Test Loss: 0.11588972, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 11/30, Train Loss: 0.01244590, Test Loss: 0.10901516, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 12/30, Train Loss: 0.01100745, Test Loss: 0.12477036, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 13/30, Train Loss: 0.00981304, Test Loss: 0.14261039, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 14/30, Train Loss: 0.00755580, Test Loss: 0.13179345, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 15/30, Train Loss: 0.00673143, Test Loss: 0.14149999, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 16/30, Train Loss: 0.00604194, Test Loss: 0.13412551, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 17/30, Train Loss: 0.00590940, Test Loss: 0.16425532, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 18/30, Train Loss: 0.00556704, Test Loss: 0.14103718, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 19/30, Train Loss: 0.00495407, Test Loss: 0.15538417, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 20/30, Train Loss: 0.00503994, Test Loss: 0.15576886, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 21/30, Train Loss: 0.00374098, Test Loss: 0.15944858, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 22/30, Train Loss: 0.00399525, Test Loss: 0.15290303, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 23/30, Train Loss: 0.00301283, Test Loss: 0.17426589, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 24/30, Train Loss: 0.00428014, Test Loss: 0.19731488, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 25/30, Train Loss: 0.00322677, Test Loss: 0.17678027, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 26/30, Train Loss: 0.00333515, Test Loss: 0.18226039, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 27/30, Train Loss: 0.00296774, Test Loss: 0.18067741, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 28/30, Train Loss: 0.00279584, Test Loss: 0.17256290, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 29/30, Train Loss: 0.00323519, Test Loss: 0.18904689, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 30/30, Train Loss: 0.00199094, Test Loss: 0.20428267, Test Accuracy: 0.98080000\n",
            "sparseness_list [tensor(0.5475, grad_fn=<MeanBackward0>), tensor(0.5692, grad_fn=<MeanBackward0>), tensor(0.5664, grad_fn=<MeanBackward0>), tensor(0.5747, grad_fn=<MeanBackward0>), tensor(0.5743, grad_fn=<MeanBackward0>), tensor(0.5798, grad_fn=<MeanBackward0>), tensor(0.5627, grad_fn=<MeanBackward0>), tensor(0.5554, grad_fn=<MeanBackward0>), tensor(0.5575, grad_fn=<MeanBackward0>), tensor(0.5541, grad_fn=<MeanBackward0>), tensor(0.5500, grad_fn=<MeanBackward0>), tensor(0.5615, grad_fn=<MeanBackward0>), tensor(0.5512, grad_fn=<MeanBackward0>), tensor(0.5377, grad_fn=<MeanBackward0>), tensor(0.5446, grad_fn=<MeanBackward0>), tensor(0.5468, grad_fn=<MeanBackward0>), tensor(0.5418, grad_fn=<MeanBackward0>), tensor(0.5475, grad_fn=<MeanBackward0>), tensor(0.5430, grad_fn=<MeanBackward0>), tensor(0.5365, grad_fn=<MeanBackward0>), tensor(0.5374, grad_fn=<MeanBackward0>), tensor(0.5364, grad_fn=<MeanBackward0>), tensor(0.5383, grad_fn=<MeanBackward0>), tensor(0.5424, grad_fn=<MeanBackward0>), tensor(0.5390, grad_fn=<MeanBackward0>), tensor(0.5383, grad_fn=<MeanBackward0>), tensor(0.5380, grad_fn=<MeanBackward0>), tensor(0.5389, grad_fn=<MeanBackward0>), tensor(0.5459, grad_fn=<MeanBackward0>), tensor(0.5431, grad_fn=<MeanBackward0>)]\n",
            "selectivity_list [0.8368550369201889, 0.6283692800615942, 0.4048455958638173, 0.6493934991427259, 0.6486562284791204, 0.6852826988841046, 0.6133414275119942, 0.740924679042702, 0.8186125708463705, 0.5546523934333868, 0.3277056564173358, 0.7173036800189848, 0.7442841067131093, 0.6986500619738903, 0.5375990790210753, 0.6470008325315727, 0.7840258453153891, 0.3851892238222812, 0.6286587720511199, 0.5434839181212312, 0.5920726507410007, 0.6611440933796603, 0.8983963147463286, 0.6744585176378297, 0.40359900703493085, 0.37625654264340863, 0.1270025642388216, 0.7764509118530444, 0.572337990192741, 0.6270687603288528, 0.39531988230032095, 0.5058370388172024, 0.4974468630052952, 0.446752933534439, 0.6486124720209074, 0.45182833681964707, 0.6695714704921265, 0.8506889637273882, 0.7026230841231892, 0.658677172492021, 0.8281954759978891, 0.5834234292293579, 0.24917689518901248, 0.5395180543942846, 0.6423332138951535, 0.8029030661008355, 0.36819074119041617, 0.6581810933009469, 0.7438900801387733, 0.01722259435366813, 0.8472962927396032, 0.7281489702884475, 0.5973185054504895, 0.6552009270595924, 0.5443937267209116, 0.7490183912793473, 0.7397458555433375, 0.2865361542044624, 0.5896296585139653, 0.7465349863277346, 0.6862751093515694, 0.6448460718207871, 0.09338535656739842, 0.07585358573627604, 0.7670947146679681, 0.5354157525719816, 0.43180353652588993, 0.5597446944829939, 0.6855507140993248, 0.7128774139823774, 0.5099963314055399, 0.30521213988562756, 0.5759741472941287, 0.6701522414270566, 0.5445400474816974, 0.7503153200518276, 0.8599014876826048, 0.6799634476400273, 0.38738261848094746, 0.7484910794559473, 0.48383989657369236, 0.4832672628733133, 0.7041893350656172, 0.5754367806601776, 0.39096655216682846, 0.751765419959621, 0.5391709071148232, 0.7274692683183179, 0.3888891688365617, 0.6953983946276655, 0.5957010313825538, 0.43621681199120454, 0.3190041223792571, 0.600851624790325, 0.04085613197979731, 0.8036508597397815, 0.3515555539143519, 0.4644752088924536, 0.35188851874238336, 0.6433968165129047, 0.6816705732918613, 0.47661123594438415, 0.0003427377764576059, 0.5236325293848674, 0.3585044566971254, 0.7010671542416106, 0.6211642806180899, 0.5378137624797844, 0.860876601721811, 0.7472173867959157, 0.5255392690693886, 0.572626731204502, 0.5718688266499855, 0.6197519430219305, 0.7204016745767724, 0.4591028855866232, 0.5377633848720642, 0.6578434538722103, 0.5618004563170981, 0.6651253691477537, 0.44280552065122375, 0.7106384380428236, 0.6899857598093154, 0.7039575187210799, 0.5773892418287673, 0.7468251888996222, 0.5279913051872399, 0.7211836568554367, 0.7029118227289086, 0.6301252408215066, 0.34883272775519536, 0.593355040709519, 0.5824706217217428, 0.5769306458349431, 0.9236970540385974, 0.6694985265904236, 0.6240850548422965, 0.7548277526403315, 0.7823050094498424, 0.5593417183645416, 0.501914281553513, 0.5883799094371746, 0.6482799089322302, 0.565378334149079, 0.4731568497832456, 0.440829977461995, 0.6666248081052927, 0.7615420015980325, 0.5734215968446985, 0.7388731148325313, 0.6149588308239687, 0.629509903764765, 0.34582369534825014, 0.700191445396114, 0.7145391313818806, 0.6968846439644324, 0.5826729791423441, 0.8233689533466894, 0.9010404557312042, 0.5688854656163352, 0.5980651238028275, 0.6316946754326581, 0.6284616099442795, 0.5005893753433359, 0.5783589373008267, 0.48557618940543296, 0.6197454348193929, 0.2921097990552735, 0.3157472159408054, 0.32199295437568776, 0.4679258567425696, 0.49508226144736955, 0.07078099416392805, 0.3919380565788368, 0.34621449562747275, 0.21975484830816924, 0.5737106171877788, 0.47777548655099944, 0.6334235309562107, 0.41138482132456033, 0.9342966171262761, 0.4158794089731763, 0.476969334638695, 0.7140220401309821, 0.3453410569540576, 0.6430004371455643, 0.6514163783170477, 0.7541168101460424, 0.45554653304401493, 0.6376231703603791, 0.7707788482643158, 0.019747826339924508, 0.7139059663351738, 0.49973565896413646, 0.324024940207161, 0.33272246696336355, 0.7895558813181852, 0.462241918409865, 0.39148853058559624, 0.5707117572001325, 0.7841418815196158, 0.4645557698508939, 0.6805764261097733, 0.7646390574747564, 0.5801611840784232, 0.30563092841013223, 0.7976885526483445, 0.6629943594925869, 0.6053169591289956, 0.7164318561141173, 0.6536701019774432, 0.690383408716801, 0.6451160553218741, 0.6329101342533703, 0.34063460422621333, 0.58323504844892, 0.5079878301550895, 0.3326608741167953, 0.6711325223480649, 0.29649012240641887, 0.7876474659463893, 0.6030251915902899, 0.5907546348257977, 0.5461566859000092, 0.7422153567580543, 0.5862768658917569, 0.4442007402258876, 0.6389282386197147, 0.7510974370961674, 0.3242144667892938, 0.5949044534655509, 0.4655361587649734, 0.47594516814328813, 0.6159687327065032, 0.6387427308573089, 0.5604097724075753, 0.3320174513362729, 0.48531002339014695, 0.6417268000020004, 0.5494939009136782, 0.6266018582123897, 0.6300326232084771, 0.8371337607251605, 0.3038554267539319, 0.7261410818746801, 0.350827936624105, 0.5873082697872132, 0.48202601052798066, 0.7894441251317186, 0.7293132618266198, 0.7687857146388777, 0.8173829440847812, 0.5100496262821126, 0.5858224905246354, 0.704619228974171, 0.8034683413932677, 0.7185001643508231, 0.8489870334157743, 0.72927522645221, 0.44214960750709287, 0.43368406280813854, 0.4370146105716601, 0.7532639348503685, 0.570586693782476, 9.079537380486677e-05, 0.5471400567016313, 0.7149209358768396, 0.5519965785585692, 0.792542481508642, 0.46376718575497294, 0.5241315963978838, 0.836216681513913, 0.8129054459339375, 0.6751406452048492, 0.7024003368283316, 0.4091636778847286, 0.6574670466571241, 0.5965362735327332, 0.6054216597106654, 0.5540975978422806, 0.8717594490348186, 0.6216153169536555, 0.6853670378737364, 0.6734188331955957, 0.7728996445926105, 0.6086560683975778, 0.6198267356213938, 0.7241245468377244, 0.49522301243142614, 0.6157363409766513, 0.5703748373072547, 0.7192654558531335, 0.3051386915443184, 0.5581717924286851, 0.5815528011135495, 0.51713280450347, 0.7074375305938151, 0.3307223402655631, 0.6037187581994083, 0.7142795341615147, 0.8171446216224025, 0.7435456792056115, 0.4239669318060699, 0.5833586853421234, 0.7830580254304897, 0.2573661581010109, 0.5516722193208288, 0.7438297641036963, 0.4962452032284873, 0.5978229113600356, 0.6631692081529628, 0.7651076297063936, 0.08788639119732554, 0.6415572801385544, 0.7792470300453646, 0.669077549323422, 0.707830685370824, 0.59736536559714, 0.6908012968480673, 0.7220595304352605, 0.47182573677248685, 0.4513319215562943, 0.7626866222303742, 0.30133125816702255, 0.532489725953025, 0.5075860921031219, 0.6694148202320871, 0.6910181075816435, 0.6123469824862994, 0.7855697372340733, 0.5198470942804504, 0.4675031713150339, 0.6174905386006928, 0.63277044835065, 0.8301329484439912, 0.5490936250889967, 0.7155615483155645, 0.7022228348037597, 0.42989398780418503, 0.7518362241089125, 0.46848068104212126, 0.7127180066994654, 0.658580852310159, 0.5820677832704381, 0.731598211722705, 0.7872716797431182, 0.6625510675572311, 0.7262355113573534, 0.5127908693644688, 0.7422114101021218, 0.2038700293931938, 0.7729207998716273, 0.48799229892633544, 0.8389013698764219, 0.7206615948380476, 0.4463524107806063, 0.3145850098648946, 0.6409138960640587, 0.6988801124861929, 0.595844040236762, 0.39498201844042874, 0.9410791295530853, 0.05687052200887257, 0.05068572382353752, 0.3510303668784652, 0.7221727770603626, 0.43556079706680895, 0.38725913749445656, 0.5369316645710183, 0.5406525907702818, 0.18669264326636104, 0.6241009625433782, 0.7527851241034501, 0.6490908510411397, 0.5978982207902919, 0.4846658909085187, 0.5746459788208803, 0.9404570847155187, 0.4811555304907559, 0.3055560226243523, 0.6283278846097532, 0.5442692947302721, 0.6604406400419944, 0.6454297210647515]\n",
            "[0.9689, 0.9677, 0.9789, 0.9787, 0.9792, 0.9776, 0.9797, 0.9791, 0.9791, 0.9807, 0.9805, 0.9812, 0.9793, 0.9821, 0.9798, 0.9808, 0.979, 0.9808, 0.9796, 0.9813, 0.9814, 0.9819, 0.9799, 0.9789, 0.9814, 0.9796, 0.9813, 0.9806, 0.981, 0.9808]\n",
            "[tensor(0.5475, grad_fn=<MeanBackward0>), tensor(0.5692, grad_fn=<MeanBackward0>), tensor(0.5664, grad_fn=<MeanBackward0>), tensor(0.5747, grad_fn=<MeanBackward0>), tensor(0.5743, grad_fn=<MeanBackward0>), tensor(0.5798, grad_fn=<MeanBackward0>), tensor(0.5627, grad_fn=<MeanBackward0>), tensor(0.5554, grad_fn=<MeanBackward0>), tensor(0.5575, grad_fn=<MeanBackward0>), tensor(0.5541, grad_fn=<MeanBackward0>), tensor(0.5500, grad_fn=<MeanBackward0>), tensor(0.5615, grad_fn=<MeanBackward0>), tensor(0.5512, grad_fn=<MeanBackward0>), tensor(0.5377, grad_fn=<MeanBackward0>), tensor(0.5446, grad_fn=<MeanBackward0>), tensor(0.5468, grad_fn=<MeanBackward0>), tensor(0.5418, grad_fn=<MeanBackward0>), tensor(0.5475, grad_fn=<MeanBackward0>), tensor(0.5430, grad_fn=<MeanBackward0>), tensor(0.5365, grad_fn=<MeanBackward0>), tensor(0.5374, grad_fn=<MeanBackward0>), tensor(0.5364, grad_fn=<MeanBackward0>), tensor(0.5383, grad_fn=<MeanBackward0>), tensor(0.5424, grad_fn=<MeanBackward0>), tensor(0.5390, grad_fn=<MeanBackward0>), tensor(0.5383, grad_fn=<MeanBackward0>), tensor(0.5380, grad_fn=<MeanBackward0>), tensor(0.5389, grad_fn=<MeanBackward0>), tensor(0.5459, grad_fn=<MeanBackward0>), tensor(0.5431, grad_fn=<MeanBackward0>)]\n",
            "0.5821094238949439\n",
            "0.17550069471825933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPS46wV5qA78",
        "outputId": "756de98b-28bf-4141-e977-442975d22ed4"
      },
      "source": [
        "model_Adam = Model(512)\n",
        "print(\"model_Adam:\", model_Adam)\n",
        "model_Adam.to(device)\n",
        "model_Adam.sigmoid.register_forward_hook(get_activation(model_Adam))\n",
        "optimizer_Adam = torch.optim.Adam(model_Adam.parameters(), lr=0.001)\n",
        "Adam_test_acc, Adam_sparsity_list, Adam_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adam, model=model_Adam, hidden_layer_neurons=512)\n",
        "print(Adam_test_acc)\n",
        "print(Adam_sparsity_list)\n",
        "print(np.average(Adam_selectivity_list))\n",
        "print(np.std(Adam_selectivity_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_Adam: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (linear_2): Linear(in_features=512, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "Epoch: 1/30, Train Loss: 0.19906497, Test Loss: 0.11302352, Test Accuracy: 0.96540000\n",
            "\n",
            "Epoch: 2/30, Train Loss: 0.08565973, Test Loss: 0.09198543, Test Accuracy: 0.97500000\n",
            "\n",
            "Epoch: 3/30, Train Loss: 0.05830932, Test Loss: 0.09634455, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 4/30, Train Loss: 0.04440080, Test Loss: 0.09184639, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 5/30, Train Loss: 0.03453563, Test Loss: 0.09585245, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 6/30, Train Loss: 0.02962860, Test Loss: 0.10369463, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 7/30, Train Loss: 0.02314996, Test Loss: 0.10049772, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 8/30, Train Loss: 0.02037488, Test Loss: 0.11393794, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 9/30, Train Loss: 0.01888872, Test Loss: 0.10699455, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 10/30, Train Loss: 0.01326043, Test Loss: 0.13178927, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 11/30, Train Loss: 0.01052461, Test Loss: 0.10854176, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 12/30, Train Loss: 0.01210548, Test Loss: 0.13885539, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 13/30, Train Loss: 0.00843966, Test Loss: 0.13799739, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 14/30, Train Loss: 0.00934168, Test Loss: 0.13230996, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 15/30, Train Loss: 0.00730632, Test Loss: 0.13934050, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 16/30, Train Loss: 0.00629956, Test Loss: 0.13591627, Test Accuracy: 0.98260000\n",
            "\n",
            "Epoch: 17/30, Train Loss: 0.00662828, Test Loss: 0.17312662, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 18/30, Train Loss: 0.00598799, Test Loss: 0.15929028, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 19/30, Train Loss: 0.00450149, Test Loss: 0.17502580, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 20/30, Train Loss: 0.00506080, Test Loss: 0.16052271, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 21/30, Train Loss: 0.00423311, Test Loss: 0.16366600, Test Accuracy: 0.98330000\n",
            "\n",
            "Epoch: 22/30, Train Loss: 0.00302610, Test Loss: 0.18137597, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 23/30, Train Loss: 0.00345288, Test Loss: 0.17012521, Test Accuracy: 0.98240000\n",
            "\n",
            "Epoch: 24/30, Train Loss: 0.00323755, Test Loss: 0.19537833, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 25/30, Train Loss: 0.00302547, Test Loss: 0.20405313, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 26/30, Train Loss: 0.00405565, Test Loss: 0.20273792, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 27/30, Train Loss: 0.00355530, Test Loss: 0.17648293, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 28/30, Train Loss: 0.00300162, Test Loss: 0.18901336, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 29/30, Train Loss: 0.00319954, Test Loss: 0.20399031, Test Accuracy: 0.98250000\n",
            "\n",
            "Epoch: 30/30, Train Loss: 0.00201850, Test Loss: 0.18945031, Test Accuracy: 0.98260000\n",
            "sparseness_list [tensor(0.5900, grad_fn=<MeanBackward0>), tensor(0.6106, grad_fn=<MeanBackward0>), tensor(0.6079, grad_fn=<MeanBackward0>), tensor(0.6161, grad_fn=<MeanBackward0>), tensor(0.6047, grad_fn=<MeanBackward0>), tensor(0.5915, grad_fn=<MeanBackward0>), tensor(0.5963, grad_fn=<MeanBackward0>), tensor(0.5805, grad_fn=<MeanBackward0>), tensor(0.5819, grad_fn=<MeanBackward0>), tensor(0.5779, grad_fn=<MeanBackward0>), tensor(0.5725, grad_fn=<MeanBackward0>), tensor(0.5655, grad_fn=<MeanBackward0>), tensor(0.5583, grad_fn=<MeanBackward0>), tensor(0.5650, grad_fn=<MeanBackward0>), tensor(0.5571, grad_fn=<MeanBackward0>), tensor(0.5592, grad_fn=<MeanBackward0>), tensor(0.5612, grad_fn=<MeanBackward0>), tensor(0.5566, grad_fn=<MeanBackward0>), tensor(0.5575, grad_fn=<MeanBackward0>), tensor(0.5621, grad_fn=<MeanBackward0>), tensor(0.5562, grad_fn=<MeanBackward0>), tensor(0.5536, grad_fn=<MeanBackward0>), tensor(0.5531, grad_fn=<MeanBackward0>), tensor(0.5500, grad_fn=<MeanBackward0>), tensor(0.5494, grad_fn=<MeanBackward0>), tensor(0.5514, grad_fn=<MeanBackward0>), tensor(0.5583, grad_fn=<MeanBackward0>), tensor(0.5493, grad_fn=<MeanBackward0>), tensor(0.5571, grad_fn=<MeanBackward0>), tensor(0.5538, grad_fn=<MeanBackward0>)]\n",
            "selectivity_list [0.38767193302281017, 0.6004946105524627, 0.4974498850937995, 0.9570172286245929, 0.723810215637206, 0.8982415880551536, 0.6141588440343444, 0.565715045386152, 0.7066308741440301, 0.564024565187308, 0.8424716396962812, 0.7092432134202774, 0.018749436692180488, 0.00010326317570094864, 0.6794910923819305, 0.10353116127887677, 0.6433443600895621, 0.7901782055855933, 0.39897748427685353, 0.8038621095603224, 0.42450831952772455, 0.533966531474093, 0.7603525703063749, 0.5056446982686995, 0.44420509104919487, 0.7089063726227528, 0.801397922897936, 0.6293354398943601, 0.6159602754792488, 0.7669631441170606, 0.6797390126376718, 0.5828331797884486, 0.5293771917944652, 0.3339421520938691, 0.21454439407702522, 0.9446472021027202, 0.6115113026860733, 0.8967572030683146, 0.6199129081521305, 0.3379872637297138, 0.3404744177825369, 0.6810967667764726, 0.9175902891859796, 0.5443580790633072, 0.773299912315092, 0.49457998962578154, 0.6181457787403338, 0.6008672659983314, 0.5270609654643538, 0.39952038479006136, 0.8794518979451589, 0.7867846558544247, 0.37902916584935964, 0.7518976803559732, 0.8190769565273132, 0.6315999047382483, 0.009871351142314638, 0.4379548762197858, 0.5817364732624175, 0.7113237267642811, 0.5365368405172773, 0.6852971596406947, 0.7064691035057702, 0.8327692398077334, 0.8205850146135808, 0.8390677499039757, 0.5830055260151233, 0.5440359788522353, 0.697290691633512, 0.8380375587732846, 0.6469056864400312, 0.6372558143496029, 0.4261897775652463, 0.6841857952906011, 0.8650256342607713, 0.6849225257428289, 0.30147194387239584, 0.43622444938525856, 0.737307808862513, 0.4943030086375175, 0.7918881629013371, 0.7866024757769309, 0.8479980502474309, 0.6547673192882566, 0.1266921848525535, 0.819304496916331, 0.9743836856378462, 0.5372436241259844, 0.37858949869234554, 0.4658670439528026, 0.3596733453601839, 0.5559460017409533, 0.5405151889955093, 0.5584158427165341, 0.4630758113798859, 0.639171537380429, 0.739066329781777, 0.5749275337468313, 0.8087657814761106, 0.2754682935091765, 0.6559724424664831, 0.470363959202061, 0.551423399020146, 0.4202938832834927, 0.8388862476562052, 0.5505961594425242, 0.7173496379467535, 0.8468600029342515, 0.6256347168671937, 0.7355723902942217, 0.4672465426744663, 0.8833600864669476, 0.6610615817367499, 0.8354524359149935, 0.5840412491707945, 0.03529309723373443, 0.7472308895480698, 0.6563238453091652, 0.6344424754126102, 0.45302377825750095, 0.6526513765938804, 0.6470504493154304, 0.5118266098305344, 0.6320545203338185, 0.5041891763308497, 0.5275144811634079, 0.7894603418556352, 0.3369670065229834, 0.8296546620479891, 0.72408486579372, 0.7619300717144792, 0.7295445613868615, 0.5931896827993628, 0.8382289097663648, 0.48394803123484637, 2.0553005827459915e-07, 0.7009692487357999, 0.7449028817003717, 0.5521720263153488, 0.1287772720507228, 0.016231236513872555, 0.7337718514665337, 0.7341967807767453, 0.6684510137058451, 0.9363005962602251, 0.7768728973229846, 0.6359586011603335, 0.6595761276288354, 0.651324036393535, 0.7069275914801815, 0.751217287115836, 0.581094465323176, 0.6652933329732457, 0.8128709285647723, 0.48475416588727366, 0.5612732167943791, 0.49630959920928946, 0.7651646692461991, 0.8667951402797478, 0.8734646079416676, 0.5479487853969941, 0.531917360403687, 0.6088558148352027, 0.5405182608602938, 0.6041142279367302, 0.5542945591164188, 0.36182525870780247, 0.5557138062484569, 0.42392056029241304, 0.6471694484542504, 0.6477129311840893, 0.5961464481614511, 0.8189867529321485, 0.6943724713142639, 0.3954567653693699, 0.8537489528336992, 0.5701246551671206, 0.5139812033297579, 0.5390982840000761, 0.22213856110469501, 0.789602591519863, 0.39293931990928654, 0.636835949157162, 0.7252566357080705, 0.3729230440681635, 0.6879620359995315, 0.8640141890173868, 0.7633301469556684, 0.880198915099198, 0.8788311137483793, 0.6655342059673246, 0.5175443947900559, 0.7217242551305691, 0.711302472513154, 0.7739546452554098, 0.8695515851411245, 0.6806373151149959, 0.6384496910302301, 0.6709429536545712, 0.6690856894182049, 0.47864637737478843, 0.4031568968812158, 0.41221713697990575, 0.5693110138407198, 0.5203783141282623, 0.791194691377753, 0.5276039658629913, 0.8172432315290015, 0.28518652254737786, 0.0005334252873277633, 0.6306707218205522, 0.801029559796914, 0.06968431578808099, 0.6654968947405162, 0.5131883852997252, 0.5234177922648728, 0.6427203402174443, 0.615479210509075, 0.8637279148575607, 0.5811922432628999, 0.6893921807993983, 0.43422527599067573, 0.7552070638630384, 0.6606022447467201, 0.1879663880089985, 0.7899626536744856, 0.6437349157202903, 0.6657286112190903, 0.6282262374087586, 0.7846857183207752, 0.7131003545183399, 0.4375199137684153, 0.4419969005750557, 0.8487274108871408, 0.5656798972613563, 0.01237832262297349, 0.6607908460307735, 0.7147357320624378, 0.37267399698832526, 0.435468065969206, 0.5807351384997778, 0.7645623140176162, 0.2881806528307256, 0.7966349214981734, 0.03417824556345753, 0.6718893887548418, 0.710746688166055, 0.30860382794563873, 0.5626926216726922, 0.28887004242663106, 0.8465863884847284, 0.9017888805944145, 0.13220958828776963, 0.44436031346328764, 0.03482165702601258, 0.6656923722762237, 0.6646305439256107, 0.5176026205602384, 1.580805586296651e-05, 0.776449095585152, 0.42334850818575354, 0.4631110170336413, 0.7603814884866181, 0.6470870778671144, 0.6121570703034808, 0.733293533748085, 0.5728076793716447, 0.6625330512516969, 0.7782233618213018, 0.5417476045949438, 0.7970914442030811, 0.5955435668160786, 0.29196004870876274, 0.9294482076613951, 0.7167655671159068, 0.8891861642610346, 0.6327778076648705, 0.4223401977603154, 0.6524864194660274, 0.523490726306858, 0.9639204193260763, 0.5026711532765546, 0.6909556344372625, 0.5339022233778393, 0.5344468393653986, 0.7265349561657047, 0.25117394660490805, 0.6139225247662942, 0.6537935177943088, 0.7356568077619539, 0.7286778188792199, 0.6612391321905863, 0.7830022019821351, 0.7223368129079885, 0.741237201788605, 0.47885172308578106, 0.6302284463592566, 0.7976450954267462, 0.3754423086027831, 0.4222599310242841, 0.6788882654280511, 0.1720199817765315, 0.9260703929644836, 0.5238844934898057, 0.4646662995830918, 0.7367885207037067, 0.6753498551204704, 0.5394104886990558, 0.657931991866835, 0.761448811526886, 0.6707515005227268, 0.4894792824773111, 0.489193949170633, 0.6532726696202317, 0.46865395225716205, 0.7525346358681219, 0.7602413942278778, 0.6801498529354758, 0.5690377648269666, 0.6386039265348096, 0.6465092948194288, 0.8323588806504427, 0.6352543841789503, 0.6725761432510247, 0.6654536835395357, 0.6700494455990533, 0.877822483591432, 0.7171139509361638, 0.6510551581391743, 0.49527040329623295, 0.7450952778925178, 0.009498646260641081, 0.4544184937371106, 0.8442491073031697, 0.719039187141917, 0.5895634747528409, 0.6510325598431302, 0.6273583910092966, 0.6065126722782075, 0.7862029052578565, 0.41427897209172176, 0.6193627832443324, 0.5074951475440848, 0.5903294538219583, 0.39341193810288605, 0.6237674971367356, 0.5755912228691211, 0.7934283442623353, 0.7817752941685128, 0.4265978778931577, 0.22780174216601218, 0.6695328843115479, 0.8123908362999785, 0.8971489478673277, 0.5570674076722932, 0.6783219829784672, 0.8225042371918535, 0.8994845456118177, 0.9053591192785694, 0.46038528342581064, 0.5833207118494598, 0.9174034583421697, 0.39019808960343555, 0.6381371282831054, 0.6755197122676212, 0.9101625114398901, 0.4880021676083578, 0.3564905846260269, 0.7570663263659769, 0.3598696881770637, 0.257497404341901, 0.7312178165229163, 0.7411639105438766, 0.21315840450265414, 0.685551337087922, 0.2981500545359356, 0.5550377412013747, 0.8019905954228798, 0.4941746615754599, 0.6516510634894234, 0.661739809087903, 0.8692495371719928, 0.7403201832798867, 0.43780889257314914, 0.7272437497952826, 0.7306640557187336, 0.8290359572740407, 0.5190553021353104, 0.6196607955512138, 0.7938557648758613, 0.5588605771539021, 0.68075373919111, 0.4354448614560053, 0.7224899111008622, 0.6281924497030617, 0.6458878889367535, 0.6684038607849139, 0.6020861356425531, 0.4862955296849319, 0.8311387493690898, 0.6760562752522168, 0.0009018947630235594, 0.7464999284109908, 0.7254760562347501, 0.4845512416476215, 0.7758379812159683, 0.9304748394127386, 0.6408591085900907, 0.599015486132943, 0.6636694896056019, 0.09653709129339218, 0.4601164555110997, 0.5593744107266204, 0.5796214019815753, 0.8918143099459388, 0.6321045626019324, 0.48341290697417216, 0.49378260069505076, 0.4320779771903484, 0.3097706274725172, 0.5159849076663092, 0.6571523971038301, 0.7614618368962482, 0.5877240836312846, 0.5444092658846477, 0.5845483072621779, 0.8715095931838407, 0.6426125134527266, 0.7390180619165674, 0.8269230398244694, 0.7011276099230139, 0.8001901774163146, 0.5826053596001595, 0.003474704371038553, 0.2859239479312914, 0.6631305108717586, 0.6050359507740103, 0.5482016590893156, 0.6335533178451688, 0.6091388245843293, 0.8016249563996005, 0.4847471831044468, 0.5293918120063844, 0.5689163978587114, 0.869199507902564, 0.6302542313296075, 0.4659531582574, 0.6762897046514061, 0.8117085272111706, 0.6253166894549115, 0.31094416932508645, 0.5124945333754796, 0.5417252823529439, 0.7132155070730862, 0.5504168465408243, 0.6869728843462292, 0.3425435539136034, 0.5100397511389657, 0.772487499580432, 0.6413076813074837, 0.43448019831560264, 0.5619535437458569, 0.4436611247728226, 0.7348086960273758, 0.363759109974439, 0.7149475497622363, 0.4905579326889712, 0.7220755991862455, 0.44077978532536427, 0.7279525247063029, 0.5981256960100455, 0.7619729711189128, 0.5370822527425329, 0.9101210766129075, 0.10871374172257202, 0.27104238659581803, 0.5657438274738678, 0.6274767614121146, 0.7500680425269821, 0.8247547456053048, 0.6314088722465678, 0.5457605173401617, 0.6236212524490699, 0.5995377671855022, 0.6096518636137607, 0.8895553330507234, 0.6924044122685364, 0.4060079331999916, 0.5938789901960174, 0.6442920632564898, 0.8421865225702728, 0.5244374395397824, 5.054852142580515e-08, 0.6797167041025038, 0.5153699547822415, 0.385654974586157, 0.3165435839426716, 0.7189520266744004, 0.7482701953729356, 0.7574773449543001, 0.21233277086601113, 0.7660056288883736, 0.13574704858227696, 0.7953155150296903, 0.30677640111546245, 0.501610351587437, 0.3521399089234953, 0.5871150973070519, 0.3882565619955746, 0.39301090173754305, 0.3902134345004535, 0.7401198387022442]\n",
            "[0.9654, 0.975, 0.977, 0.9779, 0.9779, 0.9803, 0.9799, 0.9801, 0.9806, 0.9786, 0.9823, 0.9806, 0.9797, 0.9822, 0.9816, 0.9826, 0.9786, 0.9804, 0.9786, 0.9822, 0.9833, 0.9812, 0.9824, 0.9808, 0.9795, 0.9792, 0.9822, 0.9819, 0.9825, 0.9826]\n",
            "[tensor(0.5900, grad_fn=<MeanBackward0>), tensor(0.6106, grad_fn=<MeanBackward0>), tensor(0.6079, grad_fn=<MeanBackward0>), tensor(0.6161, grad_fn=<MeanBackward0>), tensor(0.6047, grad_fn=<MeanBackward0>), tensor(0.5915, grad_fn=<MeanBackward0>), tensor(0.5963, grad_fn=<MeanBackward0>), tensor(0.5805, grad_fn=<MeanBackward0>), tensor(0.5819, grad_fn=<MeanBackward0>), tensor(0.5779, grad_fn=<MeanBackward0>), tensor(0.5725, grad_fn=<MeanBackward0>), tensor(0.5655, grad_fn=<MeanBackward0>), tensor(0.5583, grad_fn=<MeanBackward0>), tensor(0.5650, grad_fn=<MeanBackward0>), tensor(0.5571, grad_fn=<MeanBackward0>), tensor(0.5592, grad_fn=<MeanBackward0>), tensor(0.5612, grad_fn=<MeanBackward0>), tensor(0.5566, grad_fn=<MeanBackward0>), tensor(0.5575, grad_fn=<MeanBackward0>), tensor(0.5621, grad_fn=<MeanBackward0>), tensor(0.5562, grad_fn=<MeanBackward0>), tensor(0.5536, grad_fn=<MeanBackward0>), tensor(0.5531, grad_fn=<MeanBackward0>), tensor(0.5500, grad_fn=<MeanBackward0>), tensor(0.5494, grad_fn=<MeanBackward0>), tensor(0.5514, grad_fn=<MeanBackward0>), tensor(0.5583, grad_fn=<MeanBackward0>), tensor(0.5493, grad_fn=<MeanBackward0>), tensor(0.5571, grad_fn=<MeanBackward0>), tensor(0.5538, grad_fn=<MeanBackward0>)]\n",
            "0.599558553226383\n",
            "0.19884180155776962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORh8jJAQqBAy",
        "outputId": "ab3763a2-34e5-4c3c-8124-de733e3ea97a"
      },
      "source": [
        "model_Adam = Model(768)\n",
        "print(\"model_Adam:\", model_Adam)\n",
        "model_Adam.to(device)\n",
        "model_Adam.sigmoid.register_forward_hook(get_activation(model_Adam))\n",
        "optimizer_Adam = torch.optim.Adam(model_Adam.parameters(), lr=0.001)\n",
        "Adam_test_acc, Adam_sparsity_list, Adam_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adam, model=model_Adam, hidden_layer_neurons=768)\n",
        "print(Adam_test_acc)\n",
        "print(Adam_sparsity_list)\n",
        "print(np.average(Adam_selectivity_list))\n",
        "print(np.std(Adam_selectivity_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_Adam: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=768, bias=True)\n",
            "  (linear_2): Linear(in_features=768, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "Epoch: 1/30, Train Loss: 0.19656048, Test Loss: 0.10261850, Test Accuracy: 0.96920000\n",
            "\n",
            "Epoch: 2/30, Train Loss: 0.08744990, Test Loss: 0.10333888, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 3/30, Train Loss: 0.06103810, Test Loss: 0.08569655, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 4/30, Train Loss: 0.04701021, Test Loss: 0.09513980, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 5/30, Train Loss: 0.03527072, Test Loss: 0.09557252, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 6/30, Train Loss: 0.02823984, Test Loss: 0.15494753, Test Accuracy: 0.96940000\n",
            "\n",
            "Epoch: 7/30, Train Loss: 0.02761455, Test Loss: 0.11477742, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 8/30, Train Loss: 0.02185533, Test Loss: 0.11348373, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 9/30, Train Loss: 0.01844449, Test Loss: 0.12009893, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 10/30, Train Loss: 0.01718136, Test Loss: 0.13465335, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 11/30, Train Loss: 0.01499614, Test Loss: 0.13507157, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 12/30, Train Loss: 0.01254135, Test Loss: 0.13338040, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 13/30, Train Loss: 0.01081471, Test Loss: 0.14442361, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 14/30, Train Loss: 0.01188467, Test Loss: 0.15483238, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 15/30, Train Loss: 0.01023907, Test Loss: 0.16705149, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 16/30, Train Loss: 0.00948510, Test Loss: 0.16313174, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 17/30, Train Loss: 0.00847704, Test Loss: 0.17017186, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 18/30, Train Loss: 0.00765666, Test Loss: 0.18290841, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 19/30, Train Loss: 0.00773661, Test Loss: 0.16645447, Test Accuracy: 0.98220000\n",
            "\n",
            "Epoch: 20/30, Train Loss: 0.00770910, Test Loss: 0.18016602, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 21/30, Train Loss: 0.00624731, Test Loss: 0.18047191, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 22/30, Train Loss: 0.00528187, Test Loss: 0.19539050, Test Accuracy: 0.98260000\n",
            "\n",
            "Epoch: 23/30, Train Loss: 0.00524357, Test Loss: 0.21155177, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 24/30, Train Loss: 0.00537709, Test Loss: 0.20025565, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 25/30, Train Loss: 0.00452409, Test Loss: 0.21407085, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 26/30, Train Loss: 0.00465976, Test Loss: 0.19770354, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 27/30, Train Loss: 0.00399120, Test Loss: 0.21042410, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 28/30, Train Loss: 0.00441357, Test Loss: 0.22138387, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 29/30, Train Loss: 0.00253309, Test Loss: 0.23045232, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 30/30, Train Loss: 0.00557792, Test Loss: 0.25420764, Test Accuracy: 0.97950000\n",
            "sparseness_list [tensor(0.6478, grad_fn=<MeanBackward0>), tensor(0.6618, grad_fn=<MeanBackward0>), tensor(0.6655, grad_fn=<MeanBackward0>), tensor(0.6481, grad_fn=<MeanBackward0>), tensor(0.6489, grad_fn=<MeanBackward0>), tensor(0.6316, grad_fn=<MeanBackward0>), tensor(0.6334, grad_fn=<MeanBackward0>), tensor(0.6213, grad_fn=<MeanBackward0>), tensor(0.6242, grad_fn=<MeanBackward0>), tensor(0.6098, grad_fn=<MeanBackward0>), tensor(0.5984, grad_fn=<MeanBackward0>), tensor(0.6037, grad_fn=<MeanBackward0>), tensor(0.5902, grad_fn=<MeanBackward0>), tensor(0.5794, grad_fn=<MeanBackward0>), tensor(0.5828, grad_fn=<MeanBackward0>), tensor(0.5853, grad_fn=<MeanBackward0>), tensor(0.5715, grad_fn=<MeanBackward0>), tensor(0.5689, grad_fn=<MeanBackward0>), tensor(0.5683, grad_fn=<MeanBackward0>), tensor(0.5708, grad_fn=<MeanBackward0>), tensor(0.5713, grad_fn=<MeanBackward0>), tensor(0.5643, grad_fn=<MeanBackward0>), tensor(0.5628, grad_fn=<MeanBackward0>), tensor(0.5612, grad_fn=<MeanBackward0>), tensor(0.5637, grad_fn=<MeanBackward0>), tensor(0.5611, grad_fn=<MeanBackward0>), tensor(0.5590, grad_fn=<MeanBackward0>), tensor(0.5601, grad_fn=<MeanBackward0>), tensor(0.5593, grad_fn=<MeanBackward0>), tensor(0.5545, grad_fn=<MeanBackward0>)]\n",
            "selectivity_list [0.9283329368085355, 0.36117513004854024, 0.6154968914954007, 0.7065340966099741, 0.621218526336247, 0.4920314380659652, 0.10946074003261917, 0.6826489896633335, 0.8244342255075107, 0.6679065450462424, 0.7297264933359455, 0.34714264304898285, 0.34086171943949994, 0.010176289381607676, 0.4704357206797665, 0.5837549203303212, 0.12224556947774726, 0.6488413430037477, 0.7458695788807674, 0.013283686196763938, 0.8442620692630515, 0.7332867184426887, 0.7721947184393745, 0.9297848217094157, 0.5907551546961309, 0.7157644577843115, 0.2015212644535117, 0.596181815430106, 0.7621237536414953, 0.7231040383318161, 0.6206630523265476, 0.48626446340660445, 0.8127582714357313, 0.5657434799562151, 0.8362197019388192, 0.6319830792568187, 0.8100807917547279, 0.6472013917950843, 0.5354776411055502, 0.44725916760162326, 0.49927387341239365, 0.8087577552539365, 0.850008867908218, 0.9030158997842955, 0.6199253531048331, 0.0009545983520688233, 0.46253059413602043, 0.6357170142364854, 0.6887147232894238, 0.7807152857105033, 0.7146141635943357, 0.8555252401788549, 0.7342171359070042, 0.6730778653080122, 0.017140859494072333, 0.8283076219283524, 0.8000414919485298, 0.746566856141454, 0.40605807189661386, 0.6567451253030478, 0.7813448101734569, 0.8349523680394606, 0.6318195691384368, 0.741116122463311, 0.5422631486308821, 0.530748524142048, 0.37603784278900426, 0.3028123371609674, 0.7586772074618866, 0.6939469625965957, 0.0071207420160776455, 0.6224987161957698, 0.5302169032075706, 0.7035393854632148, 0.09356341238846977, 0.6539099483066196, 0.7768793799046112, 0.6338952379817028, 0.6264628213431953, 0.7965735426834005, 0.0722776868591061, 0.3109339833324463, 0.7627303848841738, 0.30702460627060035, 0.752944889322357, 0.5424438893440892, 0.002912609691305907, 0.6492397940682376, 0.010564912790667095, 0.5402180087909904, 0.4649437158149651, 0.6434583016392679, 0.6448845936251254, 0.8396987604141827, 0.7684285203595596, 0.8114972289413225, 0.6783060127993453, 0.9617473435789919, 0.9429646077375485, 0.8072731546160608, 0.7263382236739203, 0.5254233397463436, 0.6231081635411808, 0.8087238467034443, 0.6787323258000233, 0.5692910749677549, 0.3455061428360058, 0.7616978608780534, 0.4970784763859357, 0.6385645091146246, 0.6435664390481155, 0.4947802663543178, 0.4576194739731303, 0.5783205941850887, 0.3038771021072769, 0.020406463462350633, 0.5458282158590482, 0.7438441203777544, 0.9252064006104364, 0.4416612873460052, 0.6740393544268997, 0.8262117523536203, 0.4396533773863455, 0.7493281629597607, 0.858147806157242, 0.8369657590202837, 0.23916245779864814, 0.49425163580804693, 0.8408225440992524, 0.4122287796228228, 0.8604192192674078, 0.7683839235518348, 0.5504387316867051, 0.8137728589711062, 0.6143321271317088, 0.7972124966483046, 0.8217480961110898, 0.7427469949528219, 0.7016332092507314, 0.3960005267380119, 0.6836659260167558, 6.453898370249499e-06, 0.773300341414763, 0.8082040141061608, 0.6985055393007092, 0.6701839919577705, 0.6035707770943131, 0.48538984366475074, 0.8823489554544668, 0.7693824208161855, 0.4708403356040833, 0.0019716195714112638, 0.6021049842108893, 0.6442550464496571, 0.49398958582972774, 0.8501129352963368, 0.7150017096765288, 0.7827870313438344, 0.5973038133024405, 0.5952042612572606, 0.1389927665651216, 0.8701548970639763, 0.6367224800292393, 0.5492743935117118, 0.7861226883489671, 0.8664883881747956, 0.39098422541217637, 0.5090655662585474, 0.7283789222003012, 0.8004742395636483, 0.8273092579655223, 0.6891867602125283, 0.6452199997439655, 0.9413273009944307, 0.9216085635021637, 0.882830845654311, 0.024776546325540914, 0.8149520262195867, 0.8339383024388413, 0.480066460727846, 0.327658312854798, 0.7350803880355026, 0.9379473035368301, 0.7212124383301777, 0.8385330218704264, 0.49624470439655916, 0.6312976912116326, 0.8090133828297571, 0.7739433915046203, 0.010255796920513238, 0.5929977043706147, 0.5407146148291764, 0.5340624013495929, 0.8045150139719978, 0.7155730493781035, 0.7691550613628071, 0.817640793770324, 0.7627699258304541, 0.817996151827059, 0.28935046223518895, 0.576037745711296, 0.08199977945909875, 0.4428276327668276, 0.6980951371527839, 0.06790740112692877, 0.6805839833311657, 0.5714270447209351, 0.12827241515391538, 0.7221120156254012, 0.48506384335274394, 0.4463741698730214, 0.4958217724964664, 0.020984969656810553, 0.38448758395020505, 0.4267403008880292, 0.42233188671655975, 0.8203326687418844, 0.2583386371697356, 0.7071116214442056, 0.8057937439263663, 0.7525551343654602, 0.8418892083898398, 0.00602962747429221, 0.6745878798948657, 0.7420247303611338, 0.7417842016342325, 0.00033953592134857824, 0.6803134501127897, 0.5031042095888425, 0.7240097322039586, 0.895703481325411, 0.5219273313008133, 0.5915996506320893, 0.22188881919917514, 0.8026171029159267, 0.8870637037606427, 0.5864598144842421, 0.30033576587472116, 0.45298536707321596, 0.689676101096559, 0.41285135895775055, 0.3249125131304523, 0.8404174559110115, 0.5807327866033151, 0.6537686123848441, 6.224181083269916e-06, 0.46973301837950154, 0.6010409777970691, 0.39492582820735617, 0.5536939767815879, 0.7388675223866122, 0.657611424271757, 0.6715347385823511, 0.7539935767253381, 0.8164413831439397, 0.47393067558301544, 0.861539693619944, 0.3958556297822843, 0.8473467819233561, 0.5885229765268728, 0.7271363742656439, 0.03544150359232977, 0.6806000137706185, 0.4589934445036424, 0.515213030809584, 0.851145741450167, 0.7297310771142251, 0.9743221781182438, 0.9341642579987012, 0.09687558575886562, 0.5616428513823113, 0.47297675436172665, 0.7918294354452997, 0.6777442349139975, 0.6849164285786656, 0.7619351785123575, 0.915324436103136, 0.8536474174274916, 0.6861891138902371, 0.5888200857553179, 0.45932875546382373, 0.7291172320704978, 0.6359624590345632, 0.6737665393805695, 0.8670838324092296, 0.470907235280445, 0.8011262755999088, 0.5327704823905514, 0.9261410967648968, 0.8271527335127062, 0.6879082243414284, 0.01320575282143576, 0.7348965213933029, 0.49608761634984316, 0.7324235291682819, 0.9688337771358037, 0.5672203015243635, 0.8340254318375823, 1.8703177134162842e-06, 0.5825546065356199, 0.6526148951330333, 0.5099152049660699, 0.58812061145801, 0.5330131214008131, 0.8176674559777231, 0.48271015735838013, 0.6079164319777507, 0.9003853390755477, 0.6320310065771857, 0.6850760815975229, 0.7082611566190962, 0.6526889067409759, 0.4968920328901783, 0.5713607313287292, 0.7919250505867393, 0.460222414487571, 0.034259812794306975, 0.5138666247142508, 0.4618445288525122, 0.40016325221477406, 0.3542528675116346, 0.619831666817078, 0.8628070482078388, 0.6667405803174338, 0.8506932141041215, 0.7633903785073266, 0.6677800645119312, 0.6024936243245717, 0.8407664444616253, 0.5176872064000232, 0.8035973574922174, 0.668681266693099, 0.18584947667686463, 0.41497502290186006, 0.645863808321729, 0.005627449435206291, 0.7263698841847511, 0.478726015661927, 0.6047205434413425, 0.7261916133337502, 0.7306092183143724, 0.4506329825433265, 0.5631585457368644, 0.9164708732402934, 0.46927615221059144, 0.8942669465292769, 0.5787883703667392, 0.6810874078759395, 0.6420956380660559, 0.7280334173796963, 0.4518973980615826, 0.05680048215261885, 0.538676843527859, 0.6478029181883519, 0.6397121022906948, 0.5416364672825688, 0.5896020212649994, 0.9173647968814937, 0.7462639498944985, 0.8683864984383601, 0.8459837336188482, 0.7739893410285696, 0.7478056021606031, 0.8768123132637922, 0.40158793047174035, 0.647984247521292, 0.5580220539295522, 0.9470487404564368, 0.8266246030222782, 0.5525762445494233, 0.5887461966796481, 0.6050000597788213, 0.4626271773937972, 0.5197055174595836, 0.7618627744528468, 0.7741658202567943, 0.5930964703429701, 0.013363806696766121, 0.5232000497485811, 0.6598466017202943, 0.6131301177999607, 0.35555846108196215, 0.7812723383825403, 0.5116361054963158, 0.4462428272873207, 0.7789281299909362, 0.5264699378742993, 0.8950598179687861, 0.5972200245761687, 0.6923803545939221, 0.8051978428912646, 0.6141414664394234, 0.7140289638070028, 0.6430277560153895, 0.5345979993496752, 0.6994900721306188, 0.5159548956234222, 0.44323081221909766, 0.7318799274678172, 0.5823080377580394, 0.7322939504775597, 0.6625275709248979, 0.26641376519826976, 0.5657518793913193, 0.39087843244229387, 0.986951160713133, 0.7129836623389343, 0.33816970889021797, 0.6998505845125008, 0.4728266058427189, 0.6389929681548142, 0.6697866757965811, 0.7153322062998181, 0.025909439540265103, 0.7140231753611697, 0.32092603786428164, 0.5426999558970993, 0.633331068182537, 0.6915192549577562, 0.593137651784658, 0.983201420201114, 0.7087903694986459, 0.9041840069846361, 0.7720565927331613, 0.47282743056557075, 0.5858043462094665, 0.30476533827838453, 0.3969365090222654, 0.8810693980240004, 0.48899860130329714, 0.7001739528460162, 0.6565286496047349, 0.4613029541769973, 0.7194388476961974, 0.5780659103557231, 0.8560576646837708, 0.633825489757565, 0.7060663079113658, 0.439841574131982, 0.59759265346443, 0.49475662903803924, 0.5285989129295476, 0.845747726038991, 0.8024168747389278, 0.5219118294936927, 0.831859873333747, 0.8372106509276588, 0.649641175456137, 0.9054818627933923, 0.4702136808534725, 0.8468748111975718, 0.8938037936738835, 0.5053747261345728, 0.7326343777581384, 0.6193613365189878, 0.6898346128315356, 0.48198471380470814, 0.03861715698838526, 0.5942715734830265, 0.5844567404049613, 0.879206818765495, 0.6016366283137685, 0.5563413693708855, 0.46451779118833325, 0.5569464288243278, 0.8577135648921766, 0.8474354732259404, 0.7243062346678535, 0.8346318136261145, 0.7398336512000158, 0.6040472681502351, 0.8583402962068452, 0.6466603796501832, 0.5845374251863898, 0.5012949963997824, 0.4513990793755194, 0.7461238022113799, 0.6554488187390025, 6.7976553182142714e-09, 0.38122542266689796, 2.0676046113661895e-06, 0.4731688406605887, 3.2276579537364433e-06, 0.6262017983170345, 0.9013590726757483, 0.8272060565285203, 0.4196009336146429, 0.6819062594406596, 0.790894050832481, 0.500662152888979, 0.5657622898729494, 0.7063017982348134, 0.44362106680754565, 0.6489164819461337, 0.7449775527035275, 0.7475083618397916, 0.7676346314916132, 0.3661334911406939, 0.8524360826301555, 0.6883477867429979, 0.8067683164400488, 0.9522115614667473, 0.4534227653550569, 0.483300170346662, 0.6904411533823732, 0.7453499153102149, 0.45029801099512573, 5.1085954668459285e-05, 0.8278143309765692, 0.6038282008548115, 0.16617880145890887, 0.7657164435026499, 0.8403711591907392, 0.7513765997784251, 0.6304419598710173, 0.7562678532664187, 0.6092737107788477, 0.7642532495146991, 0.8370592353189972, 0.24865384586191128, 0.6611952370363203, 0.332393430993762, 0.8024312378068938, 0.7361444956646036, 0.5403996930566357, 0.5918786566066793, 0.7560810069618091, 0.8689256117075491, 0.7512116367522269, 0.6662957948213863, 0.8882496896405171, 0.773716450271716, 0.490540211604869, 0.6963957304851283, 0.4840089497438262, 0.5667495591592856, 0.4848928660051976, 0.742163952287205, 0.782747274196955, 0.7180795233356747, 0.5171657741430321, 0.7215526384118346, 0.8388892372869337, 0.652843015682709, 0.5146937813134497, 0.6846237846693597, 0.7105963381731975, 0.7190251670920035, 0.5876922693329618, 0.7638533350226213, 0.43217970681047146, 0.3399739730186764, 0.5751378578635857, 0.6607944743957049, 0.8293389390066003, 0.4733906869131242, 0.8737304780925711, 0.6139781579548362, 0.5073982642154936, 0.6373253674102716, 0.7251809745700319, 0.6944421390060707, 0.22965396193678625, 0.7226258930959194, 0.5517977550887937, 0.39904850959617155, 0.6778962067142258, 0.18948895665898688, 0.6075094046906282, 0.8231125471102403, 0.5808474589141182, 0.6113623962083676, 0.5952260211786465, 0.5518514782259397, 0.5671717441943327, 0.8672192673065237, 2.2317169088272247e-06, 0.6794507574221876, 0.48604576998420523, 0.6521166103165233, 0.6932698779627183, 0.45138375031026134, 0.7248931667733491, 0.5340896322141088, 0.0107548111243177, 0.00013200274499385067, 0.7029541293878496, 0.6493149654678658, 0.3998032893722371, 0.884042968180751, 0.8017110594057129, 0.8045969646268798, 0.23390558590685895, 0.5162767129614609, 0.4744138487032513, 0.7570418798806458, 0.30636240541666215, 0.6313318312316035, 0.46771525655526214, 0.835118978034605, 0.7648580726103681, 0.9307535019919594, 0.03894633976450919, 0.7334474336001024, 0.5720827143675041, 0.2771949125681007, 0.6318784934443068, 0.5131322580814018, 0.7567423703771605, 0.7941623826024952, 0.6934548416721629, 0.7135388723444963, 0.8158627734004319, 0.6487547322302031, 0.48777513941339845, 0.716920842377621, 0.6595553278922365, 0.7436355305579073, 0.851522911254713, 0.5472833261980037, 0.7299617613864384, 0.6817869100622561, 0.7914417471947491, 0.653517181505124, 0.25441971595885143, 1.1064442302067499e-05, 0.6200879095061337, 0.8704811384333492, 0.8003324336369046, 0.7302902170117901, 0.5789576285614391, 0.8499833225104066, 0.7591528893328313, 0.5296594983076334, 0.6572097312739591, 0.48920104423371896, 0.6560495103617032, 0.7401058661382193, 0.5044783000393138, 0.5105388171269214, 1.0641375284396662e-08, 0.8332038660130202, 0.9106579729660317, 0.3502521320761843, 0.6324522173904096, 0.33736451442485227, 0.6191158421592396, 0.5829952886104852, 0.6549540769585173, 0.754367948831672, 0.6841508644600458, 0.5709989929131697, 0.6718370168627296, 0.3654279102094072, 0.8386100262712826, 0.6348476809202082, 0.6653506797743135, 0.732555899721141, 0.5633227368653384, 0.618246456242794, 0.5000157493266392, 0.4660666243139742, 0.47044222510202693, 0.8810368967698008, 0.4397883805297668, 0.7333612482899003, 0.5862345525429193, 0.05431029969671121, 0.528484291561582, 0.41261692335662764, 0.5639359851188815, 0.4845068335985759, 0.9047956736459757, 0.6156094238761426, 0.8507898675806477, 0.6745345043247097, 0.38054598601994766, 0.7106701592571139, 1.0557239775678716e-06, 0.722804253739712, 0.789742505119983, 0.8376227940133396, 0.5713176378150654, 0.8494777073822434, 0.600028859467085, 0.7262023204876606, 0.6486777630340979, 0.7549725617662769, 0.966278132715861, 0.7202301584020016, 0.5360151379955786, 0.5648269538248063, 0.6831936421031024, 0.879985119599007, 0.7690174385535031, 0.8646327975312895, 0.5273868066760677, 0.49316857160383903, 0.3876058604458383, 0.4839533289416855, 0.47972532963497433, 0.7291923328939529, 0.7054679459401841, 0.3843302758245967, 0.6384264630189636, 0.897877878051273, 0.8334107275744448, 0.4752657310272554, 0.6536612698733818, 0.4115522734631791, 0.6454961412257847, 0.7515512904689722, 0.9502982256532357, 0.6528467163289903, 0.24992877738946603, 0.3705358708731938, 0.7000606263662135, 0.8029780275731683, 0.8818225546492967, 0.5618480799241362, 0.5378675446529539, 0.3681779120480234, 0.5996064765078175, 0.8869003886740096, 0.3804428586513297, 0.7258043964862283, 0.5390051195813469, 0.8512791496116571, 0.7773915755807362, 0.604974442511499, 0.624524509935835, 0.7752482794784518, 0.7108926274659437, 0.5473064050838883, 0.7716356694828581, 0.5886053055474524, 0.7803769594258503, 0.8453231001562892, 0.5697966296846431, 0.4781078514376966, 0.6744409028077624, 0.26362259069666155, 0.4026985149410106, 0.0011147983605788825, 0.04732003980657421, 0.71254898649373, 0.3853197043725738, 0.9664316862147035, 0.5869490950392984, 0.8180378593768649, 0.55388221890975, 0.3912103695890364, 0.5814047632895901, 0.7631147290927912, 0.7863954805317744, 0.7385261507195101, 0.5025037916405121, 0.5748373678931876, 0.7813657816786381, 0.6777904077286426, 0.5587706585528496, 0.8679951945653814, 0.9135173036222707, 0.8768980860141965, 0.8909166512367716, 0.8553592966101894, 0.735245520473051, 0.8235858020823812]\n",
            "[0.9692, 0.9745, 0.9799, 0.9786, 0.9784, 0.9694, 0.9795, 0.9806, 0.9806, 0.9812, 0.9792, 0.9817, 0.9806, 0.9804, 0.9814, 0.9796, 0.9812, 0.9807, 0.9822, 0.9808, 0.9806, 0.9826, 0.9797, 0.9812, 0.9799, 0.9807, 0.9819, 0.9807, 0.9791, 0.9795]\n",
            "[tensor(0.6478, grad_fn=<MeanBackward0>), tensor(0.6618, grad_fn=<MeanBackward0>), tensor(0.6655, grad_fn=<MeanBackward0>), tensor(0.6481, grad_fn=<MeanBackward0>), tensor(0.6489, grad_fn=<MeanBackward0>), tensor(0.6316, grad_fn=<MeanBackward0>), tensor(0.6334, grad_fn=<MeanBackward0>), tensor(0.6213, grad_fn=<MeanBackward0>), tensor(0.6242, grad_fn=<MeanBackward0>), tensor(0.6098, grad_fn=<MeanBackward0>), tensor(0.5984, grad_fn=<MeanBackward0>), tensor(0.6037, grad_fn=<MeanBackward0>), tensor(0.5902, grad_fn=<MeanBackward0>), tensor(0.5794, grad_fn=<MeanBackward0>), tensor(0.5828, grad_fn=<MeanBackward0>), tensor(0.5853, grad_fn=<MeanBackward0>), tensor(0.5715, grad_fn=<MeanBackward0>), tensor(0.5689, grad_fn=<MeanBackward0>), tensor(0.5683, grad_fn=<MeanBackward0>), tensor(0.5708, grad_fn=<MeanBackward0>), tensor(0.5713, grad_fn=<MeanBackward0>), tensor(0.5643, grad_fn=<MeanBackward0>), tensor(0.5628, grad_fn=<MeanBackward0>), tensor(0.5612, grad_fn=<MeanBackward0>), tensor(0.5637, grad_fn=<MeanBackward0>), tensor(0.5611, grad_fn=<MeanBackward0>), tensor(0.5590, grad_fn=<MeanBackward0>), tensor(0.5601, grad_fn=<MeanBackward0>), tensor(0.5593, grad_fn=<MeanBackward0>), tensor(0.5545, grad_fn=<MeanBackward0>)]\n",
            "0.6105039401891806\n",
            "0.21877447569380024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge2FgGIYqIOp",
        "outputId": "4a38d0bc-a358-4a33-df29-816301472d41"
      },
      "source": [
        "model_Adam = Model(1024)\n",
        "print(\"model_Adam:\", model_Adam)\n",
        "model_Adam.to(device)\n",
        "model_Adam.sigmoid.register_forward_hook(get_activation(model_Adam))\n",
        "optimizer_Adam = torch.optim.Adam(model_Adam.parameters(), lr=0.001)\n",
        "Adam_test_acc, Adam_sparsity_list, Adam_selectivity_list = sparsity_selectivity_trainer(optimizer=optimizer_Adam, model=model_Adam, hidden_layer_neurons=1024)\n",
        "print(Adam_test_acc)\n",
        "print(Adam_sparsity_list)\n",
        "print(np.average(Adam_selectivity_list))\n",
        "print(np.std(Adam_selectivity_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_Adam: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (linear_2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "Epoch: 1/30, Train Loss: 0.19709336, Test Loss: 0.12178474, Test Accuracy: 0.96460000\n",
            "\n",
            "Epoch: 2/30, Train Loss: 0.08684429, Test Loss: 0.12680174, Test Accuracy: 0.96420000\n",
            "\n",
            "Epoch: 3/30, Train Loss: 0.06228071, Test Loss: 0.10610934, Test Accuracy: 0.97490000\n",
            "\n",
            "Epoch: 4/30, Train Loss: 0.04630693, Test Loss: 0.11556572, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 5/30, Train Loss: 0.03800436, Test Loss: 0.10572207, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 6/30, Train Loss: 0.03112048, Test Loss: 0.14493706, Test Accuracy: 0.97490000\n",
            "\n",
            "Epoch: 7/30, Train Loss: 0.02834638, Test Loss: 0.12663943, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 8/30, Train Loss: 0.02383994, Test Loss: 0.13908864, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 9/30, Train Loss: 0.01880494, Test Loss: 0.14244552, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 10/30, Train Loss: 0.01714537, Test Loss: 0.15950506, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 11/30, Train Loss: 0.01512501, Test Loss: 0.14049095, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 12/30, Train Loss: 0.01310685, Test Loss: 0.14716836, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 13/30, Train Loss: 0.01176000, Test Loss: 0.14795354, Test Accuracy: 0.98400000\n",
            "\n",
            "Epoch: 14/30, Train Loss: 0.01021772, Test Loss: 0.14924745, Test Accuracy: 0.98320000\n",
            "\n",
            "Epoch: 15/30, Train Loss: 0.01038191, Test Loss: 0.17171060, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 16/30, Train Loss: 0.00805375, Test Loss: 0.18056932, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 17/30, Train Loss: 0.00974769, Test Loss: 0.21805704, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 18/30, Train Loss: 0.00766414, Test Loss: 0.22292578, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 19/30, Train Loss: 0.00932153, Test Loss: 0.18554302, Test Accuracy: 0.98240000\n",
            "\n",
            "Epoch: 20/30, Train Loss: 0.00892528, Test Loss: 0.20876737, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 21/30, Train Loss: 0.00737609, Test Loss: 0.19705547, Test Accuracy: 0.98230000\n",
            "\n",
            "Epoch: 22/30, Train Loss: 0.00626611, Test Loss: 0.19978778, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 23/30, Train Loss: 0.00565676, Test Loss: 0.20939358, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 24/30, Train Loss: 0.00632277, Test Loss: 0.22688613, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 25/30, Train Loss: 0.00479743, Test Loss: 0.21411463, Test Accuracy: 0.98250000\n",
            "\n",
            "Epoch: 26/30, Train Loss: 0.00465683, Test Loss: 0.20941410, Test Accuracy: 0.98390000\n",
            "\n",
            "Epoch: 27/30, Train Loss: 0.00389452, Test Loss: 0.21367633, Test Accuracy: 0.98330000\n",
            "\n",
            "Epoch: 28/30, Train Loss: 0.00482449, Test Loss: 0.21622833, Test Accuracy: 0.98320000\n",
            "\n",
            "Epoch: 29/30, Train Loss: 0.00389312, Test Loss: 0.23100272, Test Accuracy: 0.98250000\n",
            "\n",
            "Epoch: 30/30, Train Loss: 0.00386763, Test Loss: 0.26199033, Test Accuracy: 0.98170000\n",
            "sparseness_list [tensor(0.6876, grad_fn=<MeanBackward0>), tensor(0.6919, grad_fn=<MeanBackward0>), tensor(0.6863, grad_fn=<MeanBackward0>), tensor(0.6771, grad_fn=<MeanBackward0>), tensor(0.6615, grad_fn=<MeanBackward0>), tensor(0.6500, grad_fn=<MeanBackward0>), tensor(0.6522, grad_fn=<MeanBackward0>), tensor(0.6482, grad_fn=<MeanBackward0>), tensor(0.6428, grad_fn=<MeanBackward0>), tensor(0.6372, grad_fn=<MeanBackward0>), tensor(0.6295, grad_fn=<MeanBackward0>), tensor(0.6290, grad_fn=<MeanBackward0>), tensor(0.6219, grad_fn=<MeanBackward0>), tensor(0.6174, grad_fn=<MeanBackward0>), tensor(0.6156, grad_fn=<MeanBackward0>), tensor(0.6110, grad_fn=<MeanBackward0>), tensor(0.6031, grad_fn=<MeanBackward0>), tensor(0.6035, grad_fn=<MeanBackward0>), tensor(0.6013, grad_fn=<MeanBackward0>), tensor(0.5981, grad_fn=<MeanBackward0>), tensor(0.6026, grad_fn=<MeanBackward0>), tensor(0.6019, grad_fn=<MeanBackward0>), tensor(0.5900, grad_fn=<MeanBackward0>), tensor(0.5926, grad_fn=<MeanBackward0>), tensor(0.5926, grad_fn=<MeanBackward0>), tensor(0.5971, grad_fn=<MeanBackward0>), tensor(0.5898, grad_fn=<MeanBackward0>), tensor(0.5899, grad_fn=<MeanBackward0>), tensor(0.5856, grad_fn=<MeanBackward0>), tensor(0.5809, grad_fn=<MeanBackward0>)]\n",
            "selectivity_list [0.5342166671543976, 0.5303276899938271, 0.7054399436696395, 0.8032383137523688, 0.5108012607547108, 0.6892318678963669, 0.7032281610915888, 0.8659100043025963, 0.5970905058396366, 0.5123108799521674, 0.05339744602770444, 0.8398454458645515, 0.00014455687273720098, 0.6449567338627958, 0.5468174244690731, 0.7947107433311191, 0.898905191740747, 0.5587021713405351, 0.6034518754406644, 0.059809177777317986, 0.8671610244601625, 0.6895163902436534, 0.7678810163528743, 0.717539663562617, 0.44968982407521835, 0.6512420435066738, 0.5456710040881404, 0.809652136395948, 0.7211422371025836, 0.6463239007938058, 0.4217643864262607, 0.7671430822447037, 0.0014394268208963751, 0.7908839424020597, 0.7994729983477138, 0.6682981319932744, 0.8150851267069387, 0.3754384042134178, 0.7327921160158539, 0.6675426137810981, 0.00252167106865264, 0.8113526803915071, 0.5240485815706556, 0.7876185512076562, 0.0012749224850207286, 0.7206573950160035, 0.6505687091918132, 0.5780784655714857, 0.7426001404699698, 0.029857044800285157, 0.869948302308052, 0.36250139839004086, 0.4979561854944793, 0.9634560066801847, 0.7660583749605533, 0.46745590497279377, 0.8797922644851996, 0.7629829565209717, 0.09883045866455886, 0.5617860410632336, 0.7365191497369525, 0.2628920601228288, 0.7386028460023342, 0.8025688927104586, 0.7164071002353473, 0.4367917483517754, 0.5843933570356145, 0.7030518154191877, 0.5565358992726089, 0.8084777544124954, 0.9386672572235438, 0.4361589873521984, 0.4737948594330271, 0.7799003581704353, 0.4443132343278819, 0.7692670412074587, 0.6590572631194552, 0.47706004828758514, 0.6163239382798176, 0.0002842160984895876, 0.5530848713762634, 0.7587317441857342, 0.0004674704459346637, 0.3857252607416143, 0.12845623073661996, 0.7543988337104806, 0.8756529325287243, 0.15434027437045117, 0.5809994134412761, 0.8918330480860921, 0.5026666319587126, 0.6819213482626287, 0.7657958503263109, 0.49281687915218364, 0.9389480575280452, 0.9346831092761777, 0.4366729844172259, 0.3529279396596694, 0.8274553055219069, 0.7129793295185893, 0.9563487403294774, 0.8120485912297484, 0.6806054232955143, 0.13834835527502437, 0.6175582064456991, 0.5835437442376229, 0.6717641548367949, 0.6715313998531273, 0.8092668469713198, 0.894084528778893, 0.7274584044322938, 0.5932159276154426, 0.5211342086675925, 0.7753482612837113, 0.7166044382370608, 0.5261372371634794, 0.30933166212754454, 0.5954001291734387, 0.7544273010238486, 0.7435194897811676, 0.6592798547748075, 0.5879649958292933, 0.7791791839824468, 0.6234160556969602, 0.8981770813572477, 0.003480877646470603, 0.6923802475224037, 0.756453953322175, 0.7152207345121566, 0.8693639771876486, 0.46144068887504086, 0.43680995169106657, 0.7901579032652859, 0.44936005797947637, 0.9976018998949905, 0.3597647756704484, 0.5899841717486664, 0.7639332860708699, 0.5608701589017258, 0.742334954946623, 0.5896141491309408, 0.880202705297287, 0.0003119718527735669, 0.8077532685214281, 0.37262340278129324, 0.9162043358153324, 0.06935698098916286, 0.007816305223976965, 0.3794766861391458, 0.6029952342037144, 0.8147236380154772, 0.6547393348465256, 0.7636521948585807, 0.7106928555577388, 0.9745940497624479, 0.9094866155675843, 0.7386150511378089, 0.5834094730872708, 0.7562805034843895, 0.47360607725499515, 0.6878734640632519, 0.7900852787085492, 0.4639249515473138, 0.6425663268277921, 0.7088179054226895, 0.3463997200475654, 0.4717385359681769, 0.6829154968891028, 0.8263181382017376, 0.43751686665555994, 0.7848894556907875, 0.4306793385355714, 0.7890027725736967, 0.29959632663392693, 0.46173194838489284, 0.46716195131908606, 0.754336757546398, 0.21029682287540566, 0.8151765196019142, 0.8433392863285882, 0.725918627893198, 0.797639350393576, 0.7229166966991146, 0.07262705677012012, 0.7876755258164262, 0.6666353786465259, 0.625234149094168, 0.8062381078340032, 0.49700014635298706, 0.7763348952507502, 0.789121163509419, 0.6657505906447847, 0.8451905507321325, 0.757256011899349, 0.8133677759859409, 0.8060313100487241, 0.9791080979045869, 0.4644405269663106, 0.8096551245053176, 0.7536633685886556, 0.696798482719985, 0.827031038827446, 0.955125226578222, 0.7673365704176914, 0.6467800625599162, 0.5054651937552541, 0.48651223457366766, 0.8140714042083348, 0.7128586613074034, 0.8491833130498069, 0.7697211886313975, 0.7784208850479577, 0.0848668493143659, 0.8652524250159807, 0.7717040576064476, 0.7327695991659117, 0.8807856783771668, 0.5867372665972117, 0.5606785886449619, 0.7382003010471719, 0.596333268071004, 0.5368692456232714, 0.6870752257684971, 0.4737629190917305, 0.5860790481345852, 0.8335090114308245, 0.9491154726776961, 0.6324677710615543, 0.7929931064669404, 0.9774864157353025, 0.7785621212205879, 0.3927401086662671, 0.7338917483068662, 0.7898696982138456, 0.7851933663782955, 0.31504573217531895, 0.746172020922639, 0.6176983767777494, 0.8042165452144651, 0.65340371885306, 0.8274400198191735, 0.8741265859965667, 0.8169434094703633, 0.5690773914425793, 0.8100092522010425, 0.4426805222252359, 0.7244975661131724, 0.7681154880407491, 0.6379493752914848, 0.8350828198707286, 0.76731521761669, 0.83275231918975, 0.4501921166226653, 0.7459656580771441, 0.6988408750940658, 0.5026917138543072, 0.6543482507194087, 0.6347469232998715, 0.4053214423427875, 0.9139578079718428, 0.6133506188584659, 0.6457218399135489, 0.8927442872724353, 0.5281184495762148, 0.7407284146367017, 0.012830607539586834, 0.40794832967636563, 0.7179446006428328, 3.730013226418579e-08, 0.89619938644599, 0.428376770782013, 0.44534084684482983, 0.6782576336914625, 0.6714360904504476, 0.6430222883871745, 0.7538439117089972, 0.9285945749034368, 0.7616000925968994, 0.35549497181170825, 0.6940628448647757, 0.6652048340476424, 0.8253483686145577, 0.5466278622959823, 0.3597074993890135, 0.8488088426439376, 0.04972627839056937, 0.6933956642335279, 0.006824512140101917, 0.4499780136258835, 0.5578681633187478, 0.7616640003849162, 0.5056784046463432, 0.6407532426084562, 0.5003040163836482, 0.8302406113849313, 0.7353404609182507, 0.6701267680908252, 0.7009422028596015, 0.8468826854863221, 0.8476143706096108, 0.6681055430407478, 0.5488340864056748, 0.944536943928574, 0.5988732358366196, 0.8804704101112346, 0.8427852630852264, 0.7344963100656744, 0.8381346079593317, 0.6736870358578526, 0.7143911702749892, 0.7323611923908667, 0.8366117591378048, 0.8040879718771776, 0.8572387600090842, 0.6839379079297474, 0.430030930848759, 0.7090278721241728, 0.6970553624553101, 0.3282885201945094, 0.4838465412554968, 0.7826504812968053, 0.46133837796890526, 0.7150453297324376, 0.39736938968436686, 0.606162479535148, 0.9048500257166583, 0.9394516829177724, 0.9715869373500952, 0.9268169825361667, 0.8570222301239009, 0.8095573726516678, 0.25860781929392396, 0.9224429787218014, 0.7085723179281107, 0.723983021083387, 0.8093197093415311, 0.6153342254809211, 0.33315070404166575, 0.8306886676267642, 0.6868533981844714, 0.5881113867613825, 0.5743761576835091, 0.6038683026351709, 0.5959111010116767, 0.7939387968095513, 0.6382478923281169, 0.003245603414520619, 0.7861631826845452, 0.7705907759677738, 0.8533551537064312, 0.5461860973730933, 0.5688136787071683, 0.9500539597529858, 0.46712990498105916, 0.6395030698491904, 0.9809247918563042, 0.4202151041310919, 0.8217103051928808, 0.8638536681611105, 0.7837306035947391, 0.814036953563668, 0.9049768393567866, 0.6524313000118008, 0.7611042919518534, 0.8525103115177002, 0.7345200477016168, 0.9008876026966968, 0.6462665106422109, 0.731514087677477, 0.3097579694565863, 0.8204909347718066, 0.5355586870334916, 0.741047419290218, 0.4680576784819614, 0.8059946463229516, 0.5267506379716151, 0.7816777461109391, 0.575199778019109, 0.7240856043385603, 0.7521145758117932, 0.9101313499279242, 0.5137107685379723, 0.6227074225334719, 0.3912983977984544, 0.7435255681846843, 0.6918891840615619, 0.7780741484721583, 0.656252659361093, 0.7259758101185876, 0.4317971644723456, 0.5010411393137467, 0.5371106521771315, 0.8045295655622036, 0.6638345035386302, 0.8285374280781574, 0.5061567683148483, 0.816017284057241, 0.5332478301737805, 0.035234677254387746, 0.8204479089096378, 0.8928952560223459, 0.6773844860948612, 0.7842445761960972, 0.7188056164208898, 0.7180606420335731, 0.9848002860163709, 0.508012399168879, 0.5990184930497746, 0.6583745016279405, 0.5483803037202142, 0.9027530957781436, 0.4432589537068482, 0.4007266263571556, 0.8239414353495451, 0.6804577079600923, 0.902771058975706, 0.6718702611610039, 0.0012192393883187892, 0.8800026501624758, 0.7657200848280773, 0.7780992033611698, 0.6622064282336813, 0.8170734643429021, 0.03052695147316921, 0.863536737721482, 0.5755951507886574, 0.7316334673827087, 0.3642929011325885, 0.6271505074569718, 0.61811167607782, 0.6233498734056087, 0.843681914553469, 0.8063549792134701, 0.5994479103914632, 0.5879424633503781, 0.8246777523091151, 0.7515003112543014, 0.9948007963323303, 0.588559494743778, 0.829019279493959, 0.38512820051569885, 0.659055178896647, 0.4193731624664109, 0.546430582162383, 0.403045168194639, 0.5854989789002066, 6.078802004670894e-05, 0.6823831727649974, 0.6553787851770817, 0.8778799824422404, 0.6805477062251581, 0.6785355520816462, 0.30119568992956425, 0.4468996402094105, 0.6688240456277053, 0.7793444928282312, 0.7922825013601321, 0.9803097112202193, 0.8877112694190759, 0.7470693049420163, 0.4963666503531174, 0.8735344138624922, 0.8116846420381771, 0.6409006760015076, 0.7894557849173389, 0.2007344150957491, 0.5308833444196208, 0.7304724852867306, 0.6728603648412297, 0.8945233297612459, 0.758302121565805, 0.8427987889130443, 0.7304587084721925, 0.5465157675684139, 0.7509031588671666, 0.5867955779695276, 0.810387788154009, 0.6245433798991652, 0.9126065518742287, 0.650119633748408, 0.8228885175460399, 0.766436830665413, 0.34361465330414326, 0.002406219689310139, 0.787553046003269, 0.683707103559155, 0.5397300325193625, 0.41685325448553784, 0.5350969833701646, 0.9691182097328741, 0.4721161791622275, 0.7820574176119973, 0.48562093997550443, 0.6834145682499926, 0.6539447568900305, 0.5648053730949166, 0.3496401845119921, 0.0014216755563228847, 0.7207725735348335, 0.6855241907361056, 0.7250389015940822, 0.2778880831873154, 0.47785406905291095, 0.753420239905914, 0.6654568450432513, 0.8710499278550412, 0.44464430769938457, 0.9828453008215715, 0.8506978101988544, 0.6324703311979054, 0.4760971396892132, 0.7078622570033974, 0.7369032750657368, 0.8662229094897453, 0.3218150873113105, 0.5499344601724621, 0.8949046394670249, 0.9113954900230032, 0.5898518223478395, 0.40476462363688004, 0.7831776118928784, 0.42098775791235277, 0.849473317124355, 0.6997254658179657, 0.21379242599341536, 0.5314883686182202, 0.794336520115788, 0.7751541987508358, 0.7079052400252587, 0.7003917591738137, 0.8162595987614677, 0.6700158991428526, 0.5337444509510145, 0.41540502809333635, 0.6584569565077371, 0.4205411623222498, 0.00011730350716135795, 0.7652018106860358, 0.3503034708339881, 0.6160207499266527, 0.7498067678741092, 0.6739937007883867, 0.6485015201563898, 0.6839024401000917, 0.7027120933023497, 0.5519431499406628, 0.7487684598697104, 0.47296942988447416, 0.9058708048777053, 0.541049215133834, 0.9107509763641164, 0.6598687089965786, 0.7060039380965228, 0.7966040145824208, 0.4332588107193139, 0.03731243234305355, 0.2509047688718917, 0.7250879610470554, 0.3827245588121528, 0.5232310106763429, 0.7434545009218445, 0.6489020432144839, 0.4784322148962492, 0.8516424712022811, 0.8537164682446193, 0.5689469248079847, 0.6783288522173292, 0.7637091861111293, 0.5973424524358082, 0.4117248378759848, 0.5215845880359375, 0.9376457654970127, 0.6087947460756319, 0.8129898424962604, 0.7127290235205915, 0.8683013490001164, 0.6028121741541578, 0.5750995451596799, 0.848929019829558, 0.899464497235697, 0.866071587836544, 0.3257541538317744, 0.7122657741187903, 0.8091675690760313, 0.5910764743127251, 0.5899085798270999, 0.12075335516168618, 0.02175511634101603, 0.9560580031596707, 0.929329565033122, 0.9198363523189783, 0.9296655645599408, 0.42582483968119544, 0.5299906176005131, 0.8404204889893598, 0.6226933077361346, 0.7913912731227469, 0.5636288731420618, 0.74501327637058, 0.8668869303708632, 0.5948085295486184, 0.622419482720589, 0.8427898097393337, 0.669733228937063, 0.8177511939058604, 0.8805770438382998, 0.7647134223366109, 0.7821607350248952, 0.7393981768152325, 0.7082170258939385, 0.7810447333332171, 0.5879537083799575, 0.00815430888408201, 0.7786962322731971, 0.9015654206487406, 0.022571415551861786, 0.29307046046419327, 0.6808748670931002, 0.998650052861074, 0.6920214771853523, 0.7589369364972264, 0.5538778902540046, 0.6511521309548788, 0.6177226044557657, 0.6417432641332713, 0.7321219237872144, 0.5888202830554164, 0.9209611412286688, 0.792696418620704, 0.7055639368094981, 0.6698334339341787, 0.8142195442653697, 0.7358151650761497, 0.8578431156682748, 0.38885081860559634, 0.8816330258966084, 0.4089424047252709, 0.7854933267483488, 0.6636930547005839, 0.8196439044162543, 0.5832346671025287, 0.9637666172946345, 0.5050309602299535, 0.7122381458337876, 0.6694285053970143, 0.8555473895582649, 0.7020537765956771, 0.39479637529005274, 0.8221084284949606, 0.7218719944382533, 0.7077940519459173, 0.8730760797308565, 0.8274795464482285, 0.4178586840539894, 0.5590039876130715, 0.7734895064621775, 0.7755274583394464, 0.6354585071650307, 0.7294065851983866, 0.7347069747750868, 0.7117963411971648, 0.7891334780288303, 0.1817264265367016, 0.737765862345314, 0.6059541939252896, 0.8420389744080645, 0.8213781544259392, 0.8122641975159689, 0.9501406391018212, 0.8140177754816462, 0.8017956530333991, 0.6990577274233384, 0.004032980111105755, 0.5981723056923576, 0.8239142363381216, 0.6303782442432408, 0.4990051413446339, 0.5835675708427543, 0.49774350940463247, 0.7573468888501887, 0.6803178393261431, 0.4575707938054209, 0.8368919695188907, 0.513874445934205, 0.003669306259720043, 0.8804752212646626, 0.6154681402418731, 0.6884876803003795, 0.9306012100440665, 0.5212455877120273, 0.7656643205369414, 0.23097727019156025, 0.46713084818505024, 0.5467092678587132, 0.7699517249856005, 0.0218411475423423, 0.7566486238583569, 0.5689216008552388, 0.709149616161197, 0.7964934864721039, 0.9266192635650654, 0.5984760308235213, 0.6876088554083866, 0.5486943411232224, 0.7118844765483012, 0.7679857832959798, 0.24186426569386363, 0.0850629278802881, 0.4631333919000077, 0.65297032629921, 0.5501138244713374, 0.09613576270520657, 0.8825124793674031, 0.014078839799289887, 0.6063424920329643, 0.8467078778553077, 0.7278242991149354, 0.6055085922541071, 0.8769892306713272, 0.6398144204981087, 0.6561010715262388, 0.679349462386784, 0.698420352868488, 1.5738848657757954e-05, 0.6127824832714589, 0.3601544556048749, 0.6454987439845602, 0.840221718199153, 0.9422397446572054, 0.6664965968658144, 0.6026370407961388, 0.26738680658365377, 0.5937746714127771, 0.4814667711517872, 0.9900522218104894, 0.45660194588285985, 0.5060777212969938, 0.6612912600420011, 0.6178192976582816, 0.40667336217362365, 0.6003038291839706, 0.5648401287431117, 0.8752943355455124, 0.7902370225716643, 1.1223915080584164e-05, 0.47907788371411564, 0.45400314944815545, 0.46984043767406997, 0.8099305657761856, 0.6822725470503077, 0.006499746447632461, 0.8662631547063013, 0.8650661187038072, 0.44092648429191467, 0.6572261117940223, 0.5345337135348447, 0.40168784902248106, 0.7712135549897581, 0.5193648845743172, 0.7933808689359089, 0.8454398264639257, 0.9661453388484418, 0.7766805254503681, 0.48655923528140566, 0.532140763170627, 0.7523857662814598, 0.3647248224506916, 0.8442618370543918, 0.6525921422097667, 0.4943215770129231, 0.002972271647127629, 0.4964878865160075, 0.9518770475905177, 0.68420225175855, 0.45375958425383833, 0.8211309959529847, 0.6605894366874181, 0.6332718914979896, 0.7772681043442984, 0.709007664459772, 0.546494511428097, 0.035927869563051246, 0.7954083049072076, 0.49830239482475064, 0.3313468907065896, 0.47336525162815246, 0.6365291018839448, 0.8069149368117426, 0.7311033689205892, 0.5851540705798088, 0.5607212511765234, 0.03667381275260776, 0.7905833059971944, 0.8994680333439922, 0.8950416708927599, 0.38493306603403815, 0.6607005918758216, 0.2748235623224507, 1.250883572990279e-05, 0.7636295007375747, 0.6361376412168877, 0.576624559502373, 0.8544924490905642, 0.7288220785573046, 0.9033801023378693, 0.4754086551402564, 0.5257319692197499, 0.5925075063981833, 0.7706957397307379, 0.8342996322651773, 0.7591110247100757, 0.7773816583749706, 0.952791526990349, 0.002560579982263055, 0.44963320675966717, 0.8680598118907603, 0.0009631394553763805, 0.5814844336324845, 0.5277494607876891, 0.6120557884986286, 0.0006033946065735597, 0.2880724007536657, 0.5781835749988077, 0.42194322675384865, 0.7214118939325055, 0.6399462868956278, 0.7397702558450482, 0.7101017050903969, 0.5245288435674477, 0.8797279174338088, 0.06228062993491778, 0.6674306851176129, 0.9552627014667867, 0.5417028180827073, 0.40502178930477345, 0.7484764368229571, 0.479588169607362, 0.6971799631979742, 0.9559393780278121, 0.8629708264289774, 0.556355461781016, 0.7471457885169103, 0.9514200344371119, 0.9045062941089862, 0.03544898028815104, 0.7959588994416947, 0.5352408396118523, 0.39025643400039534, 0.4681760414977616, 0.4129868885868973, 0.7690515834690146, 0.5017767328237974, 0.6375225240067836, 0.5721722376901748, 0.5782021609310648, 0.8324739269867921, 0.4397209720991777, 0.27938942972781805, 0.6676625207342687, 0.607350833288045, 0.9233153211722389, 0.6928251446529504, 0.8579401247653237, 0.4417041014075307, 0.5010375574586143, 0.7045236583285427, 0.7679307842486963, 0.661983523878926, 0.5338751800935947, 0.5387192519238775, 0.7666054049778839, 0.6982696254844638, 0.5933709913132212, 0.3802286896148756, 0.6115139756881348, 0.7015027437817535, 0.7189316057232721, 0.8654913237181338, 0.624402703933557, 0.7393171135337837, 0.8634586897720017, 0.8835872057373578, 0.625590003100414, 0.04773238478518561, 0.777359951128665, 0.7041648717199945, 0.5373807168152809, 0.7225895731186779, 0.6933246958576169, 0.6728481654923798, 0.7636195193548649, 0.8819862666275621, 0.5675735680099693, 0.5094088800105453, 0.6488471691592652, 0.8425350684777102, 0.6359054756440452, 0.7885273215922449, 0.7857869174584459, 0.8584587782926205, 0.7334257160332994, 0.6546470528635663, 0.8898058801247848, 0.9957009959730605, 0.6271982680324024, 0.5305405751592635, 0.6352986671290407, 0.7276208379401932, 0.9059219337260788, 0.49016495253449344, 0.7082902388155299, 0.5298401774679792, 0.5260074105307525, 0.8227041683755737, 0.5893525706455919, 0.7448286257418492, 0.9781586627281619, 0.6510800135921349, 0.5150172045409038, 0.48845476682470196, 0.005614504833671402, 0.29446094368831205, 0.9100963851215788, 0.5960739768682353, 0.7329712824080329, 0.913268000223975, 0.37883454258249655, 0.7554067533294001, 0.7368799649120032, 0.6414514324192047, 0.9934913370469536, 0.5969826621218406, 0.4657948893531694, 0.6414831868208885, 0.5946010269758816, 0.5158350855783602, 0.7968861974244105, 0.7268930331112055, 0.6044556003769372, 0.5005249399036323, 0.7950899088635194, 0.730977992948789, 0.6523694730959814, 0.7303419960359906, 0.7137234128668015, 0.7480147936635108, 0.7918198047164957, 0.27292627374374484, 0.0008586971970736598, 0.7827351506312694, 0.764695275373659, 0.6868988823908938, 0.24238138733197337, 0.5865611618326663, 0.913012274748575, 0.8537172218722412, 0.0002901471621015374, 0.5298028787808728, 0.3452841781981661, 0.07818378035198864, 0.6642935933522077, 0.3981683303776741, 0.33845491638596725, 0.9972329140273932, 0.6615269123533316, 0.5742056786817433, 0.8484831874524538, 0.0007026330487197208, 0.35677182262530166, 0.7077733807804302, 0.5887702302308744, 0.5394440356568765, 0.7651617774141719, 0.40347713818034064, 0.6387981406831562, 0.38530986963440317, 0.8148010271445854, 0.8339496479890961, 0.5689567029675904, 0.8778306941734231, 0.8632959782688431, 0.75943479562909, 0.9570759774682083, 0.576565339117054, 0.7223423404965428, 0.6997530876140396, 0.7556727097816739, 0.8877667648362734, 0.3255674103267308, 0.9093764887268305, 0.8367966216304391, 0.6700758221170308, 0.8595249248002236, 0.46785021168313423, 0.9426504829769656, 0.9330678068460788, 0.6060352662963475, 0.8054867114802016, 0.8898417109649389, 0.47829459988745504, 0.7422236849606192, 0.4747721601753701, 0.7192433474805175, 0.6893841935280672, 0.0003448906631251027, 0.6406290302642526, 0.7107838694568954, 0.0715673651792227, 0.5766740804354001, 0.6398470319358998, 0.4640052279990948, 0.6658925527339463, 0.23586645596417288, 0.6850622284071815, 0.7133254463692256, 0.5690342883108658, 0.575984091739995, 0.643885364615548, 0.7898197595443341, 0.8587194683303648, 0.6420689531782579, 0.039356665126134956, 0.4482923552573942, 0.5222439417025025]\n",
            "[0.9646, 0.9642, 0.9749, 0.9746, 0.9794, 0.9749, 0.9791, 0.9785, 0.9794, 0.9789, 0.9819, 0.9821, 0.984, 0.9832, 0.9812, 0.9819, 0.9796, 0.9779, 0.9824, 0.981, 0.9823, 0.9813, 0.9819, 0.9791, 0.9825, 0.9839, 0.9833, 0.9832, 0.9825, 0.9817]\n",
            "[tensor(0.6876, grad_fn=<MeanBackward0>), tensor(0.6919, grad_fn=<MeanBackward0>), tensor(0.6863, grad_fn=<MeanBackward0>), tensor(0.6771, grad_fn=<MeanBackward0>), tensor(0.6615, grad_fn=<MeanBackward0>), tensor(0.6500, grad_fn=<MeanBackward0>), tensor(0.6522, grad_fn=<MeanBackward0>), tensor(0.6482, grad_fn=<MeanBackward0>), tensor(0.6428, grad_fn=<MeanBackward0>), tensor(0.6372, grad_fn=<MeanBackward0>), tensor(0.6295, grad_fn=<MeanBackward0>), tensor(0.6290, grad_fn=<MeanBackward0>), tensor(0.6219, grad_fn=<MeanBackward0>), tensor(0.6174, grad_fn=<MeanBackward0>), tensor(0.6156, grad_fn=<MeanBackward0>), tensor(0.6110, grad_fn=<MeanBackward0>), tensor(0.6031, grad_fn=<MeanBackward0>), tensor(0.6035, grad_fn=<MeanBackward0>), tensor(0.6013, grad_fn=<MeanBackward0>), tensor(0.5981, grad_fn=<MeanBackward0>), tensor(0.6026, grad_fn=<MeanBackward0>), tensor(0.6019, grad_fn=<MeanBackward0>), tensor(0.5900, grad_fn=<MeanBackward0>), tensor(0.5926, grad_fn=<MeanBackward0>), tensor(0.5926, grad_fn=<MeanBackward0>), tensor(0.5971, grad_fn=<MeanBackward0>), tensor(0.5898, grad_fn=<MeanBackward0>), tensor(0.5899, grad_fn=<MeanBackward0>), tensor(0.5856, grad_fn=<MeanBackward0>), tensor(0.5809, grad_fn=<MeanBackward0>)]\n",
            "0.6348088313812575\n",
            "0.2242820251139648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCx4AExkqWvE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}