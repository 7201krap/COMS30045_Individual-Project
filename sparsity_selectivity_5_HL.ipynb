{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparsity_selectivity_5_HL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/sparsity_selectivity_5_HL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7STrWa0P3z_",
        "outputId": "058eb622-6630-4f77-bd66-a93c98b90588"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWq7aiiEjZde",
        "outputId": "0aa315b3-4163-4a61-9b04-9001e572ab5e"
      },
      "source": [
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "root_dir = './'\n",
        "torchvision.datasets.MNIST(root=root_dir,download=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-16 01:06:28--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
            "--2021-03-16 01:06:28--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘MNIST.tar.gz’\n",
            "\n",
            "MNIST.tar.gz            [      <=>           ]  33.20M  31.8MB/s    in 1.0s    \n",
            "\n",
            "2021-03-16 01:06:29 (31.8 MB/s) - ‘MNIST.tar.gz’ saved [34813078]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte\n",
            "MNIST/raw/train-images-idx3-ubyte.gz\n",
            "MNIST/processed/\n",
            "MNIST/processed/training.pt\n",
            "MNIST/processed/test.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./\n",
              "    Split: Train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4j9WoP-UnAm"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApOU7hvb95W4"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTW5TOUnP5XY"
      },
      "source": [
        "mnist_trainset = torchvision.datasets.MNIST(root=root_dir, train=True, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "mnist_testset  = torchvision.datasets.MNIST(root=root_dir, \n",
        "                                train=False, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(mnist_trainset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=True)\n",
        "\n",
        "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXTkEUJ5P6kU"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# Define the model \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # modify this section for later use \n",
        "        self.linear_1 = torch.nn.Linear(784, 256)\n",
        "        self.linear_2 = torch.nn.Linear(256, 256)\n",
        "        self.linear_3 = torch.nn.Linear(256, 256)\n",
        "        self.linear_4 = torch.nn.Linear(256, 256)\n",
        "        self.linear_5 = torch.nn.Linear(256, 256)\n",
        "        self.linear_6 = torch.nn.Linear(256, 10)\n",
        "        self.sigmoid12  = torch.nn.Sigmoid()\n",
        "        self.sigmoid23  = torch.nn.Sigmoid()\n",
        "        self.sigmoid34  = torch.nn.Sigmoid()\n",
        "        self.sigmoid45  = torch.nn.Sigmoid()\n",
        "        self.sigmoid56  = torch.nn.Sigmoid()\n",
        "\n",
        "        self.layer_activations = dict()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # modify this section for later use \n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.sigmoid12(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.sigmoid23(x)\n",
        "        x = self.linear_3(x)\n",
        "        x = self.sigmoid34(x)\n",
        "        x = self.linear_4(x)\n",
        "        x = self.sigmoid45(x)\n",
        "        x = self.linear_5(x)\n",
        "        x = self.sigmoid56(x)\n",
        "        pred = self.linear_6(x)\n",
        "        return pred\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfgvKH6eP9Ou"
      },
      "source": [
        "def get_activation(model, layer_name):    \n",
        "    def hook(module, input, output):\n",
        "        model.layer_activations[layer_name] = output\n",
        "    return hook"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWDMw4S3jebv"
      },
      "source": [
        "def sparsity_calculator(final_spareness):\n",
        "    sparseness_list = list()\n",
        "    for single_epoch_spareness in final_spareness:\n",
        "\n",
        "        hidden_layer_activation_list = single_epoch_spareness\n",
        "        hidden_layer_activation_list = torch.stack(hidden_layer_activation_list)\n",
        "        layer_activations_list = torch.reshape(hidden_layer_activation_list, (10000, 256))\n",
        "\n",
        "        layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
        "        num_neurons = layer_activations_list.shape[1]\n",
        "        population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
        "        mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
        "\n",
        "        sparseness_list.append(mean_sparseness_per_epoch)\n",
        "\n",
        "    return sparseness_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvHGO5RSvi6I"
      },
      "source": [
        "def selectivity(hidden_layer_each_neuron):\n",
        "    __selectivity__ = list()\n",
        "    # I will now try to find the average of each class for each neuron.\n",
        "    # check out the next cell \n",
        "    avg_activations = [dict() for x in range(256)]\n",
        "    for i, neuron in enumerate(hidden_layer_each_neuron):\n",
        "        for k, v in neuron.items():\n",
        "            # v is the list of activations for hidden layer's neuron k \n",
        "            avg_activations[i][k] = sum(v) / float(len(v))\n",
        "\n",
        "    # generate 256 lists to get only values in avg_activations\n",
        "    only_activation_vals = [list() for x in range(256)]\n",
        "\n",
        "    # get only values from avg_activations\n",
        "    for i, avg_activation in enumerate(avg_activations):\n",
        "        for value in avg_activation.values():\n",
        "            only_activation_vals[i].append(value)\n",
        "\n",
        "\n",
        "    for activation_val in only_activation_vals:\n",
        "        # find u_max \n",
        "        u_max = np.max(activation_val)\n",
        "\n",
        "        # find u_minus_max \n",
        "        u_minus_max = (np.sum(activation_val) - u_max) / 9\n",
        "\n",
        "        # find selectivity \n",
        "        selectivity = (u_max - u_minus_max) / (u_max + u_minus_max)\n",
        "\n",
        "        # append selectivity value to selectivity\n",
        "        __selectivity__.append(selectivity)\n",
        "\n",
        "    avg_selectivity = np.average(__selectivity__)\n",
        "    std_selectivity = np.std(__selectivity__)\n",
        "                                 \n",
        "    return avg_selectivity, std_selectivity"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPC8QAP9jgmI"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# add a parameter to the function and calculate avg and std. Do not forget to change division by 2, 3, 4, or 5 \n",
        "def avg_std_calculator(_hidden_layer_each_neuron_12, _hidden_layer_each_neuron_23, _hidden_layer_each_neuron_34, _hidden_layer_each_neuron_45, _hidden_layer_each_neuron_56):\n",
        "\n",
        "    avg_selectivity12, std_selectivity12 = selectivity(_hidden_layer_each_neuron_12)\n",
        "    avg_selectivity23, std_selectivity23 = selectivity(_hidden_layer_each_neuron_23)\n",
        "    avg_selectivity34, std_selectivity34 = selectivity(_hidden_layer_each_neuron_34)\n",
        "    avg_selectivity45, std_selectivity45 = selectivity(_hidden_layer_each_neuron_45)\n",
        "    avg_selectivity56, std_selectivity56 = selectivity(_hidden_layer_each_neuron_56)\n",
        "\n",
        "    final_selectivity_avg = (avg_selectivity12 + avg_selectivity23 + avg_selectivity34 + avg_selectivity45 + avg_selectivity56) / 5\n",
        "    final_selecvitity_std = (std_selectivity12 + std_selectivity23 + std_selectivity34 + std_selectivity45 + std_selectivity56) / 5\n",
        "\n",
        "    return final_selectivity_avg, final_selecvitity_std\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5PUiBNqUImf"
      },
      "source": [
        "def model_factory(optimizer_name):\n",
        "    '''\n",
        "    optimizer_name : choose one of Adagrad, Adadelta, SGD, and Adam \n",
        "\n",
        "    '''\n",
        "    my_model = Model()\n",
        "    print(\"my_model:\", my_model)\n",
        "    my_model.to(device)\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    # chagen sigmoid34 an 's34'\n",
        "    my_model.sigmoid12.register_forward_hook(get_activation(my_model, 's12'))\n",
        "    my_model.sigmoid23.register_forward_hook(get_activation(my_model, 's23'))\n",
        "    my_model.sigmoid34.register_forward_hook(get_activation(my_model, 's34'))\n",
        "    my_model.sigmoid45.register_forward_hook(get_activation(my_model, 's45'))\n",
        "    my_model.sigmoid56.register_forward_hook(get_activation(my_model, 's56'))\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        my_optimizer = torch.optim.Adadelta(my_model.parameters(), lr=1.0)\n",
        "\n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        my_optimizer = torch.optim.Adagrad(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        my_optimizer = torch.optim.SGD(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        my_optimizer = torch.optim.Adam(my_model.parameters(), lr=0.001)\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")\n",
        "    \n",
        "    print(\"my_optimizer:\", my_optimizer)\n",
        "    test_acc, sparsity_avg, selectivity_list_avg, selectivity_list_std = selectivity_trainer(optimizer=my_optimizer, model=my_model)\n",
        "    # ************* modify this section for later use *************\n",
        "    # change name of the file \n",
        "    file_saver = open(f\"5HL_selectivity_sparsity_{optimizer_name}.txt\", \"w\")\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver.write(str(test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(selectivity_list_avg)+'\\n'+str(selectivity_list_std)+'\\n\\n')\n",
        "    file_saver.close()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        !cp 5HL_selectivity_sparsity_Adadelta.txt /content/drive/MyDrive\n",
        "    \n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        !cp 5HL_selectivity_sparsity_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        !cp 5HL_selectivity_sparsity_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        !cp 5HL_selectivity_sparsity_Adam.txt /content/drive/MyDrive\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXOpwTXEQFKY"
      },
      "source": [
        "no_epochs = 50\n",
        "def selectivity_trainer(optimizer, model):\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    train_loss = list()\n",
        "    test_loss  = list()\n",
        "    test_acc   = list()\n",
        "\n",
        "    best_test_loss = 1\n",
        "\n",
        "    selectivity_avg_list = list()\n",
        "    selectivity_std_list = list()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    final_spareness_12 = list()\n",
        "    final_spareness_23 = list()\n",
        "    final_spareness_34 = list()\n",
        "    final_spareness_45 = list()\n",
        "    final_spareness_56 = list()\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    for epoch in range(no_epochs):\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_activation_list_12 = list()\n",
        "        hidden_layer_activation_list_23 = list()\n",
        "        hidden_layer_activation_list_34 = list()\n",
        "        hidden_layer_activation_list_45 = list()\n",
        "        hidden_layer_activation_list_56 = list()\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_each_neuron_12 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_12 = np.array(hidden_layer_each_neuron_12)\n",
        "\n",
        "        hidden_layer_each_neuron_23 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_23 = np.array(hidden_layer_each_neuron_23)\n",
        "\n",
        "        hidden_layer_each_neuron_34 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_34 = np.array(hidden_layer_each_neuron_34)\n",
        "\n",
        "        hidden_layer_each_neuron_45 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_45 = np.array(hidden_layer_each_neuron_45)\n",
        "\n",
        "        hidden_layer_each_neuron_56 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_56 = np.array(hidden_layer_each_neuron_56)\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "\n",
        "        total_train_loss = 0\n",
        "        total_test_loss = 0\n",
        "\n",
        "        # training\n",
        "        # set up training mode \n",
        "        model.train()\n",
        "\n",
        "        for itr, (images, labels) in enumerate(train_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_train_loss = total_train_loss / (itr + 1)\n",
        "        train_loss.append(total_train_loss)\n",
        "\n",
        "        # testing \n",
        "        # change to evaluation mode \n",
        "        model.eval()\n",
        "        total = 0\n",
        "        for itr, (images, labels) in enumerate(test_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # we now need softmax because we are testing.\n",
        "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "            for i, p in enumerate(pred):\n",
        "                if labels[i] == torch.max(p.data, 0)[1]:\n",
        "                    total = total + 1\n",
        "\n",
        "            # ***************** sparsity calculation ***************** #\n",
        "            hidden_layer_activation_list_12.append(model.layer_activations['s12'])\n",
        "            hidden_layer_activation_list_23.append(model.layer_activations['s23'])\n",
        "            hidden_layer_activation_list_34.append(model.layer_activations['s34'])\n",
        "            hidden_layer_activation_list_45.append(model.layer_activations['s45'])\n",
        "            hidden_layer_activation_list_56.append(model.layer_activations['s56'])\n",
        "\n",
        "            # ************* modify this section for later use *************\n",
        "            # Do not forget to change hidden_layer_each_neuron_12 name \n",
        "            for activation, label in zip(model.layer_activations['s12'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_12[i][label].append(activation[i])\n",
        "\n",
        "            for activation, label in zip(model.layer_activations['s23'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_23[i][label].append(activation[i])\n",
        "            \n",
        "            for activation, label in zip(model.layer_activations['s34'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_34[i][label].append(activation[i])\n",
        "\n",
        "            for activation, label in zip(model.layer_activations['s45'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_45[i][label].append(activation[i])\n",
        "\n",
        "            for activation, label in zip(model.layer_activations['s56'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_56[i][label].append(activation[i])\n",
        "\n",
        "        # Add one more parameter \n",
        "        selectivity_avg, selecvitity_std = avg_std_calculator(hidden_layer_each_neuron_12, hidden_layer_each_neuron_23, hidden_layer_each_neuron_34, hidden_layer_each_neuron_45, hidden_layer_each_neuron_56)\n",
        "        # ************* modify this section for later use *************\n",
        "        \n",
        "        selectivity_avg_list.append(selectivity_avg)\n",
        "        selectivity_std_list.append(selecvitity_std)\n",
        "\n",
        "        final_spareness_12.append(hidden_layer_activation_list_12)\n",
        "        final_spareness_23.append(hidden_layer_activation_list_23)\n",
        "        final_spareness_34.append(hidden_layer_activation_list_34)\n",
        "        final_spareness_45.append(hidden_layer_activation_list_45)\n",
        "        final_spareness_56.append(hidden_layer_activation_list_56)\n",
        "        # ***************** sparsity calculation ***************** #\n",
        "\n",
        "        # caculate accuracy \n",
        "        accuracy = total / len(mnist_testset)\n",
        "\n",
        "        # append accuracy here\n",
        "        test_acc.append(accuracy)\n",
        "\n",
        "        # append test loss here \n",
        "        total_test_loss = total_test_loss / (itr + 1)\n",
        "        test_loss.append(total_test_loss)\n",
        "\n",
        "        print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Test Loss: {:.8f}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, total_train_loss, total_test_loss, accuracy))\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "    sparsity_list12 = sparsity_calculator(final_spareness_12)\n",
        "    sparsity_list23 = sparsity_calculator(final_spareness_23)\n",
        "    sparsity_list34 = sparsity_calculator(final_spareness_34)\n",
        "    sparsity_list45 = sparsity_calculator(final_spareness_45)\n",
        "    sparsity_list56 = sparsity_calculator(final_spareness_56)\n",
        "\n",
        "    print(sparsity_list12)\n",
        "    print(sparsity_list23)\n",
        "    print(sparsity_list34)\n",
        "    print(sparsity_list45)\n",
        "    print(sparsity_list56)\n",
        "\n",
        "    average_sparsity = list()\n",
        "    for i in range(no_epochs):\n",
        "        average_sparsity.append( (sparsity_list12[i].item() + sparsity_list23[i].item() + sparsity_list34[i].item() + sparsity_list45[i].item() + sparsity_list56[i].item()) / 5 )\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "\n",
        "    print(\"average_sparsity:\", average_sparsity)\n",
        "\n",
        "    return test_acc, average_sparsity, selectivity_avg_list, selectivity_std_list"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILIJTJb2UdfI"
      },
      "source": [
        "# Adadelta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UH0qDnFUfaD",
        "outputId": "2ae6f517-07ae-4ccf-9f5a-287f7761f3e7"
      },
      "source": [
        "model_factory('Adadelta')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_6): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            "  (sigmoid56): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-06\n",
            "    lr: 1.0\n",
            "    rho: 0.9\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.30755125, Test Loss: 2.30164616, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30180220, Test Loss: 2.30179365, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.30164154, Test Loss: 2.30163917, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.30165143, Test Loss: 2.30126958, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.30157960, Test Loss: 2.30127509, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.30154829, Test Loss: 2.30109043, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.30157551, Test Loss: 2.30115319, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.30144651, Test Loss: 2.30109492, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.30152204, Test Loss: 2.30146621, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.30150002, Test Loss: 2.30095169, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.30149307, Test Loss: 2.30129139, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.30142035, Test Loss: 2.30099959, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.30151436, Test Loss: 2.30136310, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.30148068, Test Loss: 2.30132865, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.30148078, Test Loss: 2.30121527, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.30138044, Test Loss: 2.30151163, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.30132344, Test Loss: 2.30071118, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.30092660, Test Loss: 2.30011426, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 2.19843385, Test Loss: 1.83227554, Test Accuracy: 0.25280000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 1.54909301, Test Loss: 1.23491005, Test Accuracy: 0.50720000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 1.10597341, Test Loss: 0.91628968, Test Accuracy: 0.65430000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.74409821, Test Loss: 0.51274829, Test Accuracy: 0.80620000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.43023869, Test Loss: 0.31522583, Test Accuracy: 0.91490000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.25973359, Test Loss: 0.22867292, Test Accuracy: 0.93700000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.20134073, Test Loss: 0.18776008, Test Accuracy: 0.94940000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.16793682, Test Loss: 0.16522446, Test Accuracy: 0.95420000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.14392228, Test Loss: 0.15053582, Test Accuracy: 0.95700000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.12330834, Test Loss: 0.14988974, Test Accuracy: 0.95720000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.10978867, Test Loss: 0.18742856, Test Accuracy: 0.94590000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.09895965, Test Loss: 0.16145032, Test Accuracy: 0.95510000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.08977338, Test Loss: 0.14810617, Test Accuracy: 0.95880000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.08012303, Test Loss: 0.15122918, Test Accuracy: 0.95820000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.07280139, Test Loss: 0.12139982, Test Accuracy: 0.96710000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.06557713, Test Loss: 0.12514173, Test Accuracy: 0.96510000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.06050015, Test Loss: 0.11758271, Test Accuracy: 0.96760000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.05554988, Test Loss: 0.10873965, Test Accuracy: 0.97130000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.04989049, Test Loss: 0.10923446, Test Accuracy: 0.97130000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.04594869, Test Loss: 0.12668358, Test Accuracy: 0.96660000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.04198659, Test Loss: 0.11742455, Test Accuracy: 0.97040000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.03977579, Test Loss: 0.11043834, Test Accuracy: 0.97390000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.03438426, Test Loss: 0.14277950, Test Accuracy: 0.96490000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.03151524, Test Loss: 0.11331568, Test Accuracy: 0.97120000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.02874050, Test Loss: 0.14659620, Test Accuracy: 0.96420000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.02653638, Test Loss: 0.12068074, Test Accuracy: 0.97260000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.02384682, Test Loss: 0.12839937, Test Accuracy: 0.97180000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.02120469, Test Loss: 0.12151354, Test Accuracy: 0.97350000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.01963685, Test Loss: 0.12853781, Test Accuracy: 0.97240000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.01704824, Test Loss: 0.13514482, Test Accuracy: 0.97050000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.01558625, Test Loss: 0.13942524, Test Accuracy: 0.97190000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.01570333, Test Loss: 0.13257417, Test Accuracy: 0.97320000\n",
            "[tensor(0.0051, grad_fn=<MeanBackward0>), tensor(0.0051, grad_fn=<MeanBackward0>), tensor(0.0052, grad_fn=<MeanBackward0>), tensor(0.0053, grad_fn=<MeanBackward0>), tensor(0.0055, grad_fn=<MeanBackward0>), tensor(0.0056, grad_fn=<MeanBackward0>), tensor(0.0058, grad_fn=<MeanBackward0>), tensor(0.0060, grad_fn=<MeanBackward0>), tensor(0.0062, grad_fn=<MeanBackward0>), tensor(0.0065, grad_fn=<MeanBackward0>), tensor(0.0069, grad_fn=<MeanBackward0>), tensor(0.0073, grad_fn=<MeanBackward0>), tensor(0.0078, grad_fn=<MeanBackward0>), tensor(0.0084, grad_fn=<MeanBackward0>), tensor(0.0092, grad_fn=<MeanBackward0>), tensor(0.0104, grad_fn=<MeanBackward0>), tensor(0.0122, grad_fn=<MeanBackward0>), tensor(0.0161, grad_fn=<MeanBackward0>), tensor(0.0690, grad_fn=<MeanBackward0>), tensor(0.3709, grad_fn=<MeanBackward0>), tensor(0.4581, grad_fn=<MeanBackward0>), tensor(0.5232, grad_fn=<MeanBackward0>), tensor(0.5516, grad_fn=<MeanBackward0>), tensor(0.5677, grad_fn=<MeanBackward0>), tensor(0.5747, grad_fn=<MeanBackward0>), tensor(0.5795, grad_fn=<MeanBackward0>), tensor(0.5815, grad_fn=<MeanBackward0>), tensor(0.5825, grad_fn=<MeanBackward0>), tensor(0.5858, grad_fn=<MeanBackward0>), tensor(0.5845, grad_fn=<MeanBackward0>), tensor(0.5863, grad_fn=<MeanBackward0>), tensor(0.5860, grad_fn=<MeanBackward0>), tensor(0.5864, grad_fn=<MeanBackward0>), tensor(0.5867, grad_fn=<MeanBackward0>), tensor(0.5902, grad_fn=<MeanBackward0>), tensor(0.5934, grad_fn=<MeanBackward0>), tensor(0.5909, grad_fn=<MeanBackward0>), tensor(0.5908, grad_fn=<MeanBackward0>), tensor(0.5939, grad_fn=<MeanBackward0>), tensor(0.5935, grad_fn=<MeanBackward0>), tensor(0.5909, grad_fn=<MeanBackward0>), tensor(0.5977, grad_fn=<MeanBackward0>), tensor(0.5992, grad_fn=<MeanBackward0>), tensor(0.5961, grad_fn=<MeanBackward0>), tensor(0.5981, grad_fn=<MeanBackward0>), tensor(0.5959, grad_fn=<MeanBackward0>), tensor(0.5955, grad_fn=<MeanBackward0>), tensor(0.5954, grad_fn=<MeanBackward0>), tensor(0.5958, grad_fn=<MeanBackward0>), tensor(0.5968, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0111, grad_fn=<MeanBackward0>), tensor(0.0107, grad_fn=<MeanBackward0>), tensor(0.0099, grad_fn=<MeanBackward0>), tensor(0.0586, grad_fn=<MeanBackward0>), tensor(0.1034, grad_fn=<MeanBackward0>), tensor(0.1309, grad_fn=<MeanBackward0>), tensor(0.1450, grad_fn=<MeanBackward0>), tensor(0.1593, grad_fn=<MeanBackward0>), tensor(0.1663, grad_fn=<MeanBackward0>), tensor(0.1771, grad_fn=<MeanBackward0>), tensor(0.1886, grad_fn=<MeanBackward0>), tensor(0.1923, grad_fn=<MeanBackward0>), tensor(0.1906, grad_fn=<MeanBackward0>), tensor(0.1911, grad_fn=<MeanBackward0>), tensor(0.1957, grad_fn=<MeanBackward0>), tensor(0.2049, grad_fn=<MeanBackward0>), tensor(0.2045, grad_fn=<MeanBackward0>), tensor(0.2074, grad_fn=<MeanBackward0>), tensor(0.2127, grad_fn=<MeanBackward0>), tensor(0.2129, grad_fn=<MeanBackward0>), tensor(0.2145, grad_fn=<MeanBackward0>), tensor(0.2185, grad_fn=<MeanBackward0>), tensor(0.2174, grad_fn=<MeanBackward0>), tensor(0.2218, grad_fn=<MeanBackward0>), tensor(0.2264, grad_fn=<MeanBackward0>), tensor(0.2288, grad_fn=<MeanBackward0>), tensor(0.2276, grad_fn=<MeanBackward0>), tensor(0.2248, grad_fn=<MeanBackward0>), tensor(0.2350, grad_fn=<MeanBackward0>), tensor(0.2389, grad_fn=<MeanBackward0>), tensor(0.2373, grad_fn=<MeanBackward0>), tensor(0.2421, grad_fn=<MeanBackward0>), tensor(0.2398, grad_fn=<MeanBackward0>), tensor(0.2390, grad_fn=<MeanBackward0>), tensor(0.2385, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0122, grad_fn=<MeanBackward0>), tensor(0.0123, grad_fn=<MeanBackward0>), tensor(0.0123, grad_fn=<MeanBackward0>), tensor(0.0123, grad_fn=<MeanBackward0>), tensor(0.0122, grad_fn=<MeanBackward0>), tensor(0.0122, grad_fn=<MeanBackward0>), tensor(0.0121, grad_fn=<MeanBackward0>), tensor(0.0121, grad_fn=<MeanBackward0>), tensor(0.0120, grad_fn=<MeanBackward0>), tensor(0.0119, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0109, grad_fn=<MeanBackward0>), tensor(0.0104, grad_fn=<MeanBackward0>), tensor(0.0097, grad_fn=<MeanBackward0>), tensor(0.0078, grad_fn=<MeanBackward0>), tensor(0.0873, grad_fn=<MeanBackward0>), tensor(0.1129, grad_fn=<MeanBackward0>), tensor(0.1664, grad_fn=<MeanBackward0>), tensor(0.1885, grad_fn=<MeanBackward0>), tensor(0.2133, grad_fn=<MeanBackward0>), tensor(0.2272, grad_fn=<MeanBackward0>), tensor(0.2334, grad_fn=<MeanBackward0>), tensor(0.2425, grad_fn=<MeanBackward0>), tensor(0.2464, grad_fn=<MeanBackward0>), tensor(0.2538, grad_fn=<MeanBackward0>), tensor(0.2514, grad_fn=<MeanBackward0>), tensor(0.2525, grad_fn=<MeanBackward0>), tensor(0.2588, grad_fn=<MeanBackward0>), tensor(0.2627, grad_fn=<MeanBackward0>), tensor(0.2663, grad_fn=<MeanBackward0>), tensor(0.2645, grad_fn=<MeanBackward0>), tensor(0.2754, grad_fn=<MeanBackward0>), tensor(0.2751, grad_fn=<MeanBackward0>), tensor(0.2795, grad_fn=<MeanBackward0>), tensor(0.2752, grad_fn=<MeanBackward0>), tensor(0.2990, grad_fn=<MeanBackward0>), tensor(0.2905, grad_fn=<MeanBackward0>), tensor(0.2771, grad_fn=<MeanBackward0>), tensor(0.2958, grad_fn=<MeanBackward0>), tensor(0.2889, grad_fn=<MeanBackward0>), tensor(0.2988, grad_fn=<MeanBackward0>), tensor(0.2949, grad_fn=<MeanBackward0>), tensor(0.2977, grad_fn=<MeanBackward0>), tensor(0.3117, grad_fn=<MeanBackward0>), tensor(0.2991, grad_fn=<MeanBackward0>), tensor(0.3054, grad_fn=<MeanBackward0>), tensor(0.3012, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0101, grad_fn=<MeanBackward0>), tensor(0.0085, grad_fn=<MeanBackward0>), tensor(0.0076, grad_fn=<MeanBackward0>), tensor(0.0072, grad_fn=<MeanBackward0>), tensor(0.0068, grad_fn=<MeanBackward0>), tensor(0.0065, grad_fn=<MeanBackward0>), tensor(0.0062, grad_fn=<MeanBackward0>), tensor(0.0060, grad_fn=<MeanBackward0>), tensor(0.0059, grad_fn=<MeanBackward0>), tensor(0.0056, grad_fn=<MeanBackward0>), tensor(0.0055, grad_fn=<MeanBackward0>), tensor(0.0053, grad_fn=<MeanBackward0>), tensor(0.0052, grad_fn=<MeanBackward0>), tensor(0.0051, grad_fn=<MeanBackward0>), tensor(0.0051, grad_fn=<MeanBackward0>), tensor(0.0049, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0042, grad_fn=<MeanBackward0>), tensor(0.0182, grad_fn=<MeanBackward0>), tensor(0.1798, grad_fn=<MeanBackward0>), tensor(0.1909, grad_fn=<MeanBackward0>), tensor(0.2642, grad_fn=<MeanBackward0>), tensor(0.2696, grad_fn=<MeanBackward0>), tensor(0.2673, grad_fn=<MeanBackward0>), tensor(0.2742, grad_fn=<MeanBackward0>), tensor(0.2748, grad_fn=<MeanBackward0>), tensor(0.2714, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2457, grad_fn=<MeanBackward0>), tensor(0.2575, grad_fn=<MeanBackward0>), tensor(0.2673, grad_fn=<MeanBackward0>), tensor(0.2631, grad_fn=<MeanBackward0>), tensor(0.2625, grad_fn=<MeanBackward0>), tensor(0.2501, grad_fn=<MeanBackward0>), tensor(0.2620, grad_fn=<MeanBackward0>), tensor(0.2543, grad_fn=<MeanBackward0>), tensor(0.2491, grad_fn=<MeanBackward0>), tensor(0.2414, grad_fn=<MeanBackward0>), tensor(0.2589, grad_fn=<MeanBackward0>), tensor(0.2509, grad_fn=<MeanBackward0>), tensor(0.2413, grad_fn=<MeanBackward0>), tensor(0.2456, grad_fn=<MeanBackward0>), tensor(0.2301, grad_fn=<MeanBackward0>), tensor(0.2585, grad_fn=<MeanBackward0>), tensor(0.2536, grad_fn=<MeanBackward0>), tensor(0.2450, grad_fn=<MeanBackward0>), tensor(0.2576, grad_fn=<MeanBackward0>), tensor(0.2497, grad_fn=<MeanBackward0>), tensor(0.2553, grad_fn=<MeanBackward0>), tensor(0.2509, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0062, grad_fn=<MeanBackward0>), tensor(0.0032, grad_fn=<MeanBackward0>), tensor(0.0016, grad_fn=<MeanBackward0>), tensor(0.0017, grad_fn=<MeanBackward0>), tensor(0.0015, grad_fn=<MeanBackward0>), tensor(0.0020, grad_fn=<MeanBackward0>), tensor(0.0013, grad_fn=<MeanBackward0>), tensor(0.0019, grad_fn=<MeanBackward0>), tensor(0.0017, grad_fn=<MeanBackward0>), tensor(0.0012, grad_fn=<MeanBackward0>), tensor(0.0015, grad_fn=<MeanBackward0>), tensor(0.0020, grad_fn=<MeanBackward0>), tensor(0.0021, grad_fn=<MeanBackward0>), tensor(0.0043, grad_fn=<MeanBackward0>), tensor(0.0070, grad_fn=<MeanBackward0>), tensor(0.0099, grad_fn=<MeanBackward0>), tensor(0.0154, grad_fn=<MeanBackward0>), tensor(0.0381, grad_fn=<MeanBackward0>), tensor(0.3657, grad_fn=<MeanBackward0>), tensor(0.6563, grad_fn=<MeanBackward0>), tensor(0.5891, grad_fn=<MeanBackward0>), tensor(0.6225, grad_fn=<MeanBackward0>), tensor(0.6429, grad_fn=<MeanBackward0>), tensor(0.6486, grad_fn=<MeanBackward0>), tensor(0.6429, grad_fn=<MeanBackward0>), tensor(0.6451, grad_fn=<MeanBackward0>), tensor(0.6574, grad_fn=<MeanBackward0>), tensor(0.6673, grad_fn=<MeanBackward0>), tensor(0.6869, grad_fn=<MeanBackward0>), tensor(0.6766, grad_fn=<MeanBackward0>), tensor(0.6631, grad_fn=<MeanBackward0>), tensor(0.6786, grad_fn=<MeanBackward0>), tensor(0.6613, grad_fn=<MeanBackward0>), tensor(0.6808, grad_fn=<MeanBackward0>), tensor(0.6605, grad_fn=<MeanBackward0>), tensor(0.6755, grad_fn=<MeanBackward0>), tensor(0.6876, grad_fn=<MeanBackward0>), tensor(0.6968, grad_fn=<MeanBackward0>), tensor(0.6700, grad_fn=<MeanBackward0>), tensor(0.6750, grad_fn=<MeanBackward0>), tensor(0.6935, grad_fn=<MeanBackward0>), tensor(0.6766, grad_fn=<MeanBackward0>), tensor(0.6946, grad_fn=<MeanBackward0>), tensor(0.6608, grad_fn=<MeanBackward0>), tensor(0.6608, grad_fn=<MeanBackward0>), tensor(0.6744, grad_fn=<MeanBackward0>), tensor(0.6562, grad_fn=<MeanBackward0>), tensor(0.6666, grad_fn=<MeanBackward0>), tensor(0.6628, grad_fn=<MeanBackward0>), tensor(0.6757, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.00902850367128849, 0.008151353430002927, 0.007696535740979016, 0.007649818598292768, 0.007559571834281087, 0.007621326157823205, 0.007435047323815525, 0.007554031279869378, 0.007510547619313001, 0.007401883858256042, 0.007455667946487665, 0.007578222034499049, 0.0076257137581706045, 0.008117577619850635, 0.008721843361854553, 0.009354039002209902, 0.010538281220942736, 0.015219835843890906, 0.11976918615400792, 0.2846583679318428, 0.30709439516067505, 0.3486900568008423, 0.3673154652118683, 0.37542482912540437, 0.3804714322090149, 0.3860863924026489, 0.3897975146770477, 0.3907734781503677, 0.3921792894601822, 0.39337149560451506, 0.39609538614749906, 0.39898026585578916, 0.3967866897583008, 0.39895394146442414, 0.40020495653152466, 0.40255609154701233, 0.40514062643051146, 0.40431372821331024, 0.40870626270771027, 0.4072700828313828, 0.40631519854068754, 0.40864716470241547, 0.4075352430343628, 0.4098538815975189, 0.40925975143909454, 0.41006646156311033, 0.41259701550006866, 0.4101256847381592, 0.4116595953702927, 0.4126238524913788]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hXfQe4vMDKB"
      },
      "source": [
        "# AdaGrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb-4TPM5MGuE",
        "outputId": "e484b0d1-bdd0-492b-f7a8-65023514d2ec"
      },
      "source": [
        "model_factory('Adagrad')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_6): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            "  (sigmoid56): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adagrad (\n",
            "Parameter Group 0\n",
            "    eps: 1e-10\n",
            "    initial_accumulator_value: 0\n",
            "    lr: 0.1\n",
            "    lr_decay: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.41126010, Test Loss: 2.32394924, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.32897436, Test Loss: 2.36121113, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.32134433, Test Loss: 2.31869691, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.31792858, Test Loss: 2.31844756, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.31580834, Test Loss: 2.31190552, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.31601657, Test Loss: 2.31554863, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.31307007, Test Loss: 2.31049158, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.31293393, Test Loss: 2.31008809, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.31280751, Test Loss: 2.31037569, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.31168190, Test Loss: 2.31775788, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.31121340, Test Loss: 2.31405701, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.31068572, Test Loss: 2.31068231, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.31046879, Test Loss: 2.31479500, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.31041237, Test Loss: 2.30308128, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.30990741, Test Loss: 2.30446376, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.30935375, Test Loss: 2.30234151, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.30856865, Test Loss: 2.31188746, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.30883907, Test Loss: 2.31044740, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 2.30939499, Test Loss: 2.30669921, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 2.30883103, Test Loss: 2.30542213, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 2.30837908, Test Loss: 2.31031523, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 2.30842606, Test Loss: 2.30488156, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 2.30803777, Test Loss: 2.31194970, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 2.30837256, Test Loss: 2.30508622, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 2.30823304, Test Loss: 2.30744039, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 2.30754379, Test Loss: 2.30671938, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 2.30752755, Test Loss: 2.30461405, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 2.30775123, Test Loss: 2.30969846, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 2.30738262, Test Loss: 2.30706236, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 2.30684886, Test Loss: 2.31648437, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 2.30748330, Test Loss: 2.30425547, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 2.30691808, Test Loss: 2.30419706, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 2.30690288, Test Loss: 2.30440267, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 2.30678642, Test Loss: 2.30941564, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 2.30631711, Test Loss: 2.30479734, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 2.30704026, Test Loss: 2.30703709, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 2.30694185, Test Loss: 2.30776670, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 2.30631107, Test Loss: 2.30756447, Test Accuracy: 0.10090000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 2.30629506, Test Loss: 2.30908956, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 2.30607986, Test Loss: 2.30392709, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 2.30648983, Test Loss: 2.30420078, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 2.30618046, Test Loss: 2.30435475, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 2.30636756, Test Loss: 2.31033406, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 2.30619037, Test Loss: 2.30756859, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 2.30621594, Test Loss: 2.30885238, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 2.30648595, Test Loss: 2.30454604, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 2.30613857, Test Loss: 2.30534246, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 2.30622853, Test Loss: 2.30539756, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 2.30589301, Test Loss: 2.30520474, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 2.30587772, Test Loss: 2.30572468, Test Accuracy: 0.09800000\n",
            "[tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>), tensor(0.2597, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>), tensor(0.3027, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>), tensor(0.2750, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>), tensor(0.3803, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.307802677154541, 0.30780268907547, 0.30780271887779237, 0.3078027665615082, 0.30780279636383057, 0.3078027606010437, 0.30780274868011476, 0.3078028321266174, 0.3078028619289398, 0.3078028678894043, 0.3078028976917267, 0.30780290365219115, 0.3078029215335846, 0.30780293941497805, 0.30780295729637147, 0.30780296921730044, 0.30780298709869386, 0.3078029990196228, 0.30780301690101625, 0.3078030288219452, 0.30780304670333863, 0.30780306458473206, 0.307803076505661, 0.3078031003475189, 0.3078031539916992, 0.3078031659126282, 0.3078031539916992, 0.30780320763587954, 0.30780318975448606, 0.3078032433986664, 0.3078032612800598, 0.30780325531959535, 0.3078032910823822, 0.3078033149242401, 0.3078033268451691, 0.3078033566474915, 0.3078033685684204, 0.3078033745288849, 0.3078033983707428, 0.30780341625213625, 0.30780344009399413, 0.30780344605445864, 0.3078034698963165, 0.3078034818172455, 0.3078034996986389, 0.30780349373817445, 0.3078035295009613, 0.3078035533428192, 0.3078035652637482, 0.3078035831451416]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmLJ4Zr2MnoS"
      },
      "source": [
        "# SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ObsEJHuMoPy",
        "outputId": "c4e2ca9d-e149-4865-df0b-3d9d4cfe0f1d"
      },
      "source": [
        "model_factory('SGD')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_6): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            "  (sigmoid56): Sigmoid()\n",
            ")\n",
            "my_optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.32514801, Test Loss: 2.31485856, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30932107, Test Loss: 2.30733581, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.30535526, Test Loss: 2.30372150, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.30402740, Test Loss: 2.30359470, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.30332511, Test Loss: 2.30275148, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.30257312, Test Loss: 2.30165909, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.30250776, Test Loss: 2.30113795, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.30266905, Test Loss: 2.30118412, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.30220000, Test Loss: 2.30260989, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.30225868, Test Loss: 2.30139722, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.30215773, Test Loss: 2.30150217, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.30198128, Test Loss: 2.30273411, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.30207878, Test Loss: 2.30145788, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.30195854, Test Loss: 2.30229507, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.30188825, Test Loss: 2.30201517, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.30204563, Test Loss: 2.30177367, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.30182799, Test Loss: 2.30209568, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.30185527, Test Loss: 2.30136093, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 2.30188141, Test Loss: 2.30132436, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 2.30178224, Test Loss: 2.30151150, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 2.30184552, Test Loss: 2.30176067, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 2.30184919, Test Loss: 2.30191328, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 2.30192880, Test Loss: 2.30132085, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 2.30182468, Test Loss: 2.30115149, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 2.30188085, Test Loss: 2.30171372, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 2.30189014, Test Loss: 2.30122393, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 2.30184617, Test Loss: 2.30207648, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 2.30185772, Test Loss: 2.30136679, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 2.30178628, Test Loss: 2.30156435, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 2.30181402, Test Loss: 2.30125367, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 2.30167712, Test Loss: 2.30168783, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 2.30185126, Test Loss: 2.30152472, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 2.30180913, Test Loss: 2.30123534, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 2.30169789, Test Loss: 2.30221615, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 2.30186145, Test Loss: 2.30153805, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 2.30176325, Test Loss: 2.30207394, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 2.30179833, Test Loss: 2.30151327, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 2.30175348, Test Loss: 2.30144468, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 2.30172016, Test Loss: 2.30153458, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 2.30177997, Test Loss: 2.30141159, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 2.30181493, Test Loss: 2.30110482, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 2.30167069, Test Loss: 2.30134452, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 2.30175330, Test Loss: 2.30190802, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 2.30169164, Test Loss: 2.30225276, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 2.30175945, Test Loss: 2.30101948, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 2.30175672, Test Loss: 2.30152033, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 2.30172992, Test Loss: 2.30151903, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 2.30168417, Test Loss: 2.30105011, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 2.30181361, Test Loss: 2.30103212, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 2.30172830, Test Loss: 2.30109319, Test Accuracy: 0.11350000\n",
            "[tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0046, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0047, grad_fn=<MeanBackward0>), tensor(0.0048, grad_fn=<MeanBackward0>), tensor(0.0048, grad_fn=<MeanBackward0>), tensor(0.0048, grad_fn=<MeanBackward0>), tensor(0.0048, grad_fn=<MeanBackward0>), tensor(0.0048, grad_fn=<MeanBackward0>), tensor(0.0048, grad_fn=<MeanBackward0>), tensor(0.0048, grad_fn=<MeanBackward0>), tensor(0.0048, grad_fn=<MeanBackward0>), tensor(0.0048, grad_fn=<MeanBackward0>), tensor(0.0048, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0116, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0119, grad_fn=<MeanBackward0>), tensor(0.0120, grad_fn=<MeanBackward0>), tensor(0.0121, grad_fn=<MeanBackward0>), tensor(0.0121, grad_fn=<MeanBackward0>), tensor(0.0122, grad_fn=<MeanBackward0>), tensor(0.0123, grad_fn=<MeanBackward0>), tensor(0.0123, grad_fn=<MeanBackward0>), tensor(0.0124, grad_fn=<MeanBackward0>), tensor(0.0124, grad_fn=<MeanBackward0>), tensor(0.0124, grad_fn=<MeanBackward0>), tensor(0.0125, grad_fn=<MeanBackward0>), tensor(0.0125, grad_fn=<MeanBackward0>), tensor(0.0125, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0127, grad_fn=<MeanBackward0>), tensor(0.0127, grad_fn=<MeanBackward0>), tensor(0.0127, grad_fn=<MeanBackward0>), tensor(0.0127, grad_fn=<MeanBackward0>), tensor(0.0127, grad_fn=<MeanBackward0>), tensor(0.0127, grad_fn=<MeanBackward0>), tensor(0.0127, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>), tensor(0.0129, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0138, grad_fn=<MeanBackward0>), tensor(0.0143, grad_fn=<MeanBackward0>), tensor(0.0143, grad_fn=<MeanBackward0>), tensor(0.0141, grad_fn=<MeanBackward0>), tensor(0.0139, grad_fn=<MeanBackward0>), tensor(0.0136, grad_fn=<MeanBackward0>), tensor(0.0134, grad_fn=<MeanBackward0>), tensor(0.0132, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0124, grad_fn=<MeanBackward0>), tensor(0.0123, grad_fn=<MeanBackward0>), tensor(0.0121, grad_fn=<MeanBackward0>), tensor(0.0120, grad_fn=<MeanBackward0>), tensor(0.0119, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0114, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0111, grad_fn=<MeanBackward0>), tensor(0.0111, grad_fn=<MeanBackward0>), tensor(0.0110, grad_fn=<MeanBackward0>), tensor(0.0109, grad_fn=<MeanBackward0>), tensor(0.0108, grad_fn=<MeanBackward0>), tensor(0.0107, grad_fn=<MeanBackward0>), tensor(0.0107, grad_fn=<MeanBackward0>), tensor(0.0106, grad_fn=<MeanBackward0>), tensor(0.0105, grad_fn=<MeanBackward0>), tensor(0.0105, grad_fn=<MeanBackward0>), tensor(0.0104, grad_fn=<MeanBackward0>), tensor(0.0104, grad_fn=<MeanBackward0>), tensor(0.0103, grad_fn=<MeanBackward0>), tensor(0.0102, grad_fn=<MeanBackward0>), tensor(0.0102, grad_fn=<MeanBackward0>), tensor(0.0101, grad_fn=<MeanBackward0>), tensor(0.0101, grad_fn=<MeanBackward0>), tensor(0.0100, grad_fn=<MeanBackward0>), tensor(0.0100, grad_fn=<MeanBackward0>), tensor(0.0099, grad_fn=<MeanBackward0>), tensor(0.0099, grad_fn=<MeanBackward0>), tensor(0.0098, grad_fn=<MeanBackward0>), tensor(0.0098, grad_fn=<MeanBackward0>), tensor(0.0097, grad_fn=<MeanBackward0>), tensor(0.0097, grad_fn=<MeanBackward0>), tensor(0.0096, grad_fn=<MeanBackward0>), tensor(0.0096, grad_fn=<MeanBackward0>), tensor(0.0095, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0066, grad_fn=<MeanBackward0>), tensor(0.0036, grad_fn=<MeanBackward0>), tensor(0.0022, grad_fn=<MeanBackward0>), tensor(0.0015, grad_fn=<MeanBackward0>), tensor(0.0011, grad_fn=<MeanBackward0>), tensor(0.0009, grad_fn=<MeanBackward0>), tensor(0.0008, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0006, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0004, grad_fn=<MeanBackward0>), tensor(0.0004, grad_fn=<MeanBackward0>), tensor(0.0004, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0004, grad_fn=<MeanBackward0>), tensor(0.0004, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0004, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0004, grad_fn=<MeanBackward0>), tensor(0.0002, grad_fn=<MeanBackward0>), tensor(0.0004, grad_fn=<MeanBackward0>), tensor(0.0002, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0004, grad_fn=<MeanBackward0>), tensor(0.0002, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0004, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>), tensor(0.0003, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.009504350740462542, 0.0090667761862278, 0.008835927722975612, 0.008679341850802302, 0.008589559746906162, 0.00851201699115336, 0.008472297643311321, 0.008421564451418817, 0.008398301783017814, 0.00833893088856712, 0.008305404731072486, 0.008280993765220046, 0.008272181893698872, 0.008228979137493297, 0.008200366934761404, 0.008177463454194367, 0.00817967068287544, 0.008165898843435571, 0.008130200917366891, 0.008121223258785903, 0.008097482810262591, 0.008083143923431634, 0.008065374556463212, 0.008050906658172607, 0.00803198556532152, 0.008027693832991645, 0.008028260990977287, 0.008005875290837138, 0.008003849239321426, 0.007985136529896408, 0.007980412850156427, 0.007963799650315195, 0.00795300147146918, 0.007966487039811909, 0.007928832480683923, 0.007949589210329577, 0.007912537158699707, 0.007920458022272214, 0.007943071977933869, 0.007889879413414746, 0.00789105391013436, 0.00789819989586249, 0.007897066156147048, 0.007879833685001359, 0.007880343904253096, 0.00787177840829827, 0.007871062308549882, 0.007865798368584365, 0.007857191131915898, 0.00785536498297006]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvQxaN_fRXLq"
      },
      "source": [
        "# Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkqfFoVkRXxP",
        "outputId": "a87fcc66-6ec3-4d1e-dacf-0d5ee6b8c783"
      },
      "source": [
        "model_factory('Adam')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_6): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            "  (sigmoid56): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 0.80638043, Test Loss: 0.29394118, Test Accuracy: 0.91810000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 0.23412891, Test Loss: 0.19497289, Test Accuracy: 0.94560000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 0.15856718, Test Loss: 0.13926358, Test Accuracy: 0.96120000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 0.12374713, Test Loss: 0.13299601, Test Accuracy: 0.96130000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.09917847, Test Loss: 0.11242942, Test Accuracy: 0.96630000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.08401953, Test Loss: 0.11302615, Test Accuracy: 0.96890000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.07111111, Test Loss: 0.09741350, Test Accuracy: 0.97300000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.05929942, Test Loss: 0.09948156, Test Accuracy: 0.97330000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.05079286, Test Loss: 0.11934904, Test Accuracy: 0.96790000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.04238930, Test Loss: 0.09797786, Test Accuracy: 0.97500000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.03798638, Test Loss: 0.09090877, Test Accuracy: 0.97610000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.03273573, Test Loss: 0.10374434, Test Accuracy: 0.97240000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.03116100, Test Loss: 0.08980980, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.02677528, Test Loss: 0.11289139, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.02485146, Test Loss: 0.09776059, Test Accuracy: 0.97640000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.02298431, Test Loss: 0.08737759, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.02071152, Test Loss: 0.11576361, Test Accuracy: 0.97420000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.02084911, Test Loss: 0.09817041, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.01874645, Test Loss: 0.09762317, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.01728803, Test Loss: 0.11790611, Test Accuracy: 0.97530000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.01511894, Test Loss: 0.10978920, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.01545028, Test Loss: 0.10351850, Test Accuracy: 0.97680000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.01290501, Test Loss: 0.12136318, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.01281452, Test Loss: 0.11589776, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.01247416, Test Loss: 0.10811739, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.01234682, Test Loss: 0.09849530, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.01330772, Test Loss: 0.11946176, Test Accuracy: 0.97600000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.01162298, Test Loss: 0.11184204, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00885963, Test Loss: 0.11792799, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00897763, Test Loss: 0.10947157, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.01061422, Test Loss: 0.12434822, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00986679, Test Loss: 0.11909649, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00838962, Test Loss: 0.10461600, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00823024, Test Loss: 0.16805160, Test Accuracy: 0.96850000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.01226708, Test Loss: 0.11942149, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00679325, Test Loss: 0.12289869, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00830148, Test Loss: 0.11592652, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00785420, Test Loss: 0.13031229, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00615115, Test Loss: 0.12197774, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00624978, Test Loss: 0.13444252, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00888086, Test Loss: 0.12185490, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00512954, Test Loss: 0.12185085, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00619709, Test Loss: 0.11387754, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00832542, Test Loss: 0.12624222, Test Accuracy: 0.97650000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00627576, Test Loss: 0.11772589, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00709740, Test Loss: 0.14567621, Test Accuracy: 0.97400000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00655005, Test Loss: 0.12360022, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00594339, Test Loss: 0.12128618, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00470455, Test Loss: 0.13550022, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00572963, Test Loss: 0.11978092, Test Accuracy: 0.98080000\n",
            "[tensor(0.2399, grad_fn=<MeanBackward0>), tensor(0.2637, grad_fn=<MeanBackward0>), tensor(0.2774, grad_fn=<MeanBackward0>), tensor(0.2888, grad_fn=<MeanBackward0>), tensor(0.2948, grad_fn=<MeanBackward0>), tensor(0.3000, grad_fn=<MeanBackward0>), tensor(0.3046, grad_fn=<MeanBackward0>), tensor(0.3069, grad_fn=<MeanBackward0>), tensor(0.3115, grad_fn=<MeanBackward0>), tensor(0.3146, grad_fn=<MeanBackward0>), tensor(0.3174, grad_fn=<MeanBackward0>), tensor(0.3143, grad_fn=<MeanBackward0>), tensor(0.3197, grad_fn=<MeanBackward0>), tensor(0.3241, grad_fn=<MeanBackward0>), tensor(0.3257, grad_fn=<MeanBackward0>), tensor(0.3276, grad_fn=<MeanBackward0>), tensor(0.3258, grad_fn=<MeanBackward0>), tensor(0.3310, grad_fn=<MeanBackward0>), tensor(0.3326, grad_fn=<MeanBackward0>), tensor(0.3335, grad_fn=<MeanBackward0>), tensor(0.3340, grad_fn=<MeanBackward0>), tensor(0.3322, grad_fn=<MeanBackward0>), tensor(0.3377, grad_fn=<MeanBackward0>), tensor(0.3354, grad_fn=<MeanBackward0>), tensor(0.3372, grad_fn=<MeanBackward0>), tensor(0.3381, grad_fn=<MeanBackward0>), tensor(0.3385, grad_fn=<MeanBackward0>), tensor(0.3387, grad_fn=<MeanBackward0>), tensor(0.3390, grad_fn=<MeanBackward0>), tensor(0.3363, grad_fn=<MeanBackward0>), tensor(0.3409, grad_fn=<MeanBackward0>), tensor(0.3418, grad_fn=<MeanBackward0>), tensor(0.3449, grad_fn=<MeanBackward0>), tensor(0.3441, grad_fn=<MeanBackward0>), tensor(0.3429, grad_fn=<MeanBackward0>), tensor(0.3427, grad_fn=<MeanBackward0>), tensor(0.3427, grad_fn=<MeanBackward0>), tensor(0.3455, grad_fn=<MeanBackward0>), tensor(0.3456, grad_fn=<MeanBackward0>), tensor(0.3500, grad_fn=<MeanBackward0>), tensor(0.3459, grad_fn=<MeanBackward0>), tensor(0.3474, grad_fn=<MeanBackward0>), tensor(0.3478, grad_fn=<MeanBackward0>), tensor(0.3466, grad_fn=<MeanBackward0>), tensor(0.3492, grad_fn=<MeanBackward0>), tensor(0.3497, grad_fn=<MeanBackward0>), tensor(0.3506, grad_fn=<MeanBackward0>), tensor(0.3507, grad_fn=<MeanBackward0>), tensor(0.3518, grad_fn=<MeanBackward0>), tensor(0.3519, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2218, grad_fn=<MeanBackward0>), tensor(0.2369, grad_fn=<MeanBackward0>), tensor(0.2460, grad_fn=<MeanBackward0>), tensor(0.2542, grad_fn=<MeanBackward0>), tensor(0.2636, grad_fn=<MeanBackward0>), tensor(0.2736, grad_fn=<MeanBackward0>), tensor(0.2799, grad_fn=<MeanBackward0>), tensor(0.2919, grad_fn=<MeanBackward0>), tensor(0.3029, grad_fn=<MeanBackward0>), tensor(0.3103, grad_fn=<MeanBackward0>), tensor(0.3103, grad_fn=<MeanBackward0>), tensor(0.3198, grad_fn=<MeanBackward0>), tensor(0.3366, grad_fn=<MeanBackward0>), tensor(0.3347, grad_fn=<MeanBackward0>), tensor(0.3444, grad_fn=<MeanBackward0>), tensor(0.3499, grad_fn=<MeanBackward0>), tensor(0.3593, grad_fn=<MeanBackward0>), tensor(0.3558, grad_fn=<MeanBackward0>), tensor(0.3604, grad_fn=<MeanBackward0>), tensor(0.3696, grad_fn=<MeanBackward0>), tensor(0.3801, grad_fn=<MeanBackward0>), tensor(0.3766, grad_fn=<MeanBackward0>), tensor(0.3809, grad_fn=<MeanBackward0>), tensor(0.3898, grad_fn=<MeanBackward0>), tensor(0.3923, grad_fn=<MeanBackward0>), tensor(0.3952, grad_fn=<MeanBackward0>), tensor(0.4007, grad_fn=<MeanBackward0>), tensor(0.4004, grad_fn=<MeanBackward0>), tensor(0.4066, grad_fn=<MeanBackward0>), tensor(0.4067, grad_fn=<MeanBackward0>), tensor(0.4101, grad_fn=<MeanBackward0>), tensor(0.4150, grad_fn=<MeanBackward0>), tensor(0.4172, grad_fn=<MeanBackward0>), tensor(0.4233, grad_fn=<MeanBackward0>), tensor(0.4312, grad_fn=<MeanBackward0>), tensor(0.4320, grad_fn=<MeanBackward0>), tensor(0.4295, grad_fn=<MeanBackward0>), tensor(0.4292, grad_fn=<MeanBackward0>), tensor(0.4317, grad_fn=<MeanBackward0>), tensor(0.4308, grad_fn=<MeanBackward0>), tensor(0.4401, grad_fn=<MeanBackward0>), tensor(0.4362, grad_fn=<MeanBackward0>), tensor(0.4404, grad_fn=<MeanBackward0>), tensor(0.4445, grad_fn=<MeanBackward0>), tensor(0.4425, grad_fn=<MeanBackward0>), tensor(0.4422, grad_fn=<MeanBackward0>), tensor(0.4430, grad_fn=<MeanBackward0>), tensor(0.4487, grad_fn=<MeanBackward0>), tensor(0.4466, grad_fn=<MeanBackward0>), tensor(0.4528, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2490, grad_fn=<MeanBackward0>), tensor(0.2666, grad_fn=<MeanBackward0>), tensor(0.2883, grad_fn=<MeanBackward0>), tensor(0.3049, grad_fn=<MeanBackward0>), tensor(0.3213, grad_fn=<MeanBackward0>), tensor(0.3285, grad_fn=<MeanBackward0>), tensor(0.3334, grad_fn=<MeanBackward0>), tensor(0.3494, grad_fn=<MeanBackward0>), tensor(0.3555, grad_fn=<MeanBackward0>), tensor(0.3617, grad_fn=<MeanBackward0>), tensor(0.3650, grad_fn=<MeanBackward0>), tensor(0.3733, grad_fn=<MeanBackward0>), tensor(0.3753, grad_fn=<MeanBackward0>), tensor(0.3726, grad_fn=<MeanBackward0>), tensor(0.3791, grad_fn=<MeanBackward0>), tensor(0.3869, grad_fn=<MeanBackward0>), tensor(0.3914, grad_fn=<MeanBackward0>), tensor(0.3932, grad_fn=<MeanBackward0>), tensor(0.4050, grad_fn=<MeanBackward0>), tensor(0.3987, grad_fn=<MeanBackward0>), tensor(0.4048, grad_fn=<MeanBackward0>), tensor(0.3990, grad_fn=<MeanBackward0>), tensor(0.4034, grad_fn=<MeanBackward0>), tensor(0.4057, grad_fn=<MeanBackward0>), tensor(0.4044, grad_fn=<MeanBackward0>), tensor(0.4183, grad_fn=<MeanBackward0>), tensor(0.4195, grad_fn=<MeanBackward0>), tensor(0.4171, grad_fn=<MeanBackward0>), tensor(0.4141, grad_fn=<MeanBackward0>), tensor(0.4180, grad_fn=<MeanBackward0>), tensor(0.4163, grad_fn=<MeanBackward0>), tensor(0.4231, grad_fn=<MeanBackward0>), tensor(0.4229, grad_fn=<MeanBackward0>), tensor(0.4194, grad_fn=<MeanBackward0>), tensor(0.4215, grad_fn=<MeanBackward0>), tensor(0.4170, grad_fn=<MeanBackward0>), tensor(0.4215, grad_fn=<MeanBackward0>), tensor(0.4234, grad_fn=<MeanBackward0>), tensor(0.4230, grad_fn=<MeanBackward0>), tensor(0.4235, grad_fn=<MeanBackward0>), tensor(0.4256, grad_fn=<MeanBackward0>), tensor(0.4293, grad_fn=<MeanBackward0>), tensor(0.4273, grad_fn=<MeanBackward0>), tensor(0.4292, grad_fn=<MeanBackward0>), tensor(0.4339, grad_fn=<MeanBackward0>), tensor(0.4267, grad_fn=<MeanBackward0>), tensor(0.4314, grad_fn=<MeanBackward0>), tensor(0.4330, grad_fn=<MeanBackward0>), tensor(0.4330, grad_fn=<MeanBackward0>), tensor(0.4284, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2639, grad_fn=<MeanBackward0>), tensor(0.2644, grad_fn=<MeanBackward0>), tensor(0.2883, grad_fn=<MeanBackward0>), tensor(0.2973, grad_fn=<MeanBackward0>), tensor(0.3004, grad_fn=<MeanBackward0>), tensor(0.3191, grad_fn=<MeanBackward0>), tensor(0.3322, grad_fn=<MeanBackward0>), tensor(0.3311, grad_fn=<MeanBackward0>), tensor(0.3390, grad_fn=<MeanBackward0>), tensor(0.3427, grad_fn=<MeanBackward0>), tensor(0.3481, grad_fn=<MeanBackward0>), tensor(0.3502, grad_fn=<MeanBackward0>), tensor(0.3546, grad_fn=<MeanBackward0>), tensor(0.3650, grad_fn=<MeanBackward0>), tensor(0.3637, grad_fn=<MeanBackward0>), tensor(0.3615, grad_fn=<MeanBackward0>), tensor(0.3598, grad_fn=<MeanBackward0>), tensor(0.3692, grad_fn=<MeanBackward0>), tensor(0.3689, grad_fn=<MeanBackward0>), tensor(0.3724, grad_fn=<MeanBackward0>), tensor(0.3752, grad_fn=<MeanBackward0>), tensor(0.3767, grad_fn=<MeanBackward0>), tensor(0.3752, grad_fn=<MeanBackward0>), tensor(0.3780, grad_fn=<MeanBackward0>), tensor(0.3815, grad_fn=<MeanBackward0>), tensor(0.3794, grad_fn=<MeanBackward0>), tensor(0.3780, grad_fn=<MeanBackward0>), tensor(0.3802, grad_fn=<MeanBackward0>), tensor(0.3800, grad_fn=<MeanBackward0>), tensor(0.3829, grad_fn=<MeanBackward0>), tensor(0.3856, grad_fn=<MeanBackward0>), tensor(0.3857, grad_fn=<MeanBackward0>), tensor(0.3839, grad_fn=<MeanBackward0>), tensor(0.3828, grad_fn=<MeanBackward0>), tensor(0.3866, grad_fn=<MeanBackward0>), tensor(0.3869, grad_fn=<MeanBackward0>), tensor(0.3892, grad_fn=<MeanBackward0>), tensor(0.3866, grad_fn=<MeanBackward0>), tensor(0.3914, grad_fn=<MeanBackward0>), tensor(0.3891, grad_fn=<MeanBackward0>), tensor(0.3910, grad_fn=<MeanBackward0>), tensor(0.3885, grad_fn=<MeanBackward0>), tensor(0.3908, grad_fn=<MeanBackward0>), tensor(0.3915, grad_fn=<MeanBackward0>), tensor(0.3959, grad_fn=<MeanBackward0>), tensor(0.3941, grad_fn=<MeanBackward0>), tensor(0.3928, grad_fn=<MeanBackward0>), tensor(0.3970, grad_fn=<MeanBackward0>), tensor(0.3952, grad_fn=<MeanBackward0>), tensor(0.3937, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2581, grad_fn=<MeanBackward0>), tensor(0.2785, grad_fn=<MeanBackward0>), tensor(0.2870, grad_fn=<MeanBackward0>), tensor(0.3013, grad_fn=<MeanBackward0>), tensor(0.3123, grad_fn=<MeanBackward0>), tensor(0.3244, grad_fn=<MeanBackward0>), tensor(0.3282, grad_fn=<MeanBackward0>), tensor(0.3376, grad_fn=<MeanBackward0>), tensor(0.3444, grad_fn=<MeanBackward0>), tensor(0.3459, grad_fn=<MeanBackward0>), tensor(0.3554, grad_fn=<MeanBackward0>), tensor(0.3595, grad_fn=<MeanBackward0>), tensor(0.3623, grad_fn=<MeanBackward0>), tensor(0.3589, grad_fn=<MeanBackward0>), tensor(0.3724, grad_fn=<MeanBackward0>), tensor(0.3765, grad_fn=<MeanBackward0>), tensor(0.3820, grad_fn=<MeanBackward0>), tensor(0.3866, grad_fn=<MeanBackward0>), tensor(0.3883, grad_fn=<MeanBackward0>), tensor(0.3844, grad_fn=<MeanBackward0>), tensor(0.3872, grad_fn=<MeanBackward0>), tensor(0.3927, grad_fn=<MeanBackward0>), tensor(0.3961, grad_fn=<MeanBackward0>), tensor(0.3947, grad_fn=<MeanBackward0>), tensor(0.4015, grad_fn=<MeanBackward0>), tensor(0.4093, grad_fn=<MeanBackward0>), tensor(0.4189, grad_fn=<MeanBackward0>), tensor(0.4141, grad_fn=<MeanBackward0>), tensor(0.4119, grad_fn=<MeanBackward0>), tensor(0.4132, grad_fn=<MeanBackward0>), tensor(0.4193, grad_fn=<MeanBackward0>), tensor(0.4225, grad_fn=<MeanBackward0>), tensor(0.4293, grad_fn=<MeanBackward0>), tensor(0.4265, grad_fn=<MeanBackward0>), tensor(0.4415, grad_fn=<MeanBackward0>), tensor(0.4436, grad_fn=<MeanBackward0>), tensor(0.4436, grad_fn=<MeanBackward0>), tensor(0.4464, grad_fn=<MeanBackward0>), tensor(0.4419, grad_fn=<MeanBackward0>), tensor(0.4401, grad_fn=<MeanBackward0>), tensor(0.4538, grad_fn=<MeanBackward0>), tensor(0.4525, grad_fn=<MeanBackward0>), tensor(0.4528, grad_fn=<MeanBackward0>), tensor(0.4681, grad_fn=<MeanBackward0>), tensor(0.4620, grad_fn=<MeanBackward0>), tensor(0.4776, grad_fn=<MeanBackward0>), tensor(0.4743, grad_fn=<MeanBackward0>), tensor(0.4760, grad_fn=<MeanBackward0>), tensor(0.4697, grad_fn=<MeanBackward0>), tensor(0.4766, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.24654164612293245, 0.2620344221591949, 0.277384015917778, 0.28927390575408934, 0.2984882056713104, 0.3091424167156219, 0.31566170454025266, 0.32335695028305056, 0.33066325783729555, 0.3350262403488159, 0.33923301100730896, 0.34343008399009706, 0.3496864020824432, 0.35108970999717715, 0.3570617914199829, 0.36048787236213686, 0.3636286497116089, 0.3671848297119141, 0.3710262179374695, 0.37170958518981934, 0.37628295421600344, 0.37542992234230044, 0.37864416241645815, 0.38075278997421264, 0.38336707949638366, 0.38805809020996096, 0.3911172032356262, 0.3900859594345093, 0.39032106995582583, 0.391413152217865, 0.39445591568946836, 0.3976324796676636, 0.39961525797843933, 0.39922271966934203, 0.404733145236969, 0.40445799231529234, 0.4053346335887909, 0.40623158812522886, 0.4067043483257294, 0.40670583248138426, 0.4112856686115265, 0.4107552945613861, 0.41181269884109495, 0.41596770882606504, 0.4166922390460968, 0.41806219816207885, 0.4184040307998657, 0.42107064127922056, 0.41927672624588014, 0.4206719398498535]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}