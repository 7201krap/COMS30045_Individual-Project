{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "selectivity_4_optim_5_nums.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/selectivity_4_optim_5_nums.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MU1dnFxiZMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37311223-b1ad-4d71-ed25-67bf99c7e696"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "import os\n",
        "import subprocess as sp\n",
        "from torchvision.datasets.mnist import MNIST, read_image_file, read_label_file\n",
        "from torchvision.datasets.utils import extract_archive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZpOCWFxYgtO"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnFXwyhGSI8S"
      },
      "source": [
        "def patched_download(self):\n",
        "    \"\"\"wget patched download method.\n",
        "    \"\"\"\n",
        "    if self._check_exists():\n",
        "        return\n",
        "\n",
        "    os.makedirs(self.raw_folder, exist_ok=True)\n",
        "    os.makedirs(self.processed_folder, exist_ok=True)\n",
        "\n",
        "    # download files\n",
        "    for url, md5 in self.resources:\n",
        "        filename = url.rpartition('/')[2]\n",
        "        download_root = os.path.expanduser(self.raw_folder)\n",
        "        extract_root = None\n",
        "        remove_finished = False\n",
        "\n",
        "        if extract_root is None:\n",
        "            extract_root = download_root\n",
        "        if not filename:\n",
        "            filename = os.path.basename(url)\n",
        "        \n",
        "        # Use wget to download archives\n",
        "        sp.run([\"wget\", url, \"-P\", download_root])\n",
        "\n",
        "        archive = os.path.join(download_root, filename)\n",
        "        print(\"Extracting {} to {}\".format(archive, extract_root))\n",
        "        extract_archive(archive, extract_root, remove_finished)\n",
        "\n",
        "    # process and save as torch files\n",
        "    print('Processing...')\n",
        "\n",
        "    training_set = (\n",
        "        read_image_file(os.path.join(self.raw_folder, 'train-images-idx3-ubyte')),\n",
        "        read_label_file(os.path.join(self.raw_folder, 'train-labels-idx1-ubyte'))\n",
        "    )\n",
        "    test_set = (\n",
        "        read_image_file(os.path.join(self.raw_folder, 't10k-images-idx3-ubyte')),\n",
        "        read_label_file(os.path.join(self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
        "    )\n",
        "    with open(os.path.join(self.processed_folder, self.training_file), 'wb') as f:\n",
        "        torch.save(training_set, f)\n",
        "    with open(os.path.join(self.processed_folder, self.test_file), 'wb') as f:\n",
        "        torch.save(test_set, f)\n",
        "\n",
        "    print('Done!')\n",
        "\n",
        "\n",
        "MNIST.download = patched_download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iJa_LOiivEN"
      },
      "source": [
        "mnist_trainset = MNIST(root='./data', train=True, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "mnist_testset  = MNIST(root='./data', \n",
        "                                train=False, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-pSE5X3i3l5"
      },
      "source": [
        "# class_inds 이거는 그냥 위에있는거를 list 로 만들어준 형태임 \n",
        "class_inds = [torch.where(mnist_trainset.targets == class_idx)[0]\n",
        "              for class_idx in mnist_trainset.class_to_idx.values()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2bTkOMQj_3t"
      },
      "source": [
        "train_dataloaders = [\n",
        "                     DataLoader(dataset=Subset(mnist_trainset, inds),\n",
        "                                batch_size=10,\n",
        "                                shuffle=True,\n",
        "                                drop_last=False\n",
        "                     )\n",
        "                     for inds in class_inds\n",
        "]\n",
        "\n",
        "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=False)\n",
        "\n",
        "print(\"Training dataset size: \", len(mnist_trainset))\n",
        "print(\"Testing dataset size: \",  len(mnist_testset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSkAos_Jq7jP"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# Define the model \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        # modify this section for later use \n",
        "        self.linear_1 = torch.nn.Linear(784, 256)\n",
        "        self.linear_2 = torch.nn.Linear(256, 10)\n",
        "        self.sigmoid12  = torch.nn.Sigmoid()\n",
        "\n",
        "        self.layer_activations = dict()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # modify this section for later use \n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.sigmoid12(x)\n",
        "        pred = self.linear_2(x)\n",
        "        return pred\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk6n-KajYdSw"
      },
      "source": [
        "def get_activation(model, layer_name):    \n",
        "    def hook(module, input, output):\n",
        "        model.layer_activations[layer_name] = output\n",
        "    return hook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf8JfDluYtay"
      },
      "source": [
        "def selectivity(hidden_layer_each_neuron):\n",
        "    __selectivity__ = list()\n",
        "    # I will now try to find the average of each class for each neuron.\n",
        "    # check out the next cell \n",
        "    avg_activations = [dict() for x in range(256)]\n",
        "    for i, neuron in enumerate(hidden_layer_each_neuron):\n",
        "        for k, v in neuron.items():\n",
        "            # v is the list of activations for hidden layer's neuron k \n",
        "            avg_activations[i][k] = sum(v) / float(len(v))\n",
        "\n",
        "    # generate 256 lists to get only values in avg_activations\n",
        "    only_activation_vals = [list() for x in range(256)]\n",
        "\n",
        "    # get only values from avg_activations\n",
        "    for i, avg_activation in enumerate(avg_activations):\n",
        "        for value in avg_activation.values():\n",
        "            only_activation_vals[i].append(value)\n",
        "\n",
        "\n",
        "    for activation_val in only_activation_vals:\n",
        "        # find u_max \n",
        "        u_max = np.max(activation_val)\n",
        "\n",
        "        # find u_minus_max \n",
        "        u_minus_max = (np.sum(activation_val) - u_max) / 9\n",
        "\n",
        "        # find selectivity \n",
        "        selectivity = (u_max - u_minus_max) / (u_max + u_minus_max)\n",
        "\n",
        "        # append selectivity value to selectivity\n",
        "        __selectivity__.append(selectivity)\n",
        "\n",
        "    avg_selectivity = np.average(__selectivity__)\n",
        "    std_selectivity = np.std(__selectivity__)\n",
        "                                 \n",
        "    return avg_selectivity, std_selectivity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2uPxAlnYvY7"
      },
      "source": [
        "def model_factory(optimizer_name):\n",
        "    '''\n",
        "    optimizer_name : choose one of Adagrad, Adadelta, SGD, and Adam \n",
        "\n",
        "    '''\n",
        "    my_model = Model()\n",
        "    print(\"my_model:\", my_model)\n",
        "    my_model.to(device)\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    my_model.sigmoid12.register_forward_hook(get_activation(my_model, 's12'))\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        my_optimizer = torch.optim.Adadelta(my_model.parameters(), lr=1.0)\n",
        "\n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        my_optimizer = torch.optim.Adagrad(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        my_optimizer = torch.optim.SGD(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        my_optimizer = torch.optim.Adam(my_model.parameters(), lr=0.001)\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")\n",
        "    \n",
        "    print(\"my_optimizer:\", my_optimizer)\n",
        "    test_acc, selectivity_list_avg, selectivity_list_std = selectivity_trainer(optimizer=my_optimizer, model=my_model)\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver = open(f\"selectivity_4_optim_5_nums_{optimizer_name}.txt\", \"w\")\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver.write(str(test_acc)+'\\n'+str(selectivity_list_avg)+'\\n'+str(selectivity_list_std)+'\\n\\n')\n",
        "    file_saver.close()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        !cp selectivity_4_optim_5_nums_Adadelta.txt /content/drive/MyDrive\n",
        "    \n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        !cp selectivity_4_optim_5_nums_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        !cp selectivity_4_optim_5_nums_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        !cp selectivity_4_optim_5_nums_Adam.txt /content/drive/MyDrive\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LkuALwAYwgK"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "def avg_std_calculator(_hidden_layer_each_neuron_12):\n",
        "\n",
        "    avg_selectivity12, std_selectivity12 = selectivity(_hidden_layer_each_neuron_12)\n",
        "\n",
        "    final_selectivity_avg = (avg_selectivity12) / 1\n",
        "    final_selecvitity_std = (std_selectivity12) / 1\n",
        "\n",
        "    return final_selectivity_avg, final_selecvitity_std\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnD7OdVYlo7H"
      },
      "source": [
        "def selectivity_trainer(optimizer, model):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    no_epochs = 30\n",
        "    test_acc   = list()\n",
        "\n",
        "    selectivity_avg_list = list()\n",
        "    selectivity_std_list = list()\n",
        "\n",
        "    for epoch in range(no_epochs):\n",
        "\n",
        "        print(f\"epoch {epoch} started\")\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_each_neuron_12 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_12 = np.array(hidden_layer_each_neuron_12)\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        # TRAINING \n",
        "        model.train()\n",
        "        iterators = list(map(iter, train_dataloaders))   \n",
        "        while iterators:\n",
        "            iterator = np.random.choice(iterators, 5, replace=False)\n",
        "            try:\n",
        "                image0, label0 = next(iterator[0]) \n",
        "                image1, label1 = next(iterator[1]) \n",
        "                image2, label2 = next(iterator[2]) \n",
        "                image3, label3 = next(iterator[3]) \n",
        "                image4, label4 = next(iterator[4]) \n",
        "\n",
        "                # concat batch_size 10 * 5 => 50\n",
        "                images = torch.cat((image0, image1, image2, image3, image4), 0)\n",
        "                labels = torch.cat((label0, label1, label2, label3, label4), 0)\n",
        "\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                pred = model(images)\n",
        "\n",
        "                loss = criterion(pred, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "            except StopIteration:\n",
        "                iterators.remove(iterator[0])\n",
        "                iterators.remove(iterator[1])\n",
        "                iterators.remove(iterator[2])\n",
        "                iterators.remove(iterator[3])\n",
        "                iterators.remove(iterator[4])\n",
        "\n",
        "\n",
        "        # TESTING\n",
        "        model.eval()\n",
        "        total = 0\n",
        "        for itr, (images, labels) in enumerate(test_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "\n",
        "            # we now need softmax because we are testing.\n",
        "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "            for i, p in enumerate(pred):\n",
        "                if labels[i] == torch.max(p.data, 0)[1]:\n",
        "                    total = total + 1\n",
        "\n",
        "            # ************* modify this section for later use *************\n",
        "            for activation, label in zip(model.layer_activations['s12'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_12[i][label].append(activation[i])\n",
        "        \n",
        "        selectivity_avg, selecvitity_std = avg_std_calculator(hidden_layer_each_neuron_12)\n",
        "        # ************* modify this section for later use *************\n",
        "            \n",
        "        selectivity_avg_list.append(selectivity_avg)\n",
        "        selectivity_std_list.append(selecvitity_std)\n",
        "\n",
        "        accuracy = total / len(mnist_testset)\n",
        "\n",
        "        # append accuracy here\n",
        "        test_acc.append(accuracy)\n",
        "\n",
        "        print('\\nEpoch: {}/{}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, accuracy))\n",
        "    \n",
        "    print(selectivity_avg_list)\n",
        "    print(selectivity_std_list)\n",
        "\n",
        "    return test_acc, selectivity_avg_list, selectivity_std_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEdfCCHUam-f"
      },
      "source": [
        "# Adadelta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7jL46x7szw0"
      },
      "source": [
        "model_factory('Adadelta')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smiDmZZ1aoS3"
      },
      "source": [
        "# Adagrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGakLBYDaYTD"
      },
      "source": [
        "model_factory('Adagrad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPYS2f8Dapah"
      },
      "source": [
        "# SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdlUNK4YaYVT"
      },
      "source": [
        "model_factory('SGD')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_u1nZEIaqOL"
      },
      "source": [
        "# Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8ga1E6jaYXJ"
      },
      "source": [
        "model_factory('Adam')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}