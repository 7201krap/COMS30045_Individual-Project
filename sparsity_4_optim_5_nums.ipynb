{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparsity_4_optim_5_nums.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/sparsity_4_optim_5_nums.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MU1dnFxiZMb"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "import os\n",
        "import subprocess as sp\n",
        "from torchvision.datasets.mnist import MNIST, read_image_file, read_label_file\n",
        "from torchvision.datasets.utils import extract_archive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOk6y_H5IWHx"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL6RNuqNSJpl"
      },
      "source": [
        "def patched_download(self):\n",
        "    \"\"\"wget patched download method.\n",
        "    \"\"\"\n",
        "    if self._check_exists():\n",
        "        return\n",
        "\n",
        "    os.makedirs(self.raw_folder, exist_ok=True)\n",
        "    os.makedirs(self.processed_folder, exist_ok=True)\n",
        "\n",
        "    # download files\n",
        "    for url, md5 in self.resources:\n",
        "        filename = url.rpartition('/')[2]\n",
        "        download_root = os.path.expanduser(self.raw_folder)\n",
        "        extract_root = None\n",
        "        remove_finished = False\n",
        "\n",
        "        if extract_root is None:\n",
        "            extract_root = download_root\n",
        "        if not filename:\n",
        "            filename = os.path.basename(url)\n",
        "        \n",
        "        # Use wget to download archives\n",
        "        sp.run([\"wget\", url, \"-P\", download_root])\n",
        "\n",
        "        archive = os.path.join(download_root, filename)\n",
        "        print(\"Extracting {} to {}\".format(archive, extract_root))\n",
        "        extract_archive(archive, extract_root, remove_finished)\n",
        "\n",
        "    # process and save as torch files\n",
        "    print('Processing...')\n",
        "\n",
        "    training_set = (\n",
        "        read_image_file(os.path.join(self.raw_folder, 'train-images-idx3-ubyte')),\n",
        "        read_label_file(os.path.join(self.raw_folder, 'train-labels-idx1-ubyte'))\n",
        "    )\n",
        "    test_set = (\n",
        "        read_image_file(os.path.join(self.raw_folder, 't10k-images-idx3-ubyte')),\n",
        "        read_label_file(os.path.join(self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
        "    )\n",
        "    with open(os.path.join(self.processed_folder, self.training_file), 'wb') as f:\n",
        "        torch.save(training_set, f)\n",
        "    with open(os.path.join(self.processed_folder, self.test_file), 'wb') as f:\n",
        "        torch.save(test_set, f)\n",
        "\n",
        "    print('Done!')\n",
        "\n",
        "\n",
        "MNIST.download = patched_download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iJa_LOiivEN"
      },
      "source": [
        "mnist_trainset = MNIST(root='./data', train=True, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "mnist_testset  = MNIST(root='./data', \n",
        "                                train=False, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-pSE5X3i3l5"
      },
      "source": [
        "# class_inds 이거는 그냥 위에있는거를 list 로 만들어준 형태임 \n",
        "class_inds = [torch.where(mnist_trainset.targets == class_idx)[0]\n",
        "              for class_idx in mnist_trainset.class_to_idx.values()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2bTkOMQj_3t"
      },
      "source": [
        "train_dataloaders = [\n",
        "                     DataLoader(dataset=Subset(mnist_trainset, inds),\n",
        "                                batch_size=10,\n",
        "                                shuffle=True,\n",
        "                                drop_last=False\n",
        "                     )\n",
        "                     for inds in class_inds\n",
        "]\n",
        "\n",
        "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=False)\n",
        "\n",
        "print(\"Training dataset size: \", len(mnist_trainset))\n",
        "print(\"Testing dataset size: \",  len(mnist_testset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSkAos_Jq7jP"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# Define the model \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # modify this section for later use \n",
        "        self.linear_1 = torch.nn.Linear(784, 256)\n",
        "        self.linear_2 = torch.nn.Linear(256, 10)\n",
        "        self.sigmoid12  = torch.nn.Sigmoid()\n",
        "\n",
        "        self.layer_activations = dict()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # modify this section for later use \n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.sigmoid12(x)\n",
        "        pred = self.linear_2(x)\n",
        "        return pred\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLpFwF1dIueK"
      },
      "source": [
        "def get_activation(model, layer_name):    \n",
        "    def hook(module, input, output):\n",
        "        model.layer_activations[layer_name] = output\n",
        "    return hook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZAZN91tIvF0"
      },
      "source": [
        "def sparsity_calculator(final_spareness):\n",
        "    sparseness_list = list()\n",
        "    for single_epoch_spareness in final_spareness:\n",
        "\n",
        "        hidden_layer_activation_list = single_epoch_spareness\n",
        "        hidden_layer_activation_list = torch.stack(hidden_layer_activation_list)\n",
        "        layer_activations_list = torch.reshape(hidden_layer_activation_list, (10000, 256))\n",
        "\n",
        "        layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
        "        num_neurons = layer_activations_list.shape[1]\n",
        "        population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
        "        mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
        "\n",
        "        sparseness_list.append(mean_sparseness_per_epoch)\n",
        "\n",
        "    return sparseness_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sT7PSGTIwjO"
      },
      "source": [
        "def model_factory(optimizer_name):\n",
        "    '''\n",
        "    optimizer_name : choose one of Adagrad, Adadelta, SGD, and Adam \n",
        "\n",
        "    '''\n",
        "    my_model = Model()\n",
        "    print(\"my_model:\", my_model)\n",
        "    my_model.to(device)\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    # modify this section for later use \n",
        "    my_model.sigmoid12.register_forward_hook(get_activation(my_model, 's12'))\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        my_optimizer = torch.optim.Adadelta(my_model.parameters(), lr=1.0)\n",
        "\n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        my_optimizer = torch.optim.Adagrad(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        my_optimizer = torch.optim.SGD(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        my_optimizer = torch.optim.Adam(my_model.parameters(), lr=0.001)\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")\n",
        "    \n",
        "    print(\"my_optimizer:\", my_optimizer)\n",
        "    test_acc, sparseness_list = sparsity_trainer(optimizer=my_optimizer, model=my_model)\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver = open(f\"sparsity_4_optim_5_nums_{optimizer_name}.txt\", \"w\")\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver.write(str(test_acc)+'\\n'+str(sparseness_list)+'\\n\\n')\n",
        "    file_saver.close()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        !cp sparsity_4_optim_5_nums_Adadelta.txt /content/drive/MyDrive\n",
        "    \n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        !cp sparsity_4_optim_5_nums_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        !cp sparsity_4_optim_5_nums_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        !cp sparsity_4_optim_5_nums_Adam.txt /content/drive/MyDrive\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnD7OdVYlo7H"
      },
      "source": [
        "def sparsity_trainer(optimizer, model):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    no_epochs = 30\n",
        "    test_acc   = list()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    final_spareness_12 = list()\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    for epoch in range(no_epochs):\n",
        "\n",
        "        print(f\"epoch {epoch} started\")\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_activation_list_12 = list()\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        # TRAINING \n",
        "        model.train()\n",
        "        iterators = list(map(iter, train_dataloaders))   \n",
        "        while iterators:    # This part is same as for loop \n",
        "            iterator = np.random.choice(iterators, 5, replace=False)\n",
        "            try:\n",
        "                image0, label0 = next(iterator[0]) \n",
        "                image1, label1 = next(iterator[1]) \n",
        "                image2, label2 = next(iterator[2]) \n",
        "                image3, label3 = next(iterator[3]) \n",
        "                image4, label4 = next(iterator[4]) \n",
        "\n",
        "                # concat batch_size 10 * 5 => 50\n",
        "                image = torch.cat((image0, image1, image2, image3, image4), 0)\n",
        "                label = torch.cat((label0, label1, label2, label3, label4), 0)\n",
        "\n",
        "                image, label = image.to(device), label.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                pred = model(image)\n",
        "\n",
        "                loss = criterion(pred, label)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "            except StopIteration:\n",
        "                iterators.remove(iterator[0])\n",
        "                iterators.remove(iterator[1])\n",
        "                iterators.remove(iterator[2])\n",
        "                iterators.remove(iterator[3])\n",
        "                iterators.remove(iterator[4])\n",
        "\n",
        "        # TESTING\n",
        "        model.eval()\n",
        "        total = 0\n",
        "        for itr, (image, label) in enumerate(test_dataloader):\n",
        "            image, label = image.to(device), label.to(device)\n",
        "\n",
        "            pred = model(image)\n",
        "\n",
        "            loss = criterion(pred, label)\n",
        "\n",
        "            # we now need softmax because we are testing.\n",
        "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "            for i, p in enumerate(pred):\n",
        "                if label[i] == torch.max(p.data, 0)[1]:\n",
        "                    total = total + 1\n",
        "\n",
        "            # ***************** sparsity calculation ***************** #\n",
        "            hidden_layer_activation_list_12.append(model.layer_activations['s12'])\n",
        "\n",
        "        # this conains activations for all epochs \n",
        "        final_spareness_12.append(hidden_layer_activation_list_12)\n",
        "        # ***************** sparsity calculation ***************** # \n",
        "\n",
        "        accuracy = total / len(mnist_testset)\n",
        "        # append accuracy here\n",
        "        test_acc.append(accuracy)\n",
        "\n",
        "        print('\\nEpoch: {}/{}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, accuracy))\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "    sparsity_list12 = sparsity_calculator(final_spareness_12)\n",
        "\n",
        "    average_sparsity = list()\n",
        "    for i in range(no_epochs):\n",
        "        average_sparsity.append( (sparsity_list12[i].item()) / 1 )\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "\n",
        "    print(\"average_sparsity:\", average_sparsity)\n",
        "\n",
        "    return test_acc, average_sparsity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFYvV3lWUdrH"
      },
      "source": [
        "# Adadelta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCJH9d7XUbNl"
      },
      "source": [
        "model_factory('Adadelta')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0epauTHjUfJ4"
      },
      "source": [
        "# Adagrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO-2R8L9UbsC"
      },
      "source": [
        "model_factory('Adagrad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xsh5pRjUhz_"
      },
      "source": [
        "# SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em71M6KcUbup"
      },
      "source": [
        "model_factory('SGD')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1YSFjUZUjtH"
      },
      "source": [
        "# Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdoLly2DUb0I"
      },
      "source": [
        "model_factory('Adam')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}