{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparsity_4_HL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/sparsity_4_HL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7STrWa0P3z_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d27a29-eb89-47e2-df81-b38a3b23ce5f"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "import os\n",
        "import subprocess as sp\n",
        "from torchvision.datasets.mnist import MNIST, read_image_file, read_label_file\n",
        "from torchvision.datasets.utils import extract_archive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfVgh6R_EDtL"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY0MSVMXmqP7"
      },
      "source": [
        "def patched_download(self):\n",
        "    \"\"\"wget patched download method.\n",
        "    \"\"\"\n",
        "    if self._check_exists():\n",
        "        return\n",
        "\n",
        "    os.makedirs(self.raw_folder, exist_ok=True)\n",
        "    os.makedirs(self.processed_folder, exist_ok=True)\n",
        "\n",
        "    # download files\n",
        "    for url, md5 in self.resources:\n",
        "        filename = url.rpartition('/')[2]\n",
        "        download_root = os.path.expanduser(self.raw_folder)\n",
        "        extract_root = None\n",
        "        remove_finished = False\n",
        "\n",
        "        if extract_root is None:\n",
        "            extract_root = download_root\n",
        "        if not filename:\n",
        "            filename = os.path.basename(url)\n",
        "        \n",
        "        # Use wget to download archives\n",
        "        sp.run([\"wget\", url, \"-P\", download_root])\n",
        "\n",
        "        archive = os.path.join(download_root, filename)\n",
        "        print(\"Extracting {} to {}\".format(archive, extract_root))\n",
        "        extract_archive(archive, extract_root, remove_finished)\n",
        "\n",
        "    # process and save as torch files\n",
        "    print('Processing...')\n",
        "\n",
        "    training_set = (\n",
        "        read_image_file(os.path.join(self.raw_folder, 'train-images-idx3-ubyte')),\n",
        "        read_label_file(os.path.join(self.raw_folder, 'train-labels-idx1-ubyte'))\n",
        "    )\n",
        "    test_set = (\n",
        "        read_image_file(os.path.join(self.raw_folder, 't10k-images-idx3-ubyte')),\n",
        "        read_label_file(os.path.join(self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
        "    )\n",
        "    with open(os.path.join(self.processed_folder, self.training_file), 'wb') as f:\n",
        "        torch.save(training_set, f)\n",
        "    with open(os.path.join(self.processed_folder, self.test_file), 'wb') as f:\n",
        "        torch.save(test_set, f)\n",
        "\n",
        "    print('Done!')\n",
        "\n",
        "\n",
        "MNIST.download = patched_download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTW5TOUnP5XY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5df5c5-1d29-434e-b5a2-37002895b0c7"
      },
      "source": [
        "mnist_trainset = MNIST(root='./data', train=True, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "mnist_testset  = MNIST(root='./data', \n",
        "                                train=False, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(mnist_trainset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=True)\n",
        "\n",
        "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=False)\n",
        "\n",
        "print(\"Training dataset size: \", len(mnist_trainset))\n",
        "print(\"Testing dataset size: \",  len(mnist_testset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset size:  60000\n",
            "Testing dataset size:  10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXTkEUJ5P6kU"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# Define the model \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # modify this section for later use \n",
        "        self.linear_1 = torch.nn.Linear(784, 256)\n",
        "        self.linear_2 = torch.nn.Linear(256, 256)\n",
        "        self.linear_3 = torch.nn.Linear(256, 256)\n",
        "        self.linear_4 = torch.nn.Linear(256, 256)\n",
        "        self.linear_5 = torch.nn.Linear(256, 10)\n",
        "        self.sigmoid12  = torch.nn.Sigmoid()\n",
        "        self.sigmoid23  = torch.nn.Sigmoid()\n",
        "        self.sigmoid34  = torch.nn.Sigmoid()\n",
        "        self.sigmoid45  = torch.nn.Sigmoid()\n",
        "\n",
        "        self.layer_activations = dict()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # modify this section for later use \n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.sigmoid12(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.sigmoid23(x)\n",
        "        x = self.linear_3(x)\n",
        "        x = self.sigmoid34(x)\n",
        "        x = self.linear_4(x)\n",
        "        x = self.sigmoid45(x)\n",
        "        pred = self.linear_5(x)\n",
        "        return pred\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfgvKH6eP9Ou"
      },
      "source": [
        "def get_activation(model, layer_name):    \n",
        "    def hook(module, input, output):\n",
        "        model.layer_activations[layer_name] = output\n",
        "    return hook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6PPVgpV57A3"
      },
      "source": [
        "def sparsity_calculator(final_spareness):\n",
        "    sparseness_list = list()\n",
        "    for single_epoch_spareness in final_spareness:\n",
        "\n",
        "        hidden_layer_activation_list = single_epoch_spareness\n",
        "        hidden_layer_activation_list = torch.stack(hidden_layer_activation_list)\n",
        "        layer_activations_list = torch.reshape(hidden_layer_activation_list, (10000, 256))\n",
        "\n",
        "        layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
        "        num_neurons = layer_activations_list.shape[1]\n",
        "        population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
        "        mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
        "\n",
        "        sparseness_list.append(mean_sparseness_per_epoch)\n",
        "\n",
        "    return sparseness_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTx6m49gEw6w"
      },
      "source": [
        "def model_factory(optimizer_name):\n",
        "    '''\n",
        "    optimizer_name : choose one of Adagrad, Adadelta, SGD, and Adam \n",
        "\n",
        "    '''\n",
        "    my_model = Model()\n",
        "    print(\"my_model:\", my_model)\n",
        "    my_model.to(device)\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    # modify this section for later use \n",
        "    my_model.sigmoid12.register_forward_hook(get_activation(my_model, 's12'))\n",
        "    my_model.sigmoid23.register_forward_hook(get_activation(my_model, 's23'))\n",
        "    my_model.sigmoid34.register_forward_hook(get_activation(my_model, 's34'))\n",
        "    my_model.sigmoid45.register_forward_hook(get_activation(my_model, 's45'))\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        my_optimizer = torch.optim.Adadelta(my_model.parameters(), lr=1.0)\n",
        "\n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        my_optimizer = torch.optim.Adagrad(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        my_optimizer = torch.optim.SGD(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        my_optimizer = torch.optim.Adam(my_model.parameters(), lr=0.001)\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")\n",
        "    \n",
        "    print(\"my_optimizer:\", my_optimizer)\n",
        "    test_acc, sparseness_list, spar12, spar23, spar34, spar45 = sparsity_trainer(optimizer=my_optimizer, model=my_model)\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver = open(f\"4HL_sparsity_new_{optimizer_name}.txt\", \"w\")\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver.write(str(test_acc)+'\\n'+str(sparseness_list)+'\\n'+str(spar12)+'\\n'+str(spar23)+'\\n'+str(spar34)+'\\n'+str(spar45)+'\\n\\n')\n",
        "    file_saver.close()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        !cp 4HL_sparsity_new_Adadelta.txt /content/drive/MyDrive\n",
        "    \n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        !cp 4HL_sparsity_new_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        !cp 4HL_sparsity_new_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        !cp 4HL_sparsity_new_Adam.txt /content/drive/MyDrive\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXOpwTXEQFKY"
      },
      "source": [
        "def sparsity_trainer(optimizer, model):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    no_epochs = 50\n",
        "\n",
        "    train_loss = list()\n",
        "    test_loss  = list()\n",
        "    test_acc   = list()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    final_spareness_12 = list()\n",
        "    final_spareness_23 = list()\n",
        "    final_spareness_34 = list()\n",
        "    final_spareness_45 = list()\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    best_test_loss = 1\n",
        "    for epoch in range(no_epochs):\n",
        "        total_train_loss = 0\n",
        "        total_test_loss = 0\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_activation_list_12 = list()\n",
        "        hidden_layer_activation_list_23 = list()\n",
        "        hidden_layer_activation_list_34 = list()\n",
        "        hidden_layer_activation_list_45 = list()\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        # training\n",
        "        # set up training mode \n",
        "        model.train()\n",
        "        for itr, (image, label) in enumerate(train_dataloader):\n",
        "            image, label = image.to(device), label.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(image)\n",
        "\n",
        "            loss = criterion(pred, label)\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_train_loss = total_train_loss / (itr + 1)\n",
        "        train_loss.append(total_train_loss)\n",
        "\n",
        "        # testing \n",
        "        # change to evaluation mode \n",
        "        model.eval()\n",
        "        total = 0\n",
        "        for itr, (image, label) in enumerate(test_dataloader):\n",
        "            image, label = image.to(device), label.to(device)\n",
        "\n",
        "            pred = model(image)\n",
        "\n",
        "            loss = criterion(pred, label)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # we now need softmax because we are testing.\n",
        "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "            for i, p in enumerate(pred):\n",
        "                if label[i] == torch.max(p.data, 0)[1]:\n",
        "                    total = total + 1\n",
        "\n",
        "            # ***************** sparsity calculation ***************** #\n",
        "            hidden_layer_activation_list_12.append(model.layer_activations['s12'])\n",
        "            hidden_layer_activation_list_23.append(model.layer_activations['s23'])\n",
        "            hidden_layer_activation_list_34.append(model.layer_activations['s34'])\n",
        "            hidden_layer_activation_list_45.append(model.layer_activations['s45'])\n",
        "\n",
        "        # this conains activations for all epochs \n",
        "        final_spareness_12.append(hidden_layer_activation_list_12)\n",
        "        final_spareness_23.append(hidden_layer_activation_list_23)\n",
        "        final_spareness_34.append(hidden_layer_activation_list_34)\n",
        "        final_spareness_45.append(hidden_layer_activation_list_45)\n",
        "        # ***************** sparsity calculation ***************** #\n",
        "\n",
        "        # caculate accuracy \n",
        "        accuracy = total / len(mnist_testset)\n",
        "\n",
        "        # append accuracy here\n",
        "        test_acc.append(accuracy)\n",
        "\n",
        "        # append test loss here \n",
        "        total_test_loss = total_test_loss / (itr + 1)\n",
        "        test_loss.append(total_test_loss)\n",
        "\n",
        "        print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Test Loss: {:.8f}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, total_train_loss, total_test_loss, accuracy))\n",
        "\n",
        "        if total_test_loss < best_test_loss:\n",
        "            best_test_loss = total_test_loss\n",
        "            print(\"Saving the model state dictionary for Epoch: {} with Test loss: {:.8f}\".format(epoch + 1, total_test_loss))\n",
        "            torch.save(model.state_dict(), \"model.dth\")\n",
        "\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "    sparsity_list12 = sparsity_calculator(final_spareness_12)\n",
        "    sparsity_list23 = sparsity_calculator(final_spareness_23)\n",
        "    sparsity_list34 = sparsity_calculator(final_spareness_34)\n",
        "    sparsity_list45 = sparsity_calculator(final_spareness_45)\n",
        "\n",
        "    print(sparsity_list12)\n",
        "    print(sparsity_list23)\n",
        "    print(sparsity_list34)\n",
        "    print(sparsity_list45)\n",
        "\n",
        "    average_sparsity = list()\n",
        "    sparsity12 = list()\n",
        "    sparsity23 = list()\n",
        "    sparsity34 = list()\n",
        "    sparsity45 = list()\n",
        "\n",
        "    for i in range(no_epochs):\n",
        "        sparsity12.append(sparsity_list12[i].item())\n",
        "        sparsity23.append(sparsity_list23[i].item())\n",
        "        sparsity34.append(sparsity_list34[i].item())\n",
        "        sparsity45.append(sparsity_list45[i].item())\n",
        "        average_sparsity.append( (sparsity_list12[i].item() + sparsity_list23[i].item() + sparsity_list34[i].item() + sparsity_list45[i].item()) / 4 )\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "\n",
        "    print(\"average_sparsity:\", average_sparsity)\n",
        "\n",
        "    return test_acc, average_sparsity, sparsity12, sparsity23, sparsity34, sparsity45"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cpDpLWKCZUR"
      },
      "source": [
        "# Adadelta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy2ZseTBB1Sq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5bc7a39-e05b-4626-fa3d-3d788272b4c6"
      },
      "source": [
        "model_factory('Adadelta')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-06\n",
            "    lr: 1.0\n",
            "    rho: 0.9\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.30746806, Test Loss: 2.30202828, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30127321, Test Loss: 2.29988619, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.18322140, Test Loss: 1.78346972, Test Accuracy: 0.35970000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 1.38807292, Test Loss: 1.05196477, Test Accuracy: 0.58940000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.73925256, Test Loss: 0.40720876, Test Accuracy: 0.89020000\n",
            "Saving the model state dictionary for Epoch: 5 with Test loss: 0.40720876\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.35055116, Test Loss: 0.30235301, Test Accuracy: 0.91540000\n",
            "Saving the model state dictionary for Epoch: 6 with Test loss: 0.30235301\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.22971141, Test Loss: 0.18080213, Test Accuracy: 0.94850000\n",
            "Saving the model state dictionary for Epoch: 7 with Test loss: 0.18080213\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.16830964, Test Loss: 0.16071009, Test Accuracy: 0.95200000\n",
            "Saving the model state dictionary for Epoch: 8 with Test loss: 0.16071009\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.13468058, Test Loss: 0.14689443, Test Accuracy: 0.95620000\n",
            "Saving the model state dictionary for Epoch: 9 with Test loss: 0.14689443\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.11125821, Test Loss: 0.13769167, Test Accuracy: 0.96050000\n",
            "Saving the model state dictionary for Epoch: 10 with Test loss: 0.13769167\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.09575459, Test Loss: 0.12158774, Test Accuracy: 0.96510000\n",
            "Saving the model state dictionary for Epoch: 11 with Test loss: 0.12158774\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.08345689, Test Loss: 0.10658882, Test Accuracy: 0.96770000\n",
            "Saving the model state dictionary for Epoch: 12 with Test loss: 0.10658882\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.07242740, Test Loss: 0.10226084, Test Accuracy: 0.97110000\n",
            "Saving the model state dictionary for Epoch: 13 with Test loss: 0.10226084\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.06346302, Test Loss: 0.12266106, Test Accuracy: 0.96430000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.05714225, Test Loss: 0.11184841, Test Accuracy: 0.97000000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.04942244, Test Loss: 0.09444607, Test Accuracy: 0.97360000\n",
            "Saving the model state dictionary for Epoch: 16 with Test loss: 0.09444607\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.04504925, Test Loss: 0.08744058, Test Accuracy: 0.97590000\n",
            "Saving the model state dictionary for Epoch: 17 with Test loss: 0.08744058\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.03980234, Test Loss: 0.09040047, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.03666472, Test Loss: 0.09473510, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.03226420, Test Loss: 0.08644381, Test Accuracy: 0.97760000\n",
            "Saving the model state dictionary for Epoch: 20 with Test loss: 0.08644381\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.02791094, Test Loss: 0.09336832, Test Accuracy: 0.97580000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.02439872, Test Loss: 0.10837251, Test Accuracy: 0.97450000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.02249789, Test Loss: 0.09321621, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.01903928, Test Loss: 0.09567787, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.01696255, Test Loss: 0.09537716, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.01522308, Test Loss: 0.09633990, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.01188127, Test Loss: 0.10120896, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.01160989, Test Loss: 0.09870279, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00893416, Test Loss: 0.09957772, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00835387, Test Loss: 0.12558413, Test Accuracy: 0.97390000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00773436, Test Loss: 0.11410533, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00713516, Test Loss: 0.11064613, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00577635, Test Loss: 0.11170420, Test Accuracy: 0.97820000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00518508, Test Loss: 0.11141245, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00432732, Test Loss: 0.11599430, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00434522, Test Loss: 0.11298601, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00291288, Test Loss: 0.11402806, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00309441, Test Loss: 0.11353930, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00236982, Test Loss: 0.11572427, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00209895, Test Loss: 0.11981597, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00173624, Test Loss: 0.12544072, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00165367, Test Loss: 0.12298006, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00122483, Test Loss: 0.12040693, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00116889, Test Loss: 0.12324028, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00101770, Test Loss: 0.12017319, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00097917, Test Loss: 0.12484312, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00095163, Test Loss: 0.13452857, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00088904, Test Loss: 0.12512112, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00081686, Test Loss: 0.12453386, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00069678, Test Loss: 0.12821129, Test Accuracy: 0.97920000\n",
            "[tensor(0.0054, grad_fn=<MeanBackward0>), tensor(0.0078, grad_fn=<MeanBackward0>), tensor(0.0597, grad_fn=<MeanBackward0>), tensor(0.2470, grad_fn=<MeanBackward0>), tensor(0.3866, grad_fn=<MeanBackward0>), tensor(0.4355, grad_fn=<MeanBackward0>), tensor(0.4512, grad_fn=<MeanBackward0>), tensor(0.4639, grad_fn=<MeanBackward0>), tensor(0.4669, grad_fn=<MeanBackward0>), tensor(0.4713, grad_fn=<MeanBackward0>), tensor(0.4783, grad_fn=<MeanBackward0>), tensor(0.4782, grad_fn=<MeanBackward0>), tensor(0.4864, grad_fn=<MeanBackward0>), tensor(0.4855, grad_fn=<MeanBackward0>), tensor(0.4901, grad_fn=<MeanBackward0>), tensor(0.4931, grad_fn=<MeanBackward0>), tensor(0.4926, grad_fn=<MeanBackward0>), tensor(0.4947, grad_fn=<MeanBackward0>), tensor(0.4964, grad_fn=<MeanBackward0>), tensor(0.4953, grad_fn=<MeanBackward0>), tensor(0.4937, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.4984, grad_fn=<MeanBackward0>), tensor(0.4980, grad_fn=<MeanBackward0>), tensor(0.4993, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.5000, grad_fn=<MeanBackward0>), tensor(0.5003, grad_fn=<MeanBackward0>), tensor(0.4990, grad_fn=<MeanBackward0>), tensor(0.5005, grad_fn=<MeanBackward0>), tensor(0.4992, grad_fn=<MeanBackward0>), tensor(0.4990, grad_fn=<MeanBackward0>), tensor(0.4998, grad_fn=<MeanBackward0>), tensor(0.5011, grad_fn=<MeanBackward0>), tensor(0.4989, grad_fn=<MeanBackward0>), tensor(0.4989, grad_fn=<MeanBackward0>), tensor(0.4990, grad_fn=<MeanBackward0>), tensor(0.5010, grad_fn=<MeanBackward0>), tensor(0.4998, grad_fn=<MeanBackward0>), tensor(0.4996, grad_fn=<MeanBackward0>), tensor(0.4973, grad_fn=<MeanBackward0>), tensor(0.4973, grad_fn=<MeanBackward0>), tensor(0.4962, grad_fn=<MeanBackward0>), tensor(0.4969, grad_fn=<MeanBackward0>), tensor(0.4964, grad_fn=<MeanBackward0>), tensor(0.4962, grad_fn=<MeanBackward0>), tensor(0.4951, grad_fn=<MeanBackward0>), tensor(0.4959, grad_fn=<MeanBackward0>), tensor(0.4953, grad_fn=<MeanBackward0>), tensor(0.4964, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0556, grad_fn=<MeanBackward0>), tensor(0.1241, grad_fn=<MeanBackward0>), tensor(0.1722, grad_fn=<MeanBackward0>), tensor(0.2126, grad_fn=<MeanBackward0>), tensor(0.2321, grad_fn=<MeanBackward0>), tensor(0.2425, grad_fn=<MeanBackward0>), tensor(0.2504, grad_fn=<MeanBackward0>), tensor(0.2448, grad_fn=<MeanBackward0>), tensor(0.2561, grad_fn=<MeanBackward0>), tensor(0.2542, grad_fn=<MeanBackward0>), tensor(0.2559, grad_fn=<MeanBackward0>), tensor(0.2572, grad_fn=<MeanBackward0>), tensor(0.2665, grad_fn=<MeanBackward0>), tensor(0.2631, grad_fn=<MeanBackward0>), tensor(0.2645, grad_fn=<MeanBackward0>), tensor(0.2626, grad_fn=<MeanBackward0>), tensor(0.2641, grad_fn=<MeanBackward0>), tensor(0.2623, grad_fn=<MeanBackward0>), tensor(0.2689, grad_fn=<MeanBackward0>), tensor(0.2643, grad_fn=<MeanBackward0>), tensor(0.2626, grad_fn=<MeanBackward0>), tensor(0.2623, grad_fn=<MeanBackward0>), tensor(0.2604, grad_fn=<MeanBackward0>), tensor(0.2613, grad_fn=<MeanBackward0>), tensor(0.2594, grad_fn=<MeanBackward0>), tensor(0.2573, grad_fn=<MeanBackward0>), tensor(0.2529, grad_fn=<MeanBackward0>), tensor(0.2533, grad_fn=<MeanBackward0>), tensor(0.2565, grad_fn=<MeanBackward0>), tensor(0.2517, grad_fn=<MeanBackward0>), tensor(0.2503, grad_fn=<MeanBackward0>), tensor(0.2516, grad_fn=<MeanBackward0>), tensor(0.2512, grad_fn=<MeanBackward0>), tensor(0.2509, grad_fn=<MeanBackward0>), tensor(0.2496, grad_fn=<MeanBackward0>), tensor(0.2459, grad_fn=<MeanBackward0>), tensor(0.2449, grad_fn=<MeanBackward0>), tensor(0.2435, grad_fn=<MeanBackward0>), tensor(0.2450, grad_fn=<MeanBackward0>), tensor(0.2413, grad_fn=<MeanBackward0>), tensor(0.2436, grad_fn=<MeanBackward0>), tensor(0.2407, grad_fn=<MeanBackward0>), tensor(0.2418, grad_fn=<MeanBackward0>), tensor(0.2416, grad_fn=<MeanBackward0>), tensor(0.2408, grad_fn=<MeanBackward0>), tensor(0.2399, grad_fn=<MeanBackward0>), tensor(0.2400, grad_fn=<MeanBackward0>), tensor(0.2385, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0097, grad_fn=<MeanBackward0>), tensor(0.0070, grad_fn=<MeanBackward0>), tensor(0.0459, grad_fn=<MeanBackward0>), tensor(0.2062, grad_fn=<MeanBackward0>), tensor(0.2501, grad_fn=<MeanBackward0>), tensor(0.2244, grad_fn=<MeanBackward0>), tensor(0.2312, grad_fn=<MeanBackward0>), tensor(0.2474, grad_fn=<MeanBackward0>), tensor(0.2480, grad_fn=<MeanBackward0>), tensor(0.2218, grad_fn=<MeanBackward0>), tensor(0.2491, grad_fn=<MeanBackward0>), tensor(0.2435, grad_fn=<MeanBackward0>), tensor(0.2594, grad_fn=<MeanBackward0>), tensor(0.2383, grad_fn=<MeanBackward0>), tensor(0.2686, grad_fn=<MeanBackward0>), tensor(0.2571, grad_fn=<MeanBackward0>), tensor(0.2511, grad_fn=<MeanBackward0>), tensor(0.2618, grad_fn=<MeanBackward0>), tensor(0.2489, grad_fn=<MeanBackward0>), tensor(0.2520, grad_fn=<MeanBackward0>), tensor(0.2580, grad_fn=<MeanBackward0>), tensor(0.2727, grad_fn=<MeanBackward0>), tensor(0.2611, grad_fn=<MeanBackward0>), tensor(0.2641, grad_fn=<MeanBackward0>), tensor(0.2590, grad_fn=<MeanBackward0>), tensor(0.2691, grad_fn=<MeanBackward0>), tensor(0.2755, grad_fn=<MeanBackward0>), tensor(0.2792, grad_fn=<MeanBackward0>), tensor(0.2785, grad_fn=<MeanBackward0>), tensor(0.2953, grad_fn=<MeanBackward0>), tensor(0.2902, grad_fn=<MeanBackward0>), tensor(0.2760, grad_fn=<MeanBackward0>), tensor(0.2761, grad_fn=<MeanBackward0>), tensor(0.2910, grad_fn=<MeanBackward0>), tensor(0.2940, grad_fn=<MeanBackward0>), tensor(0.2957, grad_fn=<MeanBackward0>), tensor(0.2950, grad_fn=<MeanBackward0>), tensor(0.2944, grad_fn=<MeanBackward0>), tensor(0.2956, grad_fn=<MeanBackward0>), tensor(0.2920, grad_fn=<MeanBackward0>), tensor(0.2999, grad_fn=<MeanBackward0>), tensor(0.2848, grad_fn=<MeanBackward0>), tensor(0.2986, grad_fn=<MeanBackward0>), tensor(0.2944, grad_fn=<MeanBackward0>), tensor(0.2973, grad_fn=<MeanBackward0>), tensor(0.3002, grad_fn=<MeanBackward0>), tensor(0.3001, grad_fn=<MeanBackward0>), tensor(0.2983, grad_fn=<MeanBackward0>), tensor(0.3022, grad_fn=<MeanBackward0>), tensor(0.2987, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0020, grad_fn=<MeanBackward0>), tensor(0.0044, grad_fn=<MeanBackward0>), tensor(0.4402, grad_fn=<MeanBackward0>), tensor(0.5325, grad_fn=<MeanBackward0>), tensor(0.5191, grad_fn=<MeanBackward0>), tensor(0.5348, grad_fn=<MeanBackward0>), tensor(0.5508, grad_fn=<MeanBackward0>), tensor(0.5308, grad_fn=<MeanBackward0>), tensor(0.5373, grad_fn=<MeanBackward0>), tensor(0.5763, grad_fn=<MeanBackward0>), tensor(0.5497, grad_fn=<MeanBackward0>), tensor(0.5700, grad_fn=<MeanBackward0>), tensor(0.5578, grad_fn=<MeanBackward0>), tensor(0.5734, grad_fn=<MeanBackward0>), tensor(0.5504, grad_fn=<MeanBackward0>), tensor(0.5682, grad_fn=<MeanBackward0>), tensor(0.5699, grad_fn=<MeanBackward0>), tensor(0.5607, grad_fn=<MeanBackward0>), tensor(0.5811, grad_fn=<MeanBackward0>), tensor(0.5779, grad_fn=<MeanBackward0>), tensor(0.5733, grad_fn=<MeanBackward0>), tensor(0.5605, grad_fn=<MeanBackward0>), tensor(0.5779, grad_fn=<MeanBackward0>), tensor(0.5730, grad_fn=<MeanBackward0>), tensor(0.5815, grad_fn=<MeanBackward0>), tensor(0.5695, grad_fn=<MeanBackward0>), tensor(0.5699, grad_fn=<MeanBackward0>), tensor(0.5683, grad_fn=<MeanBackward0>), tensor(0.5716, grad_fn=<MeanBackward0>), tensor(0.5517, grad_fn=<MeanBackward0>), tensor(0.5590, grad_fn=<MeanBackward0>), tensor(0.5715, grad_fn=<MeanBackward0>), tensor(0.5765, grad_fn=<MeanBackward0>), tensor(0.5611, grad_fn=<MeanBackward0>), tensor(0.5571, grad_fn=<MeanBackward0>), tensor(0.5526, grad_fn=<MeanBackward0>), tensor(0.5549, grad_fn=<MeanBackward0>), tensor(0.5573, grad_fn=<MeanBackward0>), tensor(0.5549, grad_fn=<MeanBackward0>), tensor(0.5593, grad_fn=<MeanBackward0>), tensor(0.5482, grad_fn=<MeanBackward0>), tensor(0.5633, grad_fn=<MeanBackward0>), tensor(0.5505, grad_fn=<MeanBackward0>), tensor(0.5574, grad_fn=<MeanBackward0>), tensor(0.5541, grad_fn=<MeanBackward0>), tensor(0.5519, grad_fn=<MeanBackward0>), tensor(0.5517, grad_fn=<MeanBackward0>), tensor(0.5528, grad_fn=<MeanBackward0>), tensor(0.5479, grad_fn=<MeanBackward0>), tensor(0.5545, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.007432476442772895, 0.007616112940013409, 0.15035035647451878, 0.27743243612349033, 0.3319634571671486, 0.35184498876333237, 0.3663038909435272, 0.3711462579667568, 0.3756405599415302, 0.3785630166530609, 0.3832824304699898, 0.3864830322563648, 0.38989973068237305, 0.38860243558883667, 0.39387910813093185, 0.3953826576471329, 0.3945397064089775, 0.3949231281876564, 0.3976452946662903, 0.39690618216991425, 0.3985056057572365, 0.39890460669994354, 0.3999796733260155, 0.3993524983525276, 0.40002214908599854, 0.3995160311460495, 0.4012063592672348, 0.40125812590122223, 0.4004858136177063, 0.4002027213573456, 0.40121761709451675, 0.3995358496904373, 0.4006773754954338, 0.40121059864759445, 0.40029845386743546, 0.39950521290302277, 0.3996046297252178, 0.39965297281742096, 0.3988223001360893, 0.3986102119088173, 0.39759935438632965, 0.3966810964047909, 0.39723527431488037, 0.3973333202302456, 0.39738747105002403, 0.3975069336593151, 0.3969082422554493, 0.3967321664094925, 0.39632951468229294, 0.39700818434357643]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMeTknSyCag1"
      },
      "source": [
        "# Adagrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnd24UwSB1VO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8456fe7d-6c7b-4c8b-c7ef-1a86c01da3b6"
      },
      "source": [
        "model_factory('Adagrad')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adagrad (\n",
            "Parameter Group 0\n",
            "    eps: 1e-10\n",
            "    initial_accumulator_value: 0\n",
            "    lr: 0.1\n",
            "    lr_decay: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.41818991, Test Loss: 2.32539597, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.32925109, Test Loss: 2.33124026, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.32166391, Test Loss: 2.32314413, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.31882392, Test Loss: 2.32085728, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.31751982, Test Loss: 2.32146739, Test Accuracy: 0.09740000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.31597325, Test Loss: 2.31645889, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.31481072, Test Loss: 2.31099015, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.31389924, Test Loss: 2.31366470, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.31191608, Test Loss: 2.31590268, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.31286547, Test Loss: 2.30808075, Test Accuracy: 0.08920000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.31184418, Test Loss: 2.32162350, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.31107131, Test Loss: 2.31958991, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.31080963, Test Loss: 2.32261508, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.31069626, Test Loss: 2.31980905, Test Accuracy: 0.08920000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.30973244, Test Loss: 2.30672133, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.30907264, Test Loss: 2.30789347, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.30989254, Test Loss: 2.31090134, Test Accuracy: 0.09580000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.30921342, Test Loss: 2.30303430, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 2.30920296, Test Loss: 2.30838287, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 2.30911811, Test Loss: 2.30733277, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 2.30856919, Test Loss: 2.30800102, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 2.30842895, Test Loss: 2.30742799, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 2.30861013, Test Loss: 2.30972126, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 2.30795143, Test Loss: 2.31337234, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 2.30812005, Test Loss: 2.30573979, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 2.30780600, Test Loss: 2.30448523, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 2.30757749, Test Loss: 2.31119840, Test Accuracy: 0.09800000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 2.30786591, Test Loss: 2.30527764, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 2.30732112, Test Loss: 2.30660927, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 2.30796772, Test Loss: 2.30350470, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 2.30737254, Test Loss: 2.30403148, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 2.30705992, Test Loss: 2.31234063, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 2.21310131, Test Loss: 1.82493974, Test Accuracy: 0.21040000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.75493335, Test Loss: 0.21477425, Test Accuracy: 0.94060000\n",
            "Saving the model state dictionary for Epoch: 34 with Test loss: 0.21477425\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.14870962, Test Loss: 0.14351636, Test Accuracy: 0.96130000\n",
            "Saving the model state dictionary for Epoch: 35 with Test loss: 0.14351636\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.09824462, Test Loss: 0.12158457, Test Accuracy: 0.96770000\n",
            "Saving the model state dictionary for Epoch: 36 with Test loss: 0.12158457\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.07133151, Test Loss: 0.11779994, Test Accuracy: 0.97070000\n",
            "Saving the model state dictionary for Epoch: 37 with Test loss: 0.11779994\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.05388656, Test Loss: 0.11552594, Test Accuracy: 0.97080000\n",
            "Saving the model state dictionary for Epoch: 38 with Test loss: 0.11552594\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.04236727, Test Loss: 0.11887056, Test Accuracy: 0.97060000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.03380225, Test Loss: 0.11963410, Test Accuracy: 0.97240000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.02760708, Test Loss: 0.12348007, Test Accuracy: 0.97080000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.02151639, Test Loss: 0.13494401, Test Accuracy: 0.97080000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.01862902, Test Loss: 0.13249978, Test Accuracy: 0.97100000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.01624867, Test Loss: 0.12581151, Test Accuracy: 0.97400000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.01397591, Test Loss: 0.12857093, Test Accuracy: 0.97410000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.01088206, Test Loss: 0.12879258, Test Accuracy: 0.97470000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00961483, Test Loss: 0.13055958, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00752028, Test Loss: 0.13690995, Test Accuracy: 0.97380000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00656766, Test Loss: 0.13630026, Test Accuracy: 0.97490000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00559000, Test Loss: 0.13950489, Test Accuracy: 0.97350000\n",
            "[tensor(0.2704, grad_fn=<MeanBackward0>), tensor(0.2705, grad_fn=<MeanBackward0>), tensor(0.2705, grad_fn=<MeanBackward0>), tensor(0.2706, grad_fn=<MeanBackward0>), tensor(0.2707, grad_fn=<MeanBackward0>), tensor(0.3032, grad_fn=<MeanBackward0>), tensor(0.3032, grad_fn=<MeanBackward0>), tensor(0.3032, grad_fn=<MeanBackward0>), tensor(0.3033, grad_fn=<MeanBackward0>), tensor(0.3033, grad_fn=<MeanBackward0>), tensor(0.3033, grad_fn=<MeanBackward0>), tensor(0.3033, grad_fn=<MeanBackward0>), tensor(0.3033, grad_fn=<MeanBackward0>), tensor(0.3033, grad_fn=<MeanBackward0>), tensor(0.3033, grad_fn=<MeanBackward0>), tensor(0.3033, grad_fn=<MeanBackward0>), tensor(0.3033, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3301, grad_fn=<MeanBackward0>), tensor(0.5536, grad_fn=<MeanBackward0>), tensor(0.5322, grad_fn=<MeanBackward0>), tensor(0.5347, grad_fn=<MeanBackward0>), tensor(0.5349, grad_fn=<MeanBackward0>), tensor(0.5362, grad_fn=<MeanBackward0>), tensor(0.5351, grad_fn=<MeanBackward0>), tensor(0.5361, grad_fn=<MeanBackward0>), tensor(0.5338, grad_fn=<MeanBackward0>), tensor(0.5356, grad_fn=<MeanBackward0>), tensor(0.5335, grad_fn=<MeanBackward0>), tensor(0.5343, grad_fn=<MeanBackward0>), tensor(0.5347, grad_fn=<MeanBackward0>), tensor(0.5354, grad_fn=<MeanBackward0>), tensor(0.5342, grad_fn=<MeanBackward0>), tensor(0.5347, grad_fn=<MeanBackward0>), tensor(0.5350, grad_fn=<MeanBackward0>), tensor(0.5343, grad_fn=<MeanBackward0>), tensor(0.5344, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3174, grad_fn=<MeanBackward0>), tensor(0.3175, grad_fn=<MeanBackward0>), tensor(0.3176, grad_fn=<MeanBackward0>), tensor(0.3178, grad_fn=<MeanBackward0>), tensor(0.3178, grad_fn=<MeanBackward0>), tensor(0.3143, grad_fn=<MeanBackward0>), tensor(0.3144, grad_fn=<MeanBackward0>), tensor(0.3145, grad_fn=<MeanBackward0>), tensor(0.3146, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3150, grad_fn=<MeanBackward0>), tensor(0.3151, grad_fn=<MeanBackward0>), tensor(0.3153, grad_fn=<MeanBackward0>), tensor(0.3155, grad_fn=<MeanBackward0>), tensor(0.3158, grad_fn=<MeanBackward0>), tensor(0.3164, grad_fn=<MeanBackward0>), tensor(0.3169, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3148, grad_fn=<MeanBackward0>), tensor(0.3150, grad_fn=<MeanBackward0>), tensor(0.4504, grad_fn=<MeanBackward0>), tensor(0.4981, grad_fn=<MeanBackward0>), tensor(0.5008, grad_fn=<MeanBackward0>), tensor(0.5047, grad_fn=<MeanBackward0>), tensor(0.5054, grad_fn=<MeanBackward0>), tensor(0.5037, grad_fn=<MeanBackward0>), tensor(0.5047, grad_fn=<MeanBackward0>), tensor(0.5040, grad_fn=<MeanBackward0>), tensor(0.5043, grad_fn=<MeanBackward0>), tensor(0.5037, grad_fn=<MeanBackward0>), tensor(0.5029, grad_fn=<MeanBackward0>), tensor(0.5005, grad_fn=<MeanBackward0>), tensor(0.5021, grad_fn=<MeanBackward0>), tensor(0.5016, grad_fn=<MeanBackward0>), tensor(0.5022, grad_fn=<MeanBackward0>), tensor(0.4998, grad_fn=<MeanBackward0>), tensor(0.5017, grad_fn=<MeanBackward0>), tensor(0.5020, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3303, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3273, grad_fn=<MeanBackward0>), tensor(0.3207, grad_fn=<MeanBackward0>), tensor(0.3601, grad_fn=<MeanBackward0>), tensor(0.3668, grad_fn=<MeanBackward0>), tensor(0.3718, grad_fn=<MeanBackward0>), tensor(0.3733, grad_fn=<MeanBackward0>), tensor(0.3723, grad_fn=<MeanBackward0>), tensor(0.3764, grad_fn=<MeanBackward0>), tensor(0.3724, grad_fn=<MeanBackward0>), tensor(0.3778, grad_fn=<MeanBackward0>), tensor(0.3752, grad_fn=<MeanBackward0>), tensor(0.3756, grad_fn=<MeanBackward0>), tensor(0.3800, grad_fn=<MeanBackward0>), tensor(0.3791, grad_fn=<MeanBackward0>), tensor(0.3786, grad_fn=<MeanBackward0>), tensor(0.3765, grad_fn=<MeanBackward0>), tensor(0.3768, grad_fn=<MeanBackward0>), tensor(0.3756, grad_fn=<MeanBackward0>), tensor(0.3745, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3984, grad_fn=<MeanBackward0>), tensor(0.3985, grad_fn=<MeanBackward0>), tensor(0.3941, grad_fn=<MeanBackward0>), tensor(0.3975, grad_fn=<MeanBackward0>), tensor(0.3992, grad_fn=<MeanBackward0>), tensor(0.3980, grad_fn=<MeanBackward0>), tensor(0.3969, grad_fn=<MeanBackward0>), tensor(0.3962, grad_fn=<MeanBackward0>), tensor(0.4003, grad_fn=<MeanBackward0>), tensor(0.3989, grad_fn=<MeanBackward0>), tensor(0.3981, grad_fn=<MeanBackward0>), tensor(0.3980, grad_fn=<MeanBackward0>), tensor(0.3970, grad_fn=<MeanBackward0>), tensor(0.3975, grad_fn=<MeanBackward0>), tensor(0.3964, grad_fn=<MeanBackward0>), tensor(0.3960, grad_fn=<MeanBackward0>), tensor(0.3950, grad_fn=<MeanBackward0>), tensor(0.3946, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.32059701532125473, 0.320630818605423, 0.32067983597517014, 0.32073458284139633, 0.3207800164818764, 0.3288341164588928, 0.32885559648275375, 0.3288813680410385, 0.3289131596684456, 0.3289514183998108, 0.32899438589811325, 0.32903702557086945, 0.329081729054451, 0.32913318276405334, 0.32922306656837463, 0.3293689414858818, 0.3294852524995804, 0.3349619135260582, 0.3349624127149582, 0.334962822496891, 0.3349631354212761, 0.334963396191597, 0.33496373891830444, 0.33496399223804474, 0.33496424555778503, 0.3349643796682358, 0.33496447652578354, 0.33496446162462234, 0.33496447652578354, 0.33496442437171936, 0.334964282810688, 0.3349602445960045, 0.43079160153865814, 0.44723984599113464, 0.44907473772764206, 0.45221441984176636, 0.45351116359233856, 0.45226551592350006, 0.45352866500616074, 0.4516070857644081, 0.4544886201620102, 0.45283208042383194, 0.4527455121278763, 0.4533326253294945, 0.4533754214644432, 0.45297273248434067, 0.45246104896068573, 0.4519011527299881, 0.45163194835186005, 0.4513765871524811]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK8bO_MTCboM"
      },
      "source": [
        "# SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_f5iGx7BUfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827fa0ae-578e-42fa-bda6-09a409b90290"
      },
      "source": [
        "model_factory('SGD')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 2.32315335, Test Loss: 2.30903592, Test Accuracy: 0.09820000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 2.30906892, Test Loss: 2.30911062, Test Accuracy: 0.10320000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 2.30566494, Test Loss: 2.30473550, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 2.30376772, Test Loss: 2.30548496, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 2.30315410, Test Loss: 2.30266627, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 2.30291336, Test Loss: 2.30233922, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 2.30252485, Test Loss: 2.30283901, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 2.30234118, Test Loss: 2.30234136, Test Accuracy: 0.10280000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 2.30218776, Test Loss: 2.30202646, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 2.30217589, Test Loss: 2.30245288, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 2.30208508, Test Loss: 2.30125003, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 2.30207717, Test Loss: 2.30117370, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 2.30191832, Test Loss: 2.30243047, Test Accuracy: 0.10100000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 2.30188111, Test Loss: 2.30183062, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 2.30170921, Test Loss: 2.30127634, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 2.30160856, Test Loss: 2.30088891, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 2.30137673, Test Loss: 2.30096326, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 2.30104898, Test Loss: 2.30090060, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 2.30040888, Test Loss: 2.29950046, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 2.29918755, Test Loss: 2.29756450, Test Accuracy: 0.11350000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 2.29594025, Test Loss: 2.29298708, Test Accuracy: 0.21030000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 2.27940425, Test Loss: 2.24576267, Test Accuracy: 0.14350000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 2.07198333, Test Loss: 1.89168994, Test Accuracy: 0.29000000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 1.79864539, Test Loss: 1.69034983, Test Accuracy: 0.35440000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 1.66169379, Test Loss: 1.59261247, Test Accuracy: 0.38050000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 1.57599356, Test Loss: 1.51248764, Test Accuracy: 0.39980000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 1.50176127, Test Loss: 1.42308501, Test Accuracy: 0.44480000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 1.41070332, Test Loss: 1.32790813, Test Accuracy: 0.49670000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 1.24923288, Test Loss: 0.98909098, Test Accuracy: 0.66370000\n",
            "Saving the model state dictionary for Epoch: 29 with Test loss: 0.98909098\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.85563588, Test Loss: 0.73560065, Test Accuracy: 0.74180000\n",
            "Saving the model state dictionary for Epoch: 30 with Test loss: 0.73560065\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.70487251, Test Loss: 0.61985320, Test Accuracy: 0.81770000\n",
            "Saving the model state dictionary for Epoch: 31 with Test loss: 0.61985320\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.60342625, Test Loss: 0.55436313, Test Accuracy: 0.83800000\n",
            "Saving the model state dictionary for Epoch: 32 with Test loss: 0.55436313\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.52255737, Test Loss: 0.49487976, Test Accuracy: 0.86330000\n",
            "Saving the model state dictionary for Epoch: 33 with Test loss: 0.49487976\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.46595162, Test Loss: 0.41601215, Test Accuracy: 0.89550000\n",
            "Saving the model state dictionary for Epoch: 34 with Test loss: 0.41601215\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.42113635, Test Loss: 0.40295407, Test Accuracy: 0.89890000\n",
            "Saving the model state dictionary for Epoch: 35 with Test loss: 0.40295407\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.38326146, Test Loss: 0.35731588, Test Accuracy: 0.90860000\n",
            "Saving the model state dictionary for Epoch: 36 with Test loss: 0.35731588\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.35746787, Test Loss: 0.36892860, Test Accuracy: 0.90110000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.32986217, Test Loss: 0.32568355, Test Accuracy: 0.91700000\n",
            "Saving the model state dictionary for Epoch: 38 with Test loss: 0.32568355\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.30912759, Test Loss: 0.29942425, Test Accuracy: 0.92330000\n",
            "Saving the model state dictionary for Epoch: 39 with Test loss: 0.29942425\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.29016535, Test Loss: 0.28343515, Test Accuracy: 0.92690000\n",
            "Saving the model state dictionary for Epoch: 40 with Test loss: 0.28343515\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.27181056, Test Loss: 0.26919646, Test Accuracy: 0.92910000\n",
            "Saving the model state dictionary for Epoch: 41 with Test loss: 0.26919646\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.25264592, Test Loss: 0.28705819, Test Accuracy: 0.92650000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.23388401, Test Loss: 0.24178807, Test Accuracy: 0.93540000\n",
            "Saving the model state dictionary for Epoch: 43 with Test loss: 0.24178807\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.21865770, Test Loss: 0.25449915, Test Accuracy: 0.93520000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.20357786, Test Loss: 0.23846973, Test Accuracy: 0.93960000\n",
            "Saving the model state dictionary for Epoch: 45 with Test loss: 0.23846973\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.19323487, Test Loss: 0.22680895, Test Accuracy: 0.94070000\n",
            "Saving the model state dictionary for Epoch: 46 with Test loss: 0.22680895\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.18324233, Test Loss: 0.20620749, Test Accuracy: 0.94470000\n",
            "Saving the model state dictionary for Epoch: 47 with Test loss: 0.20620749\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.17500359, Test Loss: 0.20668361, Test Accuracy: 0.94450000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.16426287, Test Loss: 0.20672945, Test Accuracy: 0.94380000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.15847095, Test Loss: 0.22579985, Test Accuracy: 0.93880000\n",
            "[tensor(0.0050, grad_fn=<MeanBackward0>), tensor(0.0050, grad_fn=<MeanBackward0>), tensor(0.0051, grad_fn=<MeanBackward0>), tensor(0.0051, grad_fn=<MeanBackward0>), tensor(0.0052, grad_fn=<MeanBackward0>), tensor(0.0052, grad_fn=<MeanBackward0>), tensor(0.0053, grad_fn=<MeanBackward0>), tensor(0.0053, grad_fn=<MeanBackward0>), tensor(0.0054, grad_fn=<MeanBackward0>), tensor(0.0055, grad_fn=<MeanBackward0>), tensor(0.0056, grad_fn=<MeanBackward0>), tensor(0.0057, grad_fn=<MeanBackward0>), tensor(0.0058, grad_fn=<MeanBackward0>), tensor(0.0060, grad_fn=<MeanBackward0>), tensor(0.0063, grad_fn=<MeanBackward0>), tensor(0.0066, grad_fn=<MeanBackward0>), tensor(0.0071, grad_fn=<MeanBackward0>), tensor(0.0077, grad_fn=<MeanBackward0>), tensor(0.0087, grad_fn=<MeanBackward0>), tensor(0.0104, grad_fn=<MeanBackward0>), tensor(0.0134, grad_fn=<MeanBackward0>), tensor(0.0218, grad_fn=<MeanBackward0>), tensor(0.0409, grad_fn=<MeanBackward0>), tensor(0.0313, grad_fn=<MeanBackward0>), tensor(0.0412, grad_fn=<MeanBackward0>), tensor(0.0518, grad_fn=<MeanBackward0>), tensor(0.0599, grad_fn=<MeanBackward0>), tensor(0.0686, grad_fn=<MeanBackward0>), tensor(0.0810, grad_fn=<MeanBackward0>), tensor(0.0908, grad_fn=<MeanBackward0>), tensor(0.1047, grad_fn=<MeanBackward0>), tensor(0.1211, grad_fn=<MeanBackward0>), tensor(0.1349, grad_fn=<MeanBackward0>), tensor(0.1472, grad_fn=<MeanBackward0>), tensor(0.1576, grad_fn=<MeanBackward0>), tensor(0.1678, grad_fn=<MeanBackward0>), tensor(0.1790, grad_fn=<MeanBackward0>), tensor(0.1871, grad_fn=<MeanBackward0>), tensor(0.1947, grad_fn=<MeanBackward0>), tensor(0.2019, grad_fn=<MeanBackward0>), tensor(0.2072, grad_fn=<MeanBackward0>), tensor(0.2111, grad_fn=<MeanBackward0>), tensor(0.2143, grad_fn=<MeanBackward0>), tensor(0.2168, grad_fn=<MeanBackward0>), tensor(0.2190, grad_fn=<MeanBackward0>), tensor(0.2222, grad_fn=<MeanBackward0>), tensor(0.2251, grad_fn=<MeanBackward0>), tensor(0.2277, grad_fn=<MeanBackward0>), tensor(0.2303, grad_fn=<MeanBackward0>), tensor(0.2334, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0108, grad_fn=<MeanBackward0>), tensor(0.0108, grad_fn=<MeanBackward0>), tensor(0.0109, grad_fn=<MeanBackward0>), tensor(0.0110, grad_fn=<MeanBackward0>), tensor(0.0110, grad_fn=<MeanBackward0>), tensor(0.0111, grad_fn=<MeanBackward0>), tensor(0.0111, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0113, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0111, grad_fn=<MeanBackward0>), tensor(0.0109, grad_fn=<MeanBackward0>), tensor(0.0104, grad_fn=<MeanBackward0>), tensor(0.0096, grad_fn=<MeanBackward0>), tensor(0.0083, grad_fn=<MeanBackward0>), tensor(0.0364, grad_fn=<MeanBackward0>), tensor(0.0584, grad_fn=<MeanBackward0>), tensor(0.0592, grad_fn=<MeanBackward0>), tensor(0.0599, grad_fn=<MeanBackward0>), tensor(0.0628, grad_fn=<MeanBackward0>), tensor(0.0646, grad_fn=<MeanBackward0>), tensor(0.0654, grad_fn=<MeanBackward0>), tensor(0.0740, grad_fn=<MeanBackward0>), tensor(0.0681, grad_fn=<MeanBackward0>), tensor(0.0617, grad_fn=<MeanBackward0>), tensor(0.0612, grad_fn=<MeanBackward0>), tensor(0.0648, grad_fn=<MeanBackward0>), tensor(0.0650, grad_fn=<MeanBackward0>), tensor(0.0669, grad_fn=<MeanBackward0>), tensor(0.0663, grad_fn=<MeanBackward0>), tensor(0.0691, grad_fn=<MeanBackward0>), tensor(0.0695, grad_fn=<MeanBackward0>), tensor(0.0720, grad_fn=<MeanBackward0>), tensor(0.0728, grad_fn=<MeanBackward0>), tensor(0.0756, grad_fn=<MeanBackward0>), tensor(0.0760, grad_fn=<MeanBackward0>), tensor(0.0800, grad_fn=<MeanBackward0>), tensor(0.0807, grad_fn=<MeanBackward0>), tensor(0.0831, grad_fn=<MeanBackward0>), tensor(0.0804, grad_fn=<MeanBackward0>), tensor(0.0827, grad_fn=<MeanBackward0>), tensor(0.0811, grad_fn=<MeanBackward0>), tensor(0.0815, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0134, grad_fn=<MeanBackward0>), tensor(0.0139, grad_fn=<MeanBackward0>), tensor(0.0139, grad_fn=<MeanBackward0>), tensor(0.0137, grad_fn=<MeanBackward0>), tensor(0.0135, grad_fn=<MeanBackward0>), tensor(0.0133, grad_fn=<MeanBackward0>), tensor(0.0130, grad_fn=<MeanBackward0>), tensor(0.0128, grad_fn=<MeanBackward0>), tensor(0.0126, grad_fn=<MeanBackward0>), tensor(0.0124, grad_fn=<MeanBackward0>), tensor(0.0122, grad_fn=<MeanBackward0>), tensor(0.0121, grad_fn=<MeanBackward0>), tensor(0.0118, grad_fn=<MeanBackward0>), tensor(0.0117, grad_fn=<MeanBackward0>), tensor(0.0115, grad_fn=<MeanBackward0>), tensor(0.0112, grad_fn=<MeanBackward0>), tensor(0.0110, grad_fn=<MeanBackward0>), tensor(0.0106, grad_fn=<MeanBackward0>), tensor(0.0100, grad_fn=<MeanBackward0>), tensor(0.0092, grad_fn=<MeanBackward0>), tensor(0.0076, grad_fn=<MeanBackward0>), tensor(0.0045, grad_fn=<MeanBackward0>), tensor(0.0089, grad_fn=<MeanBackward0>), tensor(0.0271, grad_fn=<MeanBackward0>), tensor(0.0492, grad_fn=<MeanBackward0>), tensor(0.0776, grad_fn=<MeanBackward0>), tensor(0.1144, grad_fn=<MeanBackward0>), tensor(0.1507, grad_fn=<MeanBackward0>), tensor(0.1149, grad_fn=<MeanBackward0>), tensor(0.1370, grad_fn=<MeanBackward0>), tensor(0.1387, grad_fn=<MeanBackward0>), tensor(0.1326, grad_fn=<MeanBackward0>), tensor(0.1346, grad_fn=<MeanBackward0>), tensor(0.1533, grad_fn=<MeanBackward0>), tensor(0.1561, grad_fn=<MeanBackward0>), tensor(0.1558, grad_fn=<MeanBackward0>), tensor(0.1505, grad_fn=<MeanBackward0>), tensor(0.1610, grad_fn=<MeanBackward0>), tensor(0.1578, grad_fn=<MeanBackward0>), tensor(0.1601, grad_fn=<MeanBackward0>), tensor(0.1565, grad_fn=<MeanBackward0>), tensor(0.1667, grad_fn=<MeanBackward0>), tensor(0.1541, grad_fn=<MeanBackward0>), tensor(0.1667, grad_fn=<MeanBackward0>), tensor(0.1635, grad_fn=<MeanBackward0>), tensor(0.1682, grad_fn=<MeanBackward0>), tensor(0.1536, grad_fn=<MeanBackward0>), tensor(0.1598, grad_fn=<MeanBackward0>), tensor(0.1528, grad_fn=<MeanBackward0>), tensor(0.1598, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0070, grad_fn=<MeanBackward0>), tensor(0.0037, grad_fn=<MeanBackward0>), tensor(0.0022, grad_fn=<MeanBackward0>), tensor(0.0016, grad_fn=<MeanBackward0>), tensor(0.0012, grad_fn=<MeanBackward0>), tensor(0.0009, grad_fn=<MeanBackward0>), tensor(0.0009, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0005, grad_fn=<MeanBackward0>), tensor(0.0006, grad_fn=<MeanBackward0>), tensor(0.0006, grad_fn=<MeanBackward0>), tensor(0.0007, grad_fn=<MeanBackward0>), tensor(0.0010, grad_fn=<MeanBackward0>), tensor(0.0013, grad_fn=<MeanBackward0>), tensor(0.0022, grad_fn=<MeanBackward0>), tensor(0.0040, grad_fn=<MeanBackward0>), tensor(0.0055, grad_fn=<MeanBackward0>), tensor(0.0089, grad_fn=<MeanBackward0>), tensor(0.0100, grad_fn=<MeanBackward0>), tensor(0.0434, grad_fn=<MeanBackward0>), tensor(0.1329, grad_fn=<MeanBackward0>), tensor(0.2291, grad_fn=<MeanBackward0>), tensor(0.2611, grad_fn=<MeanBackward0>), tensor(0.2884, grad_fn=<MeanBackward0>), tensor(0.2644, grad_fn=<MeanBackward0>), tensor(0.2654, grad_fn=<MeanBackward0>), tensor(0.3009, grad_fn=<MeanBackward0>), tensor(0.3415, grad_fn=<MeanBackward0>), tensor(0.3508, grad_fn=<MeanBackward0>), tensor(0.3402, grad_fn=<MeanBackward0>), tensor(0.3454, grad_fn=<MeanBackward0>), tensor(0.3526, grad_fn=<MeanBackward0>), tensor(0.3745, grad_fn=<MeanBackward0>), tensor(0.3528, grad_fn=<MeanBackward0>), tensor(0.3629, grad_fn=<MeanBackward0>), tensor(0.3652, grad_fn=<MeanBackward0>), tensor(0.3762, grad_fn=<MeanBackward0>), tensor(0.3611, grad_fn=<MeanBackward0>), tensor(0.3795, grad_fn=<MeanBackward0>), tensor(0.3584, grad_fn=<MeanBackward0>), tensor(0.3594, grad_fn=<MeanBackward0>), tensor(0.3586, grad_fn=<MeanBackward0>), tensor(0.3809, grad_fn=<MeanBackward0>), tensor(0.3740, grad_fn=<MeanBackward0>), tensor(0.3874, grad_fn=<MeanBackward0>), tensor(0.3816, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.009035347728058696, 0.008382768952287734, 0.00802029191982001, 0.007867650914704427, 0.007730406185146421, 0.00761897477786988, 0.007577698124805465, 0.007503567991079763, 0.007474986705346964, 0.0074540828645695, 0.007402252143947408, 0.007390893413685262, 0.007367405705736019, 0.007398378889774904, 0.0074121202051173896, 0.007467655261280015, 0.00756479604751803, 0.007666759105632082, 0.007968981692101806, 0.008500383119098842, 0.009004136314615607, 0.010887252050451934, 0.024038789328187704, 0.04004680551588535, 0.07060705032199621, 0.10460406728088856, 0.12456478457897902, 0.14306247606873512, 0.13143155351281166, 0.14178097806870937, 0.15309470891952515, 0.164234371855855, 0.17039937060326338, 0.17634675838053226, 0.1810316350311041, 0.18575504049658775, 0.19259764440357685, 0.19251402653753757, 0.19622350111603737, 0.1997651644051075, 0.2031670454889536, 0.20360440574586391, 0.2059670127928257, 0.20545146614313126, 0.20565245859324932, 0.20799820311367512, 0.20996957458555698, 0.21103666722774506, 0.2129286825656891, 0.21407948061823845]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xQTGPZHCdKn"
      },
      "source": [
        "# Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB2Kyx3TCVCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a9417b4-7510-4d8c-fdd1-e013739aa588"
      },
      "source": [
        "model_factory('Adam')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            "  (sigmoid34): Sigmoid()\n",
            "  (sigmoid45): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 0.63060463, Test Loss: 0.26807362, Test Accuracy: 0.92430000\n",
            "Saving the model state dictionary for Epoch: 1 with Test loss: 0.26807362\n",
            "\n",
            "Epoch: 2/50, Train Loss: 0.20602424, Test Loss: 0.17091360, Test Accuracy: 0.95030000\n",
            "Saving the model state dictionary for Epoch: 2 with Test loss: 0.17091360\n",
            "\n",
            "Epoch: 3/50, Train Loss: 0.13963422, Test Loss: 0.13862531, Test Accuracy: 0.96050000\n",
            "Saving the model state dictionary for Epoch: 3 with Test loss: 0.13862531\n",
            "\n",
            "Epoch: 4/50, Train Loss: 0.10679276, Test Loss: 0.11708057, Test Accuracy: 0.96550000\n",
            "Saving the model state dictionary for Epoch: 4 with Test loss: 0.11708057\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.08605077, Test Loss: 0.10161782, Test Accuracy: 0.96890000\n",
            "Saving the model state dictionary for Epoch: 5 with Test loss: 0.10161782\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.06997542, Test Loss: 0.10106371, Test Accuracy: 0.96990000\n",
            "Saving the model state dictionary for Epoch: 6 with Test loss: 0.10106371\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.05697626, Test Loss: 0.09878727, Test Accuracy: 0.97310000\n",
            "Saving the model state dictionary for Epoch: 7 with Test loss: 0.09878727\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.04864492, Test Loss: 0.08308155, Test Accuracy: 0.97660000\n",
            "Saving the model state dictionary for Epoch: 8 with Test loss: 0.08308155\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.03917281, Test Loss: 0.09710232, Test Accuracy: 0.97200000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.03353541, Test Loss: 0.08986541, Test Accuracy: 0.97510000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.02934980, Test Loss: 0.09087902, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.02703960, Test Loss: 0.07968220, Test Accuracy: 0.97920000\n",
            "Saving the model state dictionary for Epoch: 12 with Test loss: 0.07968220\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.02214539, Test Loss: 0.10103964, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.02057930, Test Loss: 0.12469582, Test Accuracy: 0.97110000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.01816297, Test Loss: 0.09816574, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.01798052, Test Loss: 0.09160909, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.01599007, Test Loss: 0.10956755, Test Accuracy: 0.97600000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.01276546, Test Loss: 0.09621905, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.01347658, Test Loss: 0.09698554, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.01203067, Test Loss: 0.12099124, Test Accuracy: 0.97640000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.01179637, Test Loss: 0.10388723, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.01092650, Test Loss: 0.13213568, Test Accuracy: 0.97330000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.00966813, Test Loss: 0.11185086, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.00971315, Test Loss: 0.10589965, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.00902569, Test Loss: 0.11987111, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.00858074, Test Loss: 0.13544777, Test Accuracy: 0.97440000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.00695648, Test Loss: 0.10459359, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.00763980, Test Loss: 0.12127137, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00636993, Test Loss: 0.12129992, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00725813, Test Loss: 0.11417820, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00764204, Test Loss: 0.12397070, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00603673, Test Loss: 0.13656780, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00801739, Test Loss: 0.11876656, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00504359, Test Loss: 0.11537302, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00619905, Test Loss: 0.14567703, Test Accuracy: 0.97540000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00579245, Test Loss: 0.12969479, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00586711, Test Loss: 0.11955673, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00393103, Test Loss: 0.13167562, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00597341, Test Loss: 0.12376471, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00455976, Test Loss: 0.13152043, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00638878, Test Loss: 0.12252544, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00579893, Test Loss: 0.11561408, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00420149, Test Loss: 0.13613159, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00473139, Test Loss: 0.12586074, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00393893, Test Loss: 0.13886614, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00541986, Test Loss: 0.10555946, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00318156, Test Loss: 0.11703440, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00455372, Test Loss: 0.12076395, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00536235, Test Loss: 0.11434369, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00367477, Test Loss: 0.15218943, Test Accuracy: 0.97790000\n",
            "[tensor(0.2696, grad_fn=<MeanBackward0>), tensor(0.2998, grad_fn=<MeanBackward0>), tensor(0.3202, grad_fn=<MeanBackward0>), tensor(0.3305, grad_fn=<MeanBackward0>), tensor(0.3425, grad_fn=<MeanBackward0>), tensor(0.3504, grad_fn=<MeanBackward0>), tensor(0.3562, grad_fn=<MeanBackward0>), tensor(0.3599, grad_fn=<MeanBackward0>), tensor(0.3647, grad_fn=<MeanBackward0>), tensor(0.3658, grad_fn=<MeanBackward0>), tensor(0.3747, grad_fn=<MeanBackward0>), tensor(0.3778, grad_fn=<MeanBackward0>), tensor(0.3746, grad_fn=<MeanBackward0>), tensor(0.3781, grad_fn=<MeanBackward0>), tensor(0.3794, grad_fn=<MeanBackward0>), tensor(0.3835, grad_fn=<MeanBackward0>), tensor(0.3850, grad_fn=<MeanBackward0>), tensor(0.3818, grad_fn=<MeanBackward0>), tensor(0.3852, grad_fn=<MeanBackward0>), tensor(0.3875, grad_fn=<MeanBackward0>), tensor(0.3883, grad_fn=<MeanBackward0>), tensor(0.3864, grad_fn=<MeanBackward0>), tensor(0.3926, grad_fn=<MeanBackward0>), tensor(0.3906, grad_fn=<MeanBackward0>), tensor(0.3910, grad_fn=<MeanBackward0>), tensor(0.3949, grad_fn=<MeanBackward0>), tensor(0.3921, grad_fn=<MeanBackward0>), tensor(0.3953, grad_fn=<MeanBackward0>), tensor(0.3973, grad_fn=<MeanBackward0>), tensor(0.3930, grad_fn=<MeanBackward0>), tensor(0.3972, grad_fn=<MeanBackward0>), tensor(0.4010, grad_fn=<MeanBackward0>), tensor(0.4012, grad_fn=<MeanBackward0>), tensor(0.3978, grad_fn=<MeanBackward0>), tensor(0.3997, grad_fn=<MeanBackward0>), tensor(0.4003, grad_fn=<MeanBackward0>), tensor(0.4037, grad_fn=<MeanBackward0>), tensor(0.4020, grad_fn=<MeanBackward0>), tensor(0.4030, grad_fn=<MeanBackward0>), tensor(0.4035, grad_fn=<MeanBackward0>), tensor(0.4024, grad_fn=<MeanBackward0>), tensor(0.4079, grad_fn=<MeanBackward0>), tensor(0.4016, grad_fn=<MeanBackward0>), tensor(0.4073, grad_fn=<MeanBackward0>), tensor(0.4098, grad_fn=<MeanBackward0>), tensor(0.4078, grad_fn=<MeanBackward0>), tensor(0.4119, grad_fn=<MeanBackward0>), tensor(0.4087, grad_fn=<MeanBackward0>), tensor(0.4090, grad_fn=<MeanBackward0>), tensor(0.4103, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2190, grad_fn=<MeanBackward0>), tensor(0.2337, grad_fn=<MeanBackward0>), tensor(0.2423, grad_fn=<MeanBackward0>), tensor(0.2540, grad_fn=<MeanBackward0>), tensor(0.2635, grad_fn=<MeanBackward0>), tensor(0.2729, grad_fn=<MeanBackward0>), tensor(0.2759, grad_fn=<MeanBackward0>), tensor(0.2840, grad_fn=<MeanBackward0>), tensor(0.2922, grad_fn=<MeanBackward0>), tensor(0.3040, grad_fn=<MeanBackward0>), tensor(0.3013, grad_fn=<MeanBackward0>), tensor(0.3172, grad_fn=<MeanBackward0>), tensor(0.3255, grad_fn=<MeanBackward0>), tensor(0.3339, grad_fn=<MeanBackward0>), tensor(0.3215, grad_fn=<MeanBackward0>), tensor(0.3478, grad_fn=<MeanBackward0>), tensor(0.3508, grad_fn=<MeanBackward0>), tensor(0.3545, grad_fn=<MeanBackward0>), tensor(0.3630, grad_fn=<MeanBackward0>), tensor(0.3667, grad_fn=<MeanBackward0>), tensor(0.3750, grad_fn=<MeanBackward0>), tensor(0.3784, grad_fn=<MeanBackward0>), tensor(0.3756, grad_fn=<MeanBackward0>), tensor(0.3853, grad_fn=<MeanBackward0>), tensor(0.3877, grad_fn=<MeanBackward0>), tensor(0.3915, grad_fn=<MeanBackward0>), tensor(0.3971, grad_fn=<MeanBackward0>), tensor(0.4028, grad_fn=<MeanBackward0>), tensor(0.3977, grad_fn=<MeanBackward0>), tensor(0.4053, grad_fn=<MeanBackward0>), tensor(0.4089, grad_fn=<MeanBackward0>), tensor(0.4139, grad_fn=<MeanBackward0>), tensor(0.4125, grad_fn=<MeanBackward0>), tensor(0.4204, grad_fn=<MeanBackward0>), tensor(0.4201, grad_fn=<MeanBackward0>), tensor(0.4243, grad_fn=<MeanBackward0>), tensor(0.4300, grad_fn=<MeanBackward0>), tensor(0.4285, grad_fn=<MeanBackward0>), tensor(0.4337, grad_fn=<MeanBackward0>), tensor(0.4274, grad_fn=<MeanBackward0>), tensor(0.4301, grad_fn=<MeanBackward0>), tensor(0.4307, grad_fn=<MeanBackward0>), tensor(0.4410, grad_fn=<MeanBackward0>), tensor(0.4318, grad_fn=<MeanBackward0>), tensor(0.4274, grad_fn=<MeanBackward0>), tensor(0.4364, grad_fn=<MeanBackward0>), tensor(0.4383, grad_fn=<MeanBackward0>), tensor(0.4447, grad_fn=<MeanBackward0>), tensor(0.4421, grad_fn=<MeanBackward0>), tensor(0.4450, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2400, grad_fn=<MeanBackward0>), tensor(0.2629, grad_fn=<MeanBackward0>), tensor(0.2751, grad_fn=<MeanBackward0>), tensor(0.2840, grad_fn=<MeanBackward0>), tensor(0.2988, grad_fn=<MeanBackward0>), tensor(0.3061, grad_fn=<MeanBackward0>), tensor(0.3156, grad_fn=<MeanBackward0>), tensor(0.3217, grad_fn=<MeanBackward0>), tensor(0.3310, grad_fn=<MeanBackward0>), tensor(0.3293, grad_fn=<MeanBackward0>), tensor(0.3438, grad_fn=<MeanBackward0>), tensor(0.3449, grad_fn=<MeanBackward0>), tensor(0.3509, grad_fn=<MeanBackward0>), tensor(0.3545, grad_fn=<MeanBackward0>), tensor(0.3651, grad_fn=<MeanBackward0>), tensor(0.3619, grad_fn=<MeanBackward0>), tensor(0.3728, grad_fn=<MeanBackward0>), tensor(0.3694, grad_fn=<MeanBackward0>), tensor(0.3741, grad_fn=<MeanBackward0>), tensor(0.3842, grad_fn=<MeanBackward0>), tensor(0.3748, grad_fn=<MeanBackward0>), tensor(0.3861, grad_fn=<MeanBackward0>), tensor(0.3884, grad_fn=<MeanBackward0>), tensor(0.3903, grad_fn=<MeanBackward0>), tensor(0.3929, grad_fn=<MeanBackward0>), tensor(0.3987, grad_fn=<MeanBackward0>), tensor(0.3978, grad_fn=<MeanBackward0>), tensor(0.3934, grad_fn=<MeanBackward0>), tensor(0.4003, grad_fn=<MeanBackward0>), tensor(0.4001, grad_fn=<MeanBackward0>), tensor(0.4018, grad_fn=<MeanBackward0>), tensor(0.4028, grad_fn=<MeanBackward0>), tensor(0.4077, grad_fn=<MeanBackward0>), tensor(0.4078, grad_fn=<MeanBackward0>), tensor(0.4100, grad_fn=<MeanBackward0>), tensor(0.4045, grad_fn=<MeanBackward0>), tensor(0.4079, grad_fn=<MeanBackward0>), tensor(0.4083, grad_fn=<MeanBackward0>), tensor(0.4107, grad_fn=<MeanBackward0>), tensor(0.4093, grad_fn=<MeanBackward0>), tensor(0.4118, grad_fn=<MeanBackward0>), tensor(0.4127, grad_fn=<MeanBackward0>), tensor(0.4127, grad_fn=<MeanBackward0>), tensor(0.4162, grad_fn=<MeanBackward0>), tensor(0.4170, grad_fn=<MeanBackward0>), tensor(0.4181, grad_fn=<MeanBackward0>), tensor(0.4171, grad_fn=<MeanBackward0>), tensor(0.4182, grad_fn=<MeanBackward0>), tensor(0.4154, grad_fn=<MeanBackward0>), tensor(0.4171, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.2533, grad_fn=<MeanBackward0>), tensor(0.2697, grad_fn=<MeanBackward0>), tensor(0.2828, grad_fn=<MeanBackward0>), tensor(0.2953, grad_fn=<MeanBackward0>), tensor(0.3106, grad_fn=<MeanBackward0>), tensor(0.3130, grad_fn=<MeanBackward0>), tensor(0.3203, grad_fn=<MeanBackward0>), tensor(0.3307, grad_fn=<MeanBackward0>), tensor(0.3361, grad_fn=<MeanBackward0>), tensor(0.3447, grad_fn=<MeanBackward0>), tensor(0.3455, grad_fn=<MeanBackward0>), tensor(0.3533, grad_fn=<MeanBackward0>), tensor(0.3571, grad_fn=<MeanBackward0>), tensor(0.3646, grad_fn=<MeanBackward0>), tensor(0.3643, grad_fn=<MeanBackward0>), tensor(0.3704, grad_fn=<MeanBackward0>), tensor(0.3717, grad_fn=<MeanBackward0>), tensor(0.3768, grad_fn=<MeanBackward0>), tensor(0.3785, grad_fn=<MeanBackward0>), tensor(0.3729, grad_fn=<MeanBackward0>), tensor(0.3822, grad_fn=<MeanBackward0>), tensor(0.3840, grad_fn=<MeanBackward0>), tensor(0.3840, grad_fn=<MeanBackward0>), tensor(0.3864, grad_fn=<MeanBackward0>), tensor(0.3874, grad_fn=<MeanBackward0>), tensor(0.3885, grad_fn=<MeanBackward0>), tensor(0.3884, grad_fn=<MeanBackward0>), tensor(0.3947, grad_fn=<MeanBackward0>), tensor(0.3873, grad_fn=<MeanBackward0>), tensor(0.3923, grad_fn=<MeanBackward0>), tensor(0.3943, grad_fn=<MeanBackward0>), tensor(0.3995, grad_fn=<MeanBackward0>), tensor(0.4051, grad_fn=<MeanBackward0>), tensor(0.4081, grad_fn=<MeanBackward0>), tensor(0.4097, grad_fn=<MeanBackward0>), tensor(0.4100, grad_fn=<MeanBackward0>), tensor(0.4090, grad_fn=<MeanBackward0>), tensor(0.4078, grad_fn=<MeanBackward0>), tensor(0.4068, grad_fn=<MeanBackward0>), tensor(0.4081, grad_fn=<MeanBackward0>), tensor(0.4164, grad_fn=<MeanBackward0>), tensor(0.4161, grad_fn=<MeanBackward0>), tensor(0.4180, grad_fn=<MeanBackward0>), tensor(0.4169, grad_fn=<MeanBackward0>), tensor(0.4167, grad_fn=<MeanBackward0>), tensor(0.4223, grad_fn=<MeanBackward0>), tensor(0.4190, grad_fn=<MeanBackward0>), tensor(0.4234, grad_fn=<MeanBackward0>), tensor(0.4284, grad_fn=<MeanBackward0>), tensor(0.4227, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.2454906739294529, 0.2665325850248337, 0.2800970785319805, 0.2909602001309395, 0.30386117100715637, 0.3106103986501694, 0.31699589639902115, 0.32405445724725723, 0.3309894800186157, 0.3359638899564743, 0.3413402736186981, 0.34832312911748886, 0.3520069941878319, 0.35779736936092377, 0.35757318139076233, 0.3658948317170143, 0.3700917661190033, 0.3706357926130295, 0.37521177530288696, 0.3778284266591072, 0.3800816610455513, 0.38372384011745453, 0.38514120131731033, 0.3881346136331558, 0.3897532746195793, 0.39339373260736465, 0.3938543051481247, 0.39653966575860977, 0.3956369459629059, 0.3976680561900139, 0.4005490764975548, 0.4042789340019226, 0.40662676841020584, 0.4085003361105919, 0.40985623747110367, 0.409788616001606, 0.41266097128391266, 0.41164247691631317, 0.4135297164320946, 0.4120713099837303, 0.41517142951488495, 0.4168693497776985, 0.41832195222377777, 0.41806602478027344, 0.4177006408572197, 0.4211563691496849, 0.42156411707401276, 0.42374276369810104, 0.4237314239144325, 0.4237944260239601]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}