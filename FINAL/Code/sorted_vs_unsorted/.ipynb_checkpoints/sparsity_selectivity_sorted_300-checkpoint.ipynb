{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/sparsity_selectivity_sorted_300.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7STrWa0P3z_",
    "outputId": "c1b844c3-e423-4b7b-8d30-90f7be846b04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIyKF1HE7uQW",
    "outputId": "333bbdfa-1b54-4404-a4e1-abd0ba5add9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-18 10:53:34--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
      "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
      "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
      "--2021-03-18 10:53:34--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
      "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-gzip]\n",
      "Saving to: ‘MNIST.tar.gz’\n",
      "\n",
      "MNIST.tar.gz            [      <=>           ]  33.20M  33.0MB/s    in 1.0s    \n",
      "\n",
      "2021-03-18 10:53:35 (33.0 MB/s) - ‘MNIST.tar.gz’ saved [34813078]\n",
      "\n",
      "MNIST/\n",
      "MNIST/raw/\n",
      "MNIST/raw/train-labels-idx1-ubyte\n",
      "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "MNIST/raw/t10k-labels-idx1-ubyte\n",
      "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "MNIST/raw/train-images-idx3-ubyte\n",
      "MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "MNIST/raw/t10k-images-idx3-ubyte\n",
      "MNIST/raw/train-images-idx3-ubyte.gz\n",
      "MNIST/processed/\n",
      "MNIST/processed/training.pt\n",
      "MNIST/processed/test.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./\n",
       "    Split: Train"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "!tar -zxvf MNIST.tar.gz\n",
    "\n",
    "root_dir = './'\n",
    "torchvision.datasets.MNIST(root=root_dir,download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z4j9WoP-UnAm"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ApOU7hvb95W4"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rTW5TOUnP5XY"
   },
   "outputs": [],
   "source": [
    "mnist_trainset = torchvision.datasets.MNIST(root=root_dir, train=True, \n",
    "                                download=True, \n",
    "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "mnist_testset  = torchvision.datasets.MNIST(root=root_dir, \n",
    "                                train=False, \n",
    "                                download=True, \n",
    "                                transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ow9Sy2SyUYgn"
   },
   "outputs": [],
   "source": [
    "new_mnist_trainset =  [ [[],[]] for i in range(10)]\n",
    "# new_mnist_testset  =  [ [[],[]] for i in range(10)]\n",
    "\n",
    "for i in range(60000):\n",
    "    for j in range(10):\n",
    "        if mnist_trainset[i][1] == j:\n",
    "            # image \n",
    "            new_mnist_trainset[j][0].append(mnist_trainset[i][0])  \n",
    "            # label\n",
    "            new_mnist_trainset[j][1].append(mnist_trainset[i][1])\n",
    "\n",
    "# for i in range(10000):\n",
    "#     for j in range(10):\n",
    "#         if mnist_testset[i][1] == j:\n",
    "#             # image \n",
    "#             new_mnist_testset[j][0].append(mnist_testset[i][0])  \n",
    "#             # label\n",
    "#             new_mnist_testset[j][1].append(mnist_testset[i][1])\n",
    "\n",
    "image_trainset = list()\n",
    "label_trainset = list()\n",
    "\n",
    "# image_testset = list()\n",
    "# label_testset = list()\n",
    "\n",
    "for i in range(10):\n",
    "    image_trainset.append(new_mnist_trainset[i][0])\n",
    "    label_trainset.append(new_mnist_trainset[i][1])\n",
    "\n",
    "# for i in range(10):\n",
    "#     image_testset.append(new_mnist_testset[i][0])\n",
    "#     label_testset.append(new_mnist_testset[i][1])\n",
    "\n",
    "flattened_image_train = list()\n",
    "flattened_label_train = list()\n",
    "\n",
    "# flattened_image_test = list()\n",
    "# flattened_label_test = list()\n",
    "\n",
    "# flattening image \n",
    "for sublist in image_trainset:\n",
    "    for val in sublist:\n",
    "        flattened_image_train.append(val)\n",
    "\n",
    "# flattening label\n",
    "for sublist in label_trainset:\n",
    "    for val in sublist:\n",
    "        flattened_label_train.append(val)\n",
    "\n",
    "# # flattening image \n",
    "# for sublist in image_testset:\n",
    "#     for val in sublist:\n",
    "#         flattened_image_test.append(val)\n",
    "\n",
    "# # flattening label\n",
    "# for sublist in label_testset:\n",
    "#     for val in sublist:\n",
    "#         flattened_label_test.append(val)\n",
    "\n",
    "flattened_image_train = torch.stack(flattened_image_train)\n",
    "flattened_label_train = torch.Tensor(flattened_label_train)\n",
    "flattened_label_train = flattened_label_train.type(torch.LongTensor)\n",
    "\n",
    "# flattened_image_test = torch.stack(flattened_image_test)\n",
    "# flattened_label_test = torch.Tensor(flattened_label_test)\n",
    "# flattened_label_test = flattened_label_test.type(torch.LongTensor)\n",
    "\n",
    "train_dataset = TensorDataset(flattened_image_train, flattened_label_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=50)\n",
    "\n",
    "# test_dataset = TensorDataset(flattened_image_test, flattened_label_test)\n",
    "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
    "                                               batch_size=50, \n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IXTkEUJ5P6kU"
   },
   "outputs": [],
   "source": [
    "# Define the model \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear_1 = torch.nn.Linear(784, 256)\n",
    "        self.linear_2 = torch.nn.Linear(256, 10)\n",
    "        self.sigmoid  = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        pred = self.linear_2(x)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BfgvKH6eP9Ou"
   },
   "outputs": [],
   "source": [
    "def get_activation(model):    \n",
    "    def hook(module, input, output):\n",
    "        model.layer_activations = output\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3uAD1thJ5JvD"
   },
   "outputs": [],
   "source": [
    "def selectivity(hidden_layer_each_neuron):\n",
    "    __selectivity__ = list()\n",
    "    # I will now try to find the average of each class for each neuron.\n",
    "    # check out the next cell \n",
    "    avg_activations = [dict() for x in range(256)]\n",
    "    for i, neuron in enumerate(hidden_layer_each_neuron):\n",
    "        for k, v in neuron.items():\n",
    "            # v is the list of activations for hidden layer's neuron k \n",
    "            avg_activations[i][k] = sum(v) / float(len(v))\n",
    "\n",
    "    # generate 256 lists to get only values in avg_activations\n",
    "    only_activation_vals = [list() for x in range(256)]\n",
    "\n",
    "    # get only values from avg_activations\n",
    "    for i, avg_activation in enumerate(avg_activations):\n",
    "        for value in avg_activation.values():\n",
    "            only_activation_vals[i].append(value)\n",
    "\n",
    "    for activation_val in only_activation_vals:\n",
    "        # find u_max \n",
    "        u_max = np.max(activation_val)\n",
    "\n",
    "        # find u_minus_max \n",
    "        u_minus_max = (np.sum(activation_val) - u_max) / 9\n",
    "\n",
    "        # find selectivity \n",
    "        selectivity = (u_max - u_minus_max) / (u_max + u_minus_max)\n",
    "\n",
    "        # append selectivity value to selectivity\n",
    "        __selectivity__.append(selectivity)\n",
    "\n",
    "    avg_selectivity = np.average(__selectivity__)\n",
    "    std_selectivity = np.std(__selectivity__)\n",
    "                                 \n",
    "    return avg_selectivity, std_selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e9kATNPUz1cA"
   },
   "outputs": [],
   "source": [
    "def sparsity_calculator(final_spareness):\n",
    "    sparseness_list = list()\n",
    "    for single_epoch_spareness in final_spareness:\n",
    "\n",
    "        hidden_layer_activation_list = single_epoch_spareness\n",
    "        hidden_layer_activation_list = torch.stack(hidden_layer_activation_list)\n",
    "        layer_activations_list = torch.reshape(hidden_layer_activation_list, (10000, 256))\n",
    "\n",
    "        layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
    "        num_neurons = layer_activations_list.shape[1]\n",
    "        population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
    "        mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
    "\n",
    "        sparseness_list.append(mean_sparseness_per_epoch)\n",
    "\n",
    "    return sparseness_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BXOpwTXEQFKY"
   },
   "outputs": [],
   "source": [
    "no_epochs = 300\n",
    "def selectivity_trainer(optimizer, model):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    train_loss = list()\n",
    "    test_loss  = list()\n",
    "    test_acc   = list()\n",
    "\n",
    "    final_spareness = list()\n",
    "    \n",
    "    final_selectivity_avg_list = list()\n",
    "    final_selectivity_std_list = list()\n",
    "\n",
    "    best_test_loss = 1\n",
    "\n",
    "    for epoch in range(no_epochs):\n",
    "\n",
    "        _hidden_layer_each_neuron_ = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
    "        _hidden_layer_each_neuron_ = np.array(_hidden_layer_each_neuron_)\n",
    "\n",
    "        hidden_layer_activation_list = list()\n",
    "\n",
    "        total_train_loss = 0\n",
    "        total_test_loss = 0\n",
    "\n",
    "        # training\n",
    "        # set up training mode \n",
    "        model.train()\n",
    "\n",
    "        for itr, (images, labels) in enumerate(train_dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(images)\n",
    "\n",
    "            loss = criterion(pred, labels)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print/Append activation of the hidden layer \n",
    "            # print(model.layer_activations.shape)\n",
    "            # model.layer_activations\n",
    "\n",
    "        total_train_loss = total_train_loss / (itr + 1)\n",
    "        train_loss.append(total_train_loss)\n",
    "\n",
    "        # testing \n",
    "        # change to evaluation mode \n",
    "        model.eval()\n",
    "        total = 0\n",
    "        for itr, (images, labels) in enumerate(test_dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            pred = model(images)\n",
    "\n",
    "            loss = criterion(pred, labels)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            # we now need softmax because we are testing.\n",
    "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "            for i, p in enumerate(pred):\n",
    "                if labels[i] == torch.max(p.data, 0)[1]:\n",
    "                    total = total + 1\n",
    "\n",
    "            hidden_layer_activation_list.append(model.layer_activations)\n",
    "\n",
    "            \n",
    "            for activation, label in zip(model.layer_activations, labels):\n",
    "                # shape of activation and label: 256 and 1 \n",
    "                \n",
    "                # get the actual value of item. This is because label is now Tensor \n",
    "                label = label.item()\n",
    "\n",
    "                # this is not part of gradient calculcation \n",
    "                with torch.no_grad():\n",
    "                    activation = activation.numpy()\n",
    "\n",
    "                # for each image/label, append activation value of neuron \n",
    "                for i in range(256):    # number of neurons in hidden layer \n",
    "                    _hidden_layer_each_neuron_[i][label].append(activation[i])\n",
    "\n",
    "        avg_selectivity, std_selectivity = selectivity(_hidden_layer_each_neuron_)\n",
    "        \n",
    "        final_selectivity_avg_list.append(avg_selectivity)\n",
    "        final_selectivity_std_list.append(std_selectivity)\n",
    "\n",
    "        final_spareness.append(hidden_layer_activation_list)\n",
    "\n",
    "        # caculate accuracy \n",
    "        accuracy = total / len(mnist_testset)\n",
    "\n",
    "        # append accuracy here\n",
    "        test_acc.append(accuracy)\n",
    "\n",
    "        # append test loss here \n",
    "        total_test_loss = total_test_loss / (itr + 1)\n",
    "        test_loss.append(total_test_loss)\n",
    "\n",
    "        print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Test Loss: {:.8f}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, total_train_loss, total_test_loss, accuracy))\n",
    "\n",
    "    sparsity_list = sparsity_calculator(final_spareness)\n",
    "\n",
    "    average_sparsity = list()\n",
    "    for i in range(no_epochs):\n",
    "        average_sparsity.append( (sparsity_list[i].item()) / 1 )\n",
    "    # ***************** sparsity calculation ***************** #\n",
    "\n",
    "    print(\"average_sparsity:\", average_sparsity)\n",
    "\n",
    "    return test_acc, average_sparsity, final_selectivity_avg_list, final_selectivity_std_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WKq9qSgMADr"
   },
   "source": [
    "# AdaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4WytqcJRZxA"
   },
   "outputs": [],
   "source": [
    "model_Adadelta = Model()\n",
    "print(\"model_Adadelta:\", model_Adadelta)\n",
    "model_Adadelta.to(device)\n",
    "model_Adadelta.sigmoid.register_forward_hook(get_activation(model_Adadelta))\n",
    "optimizer_Adadelta = torch.optim.Adadelta(model_Adadelta.parameters(), lr=1.0)\n",
    "Adadelta_test_acc, sparsity, Adadelta_avg_selectivity_list, Adadelta_std_selectivity_list = selectivity_trainer(optimizer=optimizer_Adadelta, model=model_Adadelta)\n",
    "\n",
    "f = open(\"sorted_sparsity_selectivity_Adadelta.txt\", \"w\")\n",
    "f.write(str(0)+'\\n'+str(Adadelta_test_acc)+'\\n'+str(sparsity)+'\\n'+str(Adadelta_avg_selectivity_list)+'\\n'+str(Adadelta_std_selectivity_list)+'\\n\\n')\n",
    "f.close()\n",
    "\n",
    "!cp sorted_sparsity_selectivity_Adadelta.txt /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hXfQe4vMDKB"
   },
   "source": [
    "# AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vb-4TPM5MGuE",
    "outputId": "369cb95e-5ccd-42dd-bb8d-f27e2129c185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_Adagrad: Model(\n",
      "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Epoch: 1/300, Train Loss: 0.13042085, Test Loss: 7.76099038, Test Accuracy: 0.10090000\n",
      "\n",
      "Epoch: 2/300, Train Loss: 0.09620166, Test Loss: 5.50175063, Test Accuracy: 0.10090000\n",
      "\n",
      "Epoch: 3/300, Train Loss: 0.05869379, Test Loss: 4.44656155, Test Accuracy: 0.11530000\n",
      "\n",
      "Epoch: 4/300, Train Loss: 0.04409782, Test Loss: 4.14679881, Test Accuracy: 0.20030000\n",
      "\n",
      "Epoch: 5/300, Train Loss: 0.03957421, Test Loss: 3.77036811, Test Accuracy: 0.25950000\n",
      "\n",
      "Epoch: 6/300, Train Loss: 0.03484181, Test Loss: 3.69523552, Test Accuracy: 0.26800000\n",
      "\n",
      "Epoch: 7/300, Train Loss: 0.03376216, Test Loss: 3.49587876, Test Accuracy: 0.29890000\n",
      "\n",
      "Epoch: 8/300, Train Loss: 0.03238552, Test Loss: 3.25392950, Test Accuracy: 0.32170000\n",
      "\n",
      "Epoch: 9/300, Train Loss: 0.03214524, Test Loss: 3.15860635, Test Accuracy: 0.35740000\n",
      "\n",
      "Epoch: 10/300, Train Loss: 0.03076729, Test Loss: 3.01343359, Test Accuracy: 0.36730000\n",
      "\n",
      "Epoch: 11/300, Train Loss: 0.02971740, Test Loss: 2.92935478, Test Accuracy: 0.37980000\n",
      "\n",
      "Epoch: 12/300, Train Loss: 0.02846869, Test Loss: 2.87473154, Test Accuracy: 0.39610000\n",
      "\n",
      "Epoch: 13/300, Train Loss: 0.02802577, Test Loss: 2.76391892, Test Accuracy: 0.40360000\n",
      "\n",
      "Epoch: 14/300, Train Loss: 0.02684719, Test Loss: 2.57556621, Test Accuracy: 0.40900000\n",
      "\n",
      "Epoch: 15/300, Train Loss: 0.02638409, Test Loss: 2.66438078, Test Accuracy: 0.42210000\n",
      "\n",
      "Epoch: 16/300, Train Loss: 0.02517885, Test Loss: 2.38533365, Test Accuracy: 0.45060000\n",
      "\n",
      "Epoch: 17/300, Train Loss: 0.02460295, Test Loss: 2.38308190, Test Accuracy: 0.45140000\n",
      "\n",
      "Epoch: 18/300, Train Loss: 0.02362544, Test Loss: 2.22032111, Test Accuracy: 0.47330000\n",
      "\n",
      "Epoch: 19/300, Train Loss: 0.02412544, Test Loss: 2.14633688, Test Accuracy: 0.48190000\n",
      "\n",
      "Epoch: 20/300, Train Loss: 0.02321929, Test Loss: 2.10256313, Test Accuracy: 0.49210000\n",
      "\n",
      "Epoch: 21/300, Train Loss: 0.02187667, Test Loss: 2.09769102, Test Accuracy: 0.49130000\n",
      "\n",
      "Epoch: 22/300, Train Loss: 0.02092471, Test Loss: 1.97839403, Test Accuracy: 0.51720000\n",
      "\n",
      "Epoch: 23/300, Train Loss: 0.01979493, Test Loss: 2.18259132, Test Accuracy: 0.47460000\n",
      "\n",
      "Epoch: 24/300, Train Loss: 0.01873413, Test Loss: 2.06154087, Test Accuracy: 0.49160000\n",
      "\n",
      "Epoch: 25/300, Train Loss: 0.01873686, Test Loss: 1.86905992, Test Accuracy: 0.53150000\n",
      "\n",
      "Epoch: 26/300, Train Loss: 0.01885119, Test Loss: 1.79169919, Test Accuracy: 0.53660000\n",
      "\n",
      "Epoch: 27/300, Train Loss: 0.01802162, Test Loss: 1.72872766, Test Accuracy: 0.54580000\n",
      "\n",
      "Epoch: 28/300, Train Loss: 0.01735746, Test Loss: 1.82122364, Test Accuracy: 0.53080000\n",
      "\n",
      "Epoch: 29/300, Train Loss: 0.01647207, Test Loss: 1.73831293, Test Accuracy: 0.54620000\n",
      "\n",
      "Epoch: 30/300, Train Loss: 0.01533644, Test Loss: 1.93511470, Test Accuracy: 0.52380000\n",
      "\n",
      "Epoch: 31/300, Train Loss: 0.01494211, Test Loss: 1.64331095, Test Accuracy: 0.57580000\n",
      "\n",
      "Epoch: 32/300, Train Loss: 0.01459872, Test Loss: 1.66620539, Test Accuracy: 0.56460000\n",
      "\n",
      "Epoch: 33/300, Train Loss: 0.01434371, Test Loss: 1.58220633, Test Accuracy: 0.57340000\n",
      "\n",
      "Epoch: 34/300, Train Loss: 0.01363854, Test Loss: 1.50581342, Test Accuracy: 0.59370000\n",
      "\n",
      "Epoch: 35/300, Train Loss: 0.01370832, Test Loss: 1.55081446, Test Accuracy: 0.58230000\n",
      "\n",
      "Epoch: 36/300, Train Loss: 0.01345523, Test Loss: 1.54433626, Test Accuracy: 0.58470000\n",
      "\n",
      "Epoch: 37/300, Train Loss: 0.01297670, Test Loss: 1.37382519, Test Accuracy: 0.62050000\n",
      "\n",
      "Epoch: 38/300, Train Loss: 0.01228041, Test Loss: 1.45171267, Test Accuracy: 0.60450000\n",
      "\n",
      "Epoch: 39/300, Train Loss: 0.01222946, Test Loss: 1.36928538, Test Accuracy: 0.61720000\n",
      "\n",
      "Epoch: 40/300, Train Loss: 0.01231905, Test Loss: 1.33747089, Test Accuracy: 0.62390000\n",
      "\n",
      "Epoch: 41/300, Train Loss: 0.01170529, Test Loss: 1.32143641, Test Accuracy: 0.62480000\n",
      "\n",
      "Epoch: 42/300, Train Loss: 0.01188117, Test Loss: 1.34352833, Test Accuracy: 0.62380000\n",
      "\n",
      "Epoch: 43/300, Train Loss: 0.01172292, Test Loss: 1.32569035, Test Accuracy: 0.63180000\n",
      "\n",
      "Epoch: 44/300, Train Loss: 0.01125272, Test Loss: 1.29471795, Test Accuracy: 0.64220000\n",
      "\n",
      "Epoch: 45/300, Train Loss: 0.01124346, Test Loss: 1.26548084, Test Accuracy: 0.64820000\n",
      "\n",
      "Epoch: 46/300, Train Loss: 0.01096534, Test Loss: 1.20977092, Test Accuracy: 0.66040000\n",
      "\n",
      "Epoch: 47/300, Train Loss: 0.01049442, Test Loss: 1.25165229, Test Accuracy: 0.65180000\n",
      "\n",
      "Epoch: 48/300, Train Loss: 0.01025322, Test Loss: 1.19805685, Test Accuracy: 0.66580000\n",
      "\n",
      "Epoch: 49/300, Train Loss: 0.00986333, Test Loss: 1.16457821, Test Accuracy: 0.67620000\n",
      "\n",
      "Epoch: 50/300, Train Loss: 0.00947773, Test Loss: 1.15761507, Test Accuracy: 0.67930000\n",
      "\n",
      "Epoch: 51/300, Train Loss: 0.00953074, Test Loss: 1.11278713, Test Accuracy: 0.68860000\n",
      "\n",
      "Epoch: 52/300, Train Loss: 0.00968601, Test Loss: 1.07530797, Test Accuracy: 0.69930000\n",
      "\n",
      "Epoch: 53/300, Train Loss: 0.00946399, Test Loss: 1.07472581, Test Accuracy: 0.69960000\n",
      "\n",
      "Epoch: 54/300, Train Loss: 0.00935020, Test Loss: 1.04333489, Test Accuracy: 0.70950000\n",
      "\n",
      "Epoch: 55/300, Train Loss: 0.00907732, Test Loss: 1.05992463, Test Accuracy: 0.70940000\n",
      "\n",
      "Epoch: 56/300, Train Loss: 0.00861467, Test Loss: 1.02729248, Test Accuracy: 0.71540000\n",
      "\n",
      "Epoch: 57/300, Train Loss: 0.00897244, Test Loss: 1.02421078, Test Accuracy: 0.71770000\n",
      "\n",
      "Epoch: 58/300, Train Loss: 0.00876957, Test Loss: 1.01131935, Test Accuracy: 0.72200000\n",
      "\n",
      "Epoch: 59/300, Train Loss: 0.00871610, Test Loss: 0.98995184, Test Accuracy: 0.72830000\n",
      "\n",
      "Epoch: 60/300, Train Loss: 0.00842616, Test Loss: 0.96137593, Test Accuracy: 0.73550000\n",
      "\n",
      "Epoch: 61/300, Train Loss: 0.00858039, Test Loss: 1.01942259, Test Accuracy: 0.71900000\n",
      "\n",
      "Epoch: 62/300, Train Loss: 0.00785823, Test Loss: 1.01873871, Test Accuracy: 0.72400000\n",
      "\n",
      "Epoch: 63/300, Train Loss: 0.00791547, Test Loss: 1.11015782, Test Accuracy: 0.71440000\n",
      "\n",
      "Epoch: 64/300, Train Loss: 0.00762284, Test Loss: 0.92576526, Test Accuracy: 0.74690000\n",
      "\n",
      "Epoch: 65/300, Train Loss: 0.00764514, Test Loss: 0.94280597, Test Accuracy: 0.74530000\n",
      "\n",
      "Epoch: 66/300, Train Loss: 0.00742219, Test Loss: 0.91056663, Test Accuracy: 0.75130000\n",
      "\n",
      "Epoch: 67/300, Train Loss: 0.00738832, Test Loss: 0.85808745, Test Accuracy: 0.76100000\n",
      "\n",
      "Epoch: 68/300, Train Loss: 0.00746947, Test Loss: 0.83145397, Test Accuracy: 0.76580000\n",
      "\n",
      "Epoch: 69/300, Train Loss: 0.00716344, Test Loss: 0.78611450, Test Accuracy: 0.77960000\n",
      "\n",
      "Epoch: 70/300, Train Loss: 0.00727398, Test Loss: 0.80011565, Test Accuracy: 0.77690000\n",
      "\n",
      "Epoch: 71/300, Train Loss: 0.00681546, Test Loss: 0.78552982, Test Accuracy: 0.78090000\n",
      "\n",
      "Epoch: 72/300, Train Loss: 0.00671543, Test Loss: 0.77404677, Test Accuracy: 0.78390000\n",
      "\n",
      "Epoch: 73/300, Train Loss: 0.00676629, Test Loss: 0.76562429, Test Accuracy: 0.78600000\n",
      "\n",
      "Epoch: 74/300, Train Loss: 0.00648315, Test Loss: 0.74885307, Test Accuracy: 0.79120000\n",
      "\n",
      "Epoch: 75/300, Train Loss: 0.00626508, Test Loss: 0.72800527, Test Accuracy: 0.79760000\n",
      "\n",
      "Epoch: 76/300, Train Loss: 0.00633550, Test Loss: 0.72955972, Test Accuracy: 0.79510000\n",
      "\n",
      "Epoch: 77/300, Train Loss: 0.00633989, Test Loss: 0.71498365, Test Accuracy: 0.80030000\n",
      "\n",
      "Epoch: 78/300, Train Loss: 0.00619708, Test Loss: 0.71668830, Test Accuracy: 0.79970000\n",
      "\n",
      "Epoch: 79/300, Train Loss: 0.00584506, Test Loss: 0.75209644, Test Accuracy: 0.79380000\n",
      "\n",
      "Epoch: 80/300, Train Loss: 0.00601190, Test Loss: 0.67867461, Test Accuracy: 0.81230000\n",
      "\n",
      "Epoch: 81/300, Train Loss: 0.00595556, Test Loss: 0.67329090, Test Accuracy: 0.81320000\n",
      "\n",
      "Epoch: 82/300, Train Loss: 0.00602206, Test Loss: 0.64990164, Test Accuracy: 0.81960000\n",
      "\n",
      "Epoch: 83/300, Train Loss: 0.00601115, Test Loss: 0.63389716, Test Accuracy: 0.82260000\n",
      "\n",
      "Epoch: 84/300, Train Loss: 0.00562793, Test Loss: 0.60606259, Test Accuracy: 0.82940000\n",
      "\n",
      "Epoch: 85/300, Train Loss: 0.00550527, Test Loss: 0.62599370, Test Accuracy: 0.82330000\n",
      "\n",
      "Epoch: 86/300, Train Loss: 0.00548445, Test Loss: 0.60750849, Test Accuracy: 0.82800000\n",
      "\n",
      "Epoch: 87/300, Train Loss: 0.00535401, Test Loss: 0.60375681, Test Accuracy: 0.82940000\n",
      "\n",
      "Epoch: 88/300, Train Loss: 0.00539700, Test Loss: 0.58508024, Test Accuracy: 0.83350000\n",
      "\n",
      "Epoch: 89/300, Train Loss: 0.00540660, Test Loss: 0.57241656, Test Accuracy: 0.83640000\n",
      "\n",
      "Epoch: 90/300, Train Loss: 0.00538887, Test Loss: 0.55943047, Test Accuracy: 0.83960000\n",
      "\n",
      "Epoch: 91/300, Train Loss: 0.00535751, Test Loss: 0.54716940, Test Accuracy: 0.84360000\n",
      "\n",
      "Epoch: 92/300, Train Loss: 0.00531117, Test Loss: 0.53513717, Test Accuracy: 0.84570000\n",
      "\n",
      "Epoch: 93/300, Train Loss: 0.00520604, Test Loss: 0.51491230, Test Accuracy: 0.84890000\n",
      "\n",
      "Epoch: 94/300, Train Loss: 0.00506719, Test Loss: 0.52477501, Test Accuracy: 0.84790000\n",
      "\n",
      "Epoch: 95/300, Train Loss: 0.00496770, Test Loss: 0.51782445, Test Accuracy: 0.85000000\n",
      "\n",
      "Epoch: 96/300, Train Loss: 0.00498755, Test Loss: 0.50994487, Test Accuracy: 0.85150000\n",
      "\n",
      "Epoch: 97/300, Train Loss: 0.00504415, Test Loss: 0.49477730, Test Accuracy: 0.85500000\n",
      "\n",
      "Epoch: 98/300, Train Loss: 0.00496985, Test Loss: 0.49142737, Test Accuracy: 0.85600000\n",
      "\n",
      "Epoch: 99/300, Train Loss: 0.00466606, Test Loss: 0.47064376, Test Accuracy: 0.86140000\n",
      "\n",
      "Epoch: 100/300, Train Loss: 0.00477999, Test Loss: 0.47960171, Test Accuracy: 0.86020000\n",
      "\n",
      "Epoch: 101/300, Train Loss: 0.00472663, Test Loss: 0.47840605, Test Accuracy: 0.86160000\n",
      "\n",
      "Epoch: 102/300, Train Loss: 0.00464789, Test Loss: 0.46403569, Test Accuracy: 0.86520000\n",
      "\n",
      "Epoch: 103/300, Train Loss: 0.00449461, Test Loss: 0.45402470, Test Accuracy: 0.86780000\n",
      "\n",
      "Epoch: 104/300, Train Loss: 0.00446123, Test Loss: 0.44272996, Test Accuracy: 0.87120000\n",
      "\n",
      "Epoch: 105/300, Train Loss: 0.00447555, Test Loss: 0.43268069, Test Accuracy: 0.87520000\n",
      "\n",
      "Epoch: 106/300, Train Loss: 0.00446158, Test Loss: 0.44096361, Test Accuracy: 0.87490000\n",
      "\n",
      "Epoch: 107/300, Train Loss: 0.00441553, Test Loss: 0.42903774, Test Accuracy: 0.87810000\n",
      "\n",
      "Epoch: 108/300, Train Loss: 0.00425769, Test Loss: 0.42159224, Test Accuracy: 0.88010000\n",
      "\n",
      "Epoch: 109/300, Train Loss: 0.00417969, Test Loss: 0.41639524, Test Accuracy: 0.88190000\n",
      "\n",
      "Epoch: 110/300, Train Loss: 0.00412393, Test Loss: 0.40525510, Test Accuracy: 0.88520000\n",
      "\n",
      "Epoch: 111/300, Train Loss: 0.00408992, Test Loss: 0.39437307, Test Accuracy: 0.88820000\n",
      "\n",
      "Epoch: 112/300, Train Loss: 0.00404326, Test Loss: 0.38482204, Test Accuracy: 0.89170000\n",
      "\n",
      "Epoch: 113/300, Train Loss: 0.00400489, Test Loss: 0.37710167, Test Accuracy: 0.89470000\n",
      "\n",
      "Epoch: 114/300, Train Loss: 0.00396958, Test Loss: 0.37027512, Test Accuracy: 0.89590000\n",
      "\n",
      "Epoch: 115/300, Train Loss: 0.00393408, Test Loss: 0.36385451, Test Accuracy: 0.89820000\n",
      "\n",
      "Epoch: 116/300, Train Loss: 0.00389305, Test Loss: 0.35749212, Test Accuracy: 0.89910000\n",
      "\n",
      "Epoch: 117/300, Train Loss: 0.00370628, Test Loss: 0.36320788, Test Accuracy: 0.89890000\n",
      "\n",
      "Epoch: 118/300, Train Loss: 0.00375215, Test Loss: 0.34326485, Test Accuracy: 0.90360000\n",
      "\n",
      "Epoch: 119/300, Train Loss: 0.00373999, Test Loss: 0.35755934, Test Accuracy: 0.89980000\n",
      "\n",
      "Epoch: 120/300, Train Loss: 0.00363657, Test Loss: 0.36852599, Test Accuracy: 0.89700000\n",
      "\n",
      "Epoch: 121/300, Train Loss: 0.00353453, Test Loss: 0.36190548, Test Accuracy: 0.89840000\n",
      "\n",
      "Epoch: 122/300, Train Loss: 0.00350424, Test Loss: 0.35998179, Test Accuracy: 0.89920000\n",
      "\n",
      "Epoch: 123/300, Train Loss: 0.00346425, Test Loss: 0.36692988, Test Accuracy: 0.89960000\n",
      "\n",
      "Epoch: 124/300, Train Loss: 0.00338696, Test Loss: 0.35357516, Test Accuracy: 0.90250000\n",
      "\n",
      "Epoch: 125/300, Train Loss: 0.00338901, Test Loss: 0.34383269, Test Accuracy: 0.90490000\n",
      "\n",
      "Epoch: 126/300, Train Loss: 0.00338925, Test Loss: 0.33447553, Test Accuracy: 0.90730000\n",
      "\n",
      "Epoch: 127/300, Train Loss: 0.00338337, Test Loss: 0.32355288, Test Accuracy: 0.91070000\n",
      "\n",
      "Epoch: 128/300, Train Loss: 0.00337422, Test Loss: 0.31436928, Test Accuracy: 0.91330000\n",
      "\n",
      "Epoch: 129/300, Train Loss: 0.00335538, Test Loss: 0.30768986, Test Accuracy: 0.91530000\n",
      "\n",
      "Epoch: 130/300, Train Loss: 0.00332897, Test Loss: 0.30220257, Test Accuracy: 0.91710000\n",
      "\n",
      "Epoch: 131/300, Train Loss: 0.00330433, Test Loss: 0.29763007, Test Accuracy: 0.91800000\n",
      "\n",
      "Epoch: 132/300, Train Loss: 0.00312166, Test Loss: 0.29028400, Test Accuracy: 0.91990000\n",
      "\n",
      "Epoch: 133/300, Train Loss: 0.00324258, Test Loss: 0.28807324, Test Accuracy: 0.92060000\n",
      "\n",
      "Epoch: 134/300, Train Loss: 0.00322521, Test Loss: 0.28621984, Test Accuracy: 0.92120000\n",
      "\n",
      "Epoch: 135/300, Train Loss: 0.00320431, Test Loss: 0.28243049, Test Accuracy: 0.92250000\n",
      "\n",
      "Epoch: 136/300, Train Loss: 0.00317741, Test Loss: 0.27874839, Test Accuracy: 0.92430000\n",
      "\n",
      "Epoch: 137/300, Train Loss: 0.00314999, Test Loss: 0.27515423, Test Accuracy: 0.92540000\n",
      "\n",
      "Epoch: 138/300, Train Loss: 0.00312238, Test Loss: 0.27161994, Test Accuracy: 0.92590000\n",
      "\n",
      "Epoch: 139/300, Train Loss: 0.00309245, Test Loss: 0.26821965, Test Accuracy: 0.92670000\n",
      "\n",
      "Epoch: 140/300, Train Loss: 0.00303252, Test Loss: 0.26668079, Test Accuracy: 0.92750000\n",
      "\n",
      "Epoch: 141/300, Train Loss: 0.00295823, Test Loss: 0.27016540, Test Accuracy: 0.92630000\n",
      "\n",
      "Epoch: 142/300, Train Loss: 0.00291513, Test Loss: 0.26579888, Test Accuracy: 0.92790000\n",
      "\n",
      "Epoch: 143/300, Train Loss: 0.00290537, Test Loss: 0.26199773, Test Accuracy: 0.92940000\n",
      "\n",
      "Epoch: 144/300, Train Loss: 0.00289921, Test Loss: 0.25812348, Test Accuracy: 0.93120000\n",
      "\n",
      "Epoch: 145/300, Train Loss: 0.00290365, Test Loss: 0.25308148, Test Accuracy: 0.93220000\n",
      "\n",
      "Epoch: 146/300, Train Loss: 0.00289395, Test Loss: 0.24880794, Test Accuracy: 0.93280000\n",
      "\n",
      "Epoch: 147/300, Train Loss: 0.00287208, Test Loss: 0.24583293, Test Accuracy: 0.93390000\n",
      "\n",
      "Epoch: 148/300, Train Loss: 0.00285191, Test Loss: 0.24298626, Test Accuracy: 0.93500000\n",
      "\n",
      "Epoch: 149/300, Train Loss: 0.00283031, Test Loss: 0.24022764, Test Accuracy: 0.93600000\n",
      "\n",
      "Epoch: 150/300, Train Loss: 0.00280741, Test Loss: 0.23750777, Test Accuracy: 0.93650000\n",
      "\n",
      "Epoch: 151/300, Train Loss: 0.00278296, Test Loss: 0.23475543, Test Accuracy: 0.93720000\n",
      "\n",
      "Epoch: 152/300, Train Loss: 0.00275736, Test Loss: 0.23185842, Test Accuracy: 0.93770000\n",
      "\n",
      "Epoch: 153/300, Train Loss: 0.00273203, Test Loss: 0.22881818, Test Accuracy: 0.93850000\n",
      "\n",
      "Epoch: 154/300, Train Loss: 0.00270787, Test Loss: 0.22589315, Test Accuracy: 0.93940000\n",
      "\n",
      "Epoch: 155/300, Train Loss: 0.00268361, Test Loss: 0.22321171, Test Accuracy: 0.94010000\n",
      "\n",
      "Epoch: 156/300, Train Loss: 0.00265817, Test Loss: 0.22091732, Test Accuracy: 0.94100000\n",
      "\n",
      "Epoch: 157/300, Train Loss: 0.00260770, Test Loss: 0.22252168, Test Accuracy: 0.94050000\n",
      "\n",
      "Epoch: 158/300, Train Loss: 0.00254918, Test Loss: 0.21954593, Test Accuracy: 0.94100000\n",
      "\n",
      "Epoch: 159/300, Train Loss: 0.00250722, Test Loss: 0.21654679, Test Accuracy: 0.94200000\n",
      "\n",
      "Epoch: 160/300, Train Loss: 0.00249247, Test Loss: 0.21389804, Test Accuracy: 0.94240000\n",
      "\n",
      "Epoch: 161/300, Train Loss: 0.00247776, Test Loss: 0.21138673, Test Accuracy: 0.94310000\n",
      "\n",
      "Epoch: 162/300, Train Loss: 0.00246189, Test Loss: 0.20900400, Test Accuracy: 0.94400000\n",
      "\n",
      "Epoch: 163/300, Train Loss: 0.00244502, Test Loss: 0.20675333, Test Accuracy: 0.94470000\n",
      "\n",
      "Epoch: 164/300, Train Loss: 0.00242730, Test Loss: 0.20464036, Test Accuracy: 0.94520000\n",
      "\n",
      "Epoch: 165/300, Train Loss: 0.00240884, Test Loss: 0.20265699, Test Accuracy: 0.94580000\n",
      "\n",
      "Epoch: 166/300, Train Loss: 0.00239000, Test Loss: 0.20076234, Test Accuracy: 0.94610000\n",
      "\n",
      "Epoch: 167/300, Train Loss: 0.00237127, Test Loss: 0.19892321, Test Accuracy: 0.94640000\n",
      "\n",
      "Epoch: 168/300, Train Loss: 0.00235255, Test Loss: 0.19712578, Test Accuracy: 0.94710000\n",
      "\n",
      "Epoch: 169/300, Train Loss: 0.00233364, Test Loss: 0.19535466, Test Accuracy: 0.94760000\n",
      "\n",
      "Epoch: 170/300, Train Loss: 0.00231451, Test Loss: 0.19359331, Test Accuracy: 0.94810000\n",
      "\n",
      "Epoch: 171/300, Train Loss: 0.00229524, Test Loss: 0.19182729, Test Accuracy: 0.94870000\n",
      "\n",
      "Epoch: 172/300, Train Loss: 0.00227593, Test Loss: 0.19004197, Test Accuracy: 0.94910000\n",
      "\n",
      "Epoch: 173/300, Train Loss: 0.00225666, Test Loss: 0.18823489, Test Accuracy: 0.94940000\n",
      "\n",
      "Epoch: 174/300, Train Loss: 0.00223739, Test Loss: 0.18643034, Test Accuracy: 0.95010000\n",
      "\n",
      "Epoch: 175/300, Train Loss: 0.00221805, Test Loss: 0.18466010, Test Accuracy: 0.95050000\n",
      "\n",
      "Epoch: 176/300, Train Loss: 0.00219865, Test Loss: 0.18294195, Test Accuracy: 0.95110000\n",
      "\n",
      "Epoch: 177/300, Train Loss: 0.00217923, Test Loss: 0.18127911, Test Accuracy: 0.95160000\n",
      "\n",
      "Epoch: 178/300, Train Loss: 0.00215979, Test Loss: 0.17966778, Test Accuracy: 0.95190000\n",
      "\n",
      "Epoch: 179/300, Train Loss: 0.00214036, Test Loss: 0.17810437, Test Accuracy: 0.95230000\n",
      "\n",
      "Epoch: 180/300, Train Loss: 0.00212099, Test Loss: 0.17658872, Test Accuracy: 0.95260000\n",
      "\n",
      "Epoch: 181/300, Train Loss: 0.00210180, Test Loss: 0.17511841, Test Accuracy: 0.95330000\n",
      "\n",
      "Epoch: 182/300, Train Loss: 0.00208272, Test Loss: 0.17368744, Test Accuracy: 0.95380000\n",
      "\n",
      "Epoch: 183/300, Train Loss: 0.00206369, Test Loss: 0.17229192, Test Accuracy: 0.95420000\n",
      "\n",
      "Epoch: 184/300, Train Loss: 0.00204468, Test Loss: 0.17093046, Test Accuracy: 0.95440000\n",
      "\n",
      "Epoch: 185/300, Train Loss: 0.00202573, Test Loss: 0.16960000, Test Accuracy: 0.95480000\n",
      "\n",
      "Epoch: 186/300, Train Loss: 0.00200689, Test Loss: 0.16829534, Test Accuracy: 0.95560000\n",
      "\n",
      "Epoch: 187/300, Train Loss: 0.00198814, Test Loss: 0.16701486, Test Accuracy: 0.95600000\n",
      "\n",
      "Epoch: 188/300, Train Loss: 0.00196936, Test Loss: 0.16576349, Test Accuracy: 0.95620000\n",
      "\n",
      "Epoch: 189/300, Train Loss: 0.00195064, Test Loss: 0.16454725, Test Accuracy: 0.95690000\n",
      "\n",
      "Epoch: 190/300, Train Loss: 0.00193212, Test Loss: 0.16337366, Test Accuracy: 0.95730000\n",
      "\n",
      "Epoch: 191/300, Train Loss: 0.00191361, Test Loss: 0.16225128, Test Accuracy: 0.95740000\n",
      "\n",
      "Epoch: 192/300, Train Loss: 0.00188805, Test Loss: 0.16103283, Test Accuracy: 0.95780000\n",
      "\n",
      "Epoch: 193/300, Train Loss: 0.00182053, Test Loss: 0.16240691, Test Accuracy: 0.95730000\n",
      "\n",
      "Epoch: 194/300, Train Loss: 0.00184484, Test Loss: 0.15890193, Test Accuracy: 0.95880000\n",
      "\n",
      "Epoch: 195/300, Train Loss: 0.00183557, Test Loss: 0.15812229, Test Accuracy: 0.95910000\n",
      "\n",
      "Epoch: 196/300, Train Loss: 0.00183170, Test Loss: 0.15703421, Test Accuracy: 0.95950000\n",
      "\n",
      "Epoch: 197/300, Train Loss: 0.00181421, Test Loss: 0.15600859, Test Accuracy: 0.95970000\n",
      "\n",
      "Epoch: 198/300, Train Loss: 0.00179629, Test Loss: 0.15499804, Test Accuracy: 0.96050000\n",
      "\n",
      "Epoch: 199/300, Train Loss: 0.00177870, Test Loss: 0.15398303, Test Accuracy: 0.96080000\n",
      "\n",
      "Epoch: 200/300, Train Loss: 0.00176205, Test Loss: 0.15295250, Test Accuracy: 0.96080000\n",
      "\n",
      "Epoch: 201/300, Train Loss: 0.00174505, Test Loss: 0.15196965, Test Accuracy: 0.96130000\n",
      "\n",
      "Epoch: 202/300, Train Loss: 0.00172678, Test Loss: 0.15105845, Test Accuracy: 0.96160000\n",
      "\n",
      "Epoch: 203/300, Train Loss: 0.00170964, Test Loss: 0.15020755, Test Accuracy: 0.96240000\n",
      "\n",
      "Epoch: 204/300, Train Loss: 0.00169405, Test Loss: 0.14942264, Test Accuracy: 0.96250000\n",
      "\n",
      "Epoch: 205/300, Train Loss: 0.00167413, Test Loss: 0.14888482, Test Accuracy: 0.96230000\n",
      "\n",
      "Epoch: 206/300, Train Loss: 0.00165043, Test Loss: 0.14742553, Test Accuracy: 0.96200000\n",
      "\n",
      "Epoch: 207/300, Train Loss: 0.00165028, Test Loss: 0.14673898, Test Accuracy: 0.96280000\n",
      "\n",
      "Epoch: 208/300, Train Loss: 0.00163618, Test Loss: 0.14597542, Test Accuracy: 0.96290000\n",
      "\n",
      "Epoch: 209/300, Train Loss: 0.00162074, Test Loss: 0.14517076, Test Accuracy: 0.96300000\n",
      "\n",
      "Epoch: 210/300, Train Loss: 0.00160496, Test Loss: 0.14438297, Test Accuracy: 0.96310000\n",
      "\n",
      "Epoch: 211/300, Train Loss: 0.00158973, Test Loss: 0.14360974, Test Accuracy: 0.96350000\n",
      "\n",
      "Epoch: 212/300, Train Loss: 0.00157501, Test Loss: 0.14285064, Test Accuracy: 0.96350000\n",
      "\n",
      "Epoch: 213/300, Train Loss: 0.00156072, Test Loss: 0.14210347, Test Accuracy: 0.96350000\n",
      "\n",
      "Epoch: 214/300, Train Loss: 0.00154656, Test Loss: 0.14137099, Test Accuracy: 0.96370000\n",
      "\n",
      "Epoch: 215/300, Train Loss: 0.00153229, Test Loss: 0.14065986, Test Accuracy: 0.96370000\n",
      "\n",
      "Epoch: 216/300, Train Loss: 0.00151810, Test Loss: 0.13996755, Test Accuracy: 0.96370000\n",
      "\n",
      "Epoch: 217/300, Train Loss: 0.00150408, Test Loss: 0.13928980, Test Accuracy: 0.96400000\n",
      "\n",
      "Epoch: 218/300, Train Loss: 0.00149020, Test Loss: 0.13862327, Test Accuracy: 0.96440000\n",
      "\n",
      "Epoch: 219/300, Train Loss: 0.00147645, Test Loss: 0.13796412, Test Accuracy: 0.96450000\n",
      "\n",
      "Epoch: 220/300, Train Loss: 0.00146277, Test Loss: 0.13730910, Test Accuracy: 0.96490000\n",
      "\n",
      "Epoch: 221/300, Train Loss: 0.00144911, Test Loss: 0.13665791, Test Accuracy: 0.96550000\n",
      "\n",
      "Epoch: 222/300, Train Loss: 0.00143542, Test Loss: 0.13601381, Test Accuracy: 0.96560000\n",
      "\n",
      "Epoch: 223/300, Train Loss: 0.00142168, Test Loss: 0.13538412, Test Accuracy: 0.96580000\n",
      "\n",
      "Epoch: 224/300, Train Loss: 0.00140775, Test Loss: 0.13478407, Test Accuracy: 0.96580000\n",
      "\n",
      "Epoch: 225/300, Train Loss: 0.00139007, Test Loss: 0.13433183, Test Accuracy: 0.96600000\n",
      "\n",
      "Epoch: 226/300, Train Loss: 0.00134842, Test Loss: 0.13612221, Test Accuracy: 0.96600000\n",
      "\n",
      "Epoch: 227/300, Train Loss: 0.00131826, Test Loss: 0.13521362, Test Accuracy: 0.96610000\n",
      "\n",
      "Epoch: 228/300, Train Loss: 0.00131118, Test Loss: 0.13445873, Test Accuracy: 0.96620000\n",
      "\n",
      "Epoch: 229/300, Train Loss: 0.00130349, Test Loss: 0.13376047, Test Accuracy: 0.96620000\n",
      "\n",
      "Epoch: 230/300, Train Loss: 0.00129516, Test Loss: 0.13308298, Test Accuracy: 0.96660000\n",
      "\n",
      "Epoch: 231/300, Train Loss: 0.00128625, Test Loss: 0.13241540, Test Accuracy: 0.96690000\n",
      "\n",
      "Epoch: 232/300, Train Loss: 0.00127690, Test Loss: 0.13175270, Test Accuracy: 0.96720000\n",
      "\n",
      "Epoch: 233/300, Train Loss: 0.00126725, Test Loss: 0.13108822, Test Accuracy: 0.96750000\n",
      "\n",
      "Epoch: 234/300, Train Loss: 0.00125748, Test Loss: 0.13040615, Test Accuracy: 0.96780000\n",
      "\n",
      "Epoch: 235/300, Train Loss: 0.00124789, Test Loss: 0.12961716, Test Accuracy: 0.96780000\n",
      "\n",
      "Epoch: 236/300, Train Loss: 0.00123850, Test Loss: 0.12910362, Test Accuracy: 0.96810000\n",
      "\n",
      "Epoch: 237/300, Train Loss: 0.00122432, Test Loss: 0.12859898, Test Accuracy: 0.96810000\n",
      "\n",
      "Epoch: 238/300, Train Loss: 0.00121230, Test Loss: 0.12808984, Test Accuracy: 0.96820000\n",
      "\n",
      "Epoch: 239/300, Train Loss: 0.00120109, Test Loss: 0.12758701, Test Accuracy: 0.96830000\n",
      "\n",
      "Epoch: 240/300, Train Loss: 0.00119062, Test Loss: 0.12709766, Test Accuracy: 0.96860000\n",
      "\n",
      "Epoch: 241/300, Train Loss: 0.00118060, Test Loss: 0.12661739, Test Accuracy: 0.96880000\n",
      "\n",
      "Epoch: 242/300, Train Loss: 0.00117063, Test Loss: 0.12614333, Test Accuracy: 0.96910000\n",
      "\n",
      "Epoch: 243/300, Train Loss: 0.00116062, Test Loss: 0.12567665, Test Accuracy: 0.96910000\n",
      "\n",
      "Epoch: 244/300, Train Loss: 0.00115046, Test Loss: 0.12522671, Test Accuracy: 0.96900000\n",
      "\n",
      "Epoch: 245/300, Train Loss: 0.00114023, Test Loss: 0.12479686, Test Accuracy: 0.96940000\n",
      "\n",
      "Epoch: 246/300, Train Loss: 0.00113009, Test Loss: 0.12438290, Test Accuracy: 0.96960000\n",
      "\n",
      "Epoch: 247/300, Train Loss: 0.00112012, Test Loss: 0.12398036, Test Accuracy: 0.96960000\n",
      "\n",
      "Epoch: 248/300, Train Loss: 0.00111033, Test Loss: 0.12358741, Test Accuracy: 0.96990000\n",
      "\n",
      "Epoch: 249/300, Train Loss: 0.00110071, Test Loss: 0.12320503, Test Accuracy: 0.97020000\n",
      "\n",
      "Epoch: 250/300, Train Loss: 0.00109126, Test Loss: 0.12283688, Test Accuracy: 0.97020000\n",
      "\n",
      "Epoch: 251/300, Train Loss: 0.00108209, Test Loss: 0.12249007, Test Accuracy: 0.97030000\n",
      "\n",
      "Epoch: 252/300, Train Loss: 0.00107347, Test Loss: 0.12217733, Test Accuracy: 0.97010000\n",
      "\n",
      "Epoch: 253/300, Train Loss: 0.00106533, Test Loss: 0.12201302, Test Accuracy: 0.97020000\n",
      "\n",
      "Epoch: 254/300, Train Loss: 0.00105894, Test Loss: 0.12168664, Test Accuracy: 0.97020000\n",
      "\n",
      "Epoch: 255/300, Train Loss: 0.00105046, Test Loss: 0.12116530, Test Accuracy: 0.97030000\n",
      "\n",
      "Epoch: 256/300, Train Loss: 0.00103913, Test Loss: 0.12079646, Test Accuracy: 0.97060000\n",
      "\n",
      "Epoch: 257/300, Train Loss: 0.00102788, Test Loss: 0.12054901, Test Accuracy: 0.97050000\n",
      "\n",
      "Epoch: 258/300, Train Loss: 0.00101880, Test Loss: 0.12025818, Test Accuracy: 0.97060000\n",
      "\n",
      "Epoch: 259/300, Train Loss: 0.00101022, Test Loss: 0.11995710, Test Accuracy: 0.97080000\n",
      "\n",
      "Epoch: 260/300, Train Loss: 0.00100183, Test Loss: 0.11965626, Test Accuracy: 0.97110000\n",
      "\n",
      "Epoch: 261/300, Train Loss: 0.00099355, Test Loss: 0.11936190, Test Accuracy: 0.97120000\n",
      "\n",
      "Epoch: 262/300, Train Loss: 0.00098535, Test Loss: 0.11907594, Test Accuracy: 0.97130000\n",
      "\n",
      "Epoch: 263/300, Train Loss: 0.00097719, Test Loss: 0.11879763, Test Accuracy: 0.97140000\n",
      "\n",
      "Epoch: 264/300, Train Loss: 0.00096910, Test Loss: 0.11852686, Test Accuracy: 0.97150000\n",
      "\n",
      "Epoch: 265/300, Train Loss: 0.00096108, Test Loss: 0.11826423, Test Accuracy: 0.97160000\n",
      "\n",
      "Epoch: 266/300, Train Loss: 0.00095317, Test Loss: 0.11801153, Test Accuracy: 0.97180000\n",
      "\n",
      "Epoch: 267/300, Train Loss: 0.00094541, Test Loss: 0.11777309, Test Accuracy: 0.97170000\n",
      "\n",
      "Epoch: 268/300, Train Loss: 0.00093785, Test Loss: 0.11755820, Test Accuracy: 0.97170000\n",
      "\n",
      "Epoch: 269/300, Train Loss: 0.00093052, Test Loss: 0.11737724, Test Accuracy: 0.97160000\n",
      "\n",
      "Epoch: 270/300, Train Loss: 0.00092356, Test Loss: 0.11722214, Test Accuracy: 0.97170000\n",
      "\n",
      "Epoch: 271/300, Train Loss: 0.00091648, Test Loss: 0.11706819, Test Accuracy: 0.97180000\n",
      "\n",
      "Epoch: 272/300, Train Loss: 0.00090910, Test Loss: 0.11689296, Test Accuracy: 0.97190000\n",
      "\n",
      "Epoch: 273/300, Train Loss: 0.00090162, Test Loss: 0.11668602, Test Accuracy: 0.97210000\n",
      "\n",
      "Epoch: 274/300, Train Loss: 0.00089403, Test Loss: 0.11645028, Test Accuracy: 0.97230000\n",
      "\n",
      "Epoch: 275/300, Train Loss: 0.00088639, Test Loss: 0.11619879, Test Accuracy: 0.97240000\n",
      "\n",
      "Epoch: 276/300, Train Loss: 0.00087873, Test Loss: 0.11593915, Test Accuracy: 0.97220000\n",
      "\n",
      "Epoch: 277/300, Train Loss: 0.00087074, Test Loss: 0.11571306, Test Accuracy: 0.97230000\n",
      "\n",
      "Epoch: 278/300, Train Loss: 0.00086283, Test Loss: 0.11553755, Test Accuracy: 0.97260000\n",
      "\n",
      "Epoch: 279/300, Train Loss: 0.00085540, Test Loss: 0.11537537, Test Accuracy: 0.97260000\n",
      "\n",
      "Epoch: 280/300, Train Loss: 0.00084819, Test Loss: 0.11521619, Test Accuracy: 0.97270000\n",
      "\n",
      "Epoch: 281/300, Train Loss: 0.00084111, Test Loss: 0.11505866, Test Accuracy: 0.97280000\n",
      "\n",
      "Epoch: 282/300, Train Loss: 0.00083415, Test Loss: 0.11490327, Test Accuracy: 0.97280000\n",
      "\n",
      "Epoch: 283/300, Train Loss: 0.00082731, Test Loss: 0.11475142, Test Accuracy: 0.97290000\n",
      "\n",
      "Epoch: 284/300, Train Loss: 0.00082058, Test Loss: 0.11460532, Test Accuracy: 0.97290000\n",
      "\n",
      "Epoch: 285/300, Train Loss: 0.00081397, Test Loss: 0.11446805, Test Accuracy: 0.97310000\n",
      "\n",
      "Epoch: 286/300, Train Loss: 0.00080747, Test Loss: 0.11434273, Test Accuracy: 0.97310000\n",
      "\n",
      "Epoch: 287/300, Train Loss: 0.00080109, Test Loss: 0.11423147, Test Accuracy: 0.97310000\n",
      "\n",
      "Epoch: 288/300, Train Loss: 0.00079483, Test Loss: 0.11413279, Test Accuracy: 0.97310000\n",
      "\n",
      "Epoch: 289/300, Train Loss: 0.00078866, Test Loss: 0.11404002, Test Accuracy: 0.97300000\n",
      "\n",
      "Epoch: 290/300, Train Loss: 0.00078255, Test Loss: 0.11394671, Test Accuracy: 0.97320000\n",
      "\n",
      "Epoch: 291/300, Train Loss: 0.00077646, Test Loss: 0.11385048, Test Accuracy: 0.97310000\n",
      "\n",
      "Epoch: 292/300, Train Loss: 0.00077035, Test Loss: 0.11375140, Test Accuracy: 0.97330000\n",
      "\n",
      "Epoch: 293/300, Train Loss: 0.00076423, Test Loss: 0.11364997, Test Accuracy: 0.97330000\n",
      "\n",
      "Epoch: 294/300, Train Loss: 0.00075811, Test Loss: 0.11354666, Test Accuracy: 0.97350000\n",
      "\n",
      "Epoch: 295/300, Train Loss: 0.00075199, Test Loss: 0.11344202, Test Accuracy: 0.97370000\n",
      "\n",
      "Epoch: 296/300, Train Loss: 0.00074589, Test Loss: 0.11333677, Test Accuracy: 0.97380000\n",
      "\n",
      "Epoch: 297/300, Train Loss: 0.00073984, Test Loss: 0.11323144, Test Accuracy: 0.97390000\n",
      "\n",
      "Epoch: 298/300, Train Loss: 0.00073383, Test Loss: 0.11312649, Test Accuracy: 0.97410000\n",
      "\n",
      "Epoch: 299/300, Train Loss: 0.00072787, Test Loss: 0.11302237, Test Accuracy: 0.97420000\n",
      "\n",
      "Epoch: 300/300, Train Loss: 0.00072195, Test Loss: 0.11291947, Test Accuracy: 0.97440000\n",
      "average_sparsity: [0.6329251527786255, 0.5800426006317139, 0.5371895432472229, 0.5136780142784119, 0.4975165128707886, 0.49420714378356934, 0.483918696641922, 0.4797728657722473, 0.47630518674850464, 0.4723302125930786, 0.4624687135219574, 0.45106610655784607, 0.446681946516037, 0.4532492160797119, 0.44412994384765625, 0.4393310546875, 0.43526527285575867, 0.4347837269306183, 0.4346848726272583, 0.43560555577278137, 0.4256210923194885, 0.41615238785743713, 0.4110124111175537, 0.407473087310791, 0.401856005191803, 0.3994583785533905, 0.4015134871006012, 0.3960047960281372, 0.39095020294189453, 0.38698214292526245, 0.3799575865268707, 0.37615394592285156, 0.37250638008117676, 0.3702300786972046, 0.3688688278198242, 0.3675880432128906, 0.36451256275177, 0.3581143915653229, 0.3578096330165863, 0.35792598128318787, 0.3577067255973816, 0.35747864842414856, 0.3520117402076721, 0.3509347140789032, 0.3510669469833374, 0.3484804332256317, 0.34657567739486694, 0.3474104702472687, 0.3446834683418274, 0.3389741778373718, 0.33865469694137573, 0.3385316729545593, 0.3354823887348175, 0.33495885133743286, 0.33174827694892883, 0.33149901032447815, 0.3312978446483612, 0.32841259241104126, 0.3303848206996918, 0.3299963176250458, 0.33571571111679077, 0.3280683755874634, 0.32581785321235657, 0.3227401375770569, 0.32318535447120667, 0.3203178644180298, 0.3223150074481964, 0.322074830532074, 0.3218233585357666, 0.32139918208122253, 0.3183645009994507, 0.3176368772983551, 0.3144789934158325, 0.31092485785484314, 0.31019797921180725, 0.3079433739185333, 0.3099178671836853, 0.30662718415260315, 0.30862924456596375, 0.30374711751937866, 0.30312925577163696, 0.30520907044410706, 0.30513641238212585, 0.3017880320549011, 0.2989051342010498, 0.29902100563049316, 0.29876405000686646, 0.29861342906951904, 0.29861295223236084, 0.2985597848892212, 0.2984481751918793, 0.2982657551765442, 0.2977338135242462, 0.29580575227737427, 0.29578524827957153, 0.29753902554512024, 0.29777494072914124, 0.2975296378135681, 0.2946055233478546, 0.2938266694545746, 0.29313576221466064, 0.2931009829044342, 0.29299381375312805, 0.29257911443710327, 0.2924576699733734, 0.2930850684642792, 0.2927292585372925, 0.29017600417137146, 0.2895648181438446, 0.28937798738479614, 0.28926995396614075, 0.28911593556404114, 0.2890186905860901, 0.2889164388179779, 0.28877881169319153, 0.28860634565353394, 0.2857294976711273, 0.2877962589263916, 0.28661736845970154, 0.2835734188556671, 0.2835598289966583, 0.2836005389690399, 0.28125548362731934, 0.2813369035720825, 0.2814788222312927, 0.2815857529640198, 0.28172969818115234, 0.28185272216796875, 0.28188493847846985, 0.2818755507469177, 0.28184637427330017, 0.28155258297920227, 0.2813793420791626, 0.28185009956359863, 0.2819526791572571, 0.2820298969745636, 0.2821059226989746, 0.2822025418281555, 0.28236231207847595, 0.2825937867164612, 0.2805379033088684, 0.28115156292915344, 0.2816048860549927, 0.28176257014274597, 0.2819075584411621, 0.28214892745018005, 0.2822803556919098, 0.2823411226272583, 0.28236210346221924, 0.28236180543899536, 0.2823522388935089, 0.2823474407196045, 0.2823549807071686, 0.28236016631126404, 0.28235870599746704, 0.28233492374420166, 0.28013545274734497, 0.27982181310653687, 0.27973631024360657, 0.279662549495697, 0.2796061933040619, 0.27955999970436096, 0.2795185148715973, 0.2794798016548157, 0.2794463038444519, 0.2794215679168701, 0.27940309047698975, 0.27938729524612427, 0.2793697714805603, 0.27934756875038147, 0.27932092547416687, 0.27929189801216125, 0.2792632579803467, 0.2792375981807709, 0.27921608090400696, 0.27919822931289673, 0.2791820168495178, 0.27916622161865234, 0.27915143966674805, 0.27914077043533325, 0.27913349866867065, 0.27912774682044983, 0.27912381291389465, 0.2791226804256439, 0.2791241407394409, 0.27912598848342896, 0.279127299785614, 0.2791295051574707, 0.27913275361061096, 0.2791350781917572, 0.2791379988193512, 0.2786766588687897, 0.2768457233905792, 0.27872467041015625, 0.27899008989334106, 0.2790187895298004, 0.27902716398239136, 0.27902358770370483, 0.27899980545043945, 0.2789387106895447, 0.27883267402648926, 0.27878299355506897, 0.2788452208042145, 0.27886074781417847, 0.2788531184196472, 0.2785644233226776, 0.27866947650909424, 0.27871590852737427, 0.278711199760437, 0.27869752049446106, 0.27868011593818665, 0.2786600887775421, 0.2786383330821991, 0.2786151170730591, 0.2785903811454773, 0.2785637080669403, 0.2785346508026123, 0.27850276231765747, 0.2784677743911743, 0.2784302830696106, 0.2783917784690857, 0.27835503220558167, 0.278323769569397, 0.2783028185367584, 0.27831172943115234, 0.27585816383361816, 0.2758345603942871, 0.27584293484687805, 0.2758660316467285, 0.275895893573761, 0.27592724561691284, 0.2759609818458557, 0.27600595355033875, 0.2760956287384033, 0.27647265791893005, 0.2760562598705292, 0.27585864067077637, 0.27573785185813904, 0.2756558656692505, 0.2755983769893646, 0.2755591571331024, 0.2755345106124878, 0.27552512288093567, 0.27552762627601624, 0.2755393981933594, 0.27555736899375916, 0.2755800187587738, 0.275608628988266, 0.2756476104259491, 0.27570709586143494, 0.2758244276046753, 0.27617037296295166, 0.276943176984787, 0.27706971764564514, 0.2763955593109131, 0.27570101618766785, 0.27551233768463135, 0.27545106410980225, 0.2754261791706085, 0.2754163146018982, 0.2754116356372833, 0.27541014552116394, 0.2754148542881012, 0.2754283547401428, 0.2754545509815216, 0.27550041675567627, 0.27557894587516785, 0.27570924162864685, 0.27589327096939087, 0.2761097550392151, 0.2762708365917206, 0.27632078528404236, 0.2762679159641266, 0.27612873911857605, 0.27593159675598145, 0.27569591999053955, 0.2754918336868286, 0.2753794491291046, 0.27532410621643066, 0.27529510855674744, 0.2752813398838043, 0.27527862787246704, 0.2752850651741028, 0.2753002941608429, 0.27532464265823364, 0.2753593623638153, 0.2754059433937073, 0.2754635214805603, 0.27552542090415955, 0.2755819857120514, 0.2756255269050598, 0.27565157413482666, 0.2756592631340027, 0.27565085887908936, 0.27562958002090454, 0.2755989134311676, 0.2755615711212158, 0.275519460439682, 0.27547386288642883, 0.2754257619380951]\n"
     ]
    }
   ],
   "source": [
    "model_Adagrad = Model()\n",
    "print(\"model_Adagrad:\", model_Adagrad)\n",
    "model_Adagrad.to(device)\n",
    "model_Adagrad.sigmoid.register_forward_hook(get_activation(model_Adagrad))\n",
    "optimizer_Adagrad = torch.optim.Adagrad(model_Adagrad.parameters(), lr=0.1)\n",
    "Adagrad_test_acc, sparsity, Adagrad_avg_selectivity_list, Adagrad_std_selectivity_list = selectivity_trainer(optimizer=optimizer_Adagrad, model=model_Adagrad)\n",
    "\n",
    "f = open(\"sorted_sparsity_selectivity_Adagrad.txt\", \"w\")\n",
    "f.write(str(0)+'\\n'+str(Adagrad_test_acc)+'\\n'+str(sparsity)+'\\n'+str(Adagrad_avg_selectivity_list)+'\\n'+str(Adagrad_std_selectivity_list)+'\\n\\n')\n",
    "f.close()\n",
    "\n",
    "!cp sorted_sparsity_selectivity_Adagrad.txt /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmLJ4Zr2MnoS"
   },
   "source": [
    "# SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ObsEJHuMoPy",
    "outputId": "475a4536-8238-4bc5-cb8a-376553ff6fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_SGD: Model(\n",
      "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Epoch: 1/300, Train Loss: 0.07130323, Test Loss: 8.03513433, Test Accuracy: 0.10090000\n",
      "\n",
      "Epoch: 2/300, Train Loss: 0.09522179, Test Loss: 7.73487459, Test Accuracy: 0.10090000\n",
      "\n",
      "Epoch: 3/300, Train Loss: 0.10906799, Test Loss: 7.38758298, Test Accuracy: 0.10090000\n",
      "\n",
      "Epoch: 4/300, Train Loss: 0.10622418, Test Loss: 6.70938220, Test Accuracy: 0.10090000\n",
      "\n",
      "Epoch: 5/300, Train Loss: 0.09005970, Test Loss: 5.86936565, Test Accuracy: 0.10090000\n",
      "\n",
      "Epoch: 6/300, Train Loss: 0.07472081, Test Loss: 5.22540113, Test Accuracy: 0.10560000\n",
      "\n",
      "Epoch: 7/300, Train Loss: 0.06555516, Test Loss: 4.80297432, Test Accuracy: 0.13070000\n",
      "\n",
      "Epoch: 8/300, Train Loss: 0.06068734, Test Loss: 4.50595841, Test Accuracy: 0.15850000\n",
      "\n",
      "Epoch: 9/300, Train Loss: 0.05786853, Test Loss: 4.27655988, Test Accuracy: 0.18010000\n",
      "\n",
      "Epoch: 10/300, Train Loss: 0.05595459, Test Loss: 4.09050645, Test Accuracy: 0.19700000\n",
      "\n",
      "Epoch: 11/300, Train Loss: 0.05448697, Test Loss: 3.93814013, Test Accuracy: 0.21050000\n",
      "\n",
      "Epoch: 12/300, Train Loss: 0.05329100, Test Loss: 3.81434305, Test Accuracy: 0.22320000\n",
      "\n",
      "Epoch: 13/300, Train Loss: 0.05229588, Test Loss: 3.71479845, Test Accuracy: 0.23260000\n",
      "\n",
      "Epoch: 14/300, Train Loss: 0.05146880, Test Loss: 3.63510793, Test Accuracy: 0.24150000\n",
      "\n",
      "Epoch: 15/300, Train Loss: 0.05078959, Test Loss: 3.57095001, Test Accuracy: 0.25070000\n",
      "\n",
      "Epoch: 16/300, Train Loss: 0.05023990, Test Loss: 3.51846012, Test Accuracy: 0.25590000\n",
      "\n",
      "Epoch: 17/300, Train Loss: 0.04979964, Test Loss: 3.47448181, Test Accuracy: 0.26170000\n",
      "\n",
      "Epoch: 18/300, Train Loss: 0.04944763, Test Loss: 3.43662055, Test Accuracy: 0.26750000\n",
      "\n",
      "Epoch: 19/300, Train Loss: 0.04916394, Test Loss: 3.40315063, Test Accuracy: 0.27130000\n",
      "\n",
      "Epoch: 20/300, Train Loss: 0.04893175, Test Loss: 3.37285097, Test Accuracy: 0.27440000\n",
      "\n",
      "Epoch: 21/300, Train Loss: 0.04873800, Test Loss: 3.34484273, Test Accuracy: 0.27750000\n",
      "\n",
      "Epoch: 22/300, Train Loss: 0.04857307, Test Loss: 3.31846617, Test Accuracy: 0.28000000\n",
      "\n",
      "Epoch: 23/300, Train Loss: 0.04842999, Test Loss: 3.29319012, Test Accuracy: 0.28420000\n",
      "\n",
      "Epoch: 24/300, Train Loss: 0.04830378, Test Loss: 3.26858349, Test Accuracy: 0.28690000\n",
      "\n",
      "Epoch: 25/300, Train Loss: 0.04819103, Test Loss: 3.24431689, Test Accuracy: 0.29050000\n",
      "\n",
      "Epoch: 26/300, Train Loss: 0.04808985, Test Loss: 3.22017794, Test Accuracy: 0.29280000\n",
      "\n",
      "Epoch: 27/300, Train Loss: 0.04799966, Test Loss: 3.19607327, Test Accuracy: 0.29660000\n",
      "\n",
      "Epoch: 28/300, Train Loss: 0.04792082, Test Loss: 3.17205355, Test Accuracy: 0.29890000\n",
      "\n",
      "Epoch: 29/300, Train Loss: 0.04785371, Test Loss: 3.14829312, Test Accuracy: 0.30300000\n",
      "\n",
      "Epoch: 30/300, Train Loss: 0.04779818, Test Loss: 3.12496361, Test Accuracy: 0.30610000\n",
      "\n",
      "Epoch: 31/300, Train Loss: 0.04775332, Test Loss: 3.10207924, Test Accuracy: 0.30950000\n",
      "\n",
      "Epoch: 32/300, Train Loss: 0.04771773, Test Loss: 3.07946187, Test Accuracy: 0.31220000\n",
      "\n",
      "Epoch: 33/300, Train Loss: 0.04768983, Test Loss: 3.05678703, Test Accuracy: 0.31450000\n",
      "\n",
      "Epoch: 34/300, Train Loss: 0.04766812, Test Loss: 3.03362777, Test Accuracy: 0.31820000\n",
      "\n",
      "Epoch: 35/300, Train Loss: 0.04765111, Test Loss: 3.00951381, Test Accuracy: 0.32120000\n",
      "\n",
      "Epoch: 36/300, Train Loss: 0.04763725, Test Loss: 2.98402416, Test Accuracy: 0.32440000\n",
      "\n",
      "Epoch: 37/300, Train Loss: 0.04762478, Test Loss: 2.95688782, Test Accuracy: 0.32880000\n",
      "\n",
      "Epoch: 38/300, Train Loss: 0.04761150, Test Loss: 2.92802707, Test Accuracy: 0.33190000\n",
      "\n",
      "Epoch: 39/300, Train Loss: 0.04759473, Test Loss: 2.89754081, Test Accuracy: 0.33670000\n",
      "\n",
      "Epoch: 40/300, Train Loss: 0.04757132, Test Loss: 2.86567552, Test Accuracy: 0.34200000\n",
      "\n",
      "Epoch: 41/300, Train Loss: 0.04753809, Test Loss: 2.83280488, Test Accuracy: 0.34650000\n",
      "\n",
      "Epoch: 42/300, Train Loss: 0.04749243, Test Loss: 2.79938381, Test Accuracy: 0.35090000\n",
      "\n",
      "Epoch: 43/300, Train Loss: 0.04743254, Test Loss: 2.76585199, Test Accuracy: 0.35460000\n",
      "\n",
      "Epoch: 44/300, Train Loss: 0.04735737, Test Loss: 2.73252565, Test Accuracy: 0.35900000\n",
      "\n",
      "Epoch: 45/300, Train Loss: 0.04726606, Test Loss: 2.69957533, Test Accuracy: 0.36340000\n",
      "\n",
      "Epoch: 46/300, Train Loss: 0.04715739, Test Loss: 2.66708705, Test Accuracy: 0.36700000\n",
      "\n",
      "Epoch: 47/300, Train Loss: 0.04702973, Test Loss: 2.63515150, Test Accuracy: 0.37170000\n",
      "\n",
      "Epoch: 48/300, Train Loss: 0.04688169, Test Loss: 2.60390148, Test Accuracy: 0.37500000\n",
      "\n",
      "Epoch: 49/300, Train Loss: 0.04671313, Test Loss: 2.57343762, Test Accuracy: 0.37920000\n",
      "\n",
      "Epoch: 50/300, Train Loss: 0.04652556, Test Loss: 2.54370155, Test Accuracy: 0.38290000\n",
      "\n",
      "Epoch: 51/300, Train Loss: 0.04632182, Test Loss: 2.51443062, Test Accuracy: 0.38650000\n",
      "\n",
      "Epoch: 52/300, Train Loss: 0.04610526, Test Loss: 2.48527803, Test Accuracy: 0.38910000\n",
      "\n",
      "Epoch: 53/300, Train Loss: 0.04587902, Test Loss: 2.45596111, Test Accuracy: 0.39380000\n",
      "\n",
      "Epoch: 54/300, Train Loss: 0.04564554, Test Loss: 2.42631030, Test Accuracy: 0.39870000\n",
      "\n",
      "Epoch: 55/300, Train Loss: 0.04540685, Test Loss: 2.39619884, Test Accuracy: 0.40270000\n",
      "\n",
      "Epoch: 56/300, Train Loss: 0.04516593, Test Loss: 2.36543234, Test Accuracy: 0.40830000\n",
      "\n",
      "Epoch: 57/300, Train Loss: 0.04492753, Test Loss: 2.33364880, Test Accuracy: 0.41510000\n",
      "\n",
      "Epoch: 58/300, Train Loss: 0.04469586, Test Loss: 2.30040620, Test Accuracy: 0.42100000\n",
      "\n",
      "Epoch: 59/300, Train Loss: 0.04446818, Test Loss: 2.26561565, Test Accuracy: 0.42720000\n",
      "\n",
      "Epoch: 60/300, Train Loss: 0.04423420, Test Loss: 2.22993656, Test Accuracy: 0.43370000\n",
      "\n",
      "Epoch: 61/300, Train Loss: 0.04398741, Test Loss: 2.19452481, Test Accuracy: 0.44060000\n",
      "\n",
      "Epoch: 62/300, Train Loss: 0.04372852, Test Loss: 2.16046492, Test Accuracy: 0.44600000\n",
      "\n",
      "Epoch: 63/300, Train Loss: 0.04346011, Test Loss: 2.12839974, Test Accuracy: 0.45000000\n",
      "\n",
      "Epoch: 64/300, Train Loss: 0.04318526, Test Loss: 2.09842080, Test Accuracy: 0.45520000\n",
      "\n",
      "Epoch: 65/300, Train Loss: 0.04290748, Test Loss: 2.07018994, Test Accuracy: 0.46040000\n",
      "\n",
      "Epoch: 66/300, Train Loss: 0.04262959, Test Loss: 2.04314810, Test Accuracy: 0.46460000\n",
      "\n",
      "Epoch: 67/300, Train Loss: 0.04235239, Test Loss: 2.01669264, Test Accuracy: 0.46920000\n",
      "\n",
      "Epoch: 68/300, Train Loss: 0.04207441, Test Loss: 1.99030241, Test Accuracy: 0.47380000\n",
      "\n",
      "Epoch: 69/300, Train Loss: 0.04179311, Test Loss: 1.96358836, Test Accuracy: 0.47780000\n",
      "\n",
      "Epoch: 70/300, Train Loss: 0.04150680, Test Loss: 1.93626196, Test Accuracy: 0.48220000\n",
      "\n",
      "Epoch: 71/300, Train Loss: 0.04121542, Test Loss: 1.90814229, Test Accuracy: 0.48690000\n",
      "\n",
      "Epoch: 72/300, Train Loss: 0.04091988, Test Loss: 1.87922277, Test Accuracy: 0.49140000\n",
      "\n",
      "Epoch: 73/300, Train Loss: 0.04062124, Test Loss: 1.84968588, Test Accuracy: 0.49580000\n",
      "\n",
      "Epoch: 74/300, Train Loss: 0.04032000, Test Loss: 1.81983844, Test Accuracy: 0.50070000\n",
      "\n",
      "Epoch: 75/300, Train Loss: 0.04001579, Test Loss: 1.79004662, Test Accuracy: 0.50530000\n",
      "\n",
      "Epoch: 76/300, Train Loss: 0.03970761, Test Loss: 1.76070844, Test Accuracy: 0.51000000\n",
      "\n",
      "Epoch: 77/300, Train Loss: 0.03939416, Test Loss: 1.73223724, Test Accuracy: 0.51360000\n",
      "\n",
      "Epoch: 78/300, Train Loss: 0.03907449, Test Loss: 1.70499373, Test Accuracy: 0.51750000\n",
      "\n",
      "Epoch: 79/300, Train Loss: 0.03874856, Test Loss: 1.67918336, Test Accuracy: 0.52150000\n",
      "\n",
      "Epoch: 80/300, Train Loss: 0.03841765, Test Loss: 1.65482014, Test Accuracy: 0.52560000\n",
      "\n",
      "Epoch: 81/300, Train Loss: 0.03808409, Test Loss: 1.63178329, Test Accuracy: 0.52910000\n",
      "\n",
      "Epoch: 82/300, Train Loss: 0.03775081, Test Loss: 1.60987507, Test Accuracy: 0.53410000\n",
      "\n",
      "Epoch: 83/300, Train Loss: 0.03742089, Test Loss: 1.58884105, Test Accuracy: 0.53920000\n",
      "\n",
      "Epoch: 84/300, Train Loss: 0.03709668, Test Loss: 1.56840323, Test Accuracy: 0.54360000\n",
      "\n",
      "Epoch: 85/300, Train Loss: 0.03677881, Test Loss: 1.54831445, Test Accuracy: 0.54930000\n",
      "\n",
      "Epoch: 86/300, Train Loss: 0.03646632, Test Loss: 1.52837651, Test Accuracy: 0.55290000\n",
      "\n",
      "Epoch: 87/300, Train Loss: 0.03615791, Test Loss: 1.50842744, Test Accuracy: 0.55720000\n",
      "\n",
      "Epoch: 88/300, Train Loss: 0.03585309, Test Loss: 1.48836264, Test Accuracy: 0.56120000\n",
      "\n",
      "Epoch: 89/300, Train Loss: 0.03555179, Test Loss: 1.46815583, Test Accuracy: 0.56550000\n",
      "\n",
      "Epoch: 90/300, Train Loss: 0.03525346, Test Loss: 1.44785076, Test Accuracy: 0.56940000\n",
      "\n",
      "Epoch: 91/300, Train Loss: 0.03495680, Test Loss: 1.42753387, Test Accuracy: 0.57450000\n",
      "\n",
      "Epoch: 92/300, Train Loss: 0.03466016, Test Loss: 1.40730068, Test Accuracy: 0.57950000\n",
      "\n",
      "Epoch: 93/300, Train Loss: 0.03436195, Test Loss: 1.38724319, Test Accuracy: 0.58490000\n",
      "\n",
      "Epoch: 94/300, Train Loss: 0.03406102, Test Loss: 1.36743862, Test Accuracy: 0.59110000\n",
      "\n",
      "Epoch: 95/300, Train Loss: 0.03375694, Test Loss: 1.34794558, Test Accuracy: 0.59540000\n",
      "\n",
      "Epoch: 96/300, Train Loss: 0.03345013, Test Loss: 1.32880185, Test Accuracy: 0.60040000\n",
      "\n",
      "Epoch: 97/300, Train Loss: 0.03314186, Test Loss: 1.31003162, Test Accuracy: 0.60460000\n",
      "\n",
      "Epoch: 98/300, Train Loss: 0.03283395, Test Loss: 1.29164749, Test Accuracy: 0.61010000\n",
      "\n",
      "Epoch: 99/300, Train Loss: 0.03252825, Test Loss: 1.27365323, Test Accuracy: 0.61510000\n",
      "\n",
      "Epoch: 100/300, Train Loss: 0.03222602, Test Loss: 1.25604651, Test Accuracy: 0.62010000\n",
      "\n",
      "Epoch: 101/300, Train Loss: 0.03192737, Test Loss: 1.23882474, Test Accuracy: 0.62490000\n",
      "\n",
      "Epoch: 102/300, Train Loss: 0.03163138, Test Loss: 1.22198788, Test Accuracy: 0.62930000\n",
      "\n",
      "Epoch: 103/300, Train Loss: 0.03133676, Test Loss: 1.20553596, Test Accuracy: 0.63360000\n",
      "\n",
      "Epoch: 104/300, Train Loss: 0.03104262, Test Loss: 1.18946366, Test Accuracy: 0.63900000\n",
      "\n",
      "Epoch: 105/300, Train Loss: 0.03074870, Test Loss: 1.17375572, Test Accuracy: 0.64240000\n",
      "\n",
      "Epoch: 106/300, Train Loss: 0.03045519, Test Loss: 1.15838851, Test Accuracy: 0.64770000\n",
      "\n",
      "Epoch: 107/300, Train Loss: 0.03016244, Test Loss: 1.14332887, Test Accuracy: 0.65110000\n",
      "\n",
      "Epoch: 108/300, Train Loss: 0.02987075, Test Loss: 1.12853750, Test Accuracy: 0.65500000\n",
      "\n",
      "Epoch: 109/300, Train Loss: 0.02958030, Test Loss: 1.11396988, Test Accuracy: 0.65920000\n",
      "\n",
      "Epoch: 110/300, Train Loss: 0.02929118, Test Loss: 1.09957687, Test Accuracy: 0.66380000\n",
      "\n",
      "Epoch: 111/300, Train Loss: 0.02900336, Test Loss: 1.08530774, Test Accuracy: 0.66690000\n",
      "\n",
      "Epoch: 112/300, Train Loss: 0.02871679, Test Loss: 1.07111256, Test Accuracy: 0.66960000\n",
      "\n",
      "Epoch: 113/300, Train Loss: 0.02843137, Test Loss: 1.05694907, Test Accuracy: 0.67310000\n",
      "\n",
      "Epoch: 114/300, Train Loss: 0.02814702, Test Loss: 1.04279186, Test Accuracy: 0.67790000\n",
      "\n",
      "Epoch: 115/300, Train Loss: 0.02786375, Test Loss: 1.02864306, Test Accuracy: 0.68090000\n",
      "\n",
      "Epoch: 116/300, Train Loss: 0.02758170, Test Loss: 1.01454419, Test Accuracy: 0.68400000\n",
      "\n",
      "Epoch: 117/300, Train Loss: 0.02730112, Test Loss: 1.00057101, Test Accuracy: 0.68820000\n",
      "\n",
      "Epoch: 118/300, Train Loss: 0.02702223, Test Loss: 0.98681301, Test Accuracy: 0.69210000\n",
      "\n",
      "Epoch: 119/300, Train Loss: 0.02674519, Test Loss: 0.97334450, Test Accuracy: 0.69490000\n",
      "\n",
      "Epoch: 120/300, Train Loss: 0.02647002, Test Loss: 0.96021041, Test Accuracy: 0.69880000\n",
      "\n",
      "Epoch: 121/300, Train Loss: 0.02619668, Test Loss: 0.94742187, Test Accuracy: 0.70220000\n",
      "\n",
      "Epoch: 122/300, Train Loss: 0.02592520, Test Loss: 0.93496562, Test Accuracy: 0.70540000\n",
      "\n",
      "Epoch: 123/300, Train Loss: 0.02565566, Test Loss: 0.92280936, Test Accuracy: 0.70820000\n",
      "\n",
      "Epoch: 124/300, Train Loss: 0.02538821, Test Loss: 0.91091906, Test Accuracy: 0.71180000\n",
      "\n",
      "Epoch: 125/300, Train Loss: 0.02512306, Test Loss: 0.89926099, Test Accuracy: 0.71590000\n",
      "\n",
      "Epoch: 126/300, Train Loss: 0.02486044, Test Loss: 0.88780678, Test Accuracy: 0.71930000\n",
      "\n",
      "Epoch: 127/300, Train Loss: 0.02460061, Test Loss: 0.87653704, Test Accuracy: 0.72340000\n",
      "\n",
      "Epoch: 128/300, Train Loss: 0.02434383, Test Loss: 0.86543883, Test Accuracy: 0.72540000\n",
      "\n",
      "Epoch: 129/300, Train Loss: 0.02409035, Test Loss: 0.85450481, Test Accuracy: 0.72830000\n",
      "\n",
      "Epoch: 130/300, Train Loss: 0.02384042, Test Loss: 0.84373180, Test Accuracy: 0.73110000\n",
      "\n",
      "Epoch: 131/300, Train Loss: 0.02359423, Test Loss: 0.83312101, Test Accuracy: 0.73370000\n",
      "\n",
      "Epoch: 132/300, Train Loss: 0.02335195, Test Loss: 0.82267444, Test Accuracy: 0.73730000\n",
      "\n",
      "Epoch: 133/300, Train Loss: 0.02311366, Test Loss: 0.81239445, Test Accuracy: 0.74010000\n",
      "\n",
      "Epoch: 134/300, Train Loss: 0.02287943, Test Loss: 0.80228480, Test Accuracy: 0.74400000\n",
      "\n",
      "Epoch: 135/300, Train Loss: 0.02264924, Test Loss: 0.79234667, Test Accuracy: 0.74620000\n",
      "\n",
      "Epoch: 136/300, Train Loss: 0.02242307, Test Loss: 0.78258118, Test Accuracy: 0.74880000\n",
      "\n",
      "Epoch: 137/300, Train Loss: 0.02220086, Test Loss: 0.77298798, Test Accuracy: 0.75250000\n",
      "\n",
      "Epoch: 138/300, Train Loss: 0.02198255, Test Loss: 0.76356539, Test Accuracy: 0.75560000\n",
      "\n",
      "Epoch: 139/300, Train Loss: 0.02176808, Test Loss: 0.75431004, Test Accuracy: 0.75790000\n",
      "\n",
      "Epoch: 140/300, Train Loss: 0.02155744, Test Loss: 0.74521625, Test Accuracy: 0.76080000\n",
      "\n",
      "Epoch: 141/300, Train Loss: 0.02135062, Test Loss: 0.73627736, Test Accuracy: 0.76340000\n",
      "\n",
      "Epoch: 142/300, Train Loss: 0.02114766, Test Loss: 0.72748667, Test Accuracy: 0.76610000\n",
      "\n",
      "Epoch: 143/300, Train Loss: 0.02094859, Test Loss: 0.71883626, Test Accuracy: 0.76910000\n",
      "\n",
      "Epoch: 144/300, Train Loss: 0.02075346, Test Loss: 0.71031677, Test Accuracy: 0.77150000\n",
      "\n",
      "Epoch: 145/300, Train Loss: 0.02056230, Test Loss: 0.70192116, Test Accuracy: 0.77350000\n",
      "\n",
      "Epoch: 146/300, Train Loss: 0.02037510, Test Loss: 0.69364349, Test Accuracy: 0.77480000\n",
      "\n",
      "Epoch: 147/300, Train Loss: 0.02019184, Test Loss: 0.68547738, Test Accuracy: 0.77750000\n",
      "\n",
      "Epoch: 148/300, Train Loss: 0.02001243, Test Loss: 0.67741909, Test Accuracy: 0.78060000\n",
      "\n",
      "Epoch: 149/300, Train Loss: 0.01983675, Test Loss: 0.66946453, Test Accuracy: 0.78340000\n",
      "\n",
      "Epoch: 150/300, Train Loss: 0.01966464, Test Loss: 0.66161293, Test Accuracy: 0.78600000\n",
      "\n",
      "Epoch: 151/300, Train Loss: 0.01949592, Test Loss: 0.65386298, Test Accuracy: 0.78780000\n",
      "\n",
      "Epoch: 152/300, Train Loss: 0.01933037, Test Loss: 0.64621451, Test Accuracy: 0.79070000\n",
      "\n",
      "Epoch: 153/300, Train Loss: 0.01916776, Test Loss: 0.63866815, Test Accuracy: 0.79300000\n",
      "\n",
      "Epoch: 154/300, Train Loss: 0.01900787, Test Loss: 0.63122423, Test Accuracy: 0.79580000\n",
      "\n",
      "Epoch: 155/300, Train Loss: 0.01885049, Test Loss: 0.62388379, Test Accuracy: 0.79780000\n",
      "\n",
      "Epoch: 156/300, Train Loss: 0.01869543, Test Loss: 0.61664757, Test Accuracy: 0.79970000\n",
      "\n",
      "Epoch: 157/300, Train Loss: 0.01854254, Test Loss: 0.60951534, Test Accuracy: 0.80140000\n",
      "\n",
      "Epoch: 158/300, Train Loss: 0.01839167, Test Loss: 0.60248717, Test Accuracy: 0.80330000\n",
      "\n",
      "Epoch: 159/300, Train Loss: 0.01824270, Test Loss: 0.59556368, Test Accuracy: 0.80550000\n",
      "\n",
      "Epoch: 160/300, Train Loss: 0.01809554, Test Loss: 0.58874433, Test Accuracy: 0.80790000\n",
      "\n",
      "Epoch: 161/300, Train Loss: 0.01795012, Test Loss: 0.58202908, Test Accuracy: 0.80970000\n",
      "\n",
      "Epoch: 162/300, Train Loss: 0.01780637, Test Loss: 0.57541719, Test Accuracy: 0.81140000\n",
      "\n",
      "Epoch: 163/300, Train Loss: 0.01766425, Test Loss: 0.56890942, Test Accuracy: 0.81340000\n",
      "\n",
      "Epoch: 164/300, Train Loss: 0.01752371, Test Loss: 0.56250518, Test Accuracy: 0.81540000\n",
      "\n",
      "Epoch: 165/300, Train Loss: 0.01738472, Test Loss: 0.55620418, Test Accuracy: 0.81760000\n",
      "\n",
      "Epoch: 166/300, Train Loss: 0.01724724, Test Loss: 0.55000602, Test Accuracy: 0.81900000\n",
      "\n",
      "Epoch: 167/300, Train Loss: 0.01711125, Test Loss: 0.54391072, Test Accuracy: 0.82100000\n",
      "\n",
      "Epoch: 168/300, Train Loss: 0.01697672, Test Loss: 0.53791694, Test Accuracy: 0.82350000\n",
      "\n",
      "Epoch: 169/300, Train Loss: 0.01684362, Test Loss: 0.53202412, Test Accuracy: 0.82480000\n",
      "\n",
      "Epoch: 170/300, Train Loss: 0.01671193, Test Loss: 0.52623034, Test Accuracy: 0.82720000\n",
      "\n",
      "Epoch: 171/300, Train Loss: 0.01658163, Test Loss: 0.52053491, Test Accuracy: 0.82850000\n",
      "\n",
      "Epoch: 172/300, Train Loss: 0.01645269, Test Loss: 0.51493501, Test Accuracy: 0.83080000\n",
      "\n",
      "Epoch: 173/300, Train Loss: 0.01632509, Test Loss: 0.50942847, Test Accuracy: 0.83290000\n",
      "\n",
      "Epoch: 174/300, Train Loss: 0.01619881, Test Loss: 0.50401252, Test Accuracy: 0.83470000\n",
      "\n",
      "Epoch: 175/300, Train Loss: 0.01607383, Test Loss: 0.49868332, Test Accuracy: 0.83640000\n",
      "\n",
      "Epoch: 176/300, Train Loss: 0.01595012, Test Loss: 0.49343794, Test Accuracy: 0.83840000\n",
      "\n",
      "Epoch: 177/300, Train Loss: 0.01582767, Test Loss: 0.48827203, Test Accuracy: 0.83950000\n",
      "\n",
      "Epoch: 178/300, Train Loss: 0.01570646, Test Loss: 0.48318201, Test Accuracy: 0.84210000\n",
      "\n",
      "Epoch: 179/300, Train Loss: 0.01558647, Test Loss: 0.47816319, Test Accuracy: 0.84350000\n",
      "\n",
      "Epoch: 180/300, Train Loss: 0.01546769, Test Loss: 0.47321188, Test Accuracy: 0.84510000\n",
      "\n",
      "Epoch: 181/300, Train Loss: 0.01535011, Test Loss: 0.46832357, Test Accuracy: 0.84640000\n",
      "\n",
      "Epoch: 182/300, Train Loss: 0.01523373, Test Loss: 0.46349437, Test Accuracy: 0.84810000\n",
      "\n",
      "Epoch: 183/300, Train Loss: 0.01511853, Test Loss: 0.45872087, Test Accuracy: 0.84910000\n",
      "\n",
      "Epoch: 184/300, Train Loss: 0.01500451, Test Loss: 0.45399940, Test Accuracy: 0.85170000\n",
      "\n",
      "Epoch: 185/300, Train Loss: 0.01489168, Test Loss: 0.44932799, Test Accuracy: 0.85250000\n",
      "\n",
      "Epoch: 186/300, Train Loss: 0.01478002, Test Loss: 0.44470377, Test Accuracy: 0.85380000\n",
      "\n",
      "Epoch: 187/300, Train Loss: 0.01466955, Test Loss: 0.44012577, Test Accuracy: 0.85510000\n",
      "\n",
      "Epoch: 188/300, Train Loss: 0.01456025, Test Loss: 0.43559156, Test Accuracy: 0.85700000\n",
      "\n",
      "Epoch: 189/300, Train Loss: 0.01445213, Test Loss: 0.43110188, Test Accuracy: 0.85900000\n",
      "\n",
      "Epoch: 190/300, Train Loss: 0.01434519, Test Loss: 0.42665534, Test Accuracy: 0.86130000\n",
      "\n",
      "Epoch: 191/300, Train Loss: 0.01423941, Test Loss: 0.42225202, Test Accuracy: 0.86310000\n",
      "\n",
      "Epoch: 192/300, Train Loss: 0.01413480, Test Loss: 0.41789230, Test Accuracy: 0.86450000\n",
      "\n",
      "Epoch: 193/300, Train Loss: 0.01403134, Test Loss: 0.41357610, Test Accuracy: 0.86580000\n",
      "\n",
      "Epoch: 194/300, Train Loss: 0.01392902, Test Loss: 0.40930459, Test Accuracy: 0.86750000\n",
      "\n",
      "Epoch: 195/300, Train Loss: 0.01382783, Test Loss: 0.40507733, Test Accuracy: 0.86860000\n",
      "\n",
      "Epoch: 196/300, Train Loss: 0.01372775, Test Loss: 0.40089537, Test Accuracy: 0.86990000\n",
      "\n",
      "Epoch: 197/300, Train Loss: 0.01362877, Test Loss: 0.39675954, Test Accuracy: 0.87170000\n",
      "\n",
      "Epoch: 198/300, Train Loss: 0.01353087, Test Loss: 0.39266965, Test Accuracy: 0.87320000\n",
      "\n",
      "Epoch: 199/300, Train Loss: 0.01343404, Test Loss: 0.38862683, Test Accuracy: 0.87460000\n",
      "\n",
      "Epoch: 200/300, Train Loss: 0.01333825, Test Loss: 0.38463116, Test Accuracy: 0.87580000\n",
      "\n",
      "Epoch: 201/300, Train Loss: 0.01324349, Test Loss: 0.38068322, Test Accuracy: 0.87770000\n",
      "\n",
      "Epoch: 202/300, Train Loss: 0.01314973, Test Loss: 0.37678348, Test Accuracy: 0.87860000\n",
      "\n",
      "Epoch: 203/300, Train Loss: 0.01305696, Test Loss: 0.37293232, Test Accuracy: 0.87970000\n",
      "\n",
      "Epoch: 204/300, Train Loss: 0.01296516, Test Loss: 0.36913000, Test Accuracy: 0.88170000\n",
      "\n",
      "Epoch: 205/300, Train Loss: 0.01287431, Test Loss: 0.36537670, Test Accuracy: 0.88230000\n",
      "\n",
      "Epoch: 206/300, Train Loss: 0.01278439, Test Loss: 0.36167300, Test Accuracy: 0.88310000\n",
      "\n",
      "Epoch: 207/300, Train Loss: 0.01269538, Test Loss: 0.35801895, Test Accuracy: 0.88490000\n",
      "\n",
      "Epoch: 208/300, Train Loss: 0.01260726, Test Loss: 0.35441465, Test Accuracy: 0.88660000\n",
      "\n",
      "Epoch: 209/300, Train Loss: 0.01252002, Test Loss: 0.35086096, Test Accuracy: 0.88790000\n",
      "\n",
      "Epoch: 210/300, Train Loss: 0.01243364, Test Loss: 0.34735750, Test Accuracy: 0.88940000\n",
      "\n",
      "Epoch: 211/300, Train Loss: 0.01234810, Test Loss: 0.34390440, Test Accuracy: 0.89020000\n",
      "\n",
      "Epoch: 212/300, Train Loss: 0.01226338, Test Loss: 0.34050189, Test Accuracy: 0.89120000\n",
      "\n",
      "Epoch: 213/300, Train Loss: 0.01217947, Test Loss: 0.33714982, Test Accuracy: 0.89210000\n",
      "\n",
      "Epoch: 214/300, Train Loss: 0.01209635, Test Loss: 0.33384851, Test Accuracy: 0.89310000\n",
      "\n",
      "Epoch: 215/300, Train Loss: 0.01201400, Test Loss: 0.33059751, Test Accuracy: 0.89390000\n",
      "\n",
      "Epoch: 216/300, Train Loss: 0.01193242, Test Loss: 0.32739706, Test Accuracy: 0.89460000\n",
      "\n",
      "Epoch: 217/300, Train Loss: 0.01185159, Test Loss: 0.32424676, Test Accuracy: 0.89550000\n",
      "\n",
      "Epoch: 218/300, Train Loss: 0.01177150, Test Loss: 0.32114680, Test Accuracy: 0.89660000\n",
      "\n",
      "Epoch: 219/300, Train Loss: 0.01169214, Test Loss: 0.31809676, Test Accuracy: 0.89770000\n",
      "\n",
      "Epoch: 220/300, Train Loss: 0.01161349, Test Loss: 0.31509610, Test Accuracy: 0.89800000\n",
      "\n",
      "Epoch: 221/300, Train Loss: 0.01153554, Test Loss: 0.31214456, Test Accuracy: 0.89890000\n",
      "\n",
      "Epoch: 222/300, Train Loss: 0.01145829, Test Loss: 0.30924194, Test Accuracy: 0.89950000\n",
      "\n",
      "Epoch: 223/300, Train Loss: 0.01138173, Test Loss: 0.30638772, Test Accuracy: 0.90070000\n",
      "\n",
      "Epoch: 224/300, Train Loss: 0.01130584, Test Loss: 0.30358114, Test Accuracy: 0.90160000\n",
      "\n",
      "Epoch: 225/300, Train Loss: 0.01123063, Test Loss: 0.30082167, Test Accuracy: 0.90240000\n",
      "\n",
      "Epoch: 226/300, Train Loss: 0.01115608, Test Loss: 0.29810880, Test Accuracy: 0.90310000\n",
      "\n",
      "Epoch: 227/300, Train Loss: 0.01108218, Test Loss: 0.29544199, Test Accuracy: 0.90460000\n",
      "\n",
      "Epoch: 228/300, Train Loss: 0.01100892, Test Loss: 0.29282014, Test Accuracy: 0.90540000\n",
      "\n",
      "Epoch: 229/300, Train Loss: 0.01093631, Test Loss: 0.29024285, Test Accuracy: 0.90620000\n",
      "\n",
      "Epoch: 230/300, Train Loss: 0.01086434, Test Loss: 0.28770942, Test Accuracy: 0.90710000\n",
      "\n",
      "Epoch: 231/300, Train Loss: 0.01079300, Test Loss: 0.28521865, Test Accuracy: 0.90780000\n",
      "\n",
      "Epoch: 232/300, Train Loss: 0.01072228, Test Loss: 0.28276979, Test Accuracy: 0.90870000\n",
      "\n",
      "Epoch: 233/300, Train Loss: 0.01065218, Test Loss: 0.28036208, Test Accuracy: 0.90970000\n",
      "\n",
      "Epoch: 234/300, Train Loss: 0.01058270, Test Loss: 0.27799476, Test Accuracy: 0.91040000\n",
      "\n",
      "Epoch: 235/300, Train Loss: 0.01051384, Test Loss: 0.27566666, Test Accuracy: 0.91120000\n",
      "\n",
      "Epoch: 236/300, Train Loss: 0.01044558, Test Loss: 0.27337714, Test Accuracy: 0.91180000\n",
      "\n",
      "Epoch: 237/300, Train Loss: 0.01037792, Test Loss: 0.27112510, Test Accuracy: 0.91300000\n",
      "\n",
      "Epoch: 238/300, Train Loss: 0.01031087, Test Loss: 0.26890999, Test Accuracy: 0.91390000\n",
      "\n",
      "Epoch: 239/300, Train Loss: 0.01024441, Test Loss: 0.26673076, Test Accuracy: 0.91490000\n",
      "\n",
      "Epoch: 240/300, Train Loss: 0.01017855, Test Loss: 0.26458640, Test Accuracy: 0.91560000\n",
      "\n",
      "Epoch: 241/300, Train Loss: 0.01011328, Test Loss: 0.26247630, Test Accuracy: 0.91620000\n",
      "\n",
      "Epoch: 242/300, Train Loss: 0.01004860, Test Loss: 0.26039969, Test Accuracy: 0.91710000\n",
      "\n",
      "Epoch: 243/300, Train Loss: 0.00998451, Test Loss: 0.25835565, Test Accuracy: 0.91750000\n",
      "\n",
      "Epoch: 244/300, Train Loss: 0.00992100, Test Loss: 0.25634370, Test Accuracy: 0.91800000\n",
      "\n",
      "Epoch: 245/300, Train Loss: 0.00985806, Test Loss: 0.25436280, Test Accuracy: 0.91890000\n",
      "\n",
      "Epoch: 246/300, Train Loss: 0.00979571, Test Loss: 0.25241251, Test Accuracy: 0.91960000\n",
      "\n",
      "Epoch: 247/300, Train Loss: 0.00973392, Test Loss: 0.25049178, Test Accuracy: 0.92030000\n",
      "\n",
      "Epoch: 248/300, Train Loss: 0.00967271, Test Loss: 0.24860019, Test Accuracy: 0.92090000\n",
      "\n",
      "Epoch: 249/300, Train Loss: 0.00961207, Test Loss: 0.24673708, Test Accuracy: 0.92140000\n",
      "\n",
      "Epoch: 250/300, Train Loss: 0.00955199, Test Loss: 0.24490221, Test Accuracy: 0.92190000\n",
      "\n",
      "Epoch: 251/300, Train Loss: 0.00949247, Test Loss: 0.24309449, Test Accuracy: 0.92280000\n",
      "\n",
      "Epoch: 252/300, Train Loss: 0.00943350, Test Loss: 0.24131348, Test Accuracy: 0.92370000\n",
      "\n",
      "Epoch: 253/300, Train Loss: 0.00937509, Test Loss: 0.23955881, Test Accuracy: 0.92430000\n",
      "\n",
      "Epoch: 254/300, Train Loss: 0.00931723, Test Loss: 0.23782987, Test Accuracy: 0.92480000\n",
      "\n",
      "Epoch: 255/300, Train Loss: 0.00925991, Test Loss: 0.23612605, Test Accuracy: 0.92500000\n",
      "\n",
      "Epoch: 256/300, Train Loss: 0.00920313, Test Loss: 0.23444692, Test Accuracy: 0.92540000\n",
      "\n",
      "Epoch: 257/300, Train Loss: 0.00914689, Test Loss: 0.23279203, Test Accuracy: 0.92600000\n",
      "\n",
      "Epoch: 258/300, Train Loss: 0.00909117, Test Loss: 0.23116099, Test Accuracy: 0.92680000\n",
      "\n",
      "Epoch: 259/300, Train Loss: 0.00903598, Test Loss: 0.22955324, Test Accuracy: 0.92740000\n",
      "\n",
      "Epoch: 260/300, Train Loss: 0.00898131, Test Loss: 0.22796845, Test Accuracy: 0.92800000\n",
      "\n",
      "Epoch: 261/300, Train Loss: 0.00892716, Test Loss: 0.22640593, Test Accuracy: 0.92830000\n",
      "\n",
      "Epoch: 262/300, Train Loss: 0.00887351, Test Loss: 0.22486553, Test Accuracy: 0.92900000\n",
      "\n",
      "Epoch: 263/300, Train Loss: 0.00882036, Test Loss: 0.22334680, Test Accuracy: 0.92970000\n",
      "\n",
      "Epoch: 264/300, Train Loss: 0.00876771, Test Loss: 0.22184908, Test Accuracy: 0.93080000\n",
      "\n",
      "Epoch: 265/300, Train Loss: 0.00871554, Test Loss: 0.22037221, Test Accuracy: 0.93160000\n",
      "\n",
      "Epoch: 266/300, Train Loss: 0.00866386, Test Loss: 0.21891574, Test Accuracy: 0.93170000\n",
      "\n",
      "Epoch: 267/300, Train Loss: 0.00861266, Test Loss: 0.21747928, Test Accuracy: 0.93240000\n",
      "\n",
      "Epoch: 268/300, Train Loss: 0.00856193, Test Loss: 0.21606249, Test Accuracy: 0.93320000\n",
      "\n",
      "Epoch: 269/300, Train Loss: 0.00851166, Test Loss: 0.21466496, Test Accuracy: 0.93350000\n",
      "\n",
      "Epoch: 270/300, Train Loss: 0.00846186, Test Loss: 0.21328624, Test Accuracy: 0.93370000\n",
      "\n",
      "Epoch: 271/300, Train Loss: 0.00841250, Test Loss: 0.21192612, Test Accuracy: 0.93430000\n",
      "\n",
      "Epoch: 272/300, Train Loss: 0.00836359, Test Loss: 0.21058409, Test Accuracy: 0.93480000\n",
      "\n",
      "Epoch: 273/300, Train Loss: 0.00831512, Test Loss: 0.20925995, Test Accuracy: 0.93520000\n",
      "\n",
      "Epoch: 274/300, Train Loss: 0.00826708, Test Loss: 0.20795309, Test Accuracy: 0.93590000\n",
      "\n",
      "Epoch: 275/300, Train Loss: 0.00821946, Test Loss: 0.20666356, Test Accuracy: 0.93660000\n",
      "\n",
      "Epoch: 276/300, Train Loss: 0.00817227, Test Loss: 0.20539084, Test Accuracy: 0.93700000\n",
      "\n",
      "Epoch: 277/300, Train Loss: 0.00812549, Test Loss: 0.20413457, Test Accuracy: 0.93730000\n",
      "\n",
      "Epoch: 278/300, Train Loss: 0.00807912, Test Loss: 0.20289456, Test Accuracy: 0.93810000\n",
      "\n",
      "Epoch: 279/300, Train Loss: 0.00803315, Test Loss: 0.20167041, Test Accuracy: 0.93840000\n",
      "\n",
      "Epoch: 280/300, Train Loss: 0.00798758, Test Loss: 0.20046191, Test Accuracy: 0.93880000\n",
      "\n",
      "Epoch: 281/300, Train Loss: 0.00794239, Test Loss: 0.19926874, Test Accuracy: 0.93910000\n",
      "\n",
      "Epoch: 282/300, Train Loss: 0.00789759, Test Loss: 0.19809060, Test Accuracy: 0.93960000\n",
      "\n",
      "Epoch: 283/300, Train Loss: 0.00785317, Test Loss: 0.19692727, Test Accuracy: 0.94000000\n",
      "\n",
      "Epoch: 284/300, Train Loss: 0.00780913, Test Loss: 0.19577846, Test Accuracy: 0.94020000\n",
      "\n",
      "Epoch: 285/300, Train Loss: 0.00776546, Test Loss: 0.19464396, Test Accuracy: 0.94070000\n",
      "\n",
      "Epoch: 286/300, Train Loss: 0.00772215, Test Loss: 0.19352340, Test Accuracy: 0.94150000\n",
      "\n",
      "Epoch: 287/300, Train Loss: 0.00767919, Test Loss: 0.19241689, Test Accuracy: 0.94190000\n",
      "\n",
      "Epoch: 288/300, Train Loss: 0.00763660, Test Loss: 0.19132374, Test Accuracy: 0.94230000\n",
      "\n",
      "Epoch: 289/300, Train Loss: 0.00759435, Test Loss: 0.19024419, Test Accuracy: 0.94260000\n",
      "\n",
      "Epoch: 290/300, Train Loss: 0.00755245, Test Loss: 0.18917780, Test Accuracy: 0.94300000\n",
      "\n",
      "Epoch: 291/300, Train Loss: 0.00751089, Test Loss: 0.18812422, Test Accuracy: 0.94320000\n",
      "\n",
      "Epoch: 292/300, Train Loss: 0.00746967, Test Loss: 0.18708356, Test Accuracy: 0.94380000\n",
      "\n",
      "Epoch: 293/300, Train Loss: 0.00742879, Test Loss: 0.18605543, Test Accuracy: 0.94420000\n",
      "\n",
      "Epoch: 294/300, Train Loss: 0.00738823, Test Loss: 0.18503968, Test Accuracy: 0.94470000\n",
      "\n",
      "Epoch: 295/300, Train Loss: 0.00734799, Test Loss: 0.18403624, Test Accuracy: 0.94480000\n",
      "\n",
      "Epoch: 296/300, Train Loss: 0.00730808, Test Loss: 0.18304483, Test Accuracy: 0.94510000\n",
      "\n",
      "Epoch: 297/300, Train Loss: 0.00726849, Test Loss: 0.18206526, Test Accuracy: 0.94530000\n",
      "\n",
      "Epoch: 298/300, Train Loss: 0.00722921, Test Loss: 0.18109741, Test Accuracy: 0.94590000\n",
      "\n",
      "Epoch: 299/300, Train Loss: 0.00719024, Test Loss: 0.18014110, Test Accuracy: 0.94620000\n",
      "\n",
      "Epoch: 300/300, Train Loss: 0.00715158, Test Loss: 0.17919629, Test Accuracy: 0.94630000\n",
      "average_sparsity: [0.006548745557665825, 0.011137071065604687, 0.023312317207455635, 0.0473843552172184, 0.07724131643772125, 0.10253827273845673, 0.12131894379854202, 0.13549888134002686, 0.14659158885478973, 0.1555120348930359, 0.1628708392381668, 0.16909901797771454, 0.17449548840522766, 0.17926143109798431, 0.18353226780891418, 0.18740274012088776, 0.1909431666135788, 0.19420793652534485, 0.19724062085151672, 0.20007698237895966, 0.20274831354618073, 0.205284982919693, 0.20771926641464233, 0.21008557081222534, 0.21241573989391327, 0.21473117172718048, 0.21703922748565674, 0.2193404585123062, 0.22163796424865723, 0.22393323481082916, 0.22621574997901917, 0.22847001254558563, 0.2306913584470749, 0.23288889229297638, 0.2350766360759735, 0.23726408183574677, 0.23945331573486328, 0.2416413128376007, 0.24382497370243073, 0.24600648880004883, 0.24819780886173248, 0.2504187524318695, 0.25269025564193726, 0.2550278306007385, 0.25743797421455383, 0.2599179148674011, 0.2624600827693939, 0.2650602161884308, 0.26771944761276245, 0.27043387293815613, 0.27318182587623596, 0.2759237289428711, 0.2786172330379486, 0.28123265504837036, 0.28376084566116333, 0.2862144410610199, 0.288621723651886, 0.2910107672214508, 0.2933957874774933, 0.2957794964313507, 0.2981593608856201, 0.3005252182483673, 0.30285608768463135, 0.30512815713882446, 0.3073287308216095, 0.3094605505466461, 0.3115341365337372, 0.31356117129325867, 0.31554868817329407, 0.31748834252357483, 0.3193531632423401, 0.32111304998397827, 0.3227539658546448, 0.324283629655838, 0.32572734355926514, 0.3271230161190033, 0.32851526141166687, 0.3299444615840912, 0.3314317762851715, 0.3329714238643646, 0.3345395028591156, 0.33610957860946655, 0.33766278624534607, 0.3391886055469513, 0.34068021178245544, 0.342129111289978, 0.34352177381515503, 0.34484338760375977, 0.34608641266822815, 0.34725356101989746, 0.348354697227478, 0.34940263628959656, 0.3504098355770111, 0.35138675570487976, 0.3523409068584442, 0.3532770872116089, 0.3541986048221588, 0.35510769486427307, 0.3560056686401367, 0.3568921387195587, 0.35776495933532715, 0.35862135887145996, 0.3594592213630676, 0.360277384519577, 0.36107534170150757, 0.3618524968624115, 0.3626081049442291, 0.3633410632610321, 0.36405009031295776, 0.36473333835601807, 0.3653891086578369, 0.3660159707069397, 0.36661332845687866, 0.36718204617500305, 0.3677249848842621, 0.3682462275028229, 0.3687501847743988, 0.3692396581172943, 0.36971530318260193, 0.37017586827278137, 0.3706190884113312, 0.3710428774356842, 0.3714458644390106, 0.37182775139808655, 0.37218889594078064, 0.37253043055534363, 0.3728540241718292, 0.37316158413887024, 0.37345513701438904, 0.37373650074005127, 0.37400731444358826, 0.37426915764808655, 0.37452301383018494, 0.37476983666419983, 0.37501055002212524, 0.3752458095550537, 0.3754764795303345, 0.3757033050060272, 0.37592723965644836, 0.3761492967605591, 0.3763703405857086, 0.37659117579460144, 0.376812607049942, 0.37703490257263184, 0.3772584795951843, 0.3774833381175995, 0.3777095079421997, 0.3779368996620178, 0.37816524505615234, 0.37839433550834656, 0.3786238133907318, 0.3788534104824066, 0.37908267974853516, 0.37931111454963684, 0.37953832745552063, 0.3797639012336731, 0.3799874484539032, 0.38020867109298706, 0.38042721152305603, 0.3806428611278534, 0.3808553218841553, 0.38106441497802734, 0.3812701404094696, 0.38147225975990295, 0.3816707134246826, 0.38186532258987427, 0.3820561468601227, 0.3822431266307831, 0.3824261426925659, 0.3826051652431488, 0.3827802836894989, 0.3829513490200043, 0.3831183910369873, 0.383281409740448, 0.38344046473503113, 0.38359561562538147, 0.383746862411499, 0.38389432430267334, 0.3840380012989044, 0.3841780126094818, 0.38431453704833984, 0.38444754481315613, 0.38457730412483215, 0.3847038447856903, 0.3848273456096649, 0.38494786620140076, 0.3850656747817993, 0.3851807713508606, 0.38529345393180847, 0.38540372252464294, 0.3855117857456207, 0.3856176733970642, 0.3857215940952301, 0.38582363724708557, 0.3859238922595978, 0.3860224187374115, 0.3861193060874939, 0.38621458411216736, 0.38630831241607666, 0.3864004909992218, 0.38649120926856995, 0.3865804970264435, 0.38666829466819763, 0.38675469160079956, 0.3868395984172821, 0.3869231343269348, 0.3870052695274353, 0.3870859742164612, 0.38716527819633484, 0.38724327087402344, 0.3873198926448822, 0.38739508390426636, 0.387469083070755, 0.3875417411327362, 0.3876131474971771, 0.38768336176872253, 0.3877524137496948, 0.387820303440094, 0.3878871202468872, 0.38795286417007446, 0.3880176544189453, 0.388081431388855, 0.3881443440914154, 0.3882063329219818, 0.38826751708984375, 0.3883279860019684, 0.38838762044906616, 0.38844653964042664, 0.3885047435760498, 0.38856226205825806, 0.3886191248893738, 0.3886753022670746, 0.38873085379600525, 0.38878580927848816, 0.38884004950523376, 0.388893723487854, 0.3889467716217041, 0.38899919390678406, 0.38905104994773865, 0.3891022503376007, 0.3891528248786926, 0.3892028331756592, 0.3892521858215332, 0.3893009424209595, 0.3893490731716156, 0.38939663767814636, 0.3894435167312622, 0.3894898295402527, 0.3895355463027954, 0.3895806670188904, 0.3896251320838928, 0.3896690309047699, 0.38971230387687683, 0.3897550106048584, 0.38979703187942505, 0.3898385167121887, 0.38987940549850464, 0.3899196982383728, 0.3899593651294708, 0.3899984657764435, 0.3900369703769684, 0.39007484912872314, 0.39011216163635254, 0.3901488780975342, 0.39018499851226807, 0.3902204930782318, 0.3902554512023926, 0.3902897834777832, 0.39032354950904846, 0.39035677909851074, 0.3903893828392029, 0.39042139053344727, 0.39045286178588867, 0.3904837965965271, 0.390514075756073, 0.3905438482761383, 0.39057302474975586, 0.3906016945838928, 0.39062973856925964, 0.3906572759151459, 0.39068424701690674, 0.3907106816768646, 0.39073657989501953, 0.39076194167137146, 0.3907867968082428, 0.39081108570098877, 0.39083489775657654, 0.3908582031726837, 0.3908809423446655, 0.3909032344818115, 0.39092499017715454, 0.39094626903533936, 0.39096707105636597, 0.390987366437912, 0.3910071849822998, 0.3910265266895294, 0.3910454213619232, 0.3910638093948364, 0.3910817801952362, 0.39109933376312256]\n"
     ]
    }
   ],
   "source": [
    "model_SGD = Model()\n",
    "print(\"model_SGD:\", model_SGD)\n",
    "model_SGD.to(device)\n",
    "model_SGD.sigmoid.register_forward_hook(get_activation(model_SGD))\n",
    "optimizer_SGD = torch.optim.SGD(model_SGD.parameters(), lr=0.1)\n",
    "SGD_test_acc, sparsity, SGD_avg_selectivity_list, SGD_std_selectivity_list = selectivity_trainer(optimizer=optimizer_SGD, model=model_SGD)\n",
    "\n",
    "f = open(\"sorted_sparsity_selectivity_SGD.txt\", \"w\")\n",
    "f.write(str(0)+'\\n'+str(SGD_test_acc)+'\\n'+str(sparsity)+'\\n'+str(SGD_avg_selectivity_list)+'\\n'+str(SGD_std_selectivity_list)+'\\n\\n')\n",
    "f.close()\n",
    "\n",
    "!cp sorted_sparsity_selectivity_SGD.txt /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvQxaN_fRXLq"
   },
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkqfFoVkRXxP",
    "outputId": "eac6eda7-a801-4b77-bea6-73dffe03f6c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_Adam: Model(\n",
      "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Epoch: 1/300, Train Loss: 1.04488261, Test Loss: 4.13940607, Test Accuracy: 0.10090000\n",
      "\n",
      "Epoch: 2/300, Train Loss: 0.50857559, Test Loss: 3.98666899, Test Accuracy: 0.12570000\n",
      "\n",
      "Epoch: 3/300, Train Loss: 0.22963276, Test Loss: 3.69668182, Test Accuracy: 0.24440000\n",
      "\n",
      "Epoch: 4/300, Train Loss: 0.14986563, Test Loss: 3.90408527, Test Accuracy: 0.26880000\n",
      "\n",
      "Epoch: 5/300, Train Loss: 0.12822238, Test Loss: 4.33227836, Test Accuracy: 0.26680000\n",
      "\n",
      "Epoch: 6/300, Train Loss: 0.11357996, Test Loss: 4.85693891, Test Accuracy: 0.26140000\n",
      "\n",
      "Epoch: 7/300, Train Loss: 0.10094509, Test Loss: 4.80440885, Test Accuracy: 0.30780000\n",
      "\n",
      "Epoch: 8/300, Train Loss: 0.09652053, Test Loss: 5.41518896, Test Accuracy: 0.30420000\n",
      "\n",
      "Epoch: 9/300, Train Loss: 0.08671210, Test Loss: 5.08839709, Test Accuracy: 0.33180000\n",
      "\n",
      "Epoch: 10/300, Train Loss: 0.08067263, Test Loss: 4.55569457, Test Accuracy: 0.35560000\n",
      "\n",
      "Epoch: 11/300, Train Loss: 0.07181905, Test Loss: 4.29650596, Test Accuracy: 0.39470000\n",
      "\n",
      "Epoch: 12/300, Train Loss: 0.06450128, Test Loss: 4.28331638, Test Accuracy: 0.40850000\n",
      "\n",
      "Epoch: 13/300, Train Loss: 0.06233540, Test Loss: 4.16815649, Test Accuracy: 0.40330000\n",
      "\n",
      "Epoch: 14/300, Train Loss: 0.06330251, Test Loss: 3.67078664, Test Accuracy: 0.42110000\n",
      "\n",
      "Epoch: 15/300, Train Loss: 0.05846561, Test Loss: 3.67523807, Test Accuracy: 0.42070000\n",
      "\n",
      "Epoch: 16/300, Train Loss: 0.05775745, Test Loss: 3.48690922, Test Accuracy: 0.45830000\n",
      "\n",
      "Epoch: 17/300, Train Loss: 0.05434911, Test Loss: 3.32028940, Test Accuracy: 0.48050000\n",
      "\n",
      "Epoch: 18/300, Train Loss: 0.05390446, Test Loss: 3.44309347, Test Accuracy: 0.47340000\n",
      "\n",
      "Epoch: 19/300, Train Loss: 0.05236792, Test Loss: 2.83203640, Test Accuracy: 0.52320000\n",
      "\n",
      "Epoch: 20/300, Train Loss: 0.05313469, Test Loss: 2.55819082, Test Accuracy: 0.54390000\n",
      "\n",
      "Epoch: 21/300, Train Loss: 0.04921984, Test Loss: 2.27647528, Test Accuracy: 0.56620000\n",
      "\n",
      "Epoch: 22/300, Train Loss: 0.04381938, Test Loss: 2.17409369, Test Accuracy: 0.56800000\n",
      "\n",
      "Epoch: 23/300, Train Loss: 0.03814572, Test Loss: 2.01199232, Test Accuracy: 0.59570000\n",
      "\n",
      "Epoch: 24/300, Train Loss: 0.03380242, Test Loss: 1.92155716, Test Accuracy: 0.60580000\n",
      "\n",
      "Epoch: 25/300, Train Loss: 0.03067367, Test Loss: 1.82908468, Test Accuracy: 0.62090000\n",
      "\n",
      "Epoch: 26/300, Train Loss: 0.02753807, Test Loss: 1.71990919, Test Accuracy: 0.63050000\n",
      "\n",
      "Epoch: 27/300, Train Loss: 0.02479242, Test Loss: 1.67810827, Test Accuracy: 0.63870000\n",
      "\n",
      "Epoch: 28/300, Train Loss: 0.02254203, Test Loss: 1.63172479, Test Accuracy: 0.64890000\n",
      "\n",
      "Epoch: 29/300, Train Loss: 0.02028800, Test Loss: 1.54540160, Test Accuracy: 0.66300000\n",
      "\n",
      "Epoch: 30/300, Train Loss: 0.01805609, Test Loss: 1.54499884, Test Accuracy: 0.66700000\n",
      "\n",
      "Epoch: 31/300, Train Loss: 0.01632223, Test Loss: 1.53428128, Test Accuracy: 0.67370000\n",
      "\n",
      "Epoch: 32/300, Train Loss: 0.01447823, Test Loss: 1.49976022, Test Accuracy: 0.68310000\n",
      "\n",
      "Epoch: 33/300, Train Loss: 0.01273132, Test Loss: 1.48807086, Test Accuracy: 0.68880000\n",
      "\n",
      "Epoch: 34/300, Train Loss: 0.01140785, Test Loss: 1.44180163, Test Accuracy: 0.69710000\n",
      "\n",
      "Epoch: 35/300, Train Loss: 0.01008989, Test Loss: 1.41415252, Test Accuracy: 0.70500000\n",
      "\n",
      "Epoch: 36/300, Train Loss: 0.00902642, Test Loss: 1.34862981, Test Accuracy: 0.71800000\n",
      "\n",
      "Epoch: 37/300, Train Loss: 0.00796112, Test Loss: 1.30972537, Test Accuracy: 0.72690000\n",
      "\n",
      "Epoch: 38/300, Train Loss: 0.00701297, Test Loss: 1.27052112, Test Accuracy: 0.73660000\n",
      "\n",
      "Epoch: 39/300, Train Loss: 0.00615216, Test Loss: 1.21280925, Test Accuracy: 0.74760000\n",
      "\n",
      "Epoch: 40/300, Train Loss: 0.00539352, Test Loss: 1.16289499, Test Accuracy: 0.75790000\n",
      "\n",
      "Epoch: 41/300, Train Loss: 0.00467108, Test Loss: 1.13370372, Test Accuracy: 0.76720000\n",
      "\n",
      "Epoch: 42/300, Train Loss: 0.00398280, Test Loss: 1.12167929, Test Accuracy: 0.77280000\n",
      "\n",
      "Epoch: 43/300, Train Loss: 0.00342486, Test Loss: 1.13088634, Test Accuracy: 0.77490000\n",
      "\n",
      "Epoch: 44/300, Train Loss: 0.00294106, Test Loss: 1.13331960, Test Accuracy: 0.77760000\n",
      "\n",
      "Epoch: 45/300, Train Loss: 0.00253825, Test Loss: 1.13295767, Test Accuracy: 0.78190000\n",
      "\n",
      "Epoch: 46/300, Train Loss: 0.00218687, Test Loss: 1.15409594, Test Accuracy: 0.78140000\n",
      "\n",
      "Epoch: 47/300, Train Loss: 0.00184142, Test Loss: 1.15472877, Test Accuracy: 0.78640000\n",
      "\n",
      "Epoch: 48/300, Train Loss: 0.00152853, Test Loss: 1.17900333, Test Accuracy: 0.78600000\n",
      "\n",
      "Epoch: 49/300, Train Loss: 0.00128305, Test Loss: 1.19189845, Test Accuracy: 0.78960000\n",
      "\n",
      "Epoch: 50/300, Train Loss: 0.00104888, Test Loss: 1.20653964, Test Accuracy: 0.79350000\n",
      "\n",
      "Epoch: 51/300, Train Loss: 0.00086619, Test Loss: 1.21660723, Test Accuracy: 0.79480000\n",
      "\n",
      "Epoch: 52/300, Train Loss: 0.00069686, Test Loss: 1.22950425, Test Accuracy: 0.79720000\n",
      "\n",
      "Epoch: 53/300, Train Loss: 0.00059557, Test Loss: 1.25721167, Test Accuracy: 0.79840000\n",
      "\n",
      "Epoch: 54/300, Train Loss: 0.00049702, Test Loss: 1.18153404, Test Accuracy: 0.80810000\n",
      "\n",
      "Epoch: 55/300, Train Loss: 0.00041598, Test Loss: 1.30774849, Test Accuracy: 0.79870000\n",
      "\n",
      "Epoch: 56/300, Train Loss: 0.00034943, Test Loss: 1.18520408, Test Accuracy: 0.81200000\n",
      "\n",
      "Epoch: 57/300, Train Loss: 0.00027971, Test Loss: 1.18204345, Test Accuracy: 0.81600000\n",
      "\n",
      "Epoch: 58/300, Train Loss: 0.00022685, Test Loss: 1.12601019, Test Accuracy: 0.82280000\n",
      "\n",
      "Epoch: 59/300, Train Loss: 0.00018661, Test Loss: 1.13412980, Test Accuracy: 0.82430000\n",
      "\n",
      "Epoch: 60/300, Train Loss: 0.00014756, Test Loss: 1.08159219, Test Accuracy: 0.83480000\n",
      "\n",
      "Epoch: 61/300, Train Loss: 0.00011754, Test Loss: 1.12063399, Test Accuracy: 0.83260000\n",
      "\n",
      "Epoch: 62/300, Train Loss: 0.00051876, Test Loss: 1.05364502, Test Accuracy: 0.84480000\n",
      "\n",
      "Epoch: 63/300, Train Loss: 0.00155391, Test Loss: 0.87943274, Test Accuracy: 0.86120000\n",
      "\n",
      "Epoch: 64/300, Train Loss: 0.00169010, Test Loss: 0.71415069, Test Accuracy: 0.88150000\n",
      "\n",
      "Epoch: 65/300, Train Loss: 0.00197165, Test Loss: 0.70202128, Test Accuracy: 0.88400000\n",
      "\n",
      "Epoch: 66/300, Train Loss: 0.00173777, Test Loss: 0.72501977, Test Accuracy: 0.88010000\n",
      "\n",
      "Epoch: 67/300, Train Loss: 0.00130866, Test Loss: 0.63641345, Test Accuracy: 0.89400000\n",
      "\n",
      "Epoch: 68/300, Train Loss: 0.00098367, Test Loss: 0.62607429, Test Accuracy: 0.89630000\n",
      "\n",
      "Epoch: 69/300, Train Loss: 0.00070937, Test Loss: 0.63837733, Test Accuracy: 0.89500000\n",
      "\n",
      "Epoch: 70/300, Train Loss: 0.00049463, Test Loss: 0.62050508, Test Accuracy: 0.89960000\n",
      "\n",
      "Epoch: 71/300, Train Loss: 0.00035062, Test Loss: 0.62848236, Test Accuracy: 0.89930000\n",
      "\n",
      "Epoch: 72/300, Train Loss: 0.00027098, Test Loss: 0.64059591, Test Accuracy: 0.89840000\n",
      "\n",
      "Epoch: 73/300, Train Loss: 0.00018530, Test Loss: 0.65724597, Test Accuracy: 0.89770000\n",
      "\n",
      "Epoch: 74/300, Train Loss: 0.00013948, Test Loss: 0.70012561, Test Accuracy: 0.89340000\n",
      "\n",
      "Epoch: 75/300, Train Loss: 0.00011718, Test Loss: 0.72280319, Test Accuracy: 0.89100000\n",
      "\n",
      "Epoch: 76/300, Train Loss: 0.00009940, Test Loss: 0.77949210, Test Accuracy: 0.88550000\n",
      "\n",
      "Epoch: 77/300, Train Loss: 0.00021827, Test Loss: 0.71107275, Test Accuracy: 0.89050000\n",
      "\n",
      "Epoch: 78/300, Train Loss: 0.00025681, Test Loss: 0.84317990, Test Accuracy: 0.87840000\n",
      "\n",
      "Epoch: 79/300, Train Loss: 0.00044076, Test Loss: 0.60990753, Test Accuracy: 0.90350000\n",
      "\n",
      "Epoch: 80/300, Train Loss: 0.00040006, Test Loss: 0.56165466, Test Accuracy: 0.91320000\n",
      "\n",
      "Epoch: 81/300, Train Loss: 0.00038158, Test Loss: 0.55710671, Test Accuracy: 0.91610000\n",
      "\n",
      "Epoch: 82/300, Train Loss: 0.00039545, Test Loss: 0.50408122, Test Accuracy: 0.92430000\n",
      "\n",
      "Epoch: 83/300, Train Loss: 0.00047803, Test Loss: 0.50403507, Test Accuracy: 0.92280000\n",
      "\n",
      "Epoch: 84/300, Train Loss: 0.00038968, Test Loss: 0.58514932, Test Accuracy: 0.91080000\n",
      "\n",
      "Epoch: 85/300, Train Loss: 0.00037893, Test Loss: 0.52100151, Test Accuracy: 0.92400000\n",
      "\n",
      "Epoch: 86/300, Train Loss: 0.00025769, Test Loss: 0.45092085, Test Accuracy: 0.93330000\n",
      "\n",
      "Epoch: 87/300, Train Loss: 0.00018118, Test Loss: 0.49100968, Test Accuracy: 0.92730000\n",
      "\n",
      "Epoch: 88/300, Train Loss: 0.00012607, Test Loss: 0.43985303, Test Accuracy: 0.93560000\n",
      "\n",
      "Epoch: 89/300, Train Loss: 0.00008331, Test Loss: 0.44475771, Test Accuracy: 0.93540000\n",
      "\n",
      "Epoch: 90/300, Train Loss: 0.00005646, Test Loss: 0.45094334, Test Accuracy: 0.93590000\n",
      "\n",
      "Epoch: 91/300, Train Loss: 0.00003668, Test Loss: 0.46208589, Test Accuracy: 0.93390000\n",
      "\n",
      "Epoch: 92/300, Train Loss: 0.00002405, Test Loss: 0.46436901, Test Accuracy: 0.93410000\n",
      "\n",
      "Epoch: 93/300, Train Loss: 0.00001558, Test Loss: 0.48004996, Test Accuracy: 0.93370000\n",
      "\n",
      "Epoch: 94/300, Train Loss: 0.00000995, Test Loss: 0.49166902, Test Accuracy: 0.93210000\n",
      "\n",
      "Epoch: 95/300, Train Loss: 0.00000632, Test Loss: 0.50177580, Test Accuracy: 0.93080000\n",
      "\n",
      "Epoch: 96/300, Train Loss: 0.00000404, Test Loss: 0.51471120, Test Accuracy: 0.93040000\n",
      "\n",
      "Epoch: 97/300, Train Loss: 0.00000259, Test Loss: 0.52364141, Test Accuracy: 0.93020000\n",
      "\n",
      "Epoch: 98/300, Train Loss: 0.00000168, Test Loss: 0.53963441, Test Accuracy: 0.92940000\n",
      "\n",
      "Epoch: 99/300, Train Loss: 0.00000110, Test Loss: 0.55616945, Test Accuracy: 0.92890000\n",
      "\n",
      "Epoch: 100/300, Train Loss: 0.00000074, Test Loss: 0.57143313, Test Accuracy: 0.92780000\n",
      "\n",
      "Epoch: 101/300, Train Loss: 0.00000051, Test Loss: 0.58382934, Test Accuracy: 0.92710000\n",
      "\n",
      "Epoch: 102/300, Train Loss: 0.00000036, Test Loss: 0.59336365, Test Accuracy: 0.92710000\n",
      "\n",
      "Epoch: 103/300, Train Loss: 0.00000027, Test Loss: 0.60167189, Test Accuracy: 0.92690000\n",
      "\n",
      "Epoch: 104/300, Train Loss: 0.00000021, Test Loss: 0.60816909, Test Accuracy: 0.92720000\n",
      "\n",
      "Epoch: 105/300, Train Loss: 0.00000017, Test Loss: 0.61281195, Test Accuracy: 0.92740000\n",
      "\n",
      "Epoch: 106/300, Train Loss: 0.00000014, Test Loss: 0.61549625, Test Accuracy: 0.92870000\n",
      "\n",
      "Epoch: 107/300, Train Loss: 0.00000013, Test Loss: 0.61722953, Test Accuracy: 0.92940000\n",
      "\n",
      "Epoch: 108/300, Train Loss: 0.00000012, Test Loss: 0.61776811, Test Accuracy: 0.93080000\n",
      "\n",
      "Epoch: 109/300, Train Loss: 0.00000011, Test Loss: 0.61769569, Test Accuracy: 0.93230000\n",
      "\n",
      "Epoch: 110/300, Train Loss: 0.00000010, Test Loss: 0.61610408, Test Accuracy: 0.93300000\n",
      "\n",
      "Epoch: 111/300, Train Loss: 0.00000009, Test Loss: 0.61351122, Test Accuracy: 0.93360000\n",
      "\n",
      "Epoch: 112/300, Train Loss: 0.00000009, Test Loss: 0.61057229, Test Accuracy: 0.93450000\n",
      "\n",
      "Epoch: 113/300, Train Loss: 0.00000009, Test Loss: 0.60577064, Test Accuracy: 0.93530000\n",
      "\n",
      "Epoch: 114/300, Train Loss: 0.00000008, Test Loss: 0.60210948, Test Accuracy: 0.93600000\n",
      "\n",
      "Epoch: 115/300, Train Loss: 0.00000008, Test Loss: 0.59682383, Test Accuracy: 0.93680000\n",
      "\n",
      "Epoch: 116/300, Train Loss: 0.00000008, Test Loss: 0.59158749, Test Accuracy: 0.93790000\n",
      "\n",
      "Epoch: 117/300, Train Loss: 0.00000007, Test Loss: 0.58598938, Test Accuracy: 0.93910000\n",
      "\n",
      "Epoch: 118/300, Train Loss: 0.00000007, Test Loss: 0.58104487, Test Accuracy: 0.93970000\n",
      "\n",
      "Epoch: 119/300, Train Loss: 0.00000007, Test Loss: 0.57648085, Test Accuracy: 0.94050000\n",
      "\n",
      "Epoch: 120/300, Train Loss: 0.00000007, Test Loss: 0.57104780, Test Accuracy: 0.94140000\n",
      "\n",
      "Epoch: 121/300, Train Loss: 0.00000007, Test Loss: 0.56645573, Test Accuracy: 0.94180000\n",
      "\n",
      "Epoch: 122/300, Train Loss: 0.00000006, Test Loss: 0.56084546, Test Accuracy: 0.94240000\n",
      "\n",
      "Epoch: 123/300, Train Loss: 0.00000006, Test Loss: 0.55532152, Test Accuracy: 0.94310000\n",
      "\n",
      "Epoch: 124/300, Train Loss: 0.00000006, Test Loss: 0.55016926, Test Accuracy: 0.94390000\n",
      "\n",
      "Epoch: 125/300, Train Loss: 0.00000006, Test Loss: 0.54602988, Test Accuracy: 0.94540000\n",
      "\n",
      "Epoch: 126/300, Train Loss: 0.00000006, Test Loss: 0.54262754, Test Accuracy: 0.94600000\n",
      "\n",
      "Epoch: 127/300, Train Loss: 0.00000006, Test Loss: 0.53805318, Test Accuracy: 0.94670000\n",
      "\n",
      "Epoch: 128/300, Train Loss: 0.00000006, Test Loss: 0.53330094, Test Accuracy: 0.94720000\n",
      "\n",
      "Epoch: 129/300, Train Loss: 0.00000005, Test Loss: 0.52962827, Test Accuracy: 0.94760000\n",
      "\n",
      "Epoch: 130/300, Train Loss: 0.00000005, Test Loss: 0.52538837, Test Accuracy: 0.94830000\n",
      "\n",
      "Epoch: 131/300, Train Loss: 0.00000005, Test Loss: 0.52193923, Test Accuracy: 0.94870000\n",
      "\n",
      "Epoch: 132/300, Train Loss: 0.00000005, Test Loss: 0.51674613, Test Accuracy: 0.95020000\n",
      "\n",
      "Epoch: 133/300, Train Loss: 0.00000005, Test Loss: 0.51290043, Test Accuracy: 0.95100000\n",
      "\n",
      "Epoch: 134/300, Train Loss: 0.00000005, Test Loss: 0.50821758, Test Accuracy: 0.95140000\n",
      "\n",
      "Epoch: 135/300, Train Loss: 0.00000005, Test Loss: 0.50333625, Test Accuracy: 0.95240000\n",
      "\n",
      "Epoch: 136/300, Train Loss: 0.00000005, Test Loss: 0.49732071, Test Accuracy: 0.95290000\n",
      "\n",
      "Epoch: 137/300, Train Loss: 0.00000005, Test Loss: 0.49169797, Test Accuracy: 0.95360000\n",
      "\n",
      "Epoch: 138/300, Train Loss: 0.00000004, Test Loss: 0.48474554, Test Accuracy: 0.95480000\n",
      "\n",
      "Epoch: 139/300, Train Loss: 0.00000004, Test Loss: 0.47863937, Test Accuracy: 0.95530000\n",
      "\n",
      "Epoch: 140/300, Train Loss: 0.00000004, Test Loss: 0.47426664, Test Accuracy: 0.95560000\n",
      "\n",
      "Epoch: 141/300, Train Loss: 0.00000004, Test Loss: 0.46984210, Test Accuracy: 0.95620000\n",
      "\n",
      "Epoch: 142/300, Train Loss: 0.00000004, Test Loss: 0.46598205, Test Accuracy: 0.95690000\n",
      "\n",
      "Epoch: 143/300, Train Loss: 0.00000004, Test Loss: 0.46161106, Test Accuracy: 0.95740000\n",
      "\n",
      "Epoch: 144/300, Train Loss: 0.00000004, Test Loss: 0.45777015, Test Accuracy: 0.95820000\n",
      "\n",
      "Epoch: 145/300, Train Loss: 0.00000004, Test Loss: 0.45431447, Test Accuracy: 0.95850000\n",
      "\n",
      "Epoch: 146/300, Train Loss: 0.00000004, Test Loss: 0.45105593, Test Accuracy: 0.95930000\n",
      "\n",
      "Epoch: 147/300, Train Loss: 0.00000004, Test Loss: 0.44727733, Test Accuracy: 0.95960000\n",
      "\n",
      "Epoch: 148/300, Train Loss: 0.00000004, Test Loss: 0.44390339, Test Accuracy: 0.95970000\n",
      "\n",
      "Epoch: 149/300, Train Loss: 0.00000004, Test Loss: 0.44090907, Test Accuracy: 0.96090000\n",
      "\n",
      "Epoch: 150/300, Train Loss: 0.00000003, Test Loss: 0.43788519, Test Accuracy: 0.96080000\n",
      "\n",
      "Epoch: 151/300, Train Loss: 0.00000003, Test Loss: 0.43485241, Test Accuracy: 0.96140000\n",
      "\n",
      "Epoch: 152/300, Train Loss: 0.00000003, Test Loss: 0.43230693, Test Accuracy: 0.96200000\n",
      "\n",
      "Epoch: 153/300, Train Loss: 0.00000003, Test Loss: 0.42925931, Test Accuracy: 0.96220000\n",
      "\n",
      "Epoch: 154/300, Train Loss: 0.00000003, Test Loss: 0.42586042, Test Accuracy: 0.96250000\n",
      "\n",
      "Epoch: 155/300, Train Loss: 0.00000003, Test Loss: 0.42316348, Test Accuracy: 0.96260000\n",
      "\n",
      "Epoch: 156/300, Train Loss: 0.00000003, Test Loss: 0.42024144, Test Accuracy: 0.96280000\n",
      "\n",
      "Epoch: 157/300, Train Loss: 0.00000003, Test Loss: 0.41785117, Test Accuracy: 0.96320000\n",
      "\n",
      "Epoch: 158/300, Train Loss: 0.00000003, Test Loss: 0.41514547, Test Accuracy: 0.96340000\n",
      "\n",
      "Epoch: 159/300, Train Loss: 0.00000003, Test Loss: 0.41233185, Test Accuracy: 0.96390000\n",
      "\n",
      "Epoch: 160/300, Train Loss: 0.00000003, Test Loss: 0.41010242, Test Accuracy: 0.96410000\n",
      "\n",
      "Epoch: 161/300, Train Loss: 0.00000003, Test Loss: 0.40748513, Test Accuracy: 0.96450000\n",
      "\n",
      "Epoch: 162/300, Train Loss: 0.00000003, Test Loss: 0.40532627, Test Accuracy: 0.96460000\n",
      "\n",
      "Epoch: 163/300, Train Loss: 0.00000003, Test Loss: 0.40316077, Test Accuracy: 0.96490000\n",
      "\n",
      "Epoch: 164/300, Train Loss: 0.00000003, Test Loss: 0.40054232, Test Accuracy: 0.96530000\n",
      "\n",
      "Epoch: 165/300, Train Loss: 0.00000003, Test Loss: 0.39837492, Test Accuracy: 0.96520000\n",
      "\n",
      "Epoch: 166/300, Train Loss: 0.00000003, Test Loss: 0.39574410, Test Accuracy: 0.96570000\n",
      "\n",
      "Epoch: 167/300, Train Loss: 0.00000003, Test Loss: 0.39381080, Test Accuracy: 0.96590000\n",
      "\n",
      "Epoch: 168/300, Train Loss: 0.00000002, Test Loss: 0.39118747, Test Accuracy: 0.96660000\n",
      "\n",
      "Epoch: 169/300, Train Loss: 0.00000002, Test Loss: 0.38901806, Test Accuracy: 0.96690000\n",
      "\n",
      "Epoch: 170/300, Train Loss: 0.00000002, Test Loss: 0.38748277, Test Accuracy: 0.96690000\n",
      "\n",
      "Epoch: 171/300, Train Loss: 0.00000002, Test Loss: 0.38556984, Test Accuracy: 0.96680000\n",
      "\n",
      "Epoch: 172/300, Train Loss: 0.00000002, Test Loss: 0.38372045, Test Accuracy: 0.96720000\n",
      "\n",
      "Epoch: 173/300, Train Loss: 0.00000002, Test Loss: 0.38244699, Test Accuracy: 0.96750000\n",
      "\n",
      "Epoch: 174/300, Train Loss: 0.00000002, Test Loss: 0.37973242, Test Accuracy: 0.96770000\n",
      "\n",
      "Epoch: 175/300, Train Loss: 0.00000002, Test Loss: 0.37805028, Test Accuracy: 0.96800000\n",
      "\n",
      "Epoch: 176/300, Train Loss: 0.00000002, Test Loss: 0.37614884, Test Accuracy: 0.96810000\n",
      "\n",
      "Epoch: 177/300, Train Loss: 0.00000002, Test Loss: 0.37414744, Test Accuracy: 0.96800000\n",
      "\n",
      "Epoch: 178/300, Train Loss: 0.00000002, Test Loss: 0.37218029, Test Accuracy: 0.96830000\n",
      "\n",
      "Epoch: 179/300, Train Loss: 0.00000002, Test Loss: 0.37043735, Test Accuracy: 0.96870000\n",
      "\n",
      "Epoch: 180/300, Train Loss: 0.00000002, Test Loss: 0.36795780, Test Accuracy: 0.96870000\n",
      "\n",
      "Epoch: 181/300, Train Loss: 0.00000002, Test Loss: 0.36598458, Test Accuracy: 0.96900000\n",
      "\n",
      "Epoch: 182/300, Train Loss: 0.00000002, Test Loss: 0.36410887, Test Accuracy: 0.96920000\n",
      "\n",
      "Epoch: 183/300, Train Loss: 0.00000002, Test Loss: 0.36192675, Test Accuracy: 0.96930000\n",
      "\n",
      "Epoch: 184/300, Train Loss: 0.00000002, Test Loss: 0.35980831, Test Accuracy: 0.96940000\n",
      "\n",
      "Epoch: 185/300, Train Loss: 0.00000002, Test Loss: 0.35741867, Test Accuracy: 0.96970000\n",
      "\n",
      "Epoch: 186/300, Train Loss: 0.00000002, Test Loss: 0.35531123, Test Accuracy: 0.96980000\n",
      "\n",
      "Epoch: 187/300, Train Loss: 0.00000002, Test Loss: 0.35294117, Test Accuracy: 0.97000000\n",
      "\n",
      "Epoch: 188/300, Train Loss: 0.00000002, Test Loss: 0.35052772, Test Accuracy: 0.97040000\n",
      "\n",
      "Epoch: 189/300, Train Loss: 0.00000002, Test Loss: 0.34828882, Test Accuracy: 0.97060000\n",
      "\n",
      "Epoch: 190/300, Train Loss: 0.00000002, Test Loss: 0.34655539, Test Accuracy: 0.97050000\n",
      "\n",
      "Epoch: 191/300, Train Loss: 0.00000002, Test Loss: 0.34517996, Test Accuracy: 0.97050000\n",
      "\n",
      "Epoch: 192/300, Train Loss: 0.00000002, Test Loss: 0.34397475, Test Accuracy: 0.97070000\n",
      "\n",
      "Epoch: 193/300, Train Loss: 0.00000002, Test Loss: 0.34240222, Test Accuracy: 0.97110000\n",
      "\n",
      "Epoch: 194/300, Train Loss: 0.00000001, Test Loss: 0.34045551, Test Accuracy: 0.97140000\n",
      "\n",
      "Epoch: 195/300, Train Loss: 0.00000001, Test Loss: 0.33892433, Test Accuracy: 0.97120000\n",
      "\n",
      "Epoch: 196/300, Train Loss: 0.00000001, Test Loss: 0.33752225, Test Accuracy: 0.97130000\n",
      "\n",
      "Epoch: 197/300, Train Loss: 0.00000001, Test Loss: 0.33634911, Test Accuracy: 0.97170000\n",
      "\n",
      "Epoch: 198/300, Train Loss: 0.00000001, Test Loss: 0.33487208, Test Accuracy: 0.97120000\n",
      "\n",
      "Epoch: 199/300, Train Loss: 0.00000001, Test Loss: 0.33366160, Test Accuracy: 0.97120000\n",
      "\n",
      "Epoch: 200/300, Train Loss: 0.00000001, Test Loss: 0.33246115, Test Accuracy: 0.97150000\n",
      "\n",
      "Epoch: 201/300, Train Loss: 0.00000001, Test Loss: 0.33106241, Test Accuracy: 0.97200000\n",
      "\n",
      "Epoch: 202/300, Train Loss: 0.00000001, Test Loss: 0.32969229, Test Accuracy: 0.97200000\n",
      "\n",
      "Epoch: 203/300, Train Loss: 0.00000001, Test Loss: 0.32837024, Test Accuracy: 0.97230000\n",
      "\n",
      "Epoch: 204/300, Train Loss: 0.00000001, Test Loss: 0.32697992, Test Accuracy: 0.97250000\n",
      "\n",
      "Epoch: 205/300, Train Loss: 0.00000001, Test Loss: 0.32558920, Test Accuracy: 0.97250000\n",
      "\n",
      "Epoch: 206/300, Train Loss: 0.00000001, Test Loss: 0.32434942, Test Accuracy: 0.97250000\n",
      "\n",
      "Epoch: 207/300, Train Loss: 0.00000001, Test Loss: 0.32284770, Test Accuracy: 0.97290000\n",
      "\n",
      "Epoch: 208/300, Train Loss: 0.00000001, Test Loss: 0.32150203, Test Accuracy: 0.97280000\n",
      "\n",
      "Epoch: 209/300, Train Loss: 0.00000001, Test Loss: 0.32049903, Test Accuracy: 0.97280000\n",
      "\n",
      "Epoch: 210/300, Train Loss: 0.00000001, Test Loss: 0.31914627, Test Accuracy: 0.97290000\n",
      "\n",
      "Epoch: 211/300, Train Loss: 0.00000001, Test Loss: 0.31810510, Test Accuracy: 0.97290000\n",
      "\n",
      "Epoch: 212/300, Train Loss: 0.00000001, Test Loss: 0.31732488, Test Accuracy: 0.97290000\n",
      "\n",
      "Epoch: 213/300, Train Loss: 0.00000001, Test Loss: 0.31612992, Test Accuracy: 0.97300000\n",
      "\n",
      "Epoch: 214/300, Train Loss: 0.00000001, Test Loss: 0.31534412, Test Accuracy: 0.97320000\n",
      "\n",
      "Epoch: 215/300, Train Loss: 0.00000001, Test Loss: 0.31465949, Test Accuracy: 0.97360000\n",
      "\n",
      "Epoch: 216/300, Train Loss: 0.00000001, Test Loss: 0.31391720, Test Accuracy: 0.97360000\n",
      "\n",
      "Epoch: 217/300, Train Loss: 0.00000001, Test Loss: 0.31310615, Test Accuracy: 0.97380000\n",
      "\n",
      "Epoch: 218/300, Train Loss: 0.00000001, Test Loss: 0.31190757, Test Accuracy: 0.97390000\n",
      "\n",
      "Epoch: 219/300, Train Loss: 0.00000001, Test Loss: 0.31089988, Test Accuracy: 0.97380000\n",
      "\n",
      "Epoch: 220/300, Train Loss: 0.00000001, Test Loss: 0.30998598, Test Accuracy: 0.97370000\n",
      "\n",
      "Epoch: 221/300, Train Loss: 0.00000001, Test Loss: 0.30920451, Test Accuracy: 0.97410000\n",
      "\n",
      "Epoch: 222/300, Train Loss: 0.00000001, Test Loss: 0.30848111, Test Accuracy: 0.97410000\n",
      "\n",
      "Epoch: 223/300, Train Loss: 0.00000001, Test Loss: 0.30750456, Test Accuracy: 0.97420000\n",
      "\n",
      "Epoch: 224/300, Train Loss: 0.00000001, Test Loss: 0.30706005, Test Accuracy: 0.97400000\n",
      "\n",
      "Epoch: 225/300, Train Loss: 0.00000001, Test Loss: 0.30643649, Test Accuracy: 0.97400000\n",
      "\n",
      "Epoch: 226/300, Train Loss: 0.00000001, Test Loss: 0.30632953, Test Accuracy: 0.97390000\n",
      "\n",
      "Epoch: 227/300, Train Loss: 0.00000001, Test Loss: 0.30565572, Test Accuracy: 0.97390000\n",
      "\n",
      "Epoch: 228/300, Train Loss: 0.00000001, Test Loss: 0.30498508, Test Accuracy: 0.97400000\n",
      "\n",
      "Epoch: 229/300, Train Loss: 0.00000001, Test Loss: 0.30459599, Test Accuracy: 0.97390000\n",
      "\n",
      "Epoch: 230/300, Train Loss: 0.00000001, Test Loss: 0.30458017, Test Accuracy: 0.97370000\n",
      "\n",
      "Epoch: 231/300, Train Loss: 0.00000001, Test Loss: 0.30419566, Test Accuracy: 0.97370000\n",
      "\n",
      "Epoch: 232/300, Train Loss: 0.00000001, Test Loss: 0.30417558, Test Accuracy: 0.97380000\n",
      "\n",
      "Epoch: 233/300, Train Loss: 0.00000001, Test Loss: 0.30334688, Test Accuracy: 0.97390000\n",
      "\n",
      "Epoch: 234/300, Train Loss: 0.00000001, Test Loss: 0.30373271, Test Accuracy: 0.97380000\n",
      "\n",
      "Epoch: 235/300, Train Loss: 0.00000001, Test Loss: 0.30349328, Test Accuracy: 0.97390000\n",
      "\n",
      "Epoch: 236/300, Train Loss: 0.00000000, Test Loss: 0.30329530, Test Accuracy: 0.97400000\n",
      "\n",
      "Epoch: 237/300, Train Loss: 0.00000000, Test Loss: 0.30287132, Test Accuracy: 0.97400000\n",
      "\n",
      "Epoch: 238/300, Train Loss: 0.00000000, Test Loss: 0.30222944, Test Accuracy: 0.97420000\n",
      "\n",
      "Epoch: 239/300, Train Loss: 0.00000000, Test Loss: 0.30173869, Test Accuracy: 0.97440000\n",
      "\n",
      "Epoch: 240/300, Train Loss: 0.00000000, Test Loss: 0.30140034, Test Accuracy: 0.97460000\n",
      "\n",
      "Epoch: 241/300, Train Loss: 0.00000000, Test Loss: 0.30045322, Test Accuracy: 0.97470000\n",
      "\n",
      "Epoch: 242/300, Train Loss: 0.00000000, Test Loss: 0.29987218, Test Accuracy: 0.97460000\n",
      "\n",
      "Epoch: 243/300, Train Loss: 0.00000000, Test Loss: 0.29923141, Test Accuracy: 0.97480000\n",
      "\n",
      "Epoch: 244/300, Train Loss: 0.00000000, Test Loss: 0.29823972, Test Accuracy: 0.97500000\n",
      "\n",
      "Epoch: 245/300, Train Loss: 0.00000000, Test Loss: 0.29777763, Test Accuracy: 0.97520000\n",
      "\n",
      "Epoch: 246/300, Train Loss: 0.00000000, Test Loss: 0.29739956, Test Accuracy: 0.97500000\n",
      "\n",
      "Epoch: 247/300, Train Loss: 0.00000000, Test Loss: 0.29682487, Test Accuracy: 0.97530000\n",
      "\n",
      "Epoch: 248/300, Train Loss: 0.00000000, Test Loss: 0.29609814, Test Accuracy: 0.97540000\n",
      "\n",
      "Epoch: 249/300, Train Loss: 0.00000000, Test Loss: 0.29550975, Test Accuracy: 0.97550000\n",
      "\n",
      "Epoch: 250/300, Train Loss: 0.00000000, Test Loss: 0.29498000, Test Accuracy: 0.97560000\n",
      "\n",
      "Epoch: 251/300, Train Loss: 0.00000000, Test Loss: 0.29435075, Test Accuracy: 0.97580000\n",
      "\n",
      "Epoch: 252/300, Train Loss: 0.00000000, Test Loss: 0.29355073, Test Accuracy: 0.97580000\n",
      "\n",
      "Epoch: 253/300, Train Loss: 0.00000000, Test Loss: 0.29317852, Test Accuracy: 0.97580000\n",
      "\n",
      "Epoch: 254/300, Train Loss: 0.00000000, Test Loss: 0.29305433, Test Accuracy: 0.97580000\n",
      "\n",
      "Epoch: 255/300, Train Loss: 0.00000000, Test Loss: 0.29252095, Test Accuracy: 0.97600000\n",
      "\n",
      "Epoch: 256/300, Train Loss: 0.00000000, Test Loss: 0.29255174, Test Accuracy: 0.97580000\n",
      "\n",
      "Epoch: 257/300, Train Loss: 0.00000000, Test Loss: 0.29255582, Test Accuracy: 0.97620000\n",
      "\n",
      "Epoch: 258/300, Train Loss: 0.00000000, Test Loss: 0.29244016, Test Accuracy: 0.97600000\n",
      "\n",
      "Epoch: 259/300, Train Loss: 0.00000000, Test Loss: 0.29231920, Test Accuracy: 0.97640000\n",
      "\n",
      "Epoch: 260/300, Train Loss: 0.00000000, Test Loss: 0.29232985, Test Accuracy: 0.97640000\n",
      "\n",
      "Epoch: 261/300, Train Loss: 0.00000000, Test Loss: 0.29240537, Test Accuracy: 0.97630000\n",
      "\n",
      "Epoch: 262/300, Train Loss: 0.00000000, Test Loss: 0.29270242, Test Accuracy: 0.97620000\n",
      "\n",
      "Epoch: 263/300, Train Loss: 0.00000000, Test Loss: 0.29289444, Test Accuracy: 0.97610000\n",
      "\n",
      "Epoch: 264/300, Train Loss: 0.00000000, Test Loss: 0.29308188, Test Accuracy: 0.97620000\n",
      "\n",
      "Epoch: 265/300, Train Loss: 0.00000000, Test Loss: 0.29310792, Test Accuracy: 0.97620000\n",
      "\n",
      "Epoch: 266/300, Train Loss: 0.00000000, Test Loss: 0.29320566, Test Accuracy: 0.97630000\n",
      "\n",
      "Epoch: 267/300, Train Loss: 0.00000000, Test Loss: 0.29312345, Test Accuracy: 0.97640000\n",
      "\n",
      "Epoch: 268/300, Train Loss: 0.00000000, Test Loss: 0.29335212, Test Accuracy: 0.97650000\n",
      "\n",
      "Epoch: 269/300, Train Loss: 0.00000000, Test Loss: 0.29337336, Test Accuracy: 0.97650000\n",
      "\n",
      "Epoch: 270/300, Train Loss: 0.00000000, Test Loss: 0.29316622, Test Accuracy: 0.97650000\n",
      "\n",
      "Epoch: 271/300, Train Loss: 0.00000000, Test Loss: 0.29317311, Test Accuracy: 0.97650000\n",
      "\n",
      "Epoch: 272/300, Train Loss: 0.00000000, Test Loss: 0.29319720, Test Accuracy: 0.97650000\n",
      "\n",
      "Epoch: 273/300, Train Loss: 0.00000000, Test Loss: 0.29318457, Test Accuracy: 0.97650000\n",
      "\n",
      "Epoch: 274/300, Train Loss: 0.00000000, Test Loss: 0.29311067, Test Accuracy: 0.97630000\n",
      "\n",
      "Epoch: 275/300, Train Loss: 0.00000000, Test Loss: 0.29284406, Test Accuracy: 0.97620000\n",
      "\n",
      "Epoch: 276/300, Train Loss: 0.00000000, Test Loss: 0.29287027, Test Accuracy: 0.97630000\n",
      "\n",
      "Epoch: 277/300, Train Loss: 0.00000000, Test Loss: 0.29290741, Test Accuracy: 0.97630000\n",
      "\n",
      "Epoch: 278/300, Train Loss: 0.00000000, Test Loss: 0.29265292, Test Accuracy: 0.97610000\n",
      "\n",
      "Epoch: 279/300, Train Loss: 0.00000000, Test Loss: 0.29259690, Test Accuracy: 0.97620000\n",
      "\n",
      "Epoch: 280/300, Train Loss: 0.00000000, Test Loss: 0.29258137, Test Accuracy: 0.97630000\n",
      "\n",
      "Epoch: 281/300, Train Loss: 0.00000000, Test Loss: 0.29268645, Test Accuracy: 0.97640000\n",
      "\n",
      "Epoch: 282/300, Train Loss: 0.00000000, Test Loss: 0.29235558, Test Accuracy: 0.97640000\n",
      "\n",
      "Epoch: 283/300, Train Loss: 0.00000000, Test Loss: 0.29260735, Test Accuracy: 0.97640000\n",
      "\n",
      "Epoch: 284/300, Train Loss: 0.00000000, Test Loss: 0.29234708, Test Accuracy: 0.97630000\n",
      "\n",
      "Epoch: 285/300, Train Loss: 0.00000000, Test Loss: 0.29222390, Test Accuracy: 0.97630000\n",
      "\n",
      "Epoch: 286/300, Train Loss: 0.00000000, Test Loss: 0.29224476, Test Accuracy: 0.97630000\n",
      "\n",
      "Epoch: 287/300, Train Loss: 0.00000000, Test Loss: 0.29212830, Test Accuracy: 0.97620000\n",
      "\n",
      "Epoch: 288/300, Train Loss: 0.00000000, Test Loss: 0.29207912, Test Accuracy: 0.97610000\n",
      "\n",
      "Epoch: 289/300, Train Loss: 0.00000000, Test Loss: 0.29215066, Test Accuracy: 0.97590000\n",
      "\n",
      "Epoch: 290/300, Train Loss: 0.00000000, Test Loss: 0.29176770, Test Accuracy: 0.97610000\n",
      "\n",
      "Epoch: 291/300, Train Loss: 0.00000000, Test Loss: 0.29185088, Test Accuracy: 0.97600000\n",
      "\n",
      "Epoch: 292/300, Train Loss: 0.00000000, Test Loss: 0.29182948, Test Accuracy: 0.97610000\n",
      "\n",
      "Epoch: 293/300, Train Loss: 0.00000000, Test Loss: 0.29175022, Test Accuracy: 0.97610000\n",
      "\n",
      "Epoch: 294/300, Train Loss: 0.00000000, Test Loss: 0.29178862, Test Accuracy: 0.97610000\n",
      "\n",
      "Epoch: 295/300, Train Loss: 0.00000000, Test Loss: 0.29157876, Test Accuracy: 0.97600000\n",
      "\n",
      "Epoch: 296/300, Train Loss: 0.00000000, Test Loss: 0.29164818, Test Accuracy: 0.97610000\n",
      "\n",
      "Epoch: 297/300, Train Loss: 0.00000000, Test Loss: 0.29168934, Test Accuracy: 0.97610000\n",
      "\n",
      "Epoch: 298/300, Train Loss: 0.00000000, Test Loss: 0.29159734, Test Accuracy: 0.97620000\n",
      "\n",
      "Epoch: 299/300, Train Loss: 0.00000000, Test Loss: 0.29161846, Test Accuracy: 0.97630000\n",
      "\n",
      "Epoch: 300/300, Train Loss: 0.00000000, Test Loss: 0.29171256, Test Accuracy: 0.97660000\n",
      "average_sparsity: [0.7669585347175598, 0.7155770659446716, 0.6546698808670044, 0.6282609701156616, 0.5754929780960083, 0.5584385395050049, 0.5248773694038391, 0.4981813430786133, 0.4881841838359833, 0.49445033073425293, 0.4941973090171814, 0.4909736216068268, 0.48774808645248413, 0.49485868215560913, 0.4880668520927429, 0.48779532313346863, 0.48879316449165344, 0.49118471145629883, 0.49804794788360596, 0.49288368225097656, 0.49700313806533813, 0.49718472361564636, 0.5032632946968079, 0.5033719539642334, 0.5049799084663391, 0.507380485534668, 0.5070720911026001, 0.5064823031425476, 0.5102086663246155, 0.5113553404808044, 0.5102222561836243, 0.511740505695343, 0.5107841491699219, 0.5119487047195435, 0.5112350583076477, 0.5117248296737671, 0.5108822584152222, 0.5110042691230774, 0.5112171769142151, 0.5117627382278442, 0.5113165378570557, 0.5098147392272949, 0.5095864534378052, 0.5082448720932007, 0.5078682899475098, 0.5078879594802856, 0.5081467032432556, 0.5081672072410583, 0.5080720782279968, 0.5061249136924744, 0.5065329074859619, 0.5058838129043579, 0.5042363405227661, 0.5041709542274475, 0.5044148564338684, 0.5053086280822754, 0.5046250820159912, 0.5025933384895325, 0.5021647214889526, 0.5015814304351807, 0.5002591609954834, 0.4989223778247833, 0.49652519822120667, 0.5024768710136414, 0.5049421191215515, 0.5042901635169983, 0.5038973689079285, 0.5028501749038696, 0.5019134283065796, 0.5000659227371216, 0.49983906745910645, 0.49945756793022156, 0.4986841380596161, 0.49864688515663147, 0.49806031584739685, 0.4964929223060608, 0.4991060197353363, 0.5008613467216492, 0.5021535754203796, 0.5024808645248413, 0.5007995367050171, 0.5019761323928833, 0.501119077205658, 0.5009405612945557, 0.49880850315093994, 0.49941807985305786, 0.498416930437088, 0.4964280128479004, 0.49683573842048645, 0.4961150884628296, 0.49543458223342896, 0.494951069355011, 0.49459829926490784, 0.4950140118598938, 0.49483051896095276, 0.49453017115592957, 0.49403730034828186, 0.4934593737125397, 0.49298739433288574, 0.4925239682197571, 0.49215614795684814, 0.49187329411506653, 0.49155497550964355, 0.49133145809173584, 0.49108290672302246, 0.4908313453197479, 0.4905984401702881, 0.49039629101753235, 0.4901678264141083, 0.4899665415287018, 0.4897859990596771, 0.4896126389503479, 0.48951026797294617, 0.4894067943096161, 0.4892469346523285, 0.4891652464866638, 0.48897886276245117, 0.48891669511795044, 0.48881372809410095, 0.4887017607688904, 0.48857128620147705, 0.4884829521179199, 0.48829883337020874, 0.4882426857948303, 0.4881265163421631, 0.487991601228714, 0.4879111349582672, 0.4878200590610504, 0.4876496195793152, 0.4875318706035614, 0.4874849021434784, 0.48742684721946716, 0.4873659312725067, 0.4873053729534149, 0.4873509407043457, 0.4873102009296417, 0.4873564839363098, 0.4873693287372589, 0.4873661994934082, 0.48734116554260254, 0.4872139096260071, 0.4871634840965271, 0.4870283305644989, 0.4869270920753479, 0.48681581020355225, 0.4868033230304718, 0.48672226071357727, 0.4866154193878174, 0.4865790009498596, 0.4865044951438904, 0.4864423871040344, 0.48643970489501953, 0.48641669750213623, 0.4863659143447876, 0.48626336455345154, 0.4861382246017456, 0.48606112599372864, 0.48597079515457153, 0.48588135838508606, 0.48577186465263367, 0.4857329726219177, 0.48557159304618835, 0.48546093702316284, 0.4853285253047943, 0.48519307374954224, 0.4851171374320984, 0.48501020669937134, 0.4848671853542328, 0.48476308584213257, 0.48461678624153137, 0.48442402482032776, 0.48437896370887756, 0.48413559794425964, 0.48411664366722107, 0.48397132754325867, 0.4839008152484894, 0.4837898313999176, 0.4836641252040863, 0.48347315192222595, 0.48333969712257385, 0.4832102954387665, 0.4830964207649231, 0.4829874038696289, 0.48285797238349915, 0.482729434967041, 0.48256319761276245, 0.4823845624923706, 0.4821760654449463, 0.4820791482925415, 0.48201000690460205, 0.481841504573822, 0.48163047432899475, 0.4814566969871521, 0.481412410736084, 0.48118922114372253, 0.48106229305267334, 0.4809207618236542, 0.48080068826675415, 0.48062562942504883, 0.4804302155971527, 0.48012134432792664, 0.480044424533844, 0.47985711693763733, 0.47965195775032043, 0.4794671833515167, 0.47931337356567383, 0.4790751039981842, 0.4788908064365387, 0.47859540581703186, 0.4784385859966278, 0.4782874882221222, 0.4780617654323578, 0.4779086410999298, 0.4776984453201294, 0.4774075150489807, 0.47710302472114563, 0.476764976978302, 0.4765721559524536, 0.476223349571228, 0.4758632779121399, 0.4756086766719818, 0.4752538502216339, 0.47495171427726746, 0.4746100604534149, 0.47434771060943604, 0.47400274872779846, 0.47356635332107544, 0.47322776913642883, 0.47292616963386536, 0.47265928983688354, 0.47238731384277344, 0.47212696075439453, 0.47191235423088074, 0.47148633003234863, 0.47119271755218506, 0.47089022397994995, 0.47057783603668213, 0.4702500104904175, 0.4700081944465637, 0.4696110785007477, 0.4692206084728241, 0.46888354420661926, 0.4684904217720032, 0.4681086540222168, 0.4676077663898468, 0.4673140048980713, 0.466993123292923, 0.4666524827480316, 0.46649038791656494, 0.46605801582336426, 0.46582871675491333, 0.4655396044254303, 0.46517977118492126, 0.46476444602012634, 0.4643886685371399, 0.46408408880233765, 0.4636724591255188, 0.463361918926239, 0.46308189630508423, 0.4628172218799591, 0.46242237091064453, 0.46213412284851074, 0.4617866277694702, 0.4615034759044647, 0.461299329996109, 0.4609539210796356, 0.46070215106010437, 0.4604174792766571, 0.4601331055164337, 0.45983001589775085, 0.4594308137893677, 0.4590667486190796, 0.45867910981178284, 0.4582710564136505, 0.457884281873703, 0.4574671983718872, 0.45713096857070923, 0.4568306505680084, 0.45647144317626953, 0.45612001419067383, 0.45575961470603943, 0.45545196533203125, 0.45505478978157043, 0.45488905906677246, 0.4546333849430084, 0.45431089401245117, 0.45399346947669983, 0.45368272066116333, 0.4532875120639801, 0.4529576301574707, 0.45260512828826904, 0.4522685408592224, 0.45201534032821655, 0.45169785618782043, 0.4514266550540924, 0.45111021399497986, 0.4507145881652832, 0.4504668414592743, 0.4501081109046936, 0.44971489906311035]\n"
     ]
    }
   ],
   "source": [
    "model_Adam = Model()\n",
    "print(\"model_Adam:\", model_Adam)\n",
    "model_Adam.to(device)\n",
    "model_Adam.sigmoid.register_forward_hook(get_activation(model_Adam))\n",
    "optimizer_Adam = torch.optim.Adam(model_Adam.parameters(), lr=0.001)\n",
    "Adam_test_acc, sparsity, Adam_avg_selectivity_list, Adam_std_selectivity_list = selectivity_trainer(optimizer=optimizer_Adam, model=model_Adam)\n",
    "\n",
    "f = open(\"sorted_sparsity_selectivity_Adam.txt\", \"w\")\n",
    "f.write(str(0)+'\\n'+str(Adam_test_acc)+'\\n'+str(sparsity)+'\\n'+str(Adam_avg_selectivity_list)+'\\n'+str(Adam_std_selectivity_list)+'\\n\\n')\n",
    "f.close()\n",
    "\n",
    "!cp sorted_sparsity_selectivity_Adam.txt /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_nabt-L_PHl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "sparsity_selectivity_sorted_300.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
