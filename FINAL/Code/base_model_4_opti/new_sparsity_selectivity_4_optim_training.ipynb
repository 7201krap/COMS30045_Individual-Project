{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new_selectivity_4_optim_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/new_selectivity_4_optim_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7STrWa0P3z_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b533ed2f-8d7e-4171-e9a4-fb86c0ec762a"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HltFJhKLKdox",
        "outputId": "bd2aa3b8-d468-4e1d-b2e3-540844146936"
      },
      "source": [
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "root_dir = './'\n",
        "torchvision.datasets.MNIST(root=root_dir,download=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-16 13:28:17--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
            "--2021-03-16 13:28:17--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘MNIST.tar.gz.1’\n",
            "\n",
            "MNIST.tar.gz.1          [          <=>       ]  33.20M  15.8MB/s    in 2.1s    \n",
            "\n",
            "2021-03-16 13:28:20 (15.8 MB/s) - ‘MNIST.tar.gz.1’ saved [34813078]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte\n",
            "MNIST/raw/train-images-idx3-ubyte.gz\n",
            "MNIST/processed/\n",
            "MNIST/processed/training.pt\n",
            "MNIST/processed/test.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./\n",
              "    Split: Train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4j9WoP-UnAm"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApOU7hvb95W4"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTW5TOUnP5XY"
      },
      "source": [
        "mnist_trainset = torchvision.datasets.MNIST(root=root_dir, train=True, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "mnist_testset  = torchvision.datasets.MNIST(root=root_dir, \n",
        "                                train=False, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(mnist_trainset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=True)\n",
        "\n",
        "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXTkEUJ5P6kU"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# Define the model \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        # modify this section for later use \n",
        "        self.linear_1 = torch.nn.Linear(784, 256)\n",
        "        self.linear_2 = torch.nn.Linear(256, 10)\n",
        "        self.sigmoid12  = torch.nn.Sigmoid()\n",
        "\n",
        "        self.layer_activations = dict()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # modify this section for later use \n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.sigmoid12(x)\n",
        "        pred = self.linear_2(x)\n",
        "        return pred\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfgvKH6eP9Ou"
      },
      "source": [
        "def get_activation(model, layer_name):    \n",
        "    def hook(module, input, output):\n",
        "        model.layer_activations[layer_name] = output\n",
        "    return hook"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpireCCAKkNd"
      },
      "source": [
        "def sparsity_calculator(final_spareness):\n",
        "    sparseness_list = list()\n",
        "    for single_epoch_spareness in final_spareness:\n",
        "\n",
        "        hidden_layer_activation_list = single_epoch_spareness\n",
        "        hidden_layer_activation_list = torch.stack(hidden_layer_activation_list)\n",
        "        layer_activations_list = torch.reshape(hidden_layer_activation_list, (60000, 256))\n",
        "\n",
        "        layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
        "        num_neurons = layer_activations_list.shape[1]\n",
        "        population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
        "        mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
        "\n",
        "        sparseness_list.append(mean_sparseness_per_epoch)\n",
        "\n",
        "    return sparseness_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvHGO5RSvi6I"
      },
      "source": [
        "def selectivity(hidden_layer_each_neuron):\n",
        "    __selectivity__ = list()\n",
        "    # I will now try to find the average of each class for each neuron.\n",
        "    # check out the next cell \n",
        "    avg_activations = [dict() for x in range(256)]\n",
        "    for i, neuron in enumerate(hidden_layer_each_neuron):\n",
        "        for k, v in neuron.items():\n",
        "            # v is the list of activations for hidden layer's neuron k \n",
        "            avg_activations[i][k] = sum(v) / float(len(v))\n",
        "\n",
        "    # generate 256 lists to get only values in avg_activations\n",
        "    only_activation_vals = [list() for x in range(256)]\n",
        "\n",
        "    # get only values from avg_activations\n",
        "    for i, avg_activation in enumerate(avg_activations):\n",
        "        for value in avg_activation.values():\n",
        "            only_activation_vals[i].append(value)\n",
        "\n",
        "\n",
        "    for activation_val in only_activation_vals:\n",
        "        # find u_max \n",
        "        u_max = np.max(activation_val)\n",
        "\n",
        "        # find u_minus_max \n",
        "        u_minus_max = (np.sum(activation_val) - u_max) / 9\n",
        "\n",
        "        # find selectivity \n",
        "        selectivity = (u_max - u_minus_max) / (u_max + u_minus_max)\n",
        "\n",
        "        # append selectivity value to selectivity\n",
        "        __selectivity__.append(selectivity)\n",
        "\n",
        "    avg_selectivity = np.average(__selectivity__)\n",
        "    std_selectivity = np.std(__selectivity__)\n",
        "                                 \n",
        "    return avg_selectivity, std_selectivity"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53sWAzfpKmCU"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "def avg_std_calculator(_hidden_layer_each_neuron_12):\n",
        "\n",
        "    avg_selectivity12, std_selectivity12 = selectivity(_hidden_layer_each_neuron_12)\n",
        "\n",
        "    final_selectivity_avg = (avg_selectivity12) / 1\n",
        "    final_selecvitity_std = (std_selectivity12) / 1\n",
        "\n",
        "    return final_selectivity_avg, final_selecvitity_std\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5PUiBNqUImf"
      },
      "source": [
        "def model_factory(optimizer_name):\n",
        "    '''\n",
        "    optimizer_name : choose one of Adagrad, Adadelta, SGD, and Adam \n",
        "\n",
        "    '''\n",
        "    my_model = Model()\n",
        "    print(\"my_model:\", my_model)\n",
        "    my_model.to(device)\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    my_model.sigmoid12.register_forward_hook(get_activation(my_model, 's12'))\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        my_optimizer = torch.optim.Adadelta(my_model.parameters(), lr=1.0)\n",
        "\n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        my_optimizer = torch.optim.Adagrad(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        my_optimizer = torch.optim.SGD(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        my_optimizer = torch.optim.Adam(my_model.parameters(), lr=0.001)\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")\n",
        "    \n",
        "    print(\"my_optimizer:\", my_optimizer)\n",
        "    test_acc, sparsity, selectivity_list_avg, selectivity_list_std = selectivity_trainer(optimizer=my_optimizer, model=my_model)\n",
        "\n",
        "    print(test_acc)\n",
        "    print(selectivity_list_avg)\n",
        "    print(selectivity_list_std)\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver = open(f\"sparsity_selectivity_4_optim_training_{optimizer_name}.txt\", \"w\")\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver.write(str(test_acc)+'\\n'+str(sparsity)+'\\n'+str(selectivity_list_avg)+'\\n'+str(selectivity_list_std)+'\\n\\n')\n",
        "    file_saver.close()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        !cp sparsity_selectivity_4_optim_training_Adadelta.txt /content/drive/MyDrive\n",
        "    \n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        !cp sparsity_selectivity_4_optim_training_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        !cp sparsity_selectivity_4_optim_training_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        !cp sparsity_selectivity_4_optim_training_Adam.txt /content/drive/MyDrive\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXOpwTXEQFKY"
      },
      "source": [
        "no_epochs = 30\n",
        "def selectivity_trainer(optimizer, model):\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    train_loss = list()\n",
        "    test_loss  = list()\n",
        "    test_acc   = list()\n",
        "\n",
        "    best_test_loss = 1\n",
        "\n",
        "    selectivity_avg_list = list()\n",
        "    selectivity_std_list = list()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    final_spareness_12 = list()\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    for epoch in range(no_epochs):\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_each_neuron_12 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_12 = np.array(hidden_layer_each_neuron_12)\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_activation_list_12 = list()\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "\n",
        "        total_train_loss = 0\n",
        "        total_test_loss = 0\n",
        "\n",
        "        # training\n",
        "        # set up training mode \n",
        "        model.train()\n",
        "\n",
        "        for itr, (images, labels) in enumerate(train_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # ***************** sparsity calculation ***************** #\n",
        "            hidden_layer_activation_list_12.append(model.layer_activations['s12'])\n",
        "\n",
        "            # ************* modify this section for later use *************\n",
        "            for activation, label in zip(model.layer_activations['s12'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_12[i][label].append(activation[i])\n",
        "    \n",
        "        selectivity_avg, selecvitity_std = avg_std_calculator(hidden_layer_each_neuron_12)\n",
        "        \n",
        "        selectivity_avg_list.append(selectivity_avg)\n",
        "        selectivity_std_list.append(selecvitity_std)\n",
        "         # ************* modify this section for later use *************\n",
        "\n",
        "        # this conains activations for all epochs \n",
        "        final_spareness_12.append(hidden_layer_activation_list_12)\n",
        "        # ***************** sparsity calculation ***************** #\n",
        "\n",
        "        total_train_loss = total_train_loss / (itr + 1)\n",
        "        train_loss.append(total_train_loss)\n",
        "\n",
        "        # testing \n",
        "        # change to evaluation mode \n",
        "        model.eval()\n",
        "        total = 0\n",
        "        for itr, (images, labels) in enumerate(test_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # we now need softmax because we are testing.\n",
        "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "            for i, p in enumerate(pred):\n",
        "                if labels[i] == torch.max(p.data, 0)[1]:\n",
        "                    total = total + 1\n",
        "\n",
        "        # caculate accuracy \n",
        "        accuracy = total / len(mnist_testset)\n",
        "\n",
        "        # append accuracy here\n",
        "        test_acc.append(accuracy)\n",
        "\n",
        "        # append test loss here \n",
        "        total_test_loss = total_test_loss / (itr + 1)\n",
        "        test_loss.append(total_test_loss)\n",
        "\n",
        "        print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Test Loss: {:.8f}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, total_train_loss, total_test_loss, accuracy))\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "    sparsity_list12 = sparsity_calculator(final_spareness_12)\n",
        "\n",
        "    print(sparsity_list12)\n",
        "\n",
        "    average_sparsity = list()\n",
        "    for i in range(no_epochs):\n",
        "        average_sparsity.append( (sparsity_list12[i].item()) / 1 )\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "\n",
        "    print(\"average_sparsity:\", average_sparsity)\n",
        "\n",
        "    return test_acc, average_sparsity, selectivity_avg_list, selectivity_std_list"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILIJTJb2UdfI"
      },
      "source": [
        "# Adadelta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UH0qDnFUfaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57a83262-37fc-48ab-fb1b-72b2ce68abf6"
      },
      "source": [
        "model_factory('Adadelta')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-06\n",
            "    lr: 1.0\n",
            "    rho: 0.9\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/30, Train Loss: 0.43482734, Test Loss: 0.25056329, Test Accuracy: 0.92680000\n",
            "\n",
            "Epoch: 2/30, Train Loss: 0.21552304, Test Loss: 0.17478676, Test Accuracy: 0.94670000\n",
            "\n",
            "Epoch: 3/30, Train Loss: 0.15641612, Test Loss: 0.13820603, Test Accuracy: 0.95740000\n",
            "\n",
            "Epoch: 4/30, Train Loss: 0.12157689, Test Loss: 0.11924306, Test Accuracy: 0.96300000\n",
            "\n",
            "Epoch: 5/30, Train Loss: 0.09919056, Test Loss: 0.09831683, Test Accuracy: 0.96860000\n",
            "\n",
            "Epoch: 6/30, Train Loss: 0.08336800, Test Loss: 0.09016022, Test Accuracy: 0.97190000\n",
            "\n",
            "Epoch: 7/30, Train Loss: 0.07088854, Test Loss: 0.08719796, Test Accuracy: 0.97320000\n",
            "\n",
            "Epoch: 8/30, Train Loss: 0.06153279, Test Loss: 0.07796416, Test Accuracy: 0.97520000\n",
            "\n",
            "Epoch: 9/30, Train Loss: 0.05413195, Test Loss: 0.07215949, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 10/30, Train Loss: 0.04688474, Test Loss: 0.07165625, Test Accuracy: 0.97650000\n",
            "\n",
            "Epoch: 11/30, Train Loss: 0.04203057, Test Loss: 0.07021960, Test Accuracy: 0.97680000\n",
            "\n",
            "Epoch: 12/30, Train Loss: 0.03695013, Test Loss: 0.07518681, Test Accuracy: 0.97650000\n",
            "\n",
            "Epoch: 13/30, Train Loss: 0.03292638, Test Loss: 0.06627988, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 14/30, Train Loss: 0.02905201, Test Loss: 0.06293269, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 15/30, Train Loss: 0.02590495, Test Loss: 0.06158078, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 16/30, Train Loss: 0.02330130, Test Loss: 0.06297808, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 17/30, Train Loss: 0.02085337, Test Loss: 0.05992444, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 18/30, Train Loss: 0.01860578, Test Loss: 0.06213611, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 19/30, Train Loss: 0.01655125, Test Loss: 0.06208909, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 20/30, Train Loss: 0.01466604, Test Loss: 0.05816112, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 21/30, Train Loss: 0.01332532, Test Loss: 0.06010360, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 22/30, Train Loss: 0.01189256, Test Loss: 0.05997107, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 23/30, Train Loss: 0.01074991, Test Loss: 0.06014586, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 24/30, Train Loss: 0.00972292, Test Loss: 0.06088049, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 25/30, Train Loss: 0.00874879, Test Loss: 0.06209757, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 26/30, Train Loss: 0.00798673, Test Loss: 0.06282996, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 27/30, Train Loss: 0.00720307, Test Loss: 0.05958406, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 28/30, Train Loss: 0.00660240, Test Loss: 0.06011289, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 29/30, Train Loss: 0.00585278, Test Loss: 0.05981554, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 30/30, Train Loss: 0.00549971, Test Loss: 0.06033830, Test Accuracy: 0.98150000\n",
            "[tensor(0.1503, grad_fn=<MeanBackward0>), tensor(0.2254, grad_fn=<MeanBackward0>), tensor(0.2614, grad_fn=<MeanBackward0>), tensor(0.2820, grad_fn=<MeanBackward0>), tensor(0.2939, grad_fn=<MeanBackward0>), tensor(0.3013, grad_fn=<MeanBackward0>), tensor(0.3079, grad_fn=<MeanBackward0>), tensor(0.3123, grad_fn=<MeanBackward0>), tensor(0.3149, grad_fn=<MeanBackward0>), tensor(0.3185, grad_fn=<MeanBackward0>), tensor(0.3194, grad_fn=<MeanBackward0>), tensor(0.3222, grad_fn=<MeanBackward0>), tensor(0.3222, grad_fn=<MeanBackward0>), tensor(0.3237, grad_fn=<MeanBackward0>), tensor(0.3242, grad_fn=<MeanBackward0>), tensor(0.3227, grad_fn=<MeanBackward0>), tensor(0.3246, grad_fn=<MeanBackward0>), tensor(0.3254, grad_fn=<MeanBackward0>), tensor(0.3264, grad_fn=<MeanBackward0>), tensor(0.3254, grad_fn=<MeanBackward0>), tensor(0.3249, grad_fn=<MeanBackward0>), tensor(0.3254, grad_fn=<MeanBackward0>), tensor(0.3256, grad_fn=<MeanBackward0>), tensor(0.3248, grad_fn=<MeanBackward0>), tensor(0.3256, grad_fn=<MeanBackward0>), tensor(0.3248, grad_fn=<MeanBackward0>), tensor(0.3253, grad_fn=<MeanBackward0>), tensor(0.3254, grad_fn=<MeanBackward0>), tensor(0.3251, grad_fn=<MeanBackward0>), tensor(0.3249, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.15029017627239227, 0.22535523772239685, 0.2614114284515381, 0.2820297181606293, 0.29386210441589355, 0.30131229758262634, 0.30790767073631287, 0.3123033344745636, 0.31488466262817383, 0.31846752762794495, 0.3193831741809845, 0.32216307520866394, 0.32218751311302185, 0.32371217012405396, 0.3241577446460724, 0.3226933479309082, 0.3246074318885803, 0.3253915011882782, 0.3263701796531677, 0.3253672420978546, 0.3248649835586548, 0.3253776431083679, 0.3256395161151886, 0.3247584104537964, 0.32559341192245483, 0.32481417059898376, 0.3253192603588104, 0.32538551092147827, 0.3250992000102997, 0.3248804807662964]\n",
            "[0.9268, 0.9467, 0.9574, 0.963, 0.9686, 0.9719, 0.9732, 0.9752, 0.977, 0.9765, 0.9768, 0.9765, 0.9794, 0.9803, 0.9802, 0.9792, 0.9811, 0.9804, 0.9802, 0.9815, 0.9811, 0.9814, 0.9814, 0.9809, 0.9809, 0.9813, 0.982, 0.9815, 0.981, 0.9815]\n",
            "[0.28555460152988205, 0.3526864917477972, 0.3780348975744685, 0.3911368968401212, 0.3985476203158234, 0.4013927069369916, 0.4059695055348156, 0.40752009360015096, 0.40874721335624475, 0.4099075055909235, 0.4090726561447211, 0.4100018797150028, 0.4090733719177895, 0.4088003843116357, 0.40785342983450557, 0.4056199456447455, 0.40615861672902076, 0.40587160838367387, 0.4062264345384269, 0.4042623200774071, 0.40368316718166963, 0.403446826151076, 0.40387412684055035, 0.4024552574182428, 0.4032412114096361, 0.4023346512519122, 0.4026494406599296, 0.4026898814789056, 0.4020977874357957, 0.40201174827048924]\n",
            "[0.09833513434689328, 0.14388898998272565, 0.15867205923462382, 0.16522523312121307, 0.1682470606882449, 0.17099175929227386, 0.17243994581039074, 0.17391708975392586, 0.1746262514539231, 0.17492699714074755, 0.17449736038428948, 0.17431789472743997, 0.1743537573666145, 0.17410308743025676, 0.17425876913769414, 0.17427919566122257, 0.17432093522504366, 0.17409248145356557, 0.172689860588375, 0.1730030782302599, 0.17368332424966243, 0.17290398411175445, 0.17293819646703398, 0.17213706785169572, 0.17205054677582185, 0.17201988946938307, 0.17144727019138997, 0.1720292201684773, 0.17115466974655894, 0.1716401928897793]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hXfQe4vMDKB"
      },
      "source": [
        "# AdaGrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb-4TPM5MGuE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fbb8f8a-ec3d-4f92-fa97-38dce80e567c"
      },
      "source": [
        "model_factory('Adagrad')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adagrad (\n",
            "Parameter Group 0\n",
            "    eps: 1e-10\n",
            "    initial_accumulator_value: 0\n",
            "    lr: 0.1\n",
            "    lr_decay: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/30, Train Loss: 0.23720022, Test Loss: 0.12966111, Test Accuracy: 0.95960000\n",
            "\n",
            "Epoch: 2/30, Train Loss: 0.09619678, Test Loss: 0.10075946, Test Accuracy: 0.96850000\n",
            "\n",
            "Epoch: 3/30, Train Loss: 0.06640541, Test Loss: 0.08520305, Test Accuracy: 0.97210000\n",
            "\n",
            "Epoch: 4/30, Train Loss: 0.04899912, Test Loss: 0.07719705, Test Accuracy: 0.97560000\n",
            "\n",
            "Epoch: 5/30, Train Loss: 0.03792565, Test Loss: 0.07354746, Test Accuracy: 0.97520000\n",
            "\n",
            "Epoch: 6/30, Train Loss: 0.03008004, Test Loss: 0.07184436, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 7/30, Train Loss: 0.02459523, Test Loss: 0.06897810, Test Accuracy: 0.97720000\n",
            "\n",
            "Epoch: 8/30, Train Loss: 0.02022956, Test Loss: 0.06901597, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 9/30, Train Loss: 0.01689163, Test Loss: 0.06785572, Test Accuracy: 0.97750000\n",
            "\n",
            "Epoch: 10/30, Train Loss: 0.01438798, Test Loss: 0.06610731, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 11/30, Train Loss: 0.01232129, Test Loss: 0.06671650, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 12/30, Train Loss: 0.01052789, Test Loss: 0.06710049, Test Accuracy: 0.97840000\n",
            "\n",
            "Epoch: 13/30, Train Loss: 0.00916417, Test Loss: 0.06610330, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 14/30, Train Loss: 0.00806862, Test Loss: 0.06683066, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 15/30, Train Loss: 0.00704872, Test Loss: 0.06661206, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 16/30, Train Loss: 0.00632322, Test Loss: 0.06685364, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 17/30, Train Loss: 0.00569732, Test Loss: 0.06657979, Test Accuracy: 0.97860000\n",
            "\n",
            "Epoch: 18/30, Train Loss: 0.00516997, Test Loss: 0.06767103, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 19/30, Train Loss: 0.00467289, Test Loss: 0.06769714, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 20/30, Train Loss: 0.00425627, Test Loss: 0.06756142, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 21/30, Train Loss: 0.00391845, Test Loss: 0.06765659, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 22/30, Train Loss: 0.00361093, Test Loss: 0.06818082, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 23/30, Train Loss: 0.00333458, Test Loss: 0.06857999, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 24/30, Train Loss: 0.00309907, Test Loss: 0.06842696, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 25/30, Train Loss: 0.00287972, Test Loss: 0.06887150, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 26/30, Train Loss: 0.00270318, Test Loss: 0.06908296, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 27/30, Train Loss: 0.00252195, Test Loss: 0.06995768, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 28/30, Train Loss: 0.00238148, Test Loss: 0.06914009, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 29/30, Train Loss: 0.00225066, Test Loss: 0.06938243, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 30/30, Train Loss: 0.00212237, Test Loss: 0.06982099, Test Accuracy: 0.97960000\n",
            "[tensor(0.6677, grad_fn=<MeanBackward0>), tensor(0.6499, grad_fn=<MeanBackward0>), tensor(0.6410, grad_fn=<MeanBackward0>), tensor(0.6352, grad_fn=<MeanBackward0>), tensor(0.6315, grad_fn=<MeanBackward0>), tensor(0.6287, grad_fn=<MeanBackward0>), tensor(0.6253, grad_fn=<MeanBackward0>), tensor(0.6239, grad_fn=<MeanBackward0>), tensor(0.6217, grad_fn=<MeanBackward0>), tensor(0.6192, grad_fn=<MeanBackward0>), tensor(0.6179, grad_fn=<MeanBackward0>), tensor(0.6158, grad_fn=<MeanBackward0>), tensor(0.6145, grad_fn=<MeanBackward0>), tensor(0.6132, grad_fn=<MeanBackward0>), tensor(0.6118, grad_fn=<MeanBackward0>), tensor(0.6105, grad_fn=<MeanBackward0>), tensor(0.6094, grad_fn=<MeanBackward0>), tensor(0.6083, grad_fn=<MeanBackward0>), tensor(0.6073, grad_fn=<MeanBackward0>), tensor(0.6064, grad_fn=<MeanBackward0>), tensor(0.6052, grad_fn=<MeanBackward0>), tensor(0.6047, grad_fn=<MeanBackward0>), tensor(0.6036, grad_fn=<MeanBackward0>), tensor(0.6032, grad_fn=<MeanBackward0>), tensor(0.6026, grad_fn=<MeanBackward0>), tensor(0.6017, grad_fn=<MeanBackward0>), tensor(0.6016, grad_fn=<MeanBackward0>), tensor(0.6007, grad_fn=<MeanBackward0>), tensor(0.6004, grad_fn=<MeanBackward0>), tensor(0.5999, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.6676896214485168, 0.6499239802360535, 0.6409995555877686, 0.6351637244224548, 0.6315067410469055, 0.6286928653717041, 0.6253172159194946, 0.6239030361175537, 0.6217138767242432, 0.6192038655281067, 0.6178808808326721, 0.6158379912376404, 0.6145426034927368, 0.6131526231765747, 0.6117873191833496, 0.6104682087898254, 0.60938960313797, 0.6082751750946045, 0.6072799563407898, 0.6063904762268066, 0.6052462458610535, 0.6046914458274841, 0.6036111116409302, 0.6031755805015564, 0.6025912761688232, 0.6017282605171204, 0.6015752553939819, 0.6006560325622559, 0.6004319190979004, 0.5998769402503967]\n",
            "[0.9596, 0.9685, 0.9721, 0.9756, 0.9752, 0.9759, 0.9772, 0.977, 0.9775, 0.9779, 0.9783, 0.9784, 0.979, 0.9788, 0.9781, 0.9792, 0.9786, 0.9789, 0.9785, 0.9797, 0.9791, 0.9797, 0.9796, 0.9792, 0.9793, 0.9793, 0.9795, 0.9797, 0.9798, 0.9796]\n",
            "[0.6333651803708717, 0.6955278230969741, 0.6836718345632051, 0.6760779089352036, 0.67122766151346, 0.6684550511128655, 0.666526783721124, 0.6640641907578304, 0.6639808962798774, 0.6624938490732419, 0.6603308370503378, 0.6592132853707975, 0.6575729732438056, 0.6553231777237988, 0.6542410129170307, 0.6523152408185475, 0.6505752325120558, 0.6487421955950725, 0.648279815453269, 0.6474920906721647, 0.646392452634525, 0.6462185202721367, 0.6445907136251825, 0.644145320662777, 0.6439008950942662, 0.6431869802505898, 0.6435460183988562, 0.6420076279262603, 0.6424519464693377, 0.642027088504281]\n",
            "[0.21792302002458086, 0.20553738662940943, 0.2047811203611456, 0.20377464343242355, 0.20310639315776985, 0.2026873319216653, 0.2023533290286578, 0.2021627792741097, 0.20023942916577933, 0.19928445958064736, 0.1996844584765614, 0.2005019746289249, 0.2000218196733977, 0.20050004992459677, 0.20018854577971215, 0.20042526506639044, 0.20135158417409998, 0.20206809735004694, 0.20144153829425396, 0.20161278145896344, 0.20103331614949443, 0.20093982726306786, 0.20141397964148047, 0.2009070473379526, 0.2013149061795582, 0.20098452648076945, 0.2003859573162791, 0.20094922109600577, 0.20053486323430525, 0.20035292851028438]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmLJ4Zr2MnoS"
      },
      "source": [
        "# SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ObsEJHuMoPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c22aaa-0e37-41a4-e0d6-43ae19f47d67"
      },
      "source": [
        "model_factory('SGD')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/30, Train Loss: 0.77229017, Test Loss: 0.36556621, Test Accuracy: 0.89740000\n",
            "\n",
            "Epoch: 2/30, Train Loss: 0.34719833, Test Loss: 0.30601285, Test Accuracy: 0.91130000\n",
            "\n",
            "Epoch: 3/30, Train Loss: 0.30693888, Test Loss: 0.28060609, Test Accuracy: 0.92130000\n",
            "\n",
            "Epoch: 4/30, Train Loss: 0.28483134, Test Loss: 0.27108764, Test Accuracy: 0.92220000\n",
            "\n",
            "Epoch: 5/30, Train Loss: 0.26674319, Test Loss: 0.24900325, Test Accuracy: 0.93030000\n",
            "\n",
            "Epoch: 6/30, Train Loss: 0.24984935, Test Loss: 0.23398066, Test Accuracy: 0.93260000\n",
            "\n",
            "Epoch: 7/30, Train Loss: 0.23336248, Test Loss: 0.22511649, Test Accuracy: 0.93520000\n",
            "\n",
            "Epoch: 8/30, Train Loss: 0.21804447, Test Loss: 0.20816296, Test Accuracy: 0.94010000\n",
            "\n",
            "Epoch: 9/30, Train Loss: 0.20462472, Test Loss: 0.19458868, Test Accuracy: 0.94520000\n",
            "\n",
            "Epoch: 10/30, Train Loss: 0.19214305, Test Loss: 0.18808595, Test Accuracy: 0.94570000\n",
            "\n",
            "Epoch: 11/30, Train Loss: 0.18089217, Test Loss: 0.17587552, Test Accuracy: 0.94900000\n",
            "\n",
            "Epoch: 12/30, Train Loss: 0.17059864, Test Loss: 0.16796685, Test Accuracy: 0.94990000\n",
            "\n",
            "Epoch: 13/30, Train Loss: 0.16099815, Test Loss: 0.15870855, Test Accuracy: 0.95330000\n",
            "\n",
            "Epoch: 14/30, Train Loss: 0.15282047, Test Loss: 0.15348154, Test Accuracy: 0.95580000\n",
            "\n",
            "Epoch: 15/30, Train Loss: 0.14522061, Test Loss: 0.14678261, Test Accuracy: 0.95770000\n",
            "\n",
            "Epoch: 16/30, Train Loss: 0.13832390, Test Loss: 0.13840950, Test Accuracy: 0.96100000\n",
            "\n",
            "Epoch: 17/30, Train Loss: 0.13196634, Test Loss: 0.13653209, Test Accuracy: 0.95970000\n",
            "\n",
            "Epoch: 18/30, Train Loss: 0.12606080, Test Loss: 0.12942023, Test Accuracy: 0.96280000\n",
            "\n",
            "Epoch: 19/30, Train Loss: 0.12032453, Test Loss: 0.12580063, Test Accuracy: 0.96360000\n",
            "\n",
            "Epoch: 20/30, Train Loss: 0.11546896, Test Loss: 0.12108934, Test Accuracy: 0.96530000\n",
            "\n",
            "Epoch: 21/30, Train Loss: 0.11083578, Test Loss: 0.11918582, Test Accuracy: 0.96410000\n",
            "\n",
            "Epoch: 22/30, Train Loss: 0.10652338, Test Loss: 0.11451459, Test Accuracy: 0.96600000\n",
            "\n",
            "Epoch: 23/30, Train Loss: 0.10224386, Test Loss: 0.11202619, Test Accuracy: 0.96680000\n",
            "\n",
            "Epoch: 24/30, Train Loss: 0.09870039, Test Loss: 0.10751367, Test Accuracy: 0.96800000\n",
            "\n",
            "Epoch: 25/30, Train Loss: 0.09506041, Test Loss: 0.10633760, Test Accuracy: 0.96880000\n",
            "\n",
            "Epoch: 26/30, Train Loss: 0.09156718, Test Loss: 0.10264897, Test Accuracy: 0.96910000\n",
            "\n",
            "Epoch: 27/30, Train Loss: 0.08833942, Test Loss: 0.10154049, Test Accuracy: 0.96900000\n",
            "\n",
            "Epoch: 28/30, Train Loss: 0.08558544, Test Loss: 0.09941066, Test Accuracy: 0.97010000\n",
            "\n",
            "Epoch: 29/30, Train Loss: 0.08267791, Test Loss: 0.09813298, Test Accuracy: 0.97020000\n",
            "\n",
            "Epoch: 30/30, Train Loss: 0.07989562, Test Loss: 0.09521272, Test Accuracy: 0.97110000\n",
            "[tensor(0.0564, grad_fn=<MeanBackward0>), tensor(0.0854, grad_fn=<MeanBackward0>), tensor(0.0954, grad_fn=<MeanBackward0>), tensor(0.1053, grad_fn=<MeanBackward0>), tensor(0.1153, grad_fn=<MeanBackward0>), tensor(0.1253, grad_fn=<MeanBackward0>), tensor(0.1349, grad_fn=<MeanBackward0>), tensor(0.1441, grad_fn=<MeanBackward0>), tensor(0.1528, grad_fn=<MeanBackward0>), tensor(0.1598, grad_fn=<MeanBackward0>), tensor(0.1665, grad_fn=<MeanBackward0>), tensor(0.1726, grad_fn=<MeanBackward0>), tensor(0.1775, grad_fn=<MeanBackward0>), tensor(0.1825, grad_fn=<MeanBackward0>), tensor(0.1869, grad_fn=<MeanBackward0>), tensor(0.1903, grad_fn=<MeanBackward0>), tensor(0.1939, grad_fn=<MeanBackward0>), tensor(0.1971, grad_fn=<MeanBackward0>), tensor(0.1999, grad_fn=<MeanBackward0>), tensor(0.2025, grad_fn=<MeanBackward0>), tensor(0.2046, grad_fn=<MeanBackward0>), tensor(0.2068, grad_fn=<MeanBackward0>), tensor(0.2087, grad_fn=<MeanBackward0>), tensor(0.2104, grad_fn=<MeanBackward0>), tensor(0.2122, grad_fn=<MeanBackward0>), tensor(0.2138, grad_fn=<MeanBackward0>), tensor(0.2151, grad_fn=<MeanBackward0>), tensor(0.2167, grad_fn=<MeanBackward0>), tensor(0.2180, grad_fn=<MeanBackward0>), tensor(0.2194, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.056356754153966904, 0.08541419357061386, 0.09537644684314728, 0.10528266429901123, 0.11534861475229263, 0.12533509731292725, 0.13492539525032043, 0.14408990740776062, 0.1527518928050995, 0.1598433256149292, 0.16648291051387787, 0.17257243394851685, 0.17747728526592255, 0.18246424198150635, 0.18685179948806763, 0.19026905298233032, 0.1939144730567932, 0.1971086859703064, 0.19985482096672058, 0.20246383547782898, 0.20463287830352783, 0.2067839801311493, 0.20868147909641266, 0.21039368212223053, 0.21222683787345886, 0.21377822756767273, 0.21505744755268097, 0.2167075127363205, 0.21803750097751617, 0.2194182425737381]\n",
            "[0.8974, 0.9113, 0.9213, 0.9222, 0.9303, 0.9326, 0.9352, 0.9401, 0.9452, 0.9457, 0.949, 0.9499, 0.9533, 0.9558, 0.9577, 0.961, 0.9597, 0.9628, 0.9636, 0.9653, 0.9641, 0.966, 0.9668, 0.968, 0.9688, 0.9691, 0.969, 0.9701, 0.9702, 0.9711]\n",
            "[0.17458118887203508, 0.22266900450300955, 0.2333305194282671, 0.2427656443442987, 0.25003970284483507, 0.2571440416651946, 0.26337830708046234, 0.2693549303205126, 0.2746194535896979, 0.27882467218197793, 0.28286268448251645, 0.2862956133312028, 0.2892739406006081, 0.2920242761843883, 0.29458848628698375, 0.2966417621951013, 0.29881075227637355, 0.3005998814824378, 0.3023081673000782, 0.3038331614529383, 0.3054083413603829, 0.3068746341248919, 0.307975165417122, 0.3088487769360291, 0.3102409949328321, 0.3115806998520836, 0.3122455424096543, 0.3134471748644333, 0.31441336467695385, 0.315193942064666]\n",
            "[0.06108496664049177, 0.08278528929143142, 0.09116417700883286, 0.09931741171341876, 0.1076276861833905, 0.11500075885912246, 0.12144669639154966, 0.12767541931864915, 0.1328711937823265, 0.13727396976833872, 0.14113777981185155, 0.14464456454030225, 0.14701031996747724, 0.15007785504521443, 0.15255288874197595, 0.154580587816597, 0.15672729794308823, 0.15818601997495713, 0.16002528733635618, 0.16135137410261188, 0.16301015451866765, 0.16422337352677133, 0.16521442466487504, 0.16600672954327578, 0.16712476779868374, 0.16780503825432252, 0.16874511398428366, 0.16926361223475006, 0.16998890802728409, 0.17046178322740027]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvQxaN_fRXLq"
      },
      "source": [
        "# Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkqfFoVkRXxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c827ff-de71-4e00-8f98-1012f8c7fa61"
      },
      "source": [
        "model_factory('Adam')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/30, Train Loss: 0.43013016, Test Loss: 0.23038318, Test Accuracy: 0.93350000\n",
            "\n",
            "Epoch: 2/30, Train Loss: 0.19845420, Test Loss: 0.16318714, Test Accuracy: 0.95180000\n",
            "\n",
            "Epoch: 3/30, Train Loss: 0.14409275, Test Loss: 0.13112965, Test Accuracy: 0.96150000\n",
            "\n",
            "Epoch: 4/30, Train Loss: 0.11007065, Test Loss: 0.10521114, Test Accuracy: 0.96910000\n",
            "\n",
            "Epoch: 5/30, Train Loss: 0.08675427, Test Loss: 0.09378186, Test Accuracy: 0.97180000\n",
            "\n",
            "Epoch: 6/30, Train Loss: 0.07006375, Test Loss: 0.08096124, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 7/30, Train Loss: 0.05699459, Test Loss: 0.07811322, Test Accuracy: 0.97590000\n",
            "\n",
            "Epoch: 8/30, Train Loss: 0.04634903, Test Loss: 0.07456356, Test Accuracy: 0.97660000\n",
            "\n",
            "Epoch: 9/30, Train Loss: 0.03831605, Test Loss: 0.07117033, Test Accuracy: 0.97780000\n",
            "\n",
            "Epoch: 10/30, Train Loss: 0.03116941, Test Loss: 0.06739554, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 11/30, Train Loss: 0.02555423, Test Loss: 0.06667477, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 12/30, Train Loss: 0.02080868, Test Loss: 0.06638726, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 13/30, Train Loss: 0.01690935, Test Loss: 0.06281900, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 14/30, Train Loss: 0.01330700, Test Loss: 0.06899784, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 15/30, Train Loss: 0.01106236, Test Loss: 0.06525876, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 16/30, Train Loss: 0.00848057, Test Loss: 0.06817348, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 17/30, Train Loss: 0.00698429, Test Loss: 0.06676264, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 18/30, Train Loss: 0.00547083, Test Loss: 0.06740025, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 19/30, Train Loss: 0.00480950, Test Loss: 0.07052145, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 20/30, Train Loss: 0.00362121, Test Loss: 0.07191904, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 21/30, Train Loss: 0.00258812, Test Loss: 0.07331075, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 22/30, Train Loss: 0.00234628, Test Loss: 0.07124677, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 23/30, Train Loss: 0.00173858, Test Loss: 0.07269172, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 24/30, Train Loss: 0.00202080, Test Loss: 0.07442360, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 25/30, Train Loss: 0.00109320, Test Loss: 0.07514406, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 26/30, Train Loss: 0.00076435, Test Loss: 0.07953336, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 27/30, Train Loss: 0.00227619, Test Loss: 0.08126498, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 28/30, Train Loss: 0.00053937, Test Loss: 0.08105317, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 29/30, Train Loss: 0.00035571, Test Loss: 0.08218978, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 30/30, Train Loss: 0.00033690, Test Loss: 0.08346486, Test Accuracy: 0.98130000\n",
            "[tensor(0.1610, grad_fn=<MeanBackward0>), tensor(0.2104, grad_fn=<MeanBackward0>), tensor(0.2309, grad_fn=<MeanBackward0>), tensor(0.2428, grad_fn=<MeanBackward0>), tensor(0.2511, grad_fn=<MeanBackward0>), tensor(0.2569, grad_fn=<MeanBackward0>), tensor(0.2611, grad_fn=<MeanBackward0>), tensor(0.2655, grad_fn=<MeanBackward0>), tensor(0.2677, grad_fn=<MeanBackward0>), tensor(0.2705, grad_fn=<MeanBackward0>), tensor(0.2728, grad_fn=<MeanBackward0>), tensor(0.2742, grad_fn=<MeanBackward0>), tensor(0.2754, grad_fn=<MeanBackward0>), tensor(0.2768, grad_fn=<MeanBackward0>), tensor(0.2778, grad_fn=<MeanBackward0>), tensor(0.2790, grad_fn=<MeanBackward0>), tensor(0.2804, grad_fn=<MeanBackward0>), tensor(0.2806, grad_fn=<MeanBackward0>), tensor(0.2815, grad_fn=<MeanBackward0>), tensor(0.2829, grad_fn=<MeanBackward0>), tensor(0.2826, grad_fn=<MeanBackward0>), tensor(0.2845, grad_fn=<MeanBackward0>), tensor(0.2842, grad_fn=<MeanBackward0>), tensor(0.2854, grad_fn=<MeanBackward0>), tensor(0.2861, grad_fn=<MeanBackward0>), tensor(0.2853, grad_fn=<MeanBackward0>), tensor(0.2874, grad_fn=<MeanBackward0>), tensor(0.2882, grad_fn=<MeanBackward0>), tensor(0.2865, grad_fn=<MeanBackward0>), tensor(0.2864, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.16104084253311157, 0.21042156219482422, 0.2309374064207077, 0.24280038475990295, 0.2510928511619568, 0.25688430666923523, 0.26106828451156616, 0.2654922902584076, 0.26773086190223694, 0.270490437746048, 0.2727768123149872, 0.2742057740688324, 0.27544963359832764, 0.27680253982543945, 0.2778049409389496, 0.2789705991744995, 0.28037989139556885, 0.28055810928344727, 0.28152602910995483, 0.28292787075042725, 0.28256386518478394, 0.2845227122306824, 0.2842356562614441, 0.28535738587379456, 0.28613048791885376, 0.28534582257270813, 0.2874346077442169, 0.28817200660705566, 0.28651121258735657, 0.2863735854625702]\n",
            "[0.9335, 0.9518, 0.9615, 0.9691, 0.9718, 0.9759, 0.9759, 0.9766, 0.9778, 0.98, 0.9799, 0.9795, 0.9799, 0.9789, 0.9798, 0.9792, 0.9796, 0.9801, 0.9795, 0.98, 0.9806, 0.9802, 0.9811, 0.9805, 0.9811, 0.9795, 0.9809, 0.9806, 0.981, 0.9813]\n",
            "[0.29290739642657787, 0.3341197937293985, 0.34924966217577874, 0.35632667787918715, 0.36083617949412206, 0.3634982986453449, 0.36540759133224915, 0.3668577371938721, 0.3665277072421478, 0.36738626905866434, 0.367226933413615, 0.36663746635573263, 0.3660533679526327, 0.36639775293162413, 0.36614780162276966, 0.3660660058592251, 0.36649315383380454, 0.3652565872312115, 0.36576919648311257, 0.36634491630348004, 0.3654979552211811, 0.3668216796944671, 0.3659284396852027, 0.36546905503698185, 0.3659535946560014, 0.3653275787069451, 0.3653882442293614, 0.3666645916616413, 0.3653541577770065, 0.36507383755139944]\n",
            "[0.10439160182729884, 0.14536244913800633, 0.16207607311593883, 0.17064740650318144, 0.1745772987561279, 0.17489535806963438, 0.1763852460135534, 0.17491956341168335, 0.1753181322477488, 0.1740506296004484, 0.1740804332711729, 0.17281717751898237, 0.1725564057356826, 0.17153572616079926, 0.1719772922134568, 0.17101423810197142, 0.1701853129409464, 0.17050826636727334, 0.1700337054080879, 0.17057696877927733, 0.1704654549817151, 0.17032927727042374, 0.16966402775054035, 0.17032838937187658, 0.17037503334693707, 0.16969534689986082, 0.1703050977984347, 0.17060657548764602, 0.16935378230731918, 0.16873040688969146]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}