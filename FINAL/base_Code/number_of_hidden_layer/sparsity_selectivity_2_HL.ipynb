{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparsity_selectivity_2_HL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/PYTORCH_project/blob/main/sparsity_selectivity_2_HL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7STrWa0P3z_",
        "outputId": "386319d6-b042-4c03-9c19-5f2087343f06"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHe2N_HNfGre"
      },
      "source": [
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "root_dir = './'\n",
        "torchvision.datasets.MNIST(root=root_dir,download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4j9WoP-UnAm"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApOU7hvb95W4"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTW5TOUnP5XY"
      },
      "source": [
        "mnist_trainset = torchvision.datasets.MNIST(root=root_dir, train=True, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "mnist_testset  = torchvision.datasets.MNIST(root=root_dir, \n",
        "                                train=False, \n",
        "                                download=True, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(mnist_trainset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=True)\n",
        "\n",
        "test_dataloader  = torch.utils.data.DataLoader(mnist_testset, \n",
        "                                               batch_size=50, \n",
        "                                               shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXTkEUJ5P6kU"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "# Define the model \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # modify this section for later use \n",
        "        self.linear_1 = torch.nn.Linear(784, 256)\n",
        "        self.linear_2 = torch.nn.Linear(256, 256)\n",
        "        self.linear_3 = torch.nn.Linear(256, 10)\n",
        "        self.sigmoid12  = torch.nn.Sigmoid()\n",
        "        self.sigmoid23  = torch.nn.Sigmoid()\n",
        "\n",
        "        self.layer_activations = dict()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # modify this section for later use \n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.sigmoid12(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.sigmoid23(x)\n",
        "        pred = self.linear_3(x)\n",
        "        return pred\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfgvKH6eP9Ou"
      },
      "source": [
        "def get_activation(model, layer_name):    \n",
        "    def hook(module, input, output):\n",
        "        model.layer_activations[layer_name] = output\n",
        "    return hook"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j51vkagafNh1"
      },
      "source": [
        "def sparsity_calculator(final_spareness):\n",
        "    sparseness_list = list()\n",
        "    for single_epoch_spareness in final_spareness:\n",
        "\n",
        "        hidden_layer_activation_list = single_epoch_spareness\n",
        "        hidden_layer_activation_list = torch.stack(hidden_layer_activation_list)\n",
        "        layer_activations_list = torch.reshape(hidden_layer_activation_list, (10000, 256))\n",
        "\n",
        "        layer_activations_list = torch.abs(layer_activations_list)  # modified \n",
        "        num_neurons = layer_activations_list.shape[1]\n",
        "        population_sparseness = (np.sqrt(num_neurons) - (torch.sum(layer_activations_list, dim=1) / torch.sqrt(torch.sum(layer_activations_list ** 2, dim=1)))) / (np.sqrt(num_neurons) - 1)\n",
        "        mean_sparseness_per_epoch = torch.mean(population_sparseness)\n",
        "\n",
        "        sparseness_list.append(mean_sparseness_per_epoch)\n",
        "\n",
        "    return sparseness_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvHGO5RSvi6I"
      },
      "source": [
        "def selectivity(hidden_layer_each_neuron):\n",
        "    __selectivity__ = list()\n",
        "    # I will now try to find the average of each class for each neuron.\n",
        "    # check out the next cell \n",
        "    avg_activations = [dict() for x in range(256)]\n",
        "    for i, neuron in enumerate(hidden_layer_each_neuron):\n",
        "        for k, v in neuron.items():\n",
        "            # v is the list of activations for hidden layer's neuron k \n",
        "            avg_activations[i][k] = sum(v) / float(len(v))\n",
        "\n",
        "    # generate 256 lists to get only values in avg_activations\n",
        "    only_activation_vals = [list() for x in range(256)]\n",
        "    \n",
        "    # get only values from avg_activations\n",
        "    for i, avg_activation in enumerate(avg_activations):\n",
        "        for value in avg_activation.values():\n",
        "            only_activation_vals[i].append(value)\n",
        "\n",
        "\n",
        "    for activation_val in only_activation_vals:\n",
        "        # find u_max \n",
        "        u_max = np.max(activation_val)\n",
        "\n",
        "        # find u_minus_max \n",
        "        u_minus_max = (np.sum(activation_val) - u_max) / 9\n",
        "\n",
        "        # find selectivity \n",
        "        selectivity = (u_max - u_minus_max) / (u_max + u_minus_max)\n",
        "\n",
        "        # append selectivity value to selectivity\n",
        "        __selectivity__.append(selectivity)\n",
        "\n",
        "    avg_selectivity = np.average(__selectivity__)\n",
        "    std_selectivity = np.std(__selectivity__)\n",
        "                                 \n",
        "    return avg_selectivity, std_selectivity"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYd8u8yWfPqn"
      },
      "source": [
        "# ************* modify this section for later use *************\n",
        "def avg_std_calculator(_hidden_layer_each_neuron_12, _hidden_layer_each_neuron_23):\n",
        "\n",
        "    avg_selectivity12, std_selectivity12 = selectivity(_hidden_layer_each_neuron_12)\n",
        "    avg_selectivity23, std_selectivity23 = selectivity(_hidden_layer_each_neuron_23)\n",
        "\n",
        "    final_selectivity_avg = (avg_selectivity12 + avg_selectivity23) / 2 \n",
        "    final_selecvitity_std = (std_selectivity12 + std_selectivity23) / 2 \n",
        "\n",
        "    return final_selectivity_avg, final_selecvitity_std\n",
        "# ************* modify this section for later use *************"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5PUiBNqUImf"
      },
      "source": [
        "def model_factory(optimizer_name):\n",
        "    '''\n",
        "    optimizer_name : choose one of Adagrad, Adadelta, SGD, and Adam \n",
        "\n",
        "    '''\n",
        "    my_model = Model()\n",
        "    print(\"my_model:\", my_model)\n",
        "    my_model.to(device)\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    my_model.sigmoid12.register_forward_hook(get_activation(my_model, 's12'))\n",
        "    my_model.sigmoid23.register_forward_hook(get_activation(my_model, 's23'))\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        my_optimizer = torch.optim.Adadelta(my_model.parameters(), lr=1.0)\n",
        "\n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        my_optimizer = torch.optim.Adagrad(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        my_optimizer = torch.optim.SGD(my_model.parameters(), lr=0.1)\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        my_optimizer = torch.optim.Adam(my_model.parameters(), lr=0.001)\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")\n",
        "    \n",
        "    print(\"my_optimizer:\", my_optimizer)\n",
        "    test_acc, sparsity_avg, selectivity_list_avg, selectivity_list_std = selectivity_trainer(optimizer=my_optimizer, model=my_model)\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver = open(f\"2HL_selectivity_sparsity_{optimizer_name}.txt\", \"w\")\n",
        "    # ************* modify this section for later use *************\n",
        "    file_saver.write(str(test_acc)+'\\n'+str(sparsity_avg)+'\\n'+str(selectivity_list_avg)+'\\n'+str(selectivity_list_std)+'\\n\\n')\n",
        "    file_saver.close()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    if optimizer_name == 'Adadelta':\n",
        "        !cp 2HL_selectivity_sparsity_Adadelta.txt /content/drive/MyDrive\n",
        "    \n",
        "    elif optimizer_name == 'Adagrad':\n",
        "        !cp 2HL_selectivity_sparsity_Adagrad.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'SGD':\n",
        "        !cp 2HL_selectivity_sparsity_SGD.txt /content/drive/MyDrive\n",
        "\n",
        "    elif optimizer_name == 'Adam':\n",
        "        !cp 2HL_selectivity_sparsity_Adam.txt /content/drive/MyDrive\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXOpwTXEQFKY"
      },
      "source": [
        "no_epochs = 50\n",
        "def selectivity_trainer(optimizer, model):\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    train_loss = list()\n",
        "    test_loss  = list()\n",
        "    test_acc   = list()\n",
        "\n",
        "    best_test_loss = 1\n",
        "\n",
        "    selectivity_avg_list = list()\n",
        "    selectivity_std_list = list()\n",
        "\n",
        "    # ************* modify this section for later use *************\n",
        "    final_spareness_12 = list()\n",
        "    final_spareness_23 = list()\n",
        "    # ************* modify this section for later use *************\n",
        "\n",
        "    for epoch in range(no_epochs):\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_each_neuron_12 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_12 = np.array(hidden_layer_each_neuron_12)\n",
        "\n",
        "        hidden_layer_each_neuron_23 = [{0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]} for x in range(256)]\n",
        "        hidden_layer_each_neuron_23 = np.array(hidden_layer_each_neuron_23)\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        # ************* modify this section for later use *************\n",
        "        hidden_layer_activation_list_12 = list()\n",
        "        hidden_layer_activation_list_23 = list()\n",
        "        # ************* modify this section for later use *************\n",
        "\n",
        "        total_train_loss = 0\n",
        "        total_test_loss = 0\n",
        "\n",
        "        # training\n",
        "        # set up training mode \n",
        "        model.train()\n",
        "\n",
        "        for itr, (images, labels) in enumerate(train_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_train_loss = total_train_loss / (itr + 1)\n",
        "        train_loss.append(total_train_loss)\n",
        "\n",
        "        # testing \n",
        "        # change to evaluation mode \n",
        "        model.eval()\n",
        "        total = 0\n",
        "        for itr, (images, labels) in enumerate(test_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            pred = model(images)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # we now need softmax because we are testing.\n",
        "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "            for i, p in enumerate(pred):\n",
        "                if labels[i] == torch.max(p.data, 0)[1]:\n",
        "                    total = total + 1\n",
        "\n",
        "            # ***************** sparsity calculation ***************** #\n",
        "            hidden_layer_activation_list_12.append(model.layer_activations['s12'])\n",
        "            hidden_layer_activation_list_23.append(model.layer_activations['s23'])\n",
        "\n",
        "            # ************* modify this section for later use *************\n",
        "            for activation, label in zip(model.layer_activations['s12'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_12[i][label].append(activation[i])\n",
        "\n",
        "            for activation, label in zip(model.layer_activations['s23'], labels):\n",
        "                label = label.item()\n",
        "                with torch.no_grad():\n",
        "                    activation = activation.numpy()\n",
        "                for i in range(256):    \n",
        "                    hidden_layer_each_neuron_23[i][label].append(activation[i])\n",
        "    \n",
        "        selectivity_avg, selecvitity_std = avg_std_calculator(hidden_layer_each_neuron_12, hidden_layer_each_neuron_23)\n",
        "        # ************* modify this section for later use *************\n",
        "        \n",
        "        selectivity_avg_list.append(selectivity_avg)\n",
        "        selectivity_std_list.append(selecvitity_std)\n",
        "\n",
        "        # this conains activations for all epochs \n",
        "        final_spareness_12.append(hidden_layer_activation_list_12)\n",
        "        final_spareness_23.append(hidden_layer_activation_list_23)\n",
        "        # ***************** sparsity calculation ***************** #\n",
        "\n",
        "        # caculate accuracy \n",
        "        accuracy = total / len(mnist_testset)\n",
        "\n",
        "        # append accuracy here\n",
        "        test_acc.append(accuracy)\n",
        "\n",
        "        # append test loss here \n",
        "        total_test_loss = total_test_loss / (itr + 1)\n",
        "        test_loss.append(total_test_loss)\n",
        "\n",
        "        print('\\nEpoch: {}/{}, Train Loss: {:.8f}, Test Loss: {:.8f}, Test Accuracy: {:.8f}'.format(epoch + 1, no_epochs, total_train_loss, total_test_loss, accuracy))\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "    sparsity_list12 = sparsity_calculator(final_spareness_12)\n",
        "    sparsity_list23 = sparsity_calculator(final_spareness_23)\n",
        "\n",
        "    print(sparsity_list12)\n",
        "    print(sparsity_list23)\n",
        "\n",
        "    average_sparsity = list()\n",
        "    for i in range(no_epochs):\n",
        "        average_sparsity.append( (sparsity_list12[i].item() + sparsity_list23[i].item()) / 2 )\n",
        "\n",
        "    # ***************** sparsity calculation ***************** #\n",
        "\n",
        "    print(\"average_sparsity:\", average_sparsity)\n",
        "\n",
        "    return test_acc, average_sparsity, selectivity_avg_list, selectivity_std_list"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILIJTJb2UdfI"
      },
      "source": [
        "# Adadelta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UH0qDnFUfaD",
        "outputId": "6e873bdf-5c98-4de8-c642-839530495b5c"
      },
      "source": [
        "model_factory('Adadelta')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-06\n",
            "    lr: 1.0\n",
            "    rho: 0.9\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 0.66955220, Test Loss: 0.29091561, Test Accuracy: 0.91560000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 0.23864885, Test Loss: 0.17893777, Test Accuracy: 0.94570000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 0.16415371, Test Loss: 0.14251488, Test Accuracy: 0.95830000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 0.12487799, Test Loss: 0.10738104, Test Accuracy: 0.96570000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.09811718, Test Loss: 0.09940851, Test Accuracy: 0.96730000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.08101228, Test Loss: 0.09228467, Test Accuracy: 0.97040000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.06785859, Test Loss: 0.08866796, Test Accuracy: 0.97140000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.05707939, Test Loss: 0.07453738, Test Accuracy: 0.97680000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.04893695, Test Loss: 0.07658674, Test Accuracy: 0.97500000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.04188913, Test Loss: 0.07907382, Test Accuracy: 0.97610000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.03633561, Test Loss: 0.07478659, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.03111513, Test Loss: 0.07411677, Test Accuracy: 0.97910000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.02710637, Test Loss: 0.07417858, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.02241586, Test Loss: 0.07294738, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.01807552, Test Loss: 0.07262310, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.01542590, Test Loss: 0.07768819, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.01277811, Test Loss: 0.07904738, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.01031517, Test Loss: 0.07737565, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.00880315, Test Loss: 0.07916443, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.00749228, Test Loss: 0.08437182, Test Accuracy: 0.97800000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.00596463, Test Loss: 0.08015392, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.00486791, Test Loss: 0.08034906, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.00421029, Test Loss: 0.07755168, Test Accuracy: 0.98110000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.00346229, Test Loss: 0.08204362, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.00251005, Test Loss: 0.08098144, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.00215230, Test Loss: 0.08684492, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.00167987, Test Loss: 0.08610129, Test Accuracy: 0.98120000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.00144885, Test Loss: 0.08237038, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00123362, Test Loss: 0.08542890, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00105623, Test Loss: 0.08506945, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00091279, Test Loss: 0.08574301, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00079884, Test Loss: 0.08789963, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00066917, Test Loss: 0.08765975, Test Accuracy: 0.98100000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00061405, Test Loss: 0.08729661, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00055112, Test Loss: 0.08803433, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00051557, Test Loss: 0.08878954, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00047214, Test Loss: 0.08863956, Test Accuracy: 0.98140000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00043059, Test Loss: 0.08972117, Test Accuracy: 0.98240000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00040730, Test Loss: 0.09020159, Test Accuracy: 0.98210000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00038237, Test Loss: 0.08974455, Test Accuracy: 0.98170000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00036530, Test Loss: 0.09000507, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00033928, Test Loss: 0.09118025, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00033089, Test Loss: 0.09029863, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00031177, Test Loss: 0.09124122, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00029869, Test Loss: 0.09211037, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00028776, Test Loss: 0.09185645, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00027361, Test Loss: 0.09222778, Test Accuracy: 0.98160000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00026245, Test Loss: 0.09245716, Test Accuracy: 0.98190000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00025272, Test Loss: 0.09256838, Test Accuracy: 0.98180000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00024362, Test Loss: 0.09319149, Test Accuracy: 0.98220000\n",
            "[tensor(0.2012, grad_fn=<MeanBackward0>), tensor(0.2730, grad_fn=<MeanBackward0>), tensor(0.3132, grad_fn=<MeanBackward0>), tensor(0.3355, grad_fn=<MeanBackward0>), tensor(0.3553, grad_fn=<MeanBackward0>), tensor(0.3675, grad_fn=<MeanBackward0>), tensor(0.3780, grad_fn=<MeanBackward0>), tensor(0.3852, grad_fn=<MeanBackward0>), tensor(0.3895, grad_fn=<MeanBackward0>), tensor(0.3926, grad_fn=<MeanBackward0>), tensor(0.3967, grad_fn=<MeanBackward0>), tensor(0.4008, grad_fn=<MeanBackward0>), tensor(0.4033, grad_fn=<MeanBackward0>), tensor(0.4047, grad_fn=<MeanBackward0>), tensor(0.4030, grad_fn=<MeanBackward0>), tensor(0.4051, grad_fn=<MeanBackward0>), tensor(0.4064, grad_fn=<MeanBackward0>), tensor(0.4050, grad_fn=<MeanBackward0>), tensor(0.4079, grad_fn=<MeanBackward0>), tensor(0.4054, grad_fn=<MeanBackward0>), tensor(0.4073, grad_fn=<MeanBackward0>), tensor(0.4059, grad_fn=<MeanBackward0>), tensor(0.4069, grad_fn=<MeanBackward0>), tensor(0.4058, grad_fn=<MeanBackward0>), tensor(0.4050, grad_fn=<MeanBackward0>), tensor(0.4034, grad_fn=<MeanBackward0>), tensor(0.4030, grad_fn=<MeanBackward0>), tensor(0.4028, grad_fn=<MeanBackward0>), tensor(0.4019, grad_fn=<MeanBackward0>), tensor(0.4012, grad_fn=<MeanBackward0>), tensor(0.4010, grad_fn=<MeanBackward0>), tensor(0.3999, grad_fn=<MeanBackward0>), tensor(0.3997, grad_fn=<MeanBackward0>), tensor(0.3998, grad_fn=<MeanBackward0>), tensor(0.3992, grad_fn=<MeanBackward0>), tensor(0.3989, grad_fn=<MeanBackward0>), tensor(0.3988, grad_fn=<MeanBackward0>), tensor(0.3986, grad_fn=<MeanBackward0>), tensor(0.3985, grad_fn=<MeanBackward0>), tensor(0.3985, grad_fn=<MeanBackward0>), tensor(0.3982, grad_fn=<MeanBackward0>), tensor(0.3978, grad_fn=<MeanBackward0>), tensor(0.3980, grad_fn=<MeanBackward0>), tensor(0.3978, grad_fn=<MeanBackward0>), tensor(0.3979, grad_fn=<MeanBackward0>), tensor(0.3977, grad_fn=<MeanBackward0>), tensor(0.3975, grad_fn=<MeanBackward0>), tensor(0.3974, grad_fn=<MeanBackward0>), tensor(0.3973, grad_fn=<MeanBackward0>), tensor(0.3974, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.1826, grad_fn=<MeanBackward0>), tensor(0.2026, grad_fn=<MeanBackward0>), tensor(0.2068, grad_fn=<MeanBackward0>), tensor(0.2065, grad_fn=<MeanBackward0>), tensor(0.2043, grad_fn=<MeanBackward0>), tensor(0.2040, grad_fn=<MeanBackward0>), tensor(0.1994, grad_fn=<MeanBackward0>), tensor(0.1978, grad_fn=<MeanBackward0>), tensor(0.1951, grad_fn=<MeanBackward0>), tensor(0.1903, grad_fn=<MeanBackward0>), tensor(0.1883, grad_fn=<MeanBackward0>), tensor(0.1849, grad_fn=<MeanBackward0>), tensor(0.1843, grad_fn=<MeanBackward0>), tensor(0.1814, grad_fn=<MeanBackward0>), tensor(0.1780, grad_fn=<MeanBackward0>), tensor(0.1742, grad_fn=<MeanBackward0>), tensor(0.1709, grad_fn=<MeanBackward0>), tensor(0.1662, grad_fn=<MeanBackward0>), tensor(0.1638, grad_fn=<MeanBackward0>), tensor(0.1652, grad_fn=<MeanBackward0>), tensor(0.1617, grad_fn=<MeanBackward0>), tensor(0.1597, grad_fn=<MeanBackward0>), tensor(0.1582, grad_fn=<MeanBackward0>), tensor(0.1563, grad_fn=<MeanBackward0>), tensor(0.1539, grad_fn=<MeanBackward0>), tensor(0.1538, grad_fn=<MeanBackward0>), tensor(0.1524, grad_fn=<MeanBackward0>), tensor(0.1515, grad_fn=<MeanBackward0>), tensor(0.1512, grad_fn=<MeanBackward0>), tensor(0.1509, grad_fn=<MeanBackward0>), tensor(0.1504, grad_fn=<MeanBackward0>), tensor(0.1500, grad_fn=<MeanBackward0>), tensor(0.1500, grad_fn=<MeanBackward0>), tensor(0.1492, grad_fn=<MeanBackward0>), tensor(0.1490, grad_fn=<MeanBackward0>), tensor(0.1488, grad_fn=<MeanBackward0>), tensor(0.1484, grad_fn=<MeanBackward0>), tensor(0.1483, grad_fn=<MeanBackward0>), tensor(0.1481, grad_fn=<MeanBackward0>), tensor(0.1480, grad_fn=<MeanBackward0>), tensor(0.1478, grad_fn=<MeanBackward0>), tensor(0.1478, grad_fn=<MeanBackward0>), tensor(0.1473, grad_fn=<MeanBackward0>), tensor(0.1474, grad_fn=<MeanBackward0>), tensor(0.1474, grad_fn=<MeanBackward0>), tensor(0.1472, grad_fn=<MeanBackward0>), tensor(0.1471, grad_fn=<MeanBackward0>), tensor(0.1471, grad_fn=<MeanBackward0>), tensor(0.1470, grad_fn=<MeanBackward0>), tensor(0.1470, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.19190023094415665, 0.2378183826804161, 0.2599880248308182, 0.27099692821502686, 0.2798292264342308, 0.28579042106866837, 0.28868094086647034, 0.29147837311029434, 0.2922750264406204, 0.2914329320192337, 0.292493000626564, 0.29289470613002777, 0.29378480464220047, 0.29304537922143936, 0.2904662564396858, 0.28961773961782455, 0.28864629566669464, 0.28563543409109116, 0.2858245149254799, 0.285335473716259, 0.2845066636800766, 0.2828049808740616, 0.28255555033683777, 0.2810519114136696, 0.27948147803545, 0.27857520431280136, 0.277726948261261, 0.2771633192896843, 0.2765914052724838, 0.2760351896286011, 0.27567779272794724, 0.27495017647743225, 0.27483462542295456, 0.27451518923044205, 0.2741336151957512, 0.27386294305324554, 0.27364303916692734, 0.27345889061689377, 0.2733364775776863, 0.27323780208826065, 0.27297602593898773, 0.2727954462170601, 0.2726508304476738, 0.27258332073688507, 0.27261466532945633, 0.2724515199661255, 0.2723163366317749, 0.27224384993314743, 0.2721496820449829, 0.2721605449914932]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hXfQe4vMDKB"
      },
      "source": [
        "# AdaGrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb-4TPM5MGuE",
        "outputId": "0db3c380-fc86-4c04-c445-68467088c4a6"
      },
      "source": [
        "model_factory('Adagrad')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adagrad (\n",
            "Parameter Group 0\n",
            "    eps: 1e-10\n",
            "    initial_accumulator_value: 0\n",
            "    lr: 0.1\n",
            "    lr_decay: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 0.36868044, Test Loss: 0.11248965, Test Accuracy: 0.96570000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 0.08117606, Test Loss: 0.08445396, Test Accuracy: 0.97210000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 0.05059796, Test Loss: 0.07754592, Test Accuracy: 0.97690000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 0.03388871, Test Loss: 0.07117682, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.02313452, Test Loss: 0.07472019, Test Accuracy: 0.97740000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.01545731, Test Loss: 0.07222190, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.01061224, Test Loss: 0.07550140, Test Accuracy: 0.97930000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.00761192, Test Loss: 0.07530180, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.00536287, Test Loss: 0.07591841, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.00398751, Test Loss: 0.07701518, Test Accuracy: 0.98090000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.00304564, Test Loss: 0.08140939, Test Accuracy: 0.97920000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.00236507, Test Loss: 0.08023075, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.00186442, Test Loss: 0.08129568, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.00151340, Test Loss: 0.08400254, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.00127352, Test Loss: 0.08415414, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.00105405, Test Loss: 0.08643185, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.00087577, Test Loss: 0.08661380, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.00073791, Test Loss: 0.08729609, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.00064172, Test Loss: 0.08925254, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.00055380, Test Loss: 0.08942356, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.00048498, Test Loss: 0.09120233, Test Accuracy: 0.97990000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.00043047, Test Loss: 0.09130457, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.00038197, Test Loss: 0.09186330, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.00034741, Test Loss: 0.09307892, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.00031834, Test Loss: 0.09360831, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.00029238, Test Loss: 0.09420679, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.00026979, Test Loss: 0.09495783, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.00024975, Test Loss: 0.09565893, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00023335, Test Loss: 0.09589942, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00021806, Test Loss: 0.09653643, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00020433, Test Loss: 0.09728395, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00019191, Test Loss: 0.09799119, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00018198, Test Loss: 0.09820611, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00017228, Test Loss: 0.09883445, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00016404, Test Loss: 0.09919070, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00015525, Test Loss: 0.09966821, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00014785, Test Loss: 0.10011490, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00014091, Test Loss: 0.10023727, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00013485, Test Loss: 0.10075123, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00012887, Test Loss: 0.10107250, Test Accuracy: 0.98020000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00012353, Test Loss: 0.10145833, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00011855, Test Loss: 0.10200407, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00011380, Test Loss: 0.10219556, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00010933, Test Loss: 0.10241149, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00010541, Test Loss: 0.10289882, Test Accuracy: 0.98030000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00010155, Test Loss: 0.10338596, Test Accuracy: 0.98060000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00009809, Test Loss: 0.10356757, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00009466, Test Loss: 0.10375824, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00009144, Test Loss: 0.10423860, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00008855, Test Loss: 0.10458464, Test Accuracy: 0.98050000\n",
            "[tensor(0.5370, grad_fn=<MeanBackward0>), tensor(0.5351, grad_fn=<MeanBackward0>), tensor(0.5369, grad_fn=<MeanBackward0>), tensor(0.5390, grad_fn=<MeanBackward0>), tensor(0.5370, grad_fn=<MeanBackward0>), tensor(0.5373, grad_fn=<MeanBackward0>), tensor(0.5358, grad_fn=<MeanBackward0>), tensor(0.5345, grad_fn=<MeanBackward0>), tensor(0.5350, grad_fn=<MeanBackward0>), tensor(0.5343, grad_fn=<MeanBackward0>), tensor(0.5337, grad_fn=<MeanBackward0>), tensor(0.5325, grad_fn=<MeanBackward0>), tensor(0.5332, grad_fn=<MeanBackward0>), tensor(0.5320, grad_fn=<MeanBackward0>), tensor(0.5323, grad_fn=<MeanBackward0>), tensor(0.5322, grad_fn=<MeanBackward0>), tensor(0.5319, grad_fn=<MeanBackward0>), tensor(0.5314, grad_fn=<MeanBackward0>), tensor(0.5312, grad_fn=<MeanBackward0>), tensor(0.5312, grad_fn=<MeanBackward0>), tensor(0.5311, grad_fn=<MeanBackward0>), tensor(0.5309, grad_fn=<MeanBackward0>), tensor(0.5312, grad_fn=<MeanBackward0>), tensor(0.5310, grad_fn=<MeanBackward0>), tensor(0.5304, grad_fn=<MeanBackward0>), tensor(0.5308, grad_fn=<MeanBackward0>), tensor(0.5307, grad_fn=<MeanBackward0>), tensor(0.5305, grad_fn=<MeanBackward0>), tensor(0.5306, grad_fn=<MeanBackward0>), tensor(0.5306, grad_fn=<MeanBackward0>), tensor(0.5305, grad_fn=<MeanBackward0>), tensor(0.5304, grad_fn=<MeanBackward0>), tensor(0.5305, grad_fn=<MeanBackward0>), tensor(0.5305, grad_fn=<MeanBackward0>), tensor(0.5304, grad_fn=<MeanBackward0>), tensor(0.5304, grad_fn=<MeanBackward0>), tensor(0.5303, grad_fn=<MeanBackward0>), tensor(0.5303, grad_fn=<MeanBackward0>), tensor(0.5303, grad_fn=<MeanBackward0>), tensor(0.5302, grad_fn=<MeanBackward0>), tensor(0.5302, grad_fn=<MeanBackward0>), tensor(0.5302, grad_fn=<MeanBackward0>), tensor(0.5302, grad_fn=<MeanBackward0>), tensor(0.5301, grad_fn=<MeanBackward0>), tensor(0.5300, grad_fn=<MeanBackward0>), tensor(0.5300, grad_fn=<MeanBackward0>), tensor(0.5300, grad_fn=<MeanBackward0>), tensor(0.5300, grad_fn=<MeanBackward0>), tensor(0.5300, grad_fn=<MeanBackward0>), tensor(0.5299, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.4155, grad_fn=<MeanBackward0>), tensor(0.4198, grad_fn=<MeanBackward0>), tensor(0.4145, grad_fn=<MeanBackward0>), tensor(0.4107, grad_fn=<MeanBackward0>), tensor(0.4003, grad_fn=<MeanBackward0>), tensor(0.3952, grad_fn=<MeanBackward0>), tensor(0.3875, grad_fn=<MeanBackward0>), tensor(0.3861, grad_fn=<MeanBackward0>), tensor(0.3792, grad_fn=<MeanBackward0>), tensor(0.3759, grad_fn=<MeanBackward0>), tensor(0.3752, grad_fn=<MeanBackward0>), tensor(0.3719, grad_fn=<MeanBackward0>), tensor(0.3682, grad_fn=<MeanBackward0>), tensor(0.3688, grad_fn=<MeanBackward0>), tensor(0.3660, grad_fn=<MeanBackward0>), tensor(0.3645, grad_fn=<MeanBackward0>), tensor(0.3637, grad_fn=<MeanBackward0>), tensor(0.3637, grad_fn=<MeanBackward0>), tensor(0.3622, grad_fn=<MeanBackward0>), tensor(0.3615, grad_fn=<MeanBackward0>), tensor(0.3611, grad_fn=<MeanBackward0>), tensor(0.3605, grad_fn=<MeanBackward0>), tensor(0.3595, grad_fn=<MeanBackward0>), tensor(0.3596, grad_fn=<MeanBackward0>), tensor(0.3595, grad_fn=<MeanBackward0>), tensor(0.3592, grad_fn=<MeanBackward0>), tensor(0.3593, grad_fn=<MeanBackward0>), tensor(0.3588, grad_fn=<MeanBackward0>), tensor(0.3589, grad_fn=<MeanBackward0>), tensor(0.3585, grad_fn=<MeanBackward0>), tensor(0.3584, grad_fn=<MeanBackward0>), tensor(0.3584, grad_fn=<MeanBackward0>), tensor(0.3584, grad_fn=<MeanBackward0>), tensor(0.3581, grad_fn=<MeanBackward0>), tensor(0.3582, grad_fn=<MeanBackward0>), tensor(0.3582, grad_fn=<MeanBackward0>), tensor(0.3577, grad_fn=<MeanBackward0>), tensor(0.3579, grad_fn=<MeanBackward0>), tensor(0.3577, grad_fn=<MeanBackward0>), tensor(0.3580, grad_fn=<MeanBackward0>), tensor(0.3577, grad_fn=<MeanBackward0>), tensor(0.3576, grad_fn=<MeanBackward0>), tensor(0.3576, grad_fn=<MeanBackward0>), tensor(0.3577, grad_fn=<MeanBackward0>), tensor(0.3576, grad_fn=<MeanBackward0>), tensor(0.3575, grad_fn=<MeanBackward0>), tensor(0.3575, grad_fn=<MeanBackward0>), tensor(0.3576, grad_fn=<MeanBackward0>), tensor(0.3572, grad_fn=<MeanBackward0>), tensor(0.3574, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.4762568771839142, 0.4774439036846161, 0.475690633058548, 0.4748440235853195, 0.46866530179977417, 0.4662422686815262, 0.4616864174604416, 0.46029582619667053, 0.4571046233177185, 0.4551151543855667, 0.45445074141025543, 0.4522428810596466, 0.4506925940513611, 0.4503990560770035, 0.44911907613277435, 0.448337197303772, 0.44780993461608887, 0.44755297899246216, 0.44671136140823364, 0.4463336616754532, 0.4460853785276413, 0.44572265446186066, 0.4453372657299042, 0.4453141987323761, 0.44495031237602234, 0.44500814378261566, 0.4449906200170517, 0.44465504586696625, 0.4447769373655319, 0.44456055760383606, 0.4444775730371475, 0.44441260397434235, 0.44443611800670624, 0.44431348145008087, 0.4442673921585083, 0.444298654794693, 0.4440445154905319, 0.4440891891717911, 0.4440189599990845, 0.4441000074148178, 0.44395749270915985, 0.44392772018909454, 0.4438622444868088, 0.4438823312520981, 0.44381844997406006, 0.44377030432224274, 0.443746954202652, 0.4437818229198456, 0.4436003863811493, 0.4436497986316681]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmLJ4Zr2MnoS"
      },
      "source": [
        "# SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ObsEJHuMoPy",
        "outputId": "cbfea1fe-7be2-4349-f82b-2b26c16a0795"
      },
      "source": [
        "model_factory('SGD')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            ")\n",
            "my_optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 1.69265915, Test Loss: 0.75299979, Test Accuracy: 0.76860000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 0.54849599, Test Loss: 0.42523081, Test Accuracy: 0.87280000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 0.38982520, Test Loss: 0.34583928, Test Accuracy: 0.89970000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 0.34312607, Test Loss: 0.31302324, Test Accuracy: 0.90980000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.31391889, Test Loss: 0.29038461, Test Accuracy: 0.91460000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.28898875, Test Loss: 0.27226443, Test Accuracy: 0.92060000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.26564695, Test Loss: 0.24730698, Test Accuracy: 0.92800000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.24432782, Test Loss: 0.22944423, Test Accuracy: 0.93190000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.22555595, Test Loss: 0.21526348, Test Accuracy: 0.93660000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.20883532, Test Loss: 0.20461825, Test Accuracy: 0.93830000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.19262660, Test Loss: 0.19071021, Test Accuracy: 0.94280000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.17952832, Test Loss: 0.17340062, Test Accuracy: 0.94790000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.16750424, Test Loss: 0.16394282, Test Accuracy: 0.95070000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.15693103, Test Loss: 0.15610873, Test Accuracy: 0.95400000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.14776128, Test Loss: 0.14462972, Test Accuracy: 0.95650000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.13916552, Test Loss: 0.14137395, Test Accuracy: 0.95830000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.13124293, Test Loss: 0.13786579, Test Accuracy: 0.95820000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.12440402, Test Loss: 0.12946962, Test Accuracy: 0.96120000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.11770836, Test Loss: 0.13341578, Test Accuracy: 0.96060000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.11186344, Test Loss: 0.12016613, Test Accuracy: 0.96420000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.10630881, Test Loss: 0.11287900, Test Accuracy: 0.96570000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.10135229, Test Loss: 0.11074167, Test Accuracy: 0.96580000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.09618740, Test Loss: 0.10684654, Test Accuracy: 0.96670000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.09198721, Test Loss: 0.10956699, Test Accuracy: 0.96730000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.08753607, Test Loss: 0.11048703, Test Accuracy: 0.96520000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.08429854, Test Loss: 0.09715599, Test Accuracy: 0.97000000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.08040134, Test Loss: 0.09378584, Test Accuracy: 0.97150000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.07683724, Test Loss: 0.09477380, Test Accuracy: 0.96940000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.07341151, Test Loss: 0.09817360, Test Accuracy: 0.97040000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.07050071, Test Loss: 0.09006047, Test Accuracy: 0.97190000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.06738373, Test Loss: 0.08661303, Test Accuracy: 0.97280000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.06476204, Test Loss: 0.08542473, Test Accuracy: 0.97380000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.06230762, Test Loss: 0.08570075, Test Accuracy: 0.97370000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.05966894, Test Loss: 0.08285367, Test Accuracy: 0.97390000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.05720982, Test Loss: 0.08589647, Test Accuracy: 0.97310000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.05475129, Test Loss: 0.07990010, Test Accuracy: 0.97480000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.05260998, Test Loss: 0.08208711, Test Accuracy: 0.97500000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.05067345, Test Loss: 0.07917151, Test Accuracy: 0.97650000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.04828485, Test Loss: 0.07675327, Test Accuracy: 0.97540000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.04644239, Test Loss: 0.07744274, Test Accuracy: 0.97580000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.04450753, Test Loss: 0.08109721, Test Accuracy: 0.97550000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.04269829, Test Loss: 0.07636582, Test Accuracy: 0.97620000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.04108629, Test Loss: 0.07318365, Test Accuracy: 0.97790000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.03930670, Test Loss: 0.07694888, Test Accuracy: 0.97560000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.03784707, Test Loss: 0.07193053, Test Accuracy: 0.97810000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.03634726, Test Loss: 0.07168135, Test Accuracy: 0.97770000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.03501138, Test Loss: 0.07252059, Test Accuracy: 0.97730000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.03373136, Test Loss: 0.07279964, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.03228482, Test Loss: 0.07392729, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.03091923, Test Loss: 0.07225633, Test Accuracy: 0.97800000\n",
            "[tensor(0.0650, grad_fn=<MeanBackward0>), tensor(0.0913, grad_fn=<MeanBackward0>), tensor(0.0970, grad_fn=<MeanBackward0>), tensor(0.1031, grad_fn=<MeanBackward0>), tensor(0.1108, grad_fn=<MeanBackward0>), tensor(0.1185, grad_fn=<MeanBackward0>), tensor(0.1275, grad_fn=<MeanBackward0>), tensor(0.1360, grad_fn=<MeanBackward0>), tensor(0.1444, grad_fn=<MeanBackward0>), tensor(0.1517, grad_fn=<MeanBackward0>), tensor(0.1583, grad_fn=<MeanBackward0>), tensor(0.1646, grad_fn=<MeanBackward0>), tensor(0.1700, grad_fn=<MeanBackward0>), tensor(0.1753, grad_fn=<MeanBackward0>), tensor(0.1799, grad_fn=<MeanBackward0>), tensor(0.1844, grad_fn=<MeanBackward0>), tensor(0.1880, grad_fn=<MeanBackward0>), tensor(0.1913, grad_fn=<MeanBackward0>), tensor(0.1947, grad_fn=<MeanBackward0>), tensor(0.1979, grad_fn=<MeanBackward0>), tensor(0.2001, grad_fn=<MeanBackward0>), tensor(0.2025, grad_fn=<MeanBackward0>), tensor(0.2047, grad_fn=<MeanBackward0>), tensor(0.2071, grad_fn=<MeanBackward0>), tensor(0.2089, grad_fn=<MeanBackward0>), tensor(0.2108, grad_fn=<MeanBackward0>), tensor(0.2126, grad_fn=<MeanBackward0>), tensor(0.2139, grad_fn=<MeanBackward0>), tensor(0.2153, grad_fn=<MeanBackward0>), tensor(0.2167, grad_fn=<MeanBackward0>), tensor(0.2178, grad_fn=<MeanBackward0>), tensor(0.2193, grad_fn=<MeanBackward0>), tensor(0.2201, grad_fn=<MeanBackward0>), tensor(0.2213, grad_fn=<MeanBackward0>), tensor(0.2223, grad_fn=<MeanBackward0>), tensor(0.2235, grad_fn=<MeanBackward0>), tensor(0.2243, grad_fn=<MeanBackward0>), tensor(0.2246, grad_fn=<MeanBackward0>), tensor(0.2257, grad_fn=<MeanBackward0>), tensor(0.2263, grad_fn=<MeanBackward0>), tensor(0.2270, grad_fn=<MeanBackward0>), tensor(0.2274, grad_fn=<MeanBackward0>), tensor(0.2278, grad_fn=<MeanBackward0>), tensor(0.2284, grad_fn=<MeanBackward0>), tensor(0.2291, grad_fn=<MeanBackward0>), tensor(0.2297, grad_fn=<MeanBackward0>), tensor(0.2302, grad_fn=<MeanBackward0>), tensor(0.2307, grad_fn=<MeanBackward0>), tensor(0.2314, grad_fn=<MeanBackward0>), tensor(0.2321, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.0501, grad_fn=<MeanBackward0>), tensor(0.0661, grad_fn=<MeanBackward0>), tensor(0.0759, grad_fn=<MeanBackward0>), tensor(0.0842, grad_fn=<MeanBackward0>), tensor(0.0908, grad_fn=<MeanBackward0>), tensor(0.0956, grad_fn=<MeanBackward0>), tensor(0.0991, grad_fn=<MeanBackward0>), tensor(0.1003, grad_fn=<MeanBackward0>), tensor(0.1025, grad_fn=<MeanBackward0>), tensor(0.1036, grad_fn=<MeanBackward0>), tensor(0.1028, grad_fn=<MeanBackward0>), tensor(0.1024, grad_fn=<MeanBackward0>), tensor(0.1011, grad_fn=<MeanBackward0>), tensor(0.1013, grad_fn=<MeanBackward0>), tensor(0.0993, grad_fn=<MeanBackward0>), tensor(0.0977, grad_fn=<MeanBackward0>), tensor(0.0973, grad_fn=<MeanBackward0>), tensor(0.0954, grad_fn=<MeanBackward0>), tensor(0.0938, grad_fn=<MeanBackward0>), tensor(0.0926, grad_fn=<MeanBackward0>), tensor(0.0917, grad_fn=<MeanBackward0>), tensor(0.0907, grad_fn=<MeanBackward0>), tensor(0.0894, grad_fn=<MeanBackward0>), tensor(0.0873, grad_fn=<MeanBackward0>), tensor(0.0880, grad_fn=<MeanBackward0>), tensor(0.0860, grad_fn=<MeanBackward0>), tensor(0.0847, grad_fn=<MeanBackward0>), tensor(0.0834, grad_fn=<MeanBackward0>), tensor(0.0827, grad_fn=<MeanBackward0>), tensor(0.0813, grad_fn=<MeanBackward0>), tensor(0.0811, grad_fn=<MeanBackward0>), tensor(0.0807, grad_fn=<MeanBackward0>), tensor(0.0800, grad_fn=<MeanBackward0>), tensor(0.0784, grad_fn=<MeanBackward0>), tensor(0.0777, grad_fn=<MeanBackward0>), tensor(0.0771, grad_fn=<MeanBackward0>), tensor(0.0772, grad_fn=<MeanBackward0>), tensor(0.0756, grad_fn=<MeanBackward0>), tensor(0.0763, grad_fn=<MeanBackward0>), tensor(0.0748, grad_fn=<MeanBackward0>), tensor(0.0755, grad_fn=<MeanBackward0>), tensor(0.0734, grad_fn=<MeanBackward0>), tensor(0.0739, grad_fn=<MeanBackward0>), tensor(0.0730, grad_fn=<MeanBackward0>), tensor(0.0726, grad_fn=<MeanBackward0>), tensor(0.0728, grad_fn=<MeanBackward0>), tensor(0.0720, grad_fn=<MeanBackward0>), tensor(0.0728, grad_fn=<MeanBackward0>), tensor(0.0717, grad_fn=<MeanBackward0>), tensor(0.0712, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.05755450762808323, 0.07872353866696358, 0.0864078439772129, 0.09363297000527382, 0.10082340985536575, 0.10700673609972, 0.11329208686947823, 0.11815156415104866, 0.12344875931739807, 0.12763961032032967, 0.13058065623044968, 0.13345401734113693, 0.13552473857998848, 0.13826749101281166, 0.13959621265530586, 0.14107070490717888, 0.14261950552463531, 0.14334890991449356, 0.1442207358777523, 0.1452176608145237, 0.14591902866959572, 0.14659900963306427, 0.14704744890332222, 0.14718008786439896, 0.1484658382833004, 0.14839785173535347, 0.14866113662719727, 0.1486549973487854, 0.14901376143097878, 0.14898724853992462, 0.14945601299405098, 0.14999659731984138, 0.1500404328107834, 0.14984484761953354, 0.14999183267354965, 0.1503038853406906, 0.15071412548422813, 0.15009388700127602, 0.15099602937698364, 0.15056348964571953, 0.15122513473033905, 0.15040134266018867, 0.15085376799106598, 0.15068377554416656, 0.1508517488837242, 0.1512283980846405, 0.15113838389515877, 0.15172618255019188, 0.15156808868050575, 0.15165814384818077]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvQxaN_fRXLq"
      },
      "source": [
        "# Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkqfFoVkRXxP",
        "outputId": "f63cc790-966c-4f46-acaa-ce90bb33f1b8"
      },
      "source": [
        "model_factory('Adam')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model: Model(\n",
            "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (linear_3): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (sigmoid12): Sigmoid()\n",
            "  (sigmoid23): Sigmoid()\n",
            ")\n",
            "my_optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Epoch: 1/50, Train Loss: 0.46062646, Test Loss: 0.23019592, Test Accuracy: 0.93020000\n",
            "\n",
            "Epoch: 2/50, Train Loss: 0.17090873, Test Loss: 0.13047612, Test Accuracy: 0.95990000\n",
            "\n",
            "Epoch: 3/50, Train Loss: 0.11648787, Test Loss: 0.11877710, Test Accuracy: 0.96170000\n",
            "\n",
            "Epoch: 4/50, Train Loss: 0.08605863, Test Loss: 0.09043822, Test Accuracy: 0.97080000\n",
            "\n",
            "Epoch: 5/50, Train Loss: 0.06640637, Test Loss: 0.09237713, Test Accuracy: 0.97000000\n",
            "\n",
            "Epoch: 6/50, Train Loss: 0.05035246, Test Loss: 0.07154697, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 7/50, Train Loss: 0.04009322, Test Loss: 0.07882151, Test Accuracy: 0.97500000\n",
            "\n",
            "Epoch: 8/50, Train Loss: 0.03004546, Test Loss: 0.07361695, Test Accuracy: 0.97700000\n",
            "\n",
            "Epoch: 9/50, Train Loss: 0.02533570, Test Loss: 0.07187617, Test Accuracy: 0.97880000\n",
            "\n",
            "Epoch: 10/50, Train Loss: 0.01969391, Test Loss: 0.07825238, Test Accuracy: 0.97680000\n",
            "\n",
            "Epoch: 11/50, Train Loss: 0.01594616, Test Loss: 0.08249766, Test Accuracy: 0.97710000\n",
            "\n",
            "Epoch: 12/50, Train Loss: 0.01354987, Test Loss: 0.07221691, Test Accuracy: 0.98080000\n",
            "\n",
            "Epoch: 13/50, Train Loss: 0.01049173, Test Loss: 0.07738710, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 14/50, Train Loss: 0.00932449, Test Loss: 0.08148364, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 15/50, Train Loss: 0.00801971, Test Loss: 0.08254531, Test Accuracy: 0.98050000\n",
            "\n",
            "Epoch: 16/50, Train Loss: 0.00427521, Test Loss: 0.09362144, Test Accuracy: 0.97670000\n",
            "\n",
            "Epoch: 17/50, Train Loss: 0.00834816, Test Loss: 0.09310170, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 18/50, Train Loss: 0.00546690, Test Loss: 0.09236768, Test Accuracy: 0.98150000\n",
            "\n",
            "Epoch: 19/50, Train Loss: 0.00376853, Test Loss: 0.11332293, Test Accuracy: 0.97600000\n",
            "\n",
            "Epoch: 20/50, Train Loss: 0.00478810, Test Loss: 0.09634170, Test Accuracy: 0.97850000\n",
            "\n",
            "Epoch: 21/50, Train Loss: 0.00620300, Test Loss: 0.11371623, Test Accuracy: 0.97670000\n",
            "\n",
            "Epoch: 22/50, Train Loss: 0.00229612, Test Loss: 0.09914654, Test Accuracy: 0.97970000\n",
            "\n",
            "Epoch: 23/50, Train Loss: 0.00456662, Test Loss: 0.10368257, Test Accuracy: 0.97940000\n",
            "\n",
            "Epoch: 24/50, Train Loss: 0.00367074, Test Loss: 0.10421794, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 25/50, Train Loss: 0.00219625, Test Loss: 0.10603775, Test Accuracy: 0.98040000\n",
            "\n",
            "Epoch: 26/50, Train Loss: 0.00535943, Test Loss: 0.11308874, Test Accuracy: 0.97900000\n",
            "\n",
            "Epoch: 27/50, Train Loss: 0.00189290, Test Loss: 0.10391321, Test Accuracy: 0.97950000\n",
            "\n",
            "Epoch: 28/50, Train Loss: 0.00339684, Test Loss: 0.10616338, Test Accuracy: 0.98130000\n",
            "\n",
            "Epoch: 29/50, Train Loss: 0.00370871, Test Loss: 0.10754933, Test Accuracy: 0.98000000\n",
            "\n",
            "Epoch: 30/50, Train Loss: 0.00281642, Test Loss: 0.12474217, Test Accuracy: 0.97870000\n",
            "\n",
            "Epoch: 31/50, Train Loss: 0.00362670, Test Loss: 0.11642205, Test Accuracy: 0.97960000\n",
            "\n",
            "Epoch: 32/50, Train Loss: 0.00354094, Test Loss: 0.11614737, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 33/50, Train Loss: 0.00154906, Test Loss: 0.14948410, Test Accuracy: 0.97460000\n",
            "\n",
            "Epoch: 34/50, Train Loss: 0.00483602, Test Loss: 0.12461498, Test Accuracy: 0.97830000\n",
            "\n",
            "Epoch: 35/50, Train Loss: 0.00042658, Test Loss: 0.11292917, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 36/50, Train Loss: 0.00440881, Test Loss: 0.11716136, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 37/50, Train Loss: 0.00106460, Test Loss: 0.12504489, Test Accuracy: 0.97980000\n",
            "\n",
            "Epoch: 38/50, Train Loss: 0.00444422, Test Loss: 0.11240580, Test Accuracy: 0.98070000\n",
            "\n",
            "Epoch: 39/50, Train Loss: 0.00161434, Test Loss: 0.12256952, Test Accuracy: 0.97890000\n",
            "\n",
            "Epoch: 40/50, Train Loss: 0.00347757, Test Loss: 0.12062306, Test Accuracy: 0.98010000\n",
            "\n",
            "Epoch: 41/50, Train Loss: 0.00310073, Test Loss: 0.11229475, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 42/50, Train Loss: 0.00153973, Test Loss: 0.11357223, Test Accuracy: 0.98200000\n",
            "\n",
            "Epoch: 43/50, Train Loss: 0.00009262, Test Loss: 0.11342051, Test Accuracy: 0.98270000\n",
            "\n",
            "Epoch: 44/50, Train Loss: 0.00002056, Test Loss: 0.11244276, Test Accuracy: 0.98320000\n",
            "\n",
            "Epoch: 45/50, Train Loss: 0.00001225, Test Loss: 0.11403378, Test Accuracy: 0.98330000\n",
            "\n",
            "Epoch: 46/50, Train Loss: 0.00000896, Test Loss: 0.11415949, Test Accuracy: 0.98280000\n",
            "\n",
            "Epoch: 47/50, Train Loss: 0.00000599, Test Loss: 0.11540760, Test Accuracy: 0.98320000\n",
            "\n",
            "Epoch: 48/50, Train Loss: 0.00000432, Test Loss: 0.11755486, Test Accuracy: 0.98280000\n",
            "\n",
            "Epoch: 49/50, Train Loss: 0.00579817, Test Loss: 0.13929280, Test Accuracy: 0.97760000\n",
            "\n",
            "Epoch: 50/50, Train Loss: 0.00210301, Test Loss: 0.11174996, Test Accuracy: 0.98260000\n",
            "[tensor(0.2307, grad_fn=<MeanBackward0>), tensor(0.2603, grad_fn=<MeanBackward0>), tensor(0.2758, grad_fn=<MeanBackward0>), tensor(0.2883, grad_fn=<MeanBackward0>), tensor(0.2980, grad_fn=<MeanBackward0>), tensor(0.3038, grad_fn=<MeanBackward0>), tensor(0.3069, grad_fn=<MeanBackward0>), tensor(0.3139, grad_fn=<MeanBackward0>), tensor(0.3184, grad_fn=<MeanBackward0>), tensor(0.3199, grad_fn=<MeanBackward0>), tensor(0.3240, grad_fn=<MeanBackward0>), tensor(0.3251, grad_fn=<MeanBackward0>), tensor(0.3284, grad_fn=<MeanBackward0>), tensor(0.3301, grad_fn=<MeanBackward0>), tensor(0.3313, grad_fn=<MeanBackward0>), tensor(0.3324, grad_fn=<MeanBackward0>), tensor(0.3348, grad_fn=<MeanBackward0>), tensor(0.3378, grad_fn=<MeanBackward0>), tensor(0.3398, grad_fn=<MeanBackward0>), tensor(0.3397, grad_fn=<MeanBackward0>), tensor(0.3403, grad_fn=<MeanBackward0>), tensor(0.3433, grad_fn=<MeanBackward0>), tensor(0.3431, grad_fn=<MeanBackward0>), tensor(0.3434, grad_fn=<MeanBackward0>), tensor(0.3431, grad_fn=<MeanBackward0>), tensor(0.3473, grad_fn=<MeanBackward0>), tensor(0.3452, grad_fn=<MeanBackward0>), tensor(0.3495, grad_fn=<MeanBackward0>), tensor(0.3521, grad_fn=<MeanBackward0>), tensor(0.3540, grad_fn=<MeanBackward0>), tensor(0.3504, grad_fn=<MeanBackward0>), tensor(0.3520, grad_fn=<MeanBackward0>), tensor(0.3489, grad_fn=<MeanBackward0>), tensor(0.3536, grad_fn=<MeanBackward0>), tensor(0.3518, grad_fn=<MeanBackward0>), tensor(0.3583, grad_fn=<MeanBackward0>), tensor(0.3591, grad_fn=<MeanBackward0>), tensor(0.3558, grad_fn=<MeanBackward0>), tensor(0.3575, grad_fn=<MeanBackward0>), tensor(0.3604, grad_fn=<MeanBackward0>), tensor(0.3615, grad_fn=<MeanBackward0>), tensor(0.3588, grad_fn=<MeanBackward0>), tensor(0.3572, grad_fn=<MeanBackward0>), tensor(0.3571, grad_fn=<MeanBackward0>), tensor(0.3568, grad_fn=<MeanBackward0>), tensor(0.3564, grad_fn=<MeanBackward0>), tensor(0.3562, grad_fn=<MeanBackward0>), tensor(0.3560, grad_fn=<MeanBackward0>), tensor(0.3644, grad_fn=<MeanBackward0>), tensor(0.3618, grad_fn=<MeanBackward0>)]\n",
            "[tensor(0.1996, grad_fn=<MeanBackward0>), tensor(0.1990, grad_fn=<MeanBackward0>), tensor(0.2024, grad_fn=<MeanBackward0>), tensor(0.2053, grad_fn=<MeanBackward0>), tensor(0.2083, grad_fn=<MeanBackward0>), tensor(0.2067, grad_fn=<MeanBackward0>), tensor(0.2094, grad_fn=<MeanBackward0>), tensor(0.2113, grad_fn=<MeanBackward0>), tensor(0.2151, grad_fn=<MeanBackward0>), tensor(0.2186, grad_fn=<MeanBackward0>), tensor(0.2176, grad_fn=<MeanBackward0>), tensor(0.2230, grad_fn=<MeanBackward0>), tensor(0.2220, grad_fn=<MeanBackward0>), tensor(0.2228, grad_fn=<MeanBackward0>), tensor(0.2298, grad_fn=<MeanBackward0>), tensor(0.2208, grad_fn=<MeanBackward0>), tensor(0.2368, grad_fn=<MeanBackward0>), tensor(0.2352, grad_fn=<MeanBackward0>), tensor(0.2326, grad_fn=<MeanBackward0>), tensor(0.2367, grad_fn=<MeanBackward0>), tensor(0.2469, grad_fn=<MeanBackward0>), tensor(0.2414, grad_fn=<MeanBackward0>), tensor(0.2505, grad_fn=<MeanBackward0>), tensor(0.2527, grad_fn=<MeanBackward0>), tensor(0.2479, grad_fn=<MeanBackward0>), tensor(0.2637, grad_fn=<MeanBackward0>), tensor(0.2539, grad_fn=<MeanBackward0>), tensor(0.2577, grad_fn=<MeanBackward0>), tensor(0.2677, grad_fn=<MeanBackward0>), tensor(0.2647, grad_fn=<MeanBackward0>), tensor(0.2681, grad_fn=<MeanBackward0>), tensor(0.2689, grad_fn=<MeanBackward0>), tensor(0.2771, grad_fn=<MeanBackward0>), tensor(0.2772, grad_fn=<MeanBackward0>), tensor(0.2713, grad_fn=<MeanBackward0>), tensor(0.2803, grad_fn=<MeanBackward0>), tensor(0.2730, grad_fn=<MeanBackward0>), tensor(0.2829, grad_fn=<MeanBackward0>), tensor(0.2844, grad_fn=<MeanBackward0>), tensor(0.2858, grad_fn=<MeanBackward0>), tensor(0.2889, grad_fn=<MeanBackward0>), tensor(0.2851, grad_fn=<MeanBackward0>), tensor(0.2814, grad_fn=<MeanBackward0>), tensor(0.2781, grad_fn=<MeanBackward0>), tensor(0.2752, grad_fn=<MeanBackward0>), tensor(0.2712, grad_fn=<MeanBackward0>), tensor(0.2670, grad_fn=<MeanBackward0>), tensor(0.2626, grad_fn=<MeanBackward0>), tensor(0.2854, grad_fn=<MeanBackward0>), tensor(0.2849, grad_fn=<MeanBackward0>)]\n",
            "average_sparsity: [0.21512867510318756, 0.22964683920145035, 0.2391045019030571, 0.2468099221587181, 0.2531418651342392, 0.2552357465028763, 0.2581283450126648, 0.2625899687409401, 0.2667223587632179, 0.2692561745643616, 0.270808182656765, 0.2740462124347687, 0.2752254530787468, 0.27645162492990494, 0.28057433664798737, 0.27660195529460907, 0.28582027554512024, 0.28648945689201355, 0.28623055666685104, 0.2881898134946823, 0.293602891266346, 0.292348712682724, 0.29684287309646606, 0.2980460226535797, 0.2954944968223572, 0.3054789751768112, 0.2995537370443344, 0.3035798817873001, 0.30990682542324066, 0.309320792555809, 0.3092692047357559, 0.3104582577943802, 0.31299711763858795, 0.31543175876140594, 0.31158094108104706, 0.319293737411499, 0.31605537235736847, 0.3193954676389694, 0.3209226727485657, 0.3231355845928192, 0.3251919001340866, 0.3219737112522125, 0.3193235993385315, 0.3175863176584244, 0.3159697651863098, 0.31378453969955444, 0.31163474917411804, 0.3092690110206604, 0.32491420209407806, 0.3233175426721573]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}